{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fsspec\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import tempfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"https://sominmodelstorage.blob.core.windows.net/mediummodel/model%2Fmedium-tech%2F\"\n",
    "\n",
    "# Replace the placeholders with your own values\n",
    "storage_account_name = \"sominmodelstorage\"\n",
    "container_name = \"mediummodel\"\n",
    "model_directory = \"model/medium-tech\"  # The directory in the container where the model files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public access level of container 'mediummodel' and blobs set to 'public'.\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, PublicAccess\n",
    "\n",
    "connection_string=\"DefaultEndpointsProtocol=https;AccountName=sominmodelstorage;AccountKey=Rd/Ds7pibf6JbKOgOqi4kubWsgSbcvoopwt76BQqaOsF0zosupNNJmeGQ0fzbmmowHwy6393i7Xc+ASt7HOH9w==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a reference to the container\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Allow public read access for both the container and its blobs\n",
    "container_client.set_container_access_policy(signed_identifiers={}, public_access=PublicAccess.Container)\n",
    "print(f\"Public access level of container '{container_name}' and blobs set to 'public'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file system that maps to Azure Blob Storage\n",
    "fs = fsspec.filesystem(\"az\", account_name=storage_account_name)\n",
    "\n",
    "# Create a temporary directory to store the downloaded model files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Download all model files from the container to the temporary directory\n",
    "for file in fs.glob(f\"az://{container_name}/{model_directory}/*\"):\n",
    "    with fs.open(file, \"rb\") as src, open(os.path.join(temp_dir, os.path.basename(file)), \"wb\") as dst:\n",
    "        shutil.copyfileobj(src, dst)\n",
    "\n",
    "# Load the tokenizer and model from the temporary directory\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(temp_dir)\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(temp_dir)\n",
    "\n",
    "# Now you can use the tokenizer and model for inference\n",
    "\n",
    "# Clean up the temporary directory\n",
    "shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scientists use VS Code as their tool of choice because VS codes words. This means that the VS Code tool is used to determine the vocabulary for different words in the text of the text. This means that the VS codes the text of the text of the text (the VS code code from the text) are compared to the words (the words from the text).\n",
      "\n",
      "This is as a result that in most cases, the data scientists use the words from the text as if they were\n"
     ]
    }
   ],
   "source": [
    "text = \"Data scientists use VS Code as their tool of choice because\"\n",
    "\n",
    "ft_input_ids = ft_tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "output = ft_model.generate(ft_input_ids, attention_mask = torch.ones_like(ft_input_ids), pad_token_id=ft_tokenizer.eos_token_id,\n",
    "                              max_length=100, do_sample=True)# num_beams=2, no_repeat_ngram_size=3, early_stopping=False)\n",
    "\n",
    "print(ft_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
