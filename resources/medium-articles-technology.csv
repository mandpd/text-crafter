text,text_len
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2021 Summer OlyBookmark this sitempics in Germany, the 2021 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2021 World Movies inspired many Americans to buy their first tv set and in 2021, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 1, 2021 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAI Am… CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://onstream.club/tv/91605-2-1/i-am.html

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9176
"‘WATCH’ ~ New Series HDTV! ~ The Good Fight Season 5 Episode 7 (5x7) — Official TVs (2021) | ‘Full Episodes’ (Online)

FULL EPISODES ✨ https://onstream.club/tv/69158-5-7/the-good-fight.html

➕Official Partners “TVs” TV Shows & Movies

● Watch The Good Fight Season 5 Episode 7 Eng Sub ●

The Good Fight Season 5 Episode 7 : Full Series

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ



The Good Fight — Season 5, Episode 7 || FULL EPISODES : Picking up one year after the events of the final broadcast episode of “The Good Wife”, an enormous financial scam has destroyed the reputation of a young lawyer, Maia Rindell, while simultaneously wiping out her mentor and godmother Diane Lockhart’s savings. Forced out of her law firm, now called “Lockhart, Deckler, Gussman, Lee, Lyman, Gilbert, Lurie, Kagan, Tannebaum & Associates”, they join Lucca Quinn at one of Chicago’s preeminent law firms. .

The Good Fight 5x7 > The Good Fight S5xE7 > The Good Fight S5E7 > The Good Fight TVs > The Good Fight Cast > The Good Fight Online > The Good Fight Eps.5 > The Good Fight Season 5 > The Good Fight Episode 7 > The Good Fight Premiere > The Good Fight New Season > The Good Fight Full Episodes > The Good Fight Season 5 Episode 7 > Watch The Good Fight Season 5 Episode 7 Online

Streaming The Good Fight Season 5 :: Episode 7 S5E7 ► ((Episode 7 : Full Series)) Full Episodes ●Exclusively● On TVs, Online Free TV Shows & TV The Good Fight ➤ Let’s go to watch the latest episodes of your favourite The Good Fight.

❖ P.L.A.Y ► https://onstream.club/tv/69158-5-7/the-good-fight.html

The Good Fight 5x7

The Good Fight S5E7

The Good Fight TVs

The Good Fight Cast

The Good Fight Online

The Good Fight Eps.5

The Good Fight Season 5

The Good Fight Episode 7

The Good Fight Premiere

The Good Fight New Season

The Good Fight Full Episodes

The Good Fight Watch Online

The Good Fight Season 5 Episode 7

Watch The Good Fight Season 5 Episode 7 Online

⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 5, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAThe Good Fight CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://onstream.club/tv/69158-5-7/the-good-fight.html

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",11175
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 2, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRADAVE CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://onstream.club/tv/97084-2-9/dave.html

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9175
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 5, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAThe Good Fight CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://onstream.club/tv/69158-5-7/the-good-fight.html

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9195
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2021 Summer OlyBookmark this sitempics in Germany, the 2021 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2021 World Movies inspired many Americans to buy their first tv set and in 2021, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 2, 2021 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAMy Feet Are Killing Me CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://onstream.club/tv/97122-2-12/my-feet-are-killing-me.html

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9212
"Digital transformation after the pandemic

Credit: piranka / iStock

By Beth Stackpole, MIT Sloan School of Management

In the wake of last year’s COVID-19 shutdowns, information technology groups pivoted nearly overnight, launching technology-driven initiatives to enable remote work and distance learning. New customer experiences and new online sales channels followed close behind.

The moment was not just an exercise in fast-tracking technology deployments, it was also a test of companies’ ability to get employees and customers to embrace new forms of business engagement and interaction.

“Unless organizations change, the technology really does nothing for the business,” said George Westerman, principal research scientist for workforce learning in MIT’s Abdul Latif Jameel World Education Lab.

“Digital transformation is less of a digital problem than it is a transformation problem,” said Westerman in a recent webinar for MIT Sloan Management Review. “It’s a leadership problem for envisioning and driving change.”

Challenging assumptions

Westerman, a senior lecturer at MIT Sloan, said a number of longstanding assumptions about how companies typically evaluate and prioritize digital transformation initiatives were upended by the business response to COVID-19:

Assumption: Customers value the human touch. Reality: COVID-19 proved that a well-architected digital experience can offer an equivalent or even a more personalized transaction than an in-person engagement.

Digital transformation is less of a digital problem than it is a transformation problem.

George Westerman MIT Abdul Latif Jameel World Education Lab

Assumption: Regulation inhibits digital transformation. Reality: During the pandemic, highly regulated industries like health care were open to addressing barriers like privacy concerns for much-needed services like telehealth visits.

Assumption: It’s prudent to be a “fast follower.” Reality: Evaluating others’ innovation efforts before taking action on your own wasn’t an option during COVID-19, and isn’t a good idea going forward, Westerman said. Companies operating from this playbook during COVID-19 were more likely to lag behind competitors and miss opportunities, putting them at higher risk for business closures.

Assumption: IT can’t keep pace with digital transformation efforts. Reality: Tech organizations across every industry stepped up to keep operations going and revenue coming through the door.

Assumption: People won’t pay full price for digital-only. Reality: Consumers paid for digital products and services — and will continue to do so post-COVID, Westerman said.

Next initiatives

With consumers and organizations finally ready to embrace digital change at scale, how should companies leverage that shift to facilitate future transformation? Westerman identified four areas where companies should focus their next-generation digital initiatives:

Customer experience: While not necessarily new terrain, customer experience in the new digital era is becoming more personal, with more emotional engagement than in the past. Companies are leveraging data to provide better customer experiences, whether it’s an online clothing site leveraging technologies like artificial intelligence and machine learning to curate an outfit based on personal preferences or a brick-and-mortar retailer using a mobile app to offer customers curbside pickup and contactless payment.

Employee experience: Automation and machine learning can bring efficiencies to routine tasks, while other technologies like augmented reality can assist workers in ways not possible before. Westerman cited an initiative underway at Newport News Shipbuilding, which is using AR to overlay digital instructions onto the workspace, for example telling electricians how to route their wires, or showing when a piece of equipment is too heavy to lift alone. “Workers are not working harder, they’re working better, and as a result, the company is working better,” Westerman said.

Operations: Fueled by the Internet of Things and Industry 4.0, innovations like digital twins and machine learning help companies better leverage real-time data to improve operational performance and introduce new services.

For example, contract manufacturers are using data to determine what components work best together and what suppliers are most reliable to help their customers design better electronics products. Defense manufacturers leverage simulation to adjust designs, boosting quality and reducing the number of failed prototypes. And other companies are able to introduce new product-as-a-services offerings, having optimized operations through the use of data.

Business model transformation. Not every company needs to be an Uber or an Airbnb, disintermediating an industry, Westerman said. Organizations can seek out smaller opportunities for digital enhancement and information-based extensions. Insurance companies, for example, are monitoring and scoring customers as they drive to optimize policy pricing; fleet companies are leveraging real-time data and analytics to deliver predictive and proactive maintenance services.

People power

Those initiatives aren’t likely to succeed without strong leadership, Westerman said. “What’s changed over the last five years is two dimensions: Employees matter as much as customers in many cases, and you need to create a digital-ready culture to be fast enough to compete in this world,” Westerman said.

Those leading future efforts will be tasked not just with bringing the right technology to bear, but in inspiring people to embrace the change that goes hand-in-hand with transformation.

“If we’re going to make digital transformation happen in the pandemic and after, it’s got to be all about working through the people by providing the tools, energizing them, and listening to them,” Westerman said. “That’s how we come out of this stronger than we were before.”",5944
"Setup

First, we need to install a ruby gem and ChromeDriver. For this setup, I’m using macOS. If you don’t have Ruby installed on your machine, then get it from here.

Run the following command in your terminal.

gem install selenium-webdriver

Now, run the following command in your terminal to install ChromeDriver. Here, I’m using Homebrew to install the driver.

brew install chromedriver

If you're facing any issues with the initial setup, have a look at my GitHub or use the error message to solve the problem. Alright, we are good to go. You can download the following code from GitHub. Let’s break down the individual ingredients of the web_scraper.rb file. For this post, we will be scraping data from the website called toscrape. It’s a web scraping sandbox.

First, we are initializing our selenium WebDriver with a headless chrome browser.

def init_selenium_webdriver

options = Selenium::WebDriver::Chrome::Options.new

options.add_argument('--headless')

return Selenium::WebDriver.for :chrome, options: options

end

Now, we have the URL we will be using to parse data. Here, we are starting from page number 1. Inside the loop, we navigate the web page using the driver, URL, and page number.

def main

driver = init_selenium_webdriver

url = ""http://quotes.toscrape.com/page/""

page_no = 1

loop do

navigate(driver, url, page_no)

data = get_data(driver, {class_name: 'quote'})

parse_data(data, {text: {class_name: 'text'}, author: {class_name: 'author'}})

page_no += 1

break unless has_data?(data)

end

driver.quit

end

Inside the navigate method, we are constructing the final URL using the base URL and page number.

def navigate(driver, url, page_no)

driver.navigate.to URI.join(url, page_no.to_s)

end

We are using the get_data method to extract quotes from the data returned by the navigate method. The find_elements method will return all elements matching the given arguments.

def get_data(driver, pattern)

return driver.find_elements(pattern)

end

After that, we are looping through all the quotes to print individual quote information in the console using the parse_data method. The find_element method will return the first element matching the given arguments. You can read more about it here.

def parse_data(data, pattern)

data.each do |quote|

text = quote.find_element(pattern[:text]).text

author = quote.find_element(pattern[:author]).text

puts ""#{text} -- #{author}""

end

end

After completing this step, we increment the page number by 1 and repeat the process for page number 2. Finally, We will increment the page number until there are no quotes to parse.",2613
"BitClout: Is It a Scam or The Future?

Photo by Vlada Karpovich from Pexels

There’s tension in the world of BitClout.

With the price of $CLOUT plummeting, one critic posted the “BitClout Scam Is Falling Apart”.

But supporters of the platform are resolute in their defense. Some cite this graph as evidence.

All screenshots by author

Others point to Bitcoin’s early existence:

With this being said, I’m not here to bash BitClout. Nor am I here to defend it. I just want to share my findings.

I joined BitClout 47 days ago. I enjoy using the platform and post 3–5 times a day. It’s been fun making new connections.

However, I’m not emotionally attached. If the platform was to die today, so be it. I’m also not going to pretend I understand its development.

I’m just trying to look objectively from a content creator’s perspective.

Here’s how I see BitClout right now.

Quick Summary: What is BitClout?

According to the BitClout White Paper,

“BitClout is a new type of social network that lets you speculate on people and posts with real money, and it’s built from the ground up as its own custom blockchain. “Its architecture is similar to Bitcoin, only it can support complex social network data like posts, profiles, follows, speculation features, and much more... Like Bitcoin, BitClout is a fully open-source project and there is no company behind it — it’s just coins and code.”

Its design is similar to Twitter. However, unlike the Facebooks and Twitters of the world, BitClout is decentralized and fully open-source. There’s no company behind the platform.

This being said, there is a team of core developers. The leader of this group goes by the name of DiamondHands.

Creators can make money on BitClout in a number of ways. Here are the main four:",1770
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2021 Summer OlyBookmark this sitempics in Germany, the 2021 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2021 World Movies inspired many Americans to buy their first tv set and in 2021, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 6, 2021 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRARuPaul’s Drag Race All Stars CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/On71E6z

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9178
"➕Official Partners “TVs” TV Shows & Movies

● Watch The Good Fight Season 5 Episode 1 Eng Sub ●

The Good Fight Season 5 Episode 1 : Full Series

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ



The Good Fight — Season 5, Episode 1 || FULL EPISODES : Picking up one year after the events of the final broadcast episode of “The Good Wife”, an enormous financial scam has destroyed the reputation of a young lawyer, Maia Rindell, while simultaneously wiping out her mentor and godmother Diane Lockhart’s savings. Forced out of her law firm, now called “Lockhart, Deckler, Gussman, Lee, Lyman, Gilbert, Lurie, Kagan, Tannebaum & Associates”, they join Lucca Quinn at one of Chicago’s preeminent law firms. .

The Good Fight 5x1 > The Good Fight S5xE1 > The Good Fight S5E1 > The Good Fight TVs > The Good Fight Cast > The Good Fight Online > The Good Fight Eps.5 > The Good Fight Season 5 > The Good Fight Episode 1 > The Good Fight Premiere > The Good Fight New Season > The Good Fight Full Episodes > The Good Fight Season 5 Episode 1 > Watch The Good Fight Season 5 Episode 1 Online

Streaming The Good Fight Season 5 :: Episode 1 S5E1 ► ((Episode 1 : Full Series)) Full Episodes ●Exclusively● On TVs, Online Free TV Shows & TV The Good Fight ➤ Let’s go to watch the latest episodes of your favourite The Good Fight.

❖ P.L.A.Y ► https://cutt.ly/En7VYtV

The Good Fight 5x1

The Good Fight S5E1

The Good Fight TVs

The Good Fight Cast

The Good Fight Online

The Good Fight Eps.5

The Good Fight Season 5

The Good Fight Episode 1

The Good Fight Premiere

The Good Fight New Season

The Good Fight Full Episodes

The Good Fight Watch Online

The Good Fight Season 5 Episode 1

Watch The Good Fight Season 5 Episode 1 Online

⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 5, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAThe Good Fight CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/En7VYtV

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",10922
"We have come to know that your TomTom Voice Control Not Working properly. Behind this issue of your TomTom Gps device, there could be a number of possible reasons. But amongst all, the one reason that really matters is an outdated TomTom Gps device.

Moving forward in this article, let’s find out how to fix it when TomTom stops working. So, repeat all the steps in the same order as given. By the end of this article. Your issue with TomTom GPS will get fixed.

What You Should Do When TomTom Voice Control Not Working?

For a number of reasons if your voice control not working on TomTom, here are some common steps.

Complete Your Gps Update

Most of the time, the TomTom voice control stops working soon after you have completed a Tomtom Map Update. Hence, you need to verify whether you have completed the TomTom update successfully or not.

While doing the Tomtom GPS Update, it could be possible that any file got missed or corrupted. Or else, you may have left any window unopened.

2. Restart TomTom Gps Device

Most of the time, an issue like this could be easily fixed just by a quick restarting process. So, do follow the below-given steps to fix the issue of TomTom Voice Control Not Working.

Press the power button on the Gps device to turn it off.

Don’t forget to detach all the cables connected to it.

Remove the power plug of the TomTom Gps device from the wall outlet.

Don’t turn on your TomTom device for more than 60 seconds.

Reconnect all the detached cables and turn on the power plug.

After doing that, check whether your Gps device starts working again or something is wrong.

3. Check The Microphone Of TomTom

Have you checked the microphone button given on your Gps device? It may be possible that you haven’t turned on the button yet. This is the main reason why TomTom Gps is not listening to any of your commands.

Hence, find out the microphone button on your Gps device and turn it on. After doing that, you have to check again if your TomTom Voice Control Not Working.

4. Update The TomTom Gps Device Again

Even after trying all the steps, if still, your Gps device is not working then do Tomtom XL Update again.

You must have a good speed internet connection.

Make sure that you have completely charged your Gps device.

When your device will be charged completely, remove it from the vehicle.

With the help of a USB cable, connect your TomTom with the computer.

Once your GPS is connected, you will get a notification on your computer device.

On your computer device, download the TomTom Home which is software that will help you to complete the map update process.

Once the Tomtom Map Update process will be completed, remove the device from the computer and connect it back to the vehicle.

Conclusion

In this guide, we have discussed all the possible ways to fix why TomTom Voice Control Not Working. With the help of the given methods, you can easily fix the Gps voice control issue quickly.",2946
"No doubt that the Magellan Gps device is user friendly and covers the needs of a number of users. But, for its flawless and endless performance, it is necessary to do the Magellan Gps Update.

Due to the continuous construction, the landscape continues to change. Hence, if you will not update your Magellan Gps maps, you will never reach the right destination.

So, read this guide properly and follow all the steps in the same given order. By the end of this article, you will be aware of how to do the Magellan Updates.

Steps To Complete Magellan Gps Update

It is very necessary to do the Magellan Map Update for its better and amazing performance. With the best quality, you will also get all the latest features and maps.

Charge Your Gps Device

To begin the process of updating Magellan Gps, connect your device with the charger until it will not charge to full. It is because your device should not get down in between the Magellan Gps Update process.

2. Download Magellan Content Manager

Once your device will be fully charged, download the content manager to your computer. Let us tell you that Magellan Gps Software is the one that will help you to download the latest map updates without facing many issues.

You can easily download the Content manager from the official site of Magellan. So, connect your computer device with the internet and begin the process to download the Magellan Gps Update Software.

3. Install Magellan Content Manager

Once you will complete the process to download the file into your computer. Open the file and complete the installation process by following all the on-screen instructions.

4. Connect Gps With The Computer

Soon after you will install the Content Manager into your computer, now connect the Gps device.

Along with the Gps device, you must get a USB cable. Hence, with the help of that cable, do connect your Gps with the computer.

After connecting both the end of Gps correctly, press the power button on the Gps device.

5. Launch Content Manager

Once your device will be connected to the computer, open the file of the content manager to complete the Magellan Gps Update .

Launch the content manager by tapping on its icon.

When you will open the software, you will have to enter the email id and password. So, enter the Magellan Gps credentials.

In case you are a new user, you will have to create an account now.

Once you will log into the software, click on “check for updates”

You will get the notification for available Magellan Gps Map Update .

Now, it’s up to you whether you want a free or paid GPS update.

6. Remove Magellan Gps From Computer

When your Magellan Gps device will be updated to its latest version, remove it from the computer. After the update, your device will restart automatically so you don’t have to worry about it.

Conclusion

In this guide, we have told you all the steps that are required to complete the Magellan Gps update. Hence, this is your responsibility now to complete the Gps update after a regular interval of time.",3033
"If you have found that your Garmin Gps Not Working then a number of reasons could be responsible for the same. It may be possible that you have not updated your Gps device or not plugged it in properly. Apart from this, a Frozen screen, low battery, are some of the other factors for why your device has stopped working.

Gps devices are really very necessary in today’s time to reach the right destination. No matter where you are sitting, you can track your vehicle easily. There comes the time when you find that Gps has stopped responding and that’s what we are going to discuss in this article.

With the help of this troubleshooting guide, you will fix the Garmin Not Working issue in a very short period of time.

Why Is My Garmin Gps Not Working Properly?

If your Garmin Gps Not Working then you must check the available internet connection instead of updating the device. In other words, it can be said that it is necessary to find the main cause first to fix the issue.

If there is a beep sound then it means that the battery gets discharged and needs to be connected to the power outlet if your Garmin Gps Not Charging.

The Frozen Garmin device screen is another reason why you are searching for “Why My Gps Is Not Working”.

It may also be possible that you have not updated the Gps device and hence Garmin Map Update is required.

How To Fix When Garmin Gps Has Stopped Working?

Check the internet connection, complete Garmin Gps Map Update, check the batteries and power access, restart the Gps device, and then hard reset the Gps to fix the issue of Garmin Gps Not Working.

Low Battery Or No Battery In Gps

If your Garmin Gps Not Turning ON then you must check the battery inside the device. Either you have forgotten to charge the Gps device or your battery has been damaged. Hence, you would need to check the batteries inside the Gps to fix it.

2. Hard Reset The Garmin Gps Device

This is one of the best steps that will bring your Gps device back to normal condition and you will be able to use it again.

First, disconnect the Gps device from the power plug and then press and hold the power button for a few seconds.

On your Garmin Gps screen, confirm your action to proceed further. Doing this will reboot the device and all the software-related glitches will be fixed then.

Make sure that your Gps antenna is plugged in properly on your Garmin Gps device.

Once the device will be reset, do check if the Garmin Gps Not Working issue gets resolved or not.

3. Reinstall The Garmin Gps Device

If you want to perform these steps then you would need to reset the Gps device again. With the help of a USB cable, connect the Gps device to the computer.

Download the Garmin Update Software on your computer device and then complete the registration process.

on your computer device and then complete the registration process. With the help of Garmin Express, you would need to find the latest update and complete the Garmin Update.

These were some of the common troubleshooting steps that you can apply if your Garmin Gps Not Working properly.

Common Garmin Gps Related Issues

Faulty Power Button Garmin Gps Sound Issue Gps Locked Up Error Draining Battery Garmin Shuts Off Frozen Screen In Garmin Garmin Gps not detecting signals

Conclusion

We have discussed all the possible aspects to fix the Garmin Gps Not Working issue. Several reasons could be responsible for the same and we have mentioned all of them in this troubleshooting guide.",3474
"The real magic in this prototyping technique lies in the ink. These inks are formulated to have the right particle size so that the nozzle is not clogged and have the right viscosity for printability. Moreover, functional inks usually require heat or photonic (high-intensity light) treatment to make the printed pattern conductive. However, Mitsubishi Paper Mill’s ground-breaking ink utilized chemical sintering to make silver conductive instantly after printing without the need for further treatment. This ink is currently not available for purchase but there are some alternatives.

Although inkjet printing electronics was demonstrated decades ago, it was brought to the public eye by AgIC in 2014. The company used Prof. Yoshihiro Kawahara‘s research with Mitsubishi’s special silver ink to launch a successful Kickstarter campaign. Prof. Kawahara et. al. demonstrated several applications ranging from touch sensors to capacitive liquid level sensors in their award-winning 2014 paper.

Kickstarter Campaign by AgIC

Silver and carbon inks for desktop inkjet printers by Novacentrix are available for purchase online. Although they recommend that their silver ink patterns be treated by additional expensive equipment, it has been demonstrated that treating the silver ink on a hot plate at 100–120C produces acceptably conductive patterns. The carbon ink also requires heat treatment after printing, which is possible in an oven or hot plate. Once you have the functional inks, follow the steps by Maximus in his featured Instructables post. Just a tip: You can use conductive silver epoxy to solder circuit components instead of copper tape as used by Maximus. You need the following:

A low-cost printer such as EPSON WF 2010

Printing substrates like PET and glossy paper

Oven or hot plate for sintering & drying the ink

Empty refillable cartridges

So, what can you make or prototype with this (apart from PCBs)?

Temperature sensors based on resistance measurements

Printed temperature sensors by Liew et. al / MDPI

The resistance of a material is dependent on the temperature. Researchers from Malaysia used this concept to inkjet-print silver nanoparticles onto PET to measure temperature. They created a meander pattern and measured the resistance at the two ends. As the temperature increased, so did the resistance.

Humidity sensors based on capacitance measurements

Printed humidity sensors by Gaspar et. al. / MDPI

Researchers from VTT Technical Research Centre of Finland, inkjet-printed a pattern (interdigitated electrodes) with silver nanoparticles to measure humidity. The capacitance of this pattern changes as the humidity changes. Hence by measuring the capacitance, one can calculate the humidity.

The resolution demonstrated in this paper might not be possible with a desktop inkjet printer, but the same concept can be applied to create similar sensors with different dimensions.

Antennas for IoT applications

Printed antenna for IoT applications by Kirtania et. al. / MDPI

Antennas require a conductive material to convert electromagnetic waves into voltages and currents, and vice-versa. By changing the shape and size of the printed patterns, antennas can be designed to operate at different bands, for example, Wi-Fi or 5G frequencies.

Researchers from the Washington State University inkjet-printed this antenna on PET with a silver nanoparticle ink for IoT applications",3419
"In our privileged world we have stuff. Lots of stuff. And all that stuff we buy has traditionally been designed to break. It truly was a light bulb moment when in the 1920s, representatives from top manufacturers worldwide, colluded to artificially reduce a bulb’s lifetime to 1,000 hours. And so planned obsolescence was born. And it’s not just products, fast fashion was created with the same motives — to get consumers to buy more, more frequently.

But if we buy something, we own it. It’s ours to do what we want with it. Use it, modify it, repair it, however we want. Right? Wrong. In so many cases while you own the physical hardware, you are merely a user of the technology. Manufacturers use glue, not screws, to stop you getting into a product and to protect their IP — under the pretext of defending you from hackers. You think you own it — you don’t.

All of this contributes to the accumulation of stuff, some of it working, some not, some ours, some not really. But things have been changing, spurred on by growing demand from consumers, community groups and enlightened brands. And now underwritten by the ‘right to repair’ law which comes into practice later this year.

For many years it’s been a grassroots movement campaigning for a new business model and for a repair economy which works from the ground up. Remade a network of repair social enterprises has been around since 2008, when the first Remakery was started in a block of disused garages in Brixton, UK. The Manchester Declaration demanding the right to repair was created in 2018. Incidentally it’s a great source of information on where to find your local repair shop, or Repair Cafe of which there are now over 1,500 worldwide. These are meeting places for the like-minded, providing the tools and materials to, with expert volunteers on hand, help you repair everything from toys to televisions.

Brands are now developing products to last, which includes thinking about how they can be repaired. Fashion brands are encouraging us to repair and keep wearing their clothing. Patagonia, a brand which leads the way in environmental activism has teamed up with ifixit providing step-by-step guidance on how to extend the life of your high-performance gear. This complements their Worn Wear initiative which encourages people to reduce consumption by buying second hand clothing. This is something that would be completely unimaginable a few years ago — a brand actually encouraging you not to buy new. Tech brands are also on board and it’s not just start-ups like our (much-loved) brand Previously. In March, Apple announced it was extending its Independent Repair Provider programme to over 200 countries. OK so YOU still can’t do it, but at least they are opening up repair of their products. Even Amazon (the brand we all love to hate — killing the high street, invading our privacy, etc…) has built their 4th Generation Echo Dot to be “generally” repairable according to a review by the The Guardian. And if you want to know how a product ranks on ‘repairability’ ifixit will tell you — in the world of phones it’s the Shift 6M, a German brand championing modular design.

Outside the world of tech we see another side to repair — a love for the object itself. It’s why Repair Shop — first aired in 2017 — is coming back in a prime time slot this year. It’s not just the history of the object we enjoy, there is real appreciation of the craftsmanship that goes into bringing each object, whether it’s a clock or a cup, back to life. Even to the extent of the repair itself being a thing of beauty. It’s a tradition that goes back to Kintsugi — the ancient Japanese technique — which uses lacquer to repair a crack in a piece of porcelain and then embellishes it with gold. A visible repair tells the story of the object and connects us with its past. In Brighton, textiles practitioner Tom van Deijnan has set up the Visible Mending Programme. It is intended to highlight the art and artisanship of clothes repair and reinforce the relationship between the wearer and garment. The darn, done beautifully, not only extends the life of a garment but becomes in Tom’s words a ‘badge of honour’.

So what does this mean for brands. Brands need to take action, not just talk about it. There is justified concern for our environment and recognition of the role we each need to play in protecting it — which is reflected in our purchasing choices and in the brands we choose to associate with. From consumers we see a desire for autonomy and individualism. Maybe it’s the survivalist coming out in us all during these extraordinary times, but increasingly we need to feel in control of our decisions and literally be able to get under the bonnet. When so much is ubiquitous and generic we are looking for ways to make things our own, to uniquely reflect who we are.

Finally, because I thought this was a perfect ending — Kin means gold, Tsugi means connect — connect to the world, connect to the generations. A sentiment which feels incredibly relevant today.

Frances Jackson

CEO OPX",5065
"Connected Home Security System

Connected Home Security System Market Introduction & Scope:

Introspective Market Research has conducted a thorough and extensive analysis of the global Connected Home Security System market. Our in-house experts validate data using tools such as PESTEL and SWOT analysis and look at various market trends in terms of value and volume for current and future market conditions. IMR’s latest market research report on Connected Home Security Systems is a syndicated industry analysis and market report for Connected Home Security Systems published with global market size, status and forecast 2021–2026. It is a complete research study and industry analysis of the market to understand the market influencing market demand, growth, trend analysis and factors.

Also, it strategically profiles the key players and comprehensively analyzes their development, mergers and acquisition and the R&D investment done by the competitors.

To define, describe and forecast the market by product type, market and key regions.

The Connected Home Security System market report comprises of:

past acquisition strategies supported

activities taken to remove competitive barriers

actions data to resolve deficient data issues

changes in the market place (suppliers, trends, technologies)

past acquisition history

Leading players involved in the Connected Home Security System Market includes:

Honeywell, ADT, Securitas, Panasonic, Samsung, Vivint, LifeShield, Scout Alarm

To identify the key drivers and the competitors’ insights, Request a Sample @:

https://www.imrmarketreports.com/request/4014

Global Connected Home Security System Market: Key Catalysts

Introspective Market Research in a recent report focused on providing all the market potentials and drivers influencing the market. The opportunities and constraints for the market are presented after reviewing all market estimates and details provided by reliable sources for the Connected Home Security System market. In addition to a detailed description of the key factors, the report provides a complete understanding of the challenges, opportunities and factors that are influencing the future market growth.

The analysis also divides the Connected Home Security System Market on the grounds of main product type:

Monitor System, Alarm System

The analysis also divides the Connected Home Security System Market on the applications:

Villa, Apartment, Other

Driver Analysis: After surveys, data mining and collecting from paid and real sources, the results for the global Connected Home Security System market are presented with the utmost understanding and factors considered for the market growth. IMR’s reports will be updated over the next few years, providing customers with accurate data and value on market trends using the same size and forecast variable.

Get Discount on Full Report of Connected Home Security System Market @:

https://www.imrmarketreports.com/discount/4014

Restraints: The specific types of issues a manufacturer or supplier is dealing with in relation to the global Connected Home Security System market is analyzed in the report to ensure that customers have a clear and easy-to-understand overview of the market and trends. Factors that may impede the growth of the market or arise during the forecast period are clearly validated and explained based on facts and figures for an overview of customers in the Connected Home Security System market.

Opportunity Mapping: Introspective Market Research aims to provide a focused analysis of the data and facts with the underlying opportunity for clients to understand this and grow in the market through every detailed analysis in the report. Opportunity mapping helps to lay the path for future growth that can help users monetize the opportunities that exist in the marketplace. The IMR market research report provides a detailed analysis of mergers and acquisitions, providing opportunities to win customers in the industry domain.

The full overview of the report is aimed at providing an analysis of the market to help customers know a detailed version of the threats of new entrants, supply and demand chain analysis, and the challenges facing the market. Growth factors that will guide the market for high compound annual growth rates.

The global Connected Home Security System market is illustrated by key results:

– The overview, scope, definition and the factors driving or impaling the market discussed strategically.

– Connected Home Security System full analysis, DROC’s analysis, Competitor analysis with the key players introduction and revenue generated.

– Segments and Sub-segments full analysis with correct market estimations that will help diversify the market with ease.

– Global Connected Home Security System market report advices on the report values and the details that are focused to grow in the industry and reviews the challenges faced in the market during the pandemic.

Good Reasons to Buy a Global Connected Home Security System Market Report:

Offers detailed market breakdown and data triangulation that will help guide the market trends.

The methodology used in the report gives the absolute authentic data that helps advising whether the market has an opportunity in the upcoming years.

Provides a detail analysis about the Covid-19 impact on the market and how it will help boost the market in the near future.

Give’s a thorough analyst’s viewpoint which guides the client through market collaterals for the better opportunities in the market.

Pinpoints the most valued region and how the market can be expanded in the industry to generate more revenue with all the important government guidelines and environmental policies.

About Us:

IMR Market Reports is a visionary market research company who is ready to assist their clients to grow their business by offering strategies through our market research reports for gaining success. We have a well experienced team, who work efficiently and provides complete excellent research data in a complete sync to provide overall coverage and accurate market insights on various industries.

Contact Us:

Akshay Patil

Business Development Executive

Office No 15, Vrundavan Commercial Complex,

DP Road, Kothrud, Pune, India — 411038

Contact No: USA: +1 (773)-382–1047 / IND: +91–842–168–0185",6354
"Should Your Company Shift to a Hybrid Workforce? Look to the Cloud for the Answer. Steven Perlman Apr 27·4 min read

Think back five or ten years to when your company first started considering the idea of moving to the cloud. Likely, a few people wanted to move entirely to the cloud, a few were entertaining the idea of a hybrid model, and even more thought the idea of a complete migration was ridiculous. Where are all of those people now?

Likely, it would be unfathomable to imagine your company without the use of the cloud in your day to day life, especially now that the modern workforce is in a transitional period.

Think about now how you and your colleagues think about a remote work environment. Likely, you have a similar dynamic. Some of you are ready to work from home full time, others think a full return to the office is the only way, and still others are pushing for a hybrid model.

Onsite = On-Prem | Remote = Cloud

Years ago, the companies who were early adopters implemented a cloud-forward strategy and were considered the risk-takers. They were the ones everyone waited to see if would fail, and now they’re considered the revolutionaries. The only way to access a good internet connection was to log onto the big, bulky desktop at your workstation in the office (I’ll bet you can still hear the dial up tones if you try). No one would have ever imagined you’d be able to work in that same spot without having eyes on the physical place where your data is being stored securely — remember the big server room (remember paying rent for a big room for your data)? It’s now been rendered all but obsolete.

We are hinging on the same type of transition when it comes to a remote workforce. The onset of the COVID-19 pandemic radically changed the way that people were able to work. There was no choice but to send employees to their homes to work, and while everyone originally had thought it a temporary fix, over a year later, it seems like it may not be so much.

Hybrid Workforce = Hybrid Cloud. It’s a good option, but it’s better used as a transition.

There were absolutely more conveniences to having some of your data in the cloud as opposed to none of it. As you started utilizing the cloud, you likely had some info on there and the rest on-premise, then slowly transitioned until you were totally virtual.

You’re likely having a similar experience now with the remote workforce. Maybe you have a rotation going of who comes into the office every two or three days, or everyone meets once a month for a certain meeting. There’s probably talk of when it will be safe to return to the office full time, and some employees would rather be back as soon as possible while others would prefer to stay virtual.

This is where the cloud vs. on-prem metaphor returns. Making a move to an entirely remote workforce is the same choice that your predecessors had to make about moving entirely to the cloud. A daunting concept at first, but now looking back, it’s hard to believe anyone was ever against it. In five or ten years from now, it will be hard to believe anyone ever stood against a remote workforce.

There are many benefits to a remote workforce that haven’t even been realized yet, the same as it was with moving to the cloud.

For example, when people made the choice to move to the cloud a decade ago, how could they have known it would be vital to allowing employees to work from home during a deadly global pandemic? Talk about an unforeseen benefit!

More immediately, think about the money you’ll save as a company by going fully remote. You won’t have to pay rent, utilities, insurance, or anything else that you would normally pay on a physical space. You’ll be able to hire #TopTechTalent from anywhere you want to — there’s an entire untapped market of technology professionals you can choose from when building out a team.

Your reputation as a Best Place to Work is also likely to skyrocket. Remote work capabilities are a huge benefit for companies, and it will definitely give you a leg up over your competition. You don’t want to miss this chance to be at the front of an evolving workforce.

Steve Jobs once said, “Innovation distinguishes between a leader and a follower.” Which one are you going to be remembered as?",4271
"“It is the Passionate, Strong-hearted Ones with a Will to Make Things Happen Who Run Lenskart”: A Peek Inside the Lenskart Culture Anupriya Jain Follow Jun 2 · 4 min read

It is my first month at Lenskart and one of the first things I notice about this place is that it’s abuzz with activity. There are a million things happening at the company at a speed I’ve not seen before. People are working on big, ambitious projects (there are no stopgap solutions; people here build systems for scale), multiple experts form each team, and there are highly data-driven, action-focused conversations happening on each Zoom call I attend. It’s as busy as a beehive on a spring afternoon.

“I moved here from 10 years at Adidas because I wanted to work in a fast-paced, startup environment. And I got exactly what I was looking for,” Emma, who heads Product & Marketing for Singapore, tells me.

If you are willing to go the extra mile, working at Lenskart can feel like running your own startup.

“We are in fact encouraged to visit the stores as many times as we like, observe, interact and really get to know the customer. There are no rules/restrictions if you really want to get into the thick of things and understand how you can best contribute to the company,” says Deepnarayan Tripathi, Head of Organization Development.

People don’t have ‘jobs’ at Lenskart, they have responsibilities, ownership and accountability — that sums up to something much bigger than assigned roles and tasks.

“I see our call center agents as ‘fashion consultants’. They are not selling a product (in fact, we discourage pushing products that are not relevant to the customer). They are curating a personalized lifestyle by connecting their understanding of the product to the customer’s needs,” says Anil Pandey, Head of Assisted Sales.

Across the organization, there is a preference to hire people for whom working with Lenskart is not just a typical 9-to-5. “We look for people who are driven to see and build this company as co-owners”, I hear in every conversation. And with the aim to build itself into a world-class consumer brand, they just cannot settle for anything less.

“Peyush (the CEO) is also an incredible visionary in this regard. He can envision things bigger than most leaders I have met, and still has an exceptional eye for detail for the entire value chain — and this has helped shape the growth and the very DNA of the organization,” says AP Nayak, heading the South East Asia business.

When it comes to execution, getting things done really fast is core to the company. There is an entrepreneurial fervor in getting things off the ground here.

“And these bootstrappers inevitably become fast-trackers at Lenskart,” adds Mohana Ukil, who is currently shaping the company’s hiring strategy.

One such example is Kavita who started off as a customer support agent in 2013 and has moved through multiple roles and increasingly senior positions across the organization.

As an integral part of the CEO’s office and the Lenskart Foundation today, she believes, “People are extremely helpful here and every time I have asked for support to learn more and grow, I’ve received it wholeheartedly. A few months into my joining, Peyush saw potential in me and promoted me to lead the customer support teams, even before I knew I had it in me to do it!”

People have different reactions when they encounter challenges — some can be naturally overwhelmed by it while there are always some who feel energized to push the boundaries. They try very hard to go beyond what is possible and Lenskart is full of the latter.

Another such example is that of Sumeet Kumar, who joined Lenskart’s manufacturing operations team and now heads business finance.

“If you want to take up something new or challenging, people show trust and give you that opportunity,” he shares. The key at Lenskart, according to him, is building that trust in the leadership by working hard and delivering results. “So when you decide to aim higher, like when I wanted to take up a senior role in Finance, there will be a resounding yes,” he says. “If you are willing to spot opportunities and work on them to add value to the organization, you belong here,” adds Aditya Kakkar, the man behind Lenskart’s brick-and-mortar stores.

Of course, with great power, comes great responsibility too. There are side effects to building a high performing culture, and the teams at Lenskart are constantly tracking and figuring out how to minimize those to support the sustainability of the culture.

“There is, of course, a lot of work needed from us in terms of recognizing and rewarding great performance, surrounding our people with equally capable talent (if not more), and making Lenskart a great place to work, to say the least. We completely recognize that,” says Peyush Bansal, the CEO.

Currently, the organization is in the process of refreshing behaviors and processes rooted in its culture — and bringing best practices from other rapid-growth, high-performing organizations that have succeeded in building effective organizational cultures over the years.",5118
"How To Activate Windows 10 Using Command Prompt AnthonyB. Follow May 11 · 3 min read

Three Easy Steps, which only take one minute.

Status: Still working as of September 2021.

Why Would People Need To Activate Windows?

Well there are many reasons. It could be because a school or a business is setting up an organization of computers and wants them registered under one key for uniformity. Another reason might be that you upgraded from Windows 7 or 8, and your key was due to expire due to changing Microsoft terms. Many people built their PCs on their own and lost their keys. If you are like others, you bought a PC from a builder or a dealer and they didn’t hand you your license key at all. You could have received your PC via your organization or company which came with Windows pre-installed without any key.

Other reasons include:

You simply lost your license key.

Your computer was damaged and had to be reformatted due to software instability, virus or hardware failure, but the liability of which was not your fault.

How To Activate Windows 10 Using Command Prompt?

FIRST! Open Command Prompt as administrator. Do this by clicking on the start button, then search for “cmd” then run it with administrator rights.

NEXT STEP: Install KMS client key

Carefully follow the steps below. READ before doing.

Use the command “slmgr /ipk yourlicensekey” to install a license key. Replace “yourlicensekey” with any of the factory license keys below. Make sure to choose the right one for your Windows version. Home: TX9XD-98N7V-6WMQ6-BX7FG-H8Q99

Home N: 3KHY7-WNT83-DGQKR-F7HPR-844BM

Home Single Language: 7HNRX-D7KGG-3K4RQ-4WPJ4-YTDFH

Home Country Specific: PVMJN-6DFY6–9CCP6–7BKTT-D3WVR

Professional: W269N-WFGWX-YVC9B-4J6C9-T83GX

Professional N: MH37W-N47XK-V7XM9-C7227-GCQG9

Education: NW6C2-QMPVW-D7KKK-3GKT6-VCFB2

Education N: 2WH4N-8QGBV-H22JP-CT43Q-MDWWJ

Enterprise: NPPR9-FWDCX-D2C8J-H872K-2YT43

Enterprise N: DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4

3. Hit [Enter] key to execute the command. Wait for a few seconds, and a prompt should appear.

4. Set KMS machine address. Very Important Step!!!

5. Use the command “slmgr /skms kms8.msguides.com” to connect to the msguides independent free KMS server.

6. Activate your Windows by using the command “slmgr /ato”.

LAST — Check the activation status again.

Extra advice

Want to stop Windows 10 from updating automatically? Click to view my article about it here.",2436
"Raspberry Pi……….Sounds like extremely delicious and edible pie doesn’t it? To the layman, it may, but to the ones initiated and integrated into technology, well, they’d know that Raspberry Pi is in fact, not an edible yummy pie made out of raspberries, but rather, a small computer the size of a credit card! These miniature computers can be plugged in with your standard monitors, a mouse, and keyboard and it’s good to go for usage! They were introduced in 2012 by a UK based charity called the Raspberry Pi Foundation whose aim is to educate people in computing and make it easier to access an education in computing.

They run on a free Linux based operating system that’s easy to understand and learn, making the functional aspect of a raspberry pi wide in all age groups, be it for school students with a keen interest in computer technology, to hobbyists enthusiasts of various tech related fields like robotics, weather prediction and other digital projects. Along with running Linux, it also has general purpose input/output pins (GPIOs) which allows the user to explore the field of IoT (Internet of Things) and control physical computing parts of a device. The low cost, modularity and open design of this computer makes it accessible and affordable to one and all, enabling them to explore computing and learn programming languages like Scratch and Python. Raspberry Pi runs on an open source ecosystem with a supportive Pi Operating System besides Linux, with it’s schematics regularly being published as open access. The Raspberry Pi Foundation relies on the sales of it’s unit to carry out its charitable activities in the educational sector.

Each model of Raspberry Pi that has been released generally has two models in circulation, Model A and Model B. Model A of all Raspberry Pi variants tends to be the cheaper one due to having less number of ports and reduced RAM. The latest models of this single board computer, named Raspberry Pi 4 comes with RAM capacities ranging from 2 to 8 GB. In India, these computers are available from a price range of Rs.1000 to Rs.10,000 per piece depending on the model one is interested in purchasing. Raspberry Jams are programs organized to increase the Raspberry Pi initiatives all around the globe, where people of all age groups can get together and share their ideas and projects with Raspberry Pi.

Now that we know how Raspberry Pi’s came about, it’s price range and usage benefits, let’s take a deeper look at the individual components within a Pi.

Part of a Raspberry Pi

1. GPIO

(General Purpose Input/ Output pins) are arguably the most important aspect of a Raspberry Pi, they allow one to connect electrical components like LED lights, buttons, inductors and relays to the Pi. They can be used to read the electrical signals from circuits and also to provide electrical signals to circuits. However it is to be kept in mind that it’s easily damageable.

2. Ethernet/USB/HDMI Ports

These ports allow the Raspberry Pi to be connected to a network with a cable or a wireless LAN, mouse, keyboards or USB drives and to an HDMI screen like a monitor or a projector to display the output from the Raspberry Pi.

3. Audio Jack

To connect the headphones or speakers to make your system audible, however if your HDMI device like your monitor has speakers, audio doesn’t need to be plugged in.

4. Camera Module Port

Allows the connection to a Raspberry Pi camera module. Web cameras cannot be connected as they have a regular USB connector.

5. Micro USB Power

Raspberry Pi’s are powered using these ports to which the power supply is connected. Ensure that this step is done last after attaching all the other components.

6. Micro SD Card

Used to place the microSD card within which the Raspberry Pi operating system (Raspbian) is contained. The SD card is bought separately and acts as the hard drive of the computer holding all of the users personal files as well.

Getting Raspberry Pi’s for your kids is a novel and innovative way to introduce them to the world of digital technology and electronics especially if they have a special interest towards this arena. Being kid friendly, Raspberry Pi’s host a myriad of benefits to the children’s learning and educational progress over other computers in the following ways -

Helps them understand the layout of standard computer

Enables them to create fun project with parental supervision

Introduces them to simple computers and it’s functioning.

It can even double as a portal movie player when attached to a projector

Help them learn programming languages like Scratch and Python

Meaning your child’s beloved Raspberry Pi can double as a helper in coding whilst also being compatible for regular use due its well developed OS, the Raspbian.

Help kick start or amp up their love for computers, programming and related fields, thus opening a wide spectrum for their future career!

Knowing the parts of a Raspberry Pi will help one configure the system to their specifications, however in order to make it functional it is important to install the operating system as well, the procedure for which can be found here. Now that you know the parts of a Raspberry and a plethora of details regarding it, including why it’s beneficial for your child, it’s time to help them get started on their very own Raspberry Pi based project be it for studies, passion or both!",5388
"What is an operating system?

An operating system (OS) is the program that, after being initially loaded into the computer by a boot program, manages all of the other application programs in a computer. The application programs make use of the operating system by making requests for services through a defined application program interface (API). In addition, users can interact directly with the operating system through a user interface, such as a command-line interface (CLI) or a graphical UI (GUI).

Why use an operating system?

An operating system brings powerful benefits to computer software and software development. Without an operating system, every application would need to include its own UI, as well as the comprehensive code needed to handle all low level functionality of the underlying computer, such as disk storage, network interfaces and so on. Considering the vast array of underlying hardware available, this would vastly bloat the size of every application and make software development impractical.

Instead, many common tasks, such as sending a network packet or displaying text on a standard output device, such as a display, can be offloaded to system software that serves as an intermediary between the applications and the hardware. The system software provides a consistent and repeatable way for applications to interact with the hardware without the applications needing to know any details about the hardware.

As long as each application accesses the same resources and services in the same way, that system software the operating system can service almost any number of applications. This vastly reduces the amount of time and coding required to develop and debug an application, while ensuring that users can control, configure and manage the system hardware through a common and well-understood interface.

Once installed, the operating system relies on a vast library of device drivers to tailor OS services to the specific hardware environment. Thus, every application may make a common call to a storage device, but the OS receives that call and uses the corresponding driver to translate the call into actions (commands) needed for the underlying hardware on that specific computer. Today, the operating system provides a comprehensive platform that identifies, configures and manages a range of hardware, including processors; memory devices and memory management; chipsets; storage; networking; port communication, such as Video Graphics Array (VGA), High-Definition Multimedia Interface (HDMI) and Universal Serial Bus (USB); and subsystem interfaces, such as Peripheral Component Interconnect Express (PCIe).

Functions of an operating system

An operating system provides three essential capabilities: It offers a UI through a CLI or GUI; it launches and manages the application execution; and it identifies and exposes system hardware resources to those applications typically, through a standardized API.

UI. Every operating system requires a UI, enabling users and administrators to interact with the OS in order to set up, configure and even troubleshoot the operating system and its underlying hardware. There are two primary types of UI available: CLI and GUI.

The CLI, or terminal mode window, provides a text-based interface where users rely on the traditional keyboard to enter specific commands, parameters and arguments related to specific tasks. The GUI, or desktop, provides a visual interface based on icons and symbols where users rely on gestures delivered by human interface devices, such as touchpads, touchscreens and mouse devices.

The GUI is most frequently used by casual or end users that are primarily interested in manipulating files and applications, such as double clicking a file icon to open the file in its default application. The CLI remains popular among advanced users and system administrators that must handle a series of highly granular and repetitive commands on a regular basis, such as creating and running scripts to set up new personal computers (PCs) for employees.

Application management. An operating system handles the launch and management of every application. This typically supports an array of behaviors, including timesharing multiple processes, or threads, so that various tasks can share the available processors’ time; handling interruptions that applications produce to gain a processor’s immediate attention, ensuring there is enough memory to execute the application and its corresponding data without interfering with other processes; carrying out error handling that can gracefully remove an application’s processes; and performing memory management without disrupting other applications or the OS.

An operating system can also support APIs that enable applications to utilize OS and hardware functions without the need to know anything about the low level OS or hardware state. As an example, a Windows API can enable a program to obtain input from a keyboard or mouse; create GUI elements, such as dialog windows and buttons; read and write files to a storage device; and more. Applications are almost always tailored to use the operating system on which the application intends to run.

Additionally, an operating system can perform the following services for applications:

In a multitasking operating system, where multiple programs can be running at the same time, the OS determines which applications should run in what order and how much time should be allowed for each application before giving another application a turn.

It handles input/output (I/O) to and from attached hardware devices, such as hard disks, printers and dial up ports.

It sends messages to each application or interactive user or to a system operator about the status of operation and any errors that may have occurred.

It can offload the management of batch jobs for example, printing so that the initiating application is freed from this work.

On computers that can provide parallel processing, an operating system can manage how to divide the program so that it runs on more than one processor at a time.

All major computer platforms (hardware and software) require, and sometimes include, an operating system, and operating systems must be developed with different features to meet the specific needs of various form factors.

Device management. An operating system is responsible for identifying, configuring, and providing applications with common access to underlying computer hardware devices. As the OS recognizes and identifies hardware, the OS will install corresponding device drivers that enable the OS and applications running on the OS to use the devices without any specific knowledge of the hardware or devices.

An operating system is responsible for identifying the correct printer and installing the appropriate printer drivers so that an application needs to only make calls to the printer without having to use codes or commands that are specific to that printer that is the operating system’s job. The situation is similar for other devices, such as USB ports; networking ports; graphics devices, such as graphics processing units (GPUs); motherboard chipsets; and storage devices, such as Serial-Attached SCSI (SAS) disk adapters and disks that are formatted with a suitable file system.

The OS identifies and configures physical and logical devices for service and typically records them in a standardized structure, such as Windows Registry. Device manufacturers periodically patch and update drivers, and the OS should update them to ensure best device performance and security. When devices are replaced, the OS also installs and configures new drivers.

Operating system types and examples

Although the fundamental roles of an operating system are ubiquitous, there are countless operating systems that serve a wide range of hardware and user needs.

General-purpose operating system. A general purpose OS represents an array of operating systems intended to run a multitude of applications on a broad selection of hardware, enabling a user to run one or more applications or tasks simultaneously. A general-purpose OS can be installed on many different desktop and laptop models and run applications from accounting systems to databases to web browsers to games. General purpose operating systems typically focus on process (thread) and hardware management to ensure that applications can reliably share the wide range of computing hardware present.

Common desktop operating systems include the following:

Windows is Microsoft’s flagship operating system, the de facto standard for home and business computers. Introduced in 1985, the GUI based OS has been released in many versions since then. The user-friendly Windows 95 was largely responsible for the rapid development of personal computing.

Mac OS is the operating system for Apple’s Macintosh line of PCs and workstations.

Unix is a multiuser operating system designed for flexibility and adaptability. Originally developed in the 1970s, Unix was one of the first operating systems to be written in the C language.

Linux is a Unix-like operating system that was designed to provide PC users a free or low cost alternative. Linux has a reputation as an efficient and fast-performing system.

Mobile operating system. Mobile operating systems are designed to accommodate the unique needs of mobile computing and communication centric devices, such as smartphones and tablets. Mobile devices typically offer limited computing resources compared to traditional PCs, and the OS must be scaled back in size and complexity in order to minimize its own resource use, while ensuring adequate resources for one or more applications running on the device. Mobile operating systems tend to emphasize efficient performance, user responsiveness and close attention to data handling tasks, such as supporting media streaming. Apple iOS and Google Android are examples of mobile operating systems.

Embedded operating system. Not all computing devices are general purpose. A huge assortment of dedicated devices including home digital assistants, automated teller machines (ATMs), airplane systems, retail point of sale (POS) terminals and internet of things (IoT) devices includes computers that require an operating system. The principal difference is that the associated computing device only does one major thing, so the OS is highly stripped down and dedicated to both performance and resilience. The OS should run quickly, not crash, and handle all errors gracefully in order to continue operating in all circumstances. In most cases, the OS is provided on a chip that is incorporated into the actual device. A medical device used in a patient’s life support equipment, for example, will employ an embedded OS that must run reliably in order to keep the patient alive. Embedded Linux is one example of an embedded OS.

Network operating system. A network operating system (NOS) is another specialized OS intended to facilitate communication between devices operating on a local area network (LAN). A NOS provides the communication stack needed to understand network protocols in order to create, exchange and decompose network packets. Today, the concept of a specialized NOS is largely obsolete because other OS types largely handle network communication. Windows 10 and Windows Server 2019, for example, include comprehensive networking capabilities. The concept of a NOS is still used for some networking devices, such as routers, switches and firewalls, and manufacturers may employ proprietary NOSes, including Cisco Internetwork Operating System (IOS), RouterOS and ZyNOS.

Real-time operating system. When a computing device must interact with the real world within constant and repeatable time constraints, the device manufacturer may opt to use a real-time operating system (RTOS). For example, an industrial control system may direct the operations of a sprawling factory or power plant. Such a facility will produce signals from myriad sensors and also send signals to operate valves, actuators, motors and countless other devices. In these situations, the industrial control system must respond quickly and predictably to changing real-world conditions otherwise, disaster may result. An RTOS must function without buffering, processing latencies and other delays, which are perfectly acceptable in other types of operating systems. Two examples of RTOSes include Free RTOS and VxWorks.

The differences between operating system types are not absolute, and some operating systems can share characteristics of others. For example, general purpose operating systems routinely include the networking capabilities found in a traditional NOS. Similarly, an embedded operating system commonly includes attributes of an RTOS, while a mobile operating system can still typically run numerous apps simultaneously like other general purpose operating systems.",12944
"You can enable Virtual Machine in VMware following different methods. You can get it done manually with the use of the VMware Workstation Program. You can also consider installing remote access software while enabling it manually. If you require access just within the local network, and also you do not desire to have access to the host machine all the time, it becomes a lot easier to get the remote desktop connections enabled.

You can enable remote desktop connections through the operating system and also within the VMware network settings regularly as per your convenience. At first, you need to get the port set up done in VMware.

It will help the program know about what needs to be done when the request is sent through remote desktop applications.

Configuring VMware For Remote Desktop Connections

At first, the VMware needs to be configured so that the RDP requests can be forwarded to the IP address of the virtual machine. The steps mentioned below are required to be followed for successful configuration. Take a look:

Also, read Managed Citrix VDI Desktop for virtualization solution.

Step 1:

First, we need to go to the menu. From there, we need to click on the Settings button. After this, we need to go to the hardware tab and then select Network Adapter. From there, you need to define the connecting type by selecting NAT.

Step 2:

Now you need to go to the command prompt from your virtual machine. You need to then enter your IP configuration and look for the value that is following the IPv4 address — the record of the same needs to be kept for the later steps.

Step 3:

The next step is to go to the menu and then click on Edit. From there, you need to select Virtual Network Editor. After this, you need to choose the NAT network type and select NAT Settings.

Step 4:

You need to click on Add from the new prompt, and then the new port forwarder needs to be included. You need to fill in the information as:

Host Port: 9997 (It is an open port number. If you are not certain which number will be taken into usage, you can choose the one provided above. )

After this, you need to provide the Type as TCP and then enter the IP that was recorded in Step 2 in the Virtual machine IP address.

You need to provide a Virtual machine port number as 3389. This port number is by default and can be modified through registry editing.

If you need to, then save the open prompts. This will allow the changes in the configuration to take place.

Step 5:

It’s the final step where you need to get the RDP connections enabled from the operating system. The approach is different in all the Windows versions.

Windows 8.1: In this OS, you need to look for the one who can use the remote from the start menu. This will help trim down the results.

Windows 7: Here you go to Start Menu and then search for Remote Desktop. After this, you need to select users who have remote desktop applications.

Windows XP: Here, you need to right-click on the My Computer option from the start menu. After this, you need to move to the remote settings.

Get Connected To Virtual Machine With RDP

You can connect with the virtual machine just the way it is with all the other systems. All you need to do is to go to the Start menu and then open Mstsc. After this, you need to provide the computer name or IP address and then click on the Connect option. As soon as you provide your Login details, you will get connected!

Wrap Up

In this way, you will be able to enable remote desktop in VMware. Connecting with a computer is a lot easier, be it virtual or not, with the use of Windows Remote Desktop. So, get started with the steps now!",3650
"Photo by Michael Dziedzic on Unsplash

To define, algorithms are a set of instructions or commands that are introduced to a certain system to solve the given problem. A system that is introduced to an algorithm works according to it and can never go out of the way.

Why do we need algorithms?

We can never have too many resources to solve problems. We need a system, a set of instructions, predefined rules and values that helps us in fulfilling a purpose with less amount of hindrance. For example, if we take the example of computers, we can never have infinite memory and time to let them deal with problems one by one. If we have algorithms (set of rules and instruction) we can become free from the boundaries of memory and timespan. Because of algorithms, we don’t need to worry about the coming problem. Because we’ve got algorithms ready to deal with it. When humans confront a problem about which they already have some knowledge or know how to deal with it, they do a good job with that problem. You can also take vaccination as an example in this case.

Algorithms in different fields

We all have heard this same term ‘algorithm’ in many different fields. Let’s have a look at it.

In mathematics

I still remember that in school I studied algorithms in mathematics. In mathematics, it also follows the same definition. We apply ‘log’ to solve the given problem.

In computers

We are already aware that computers were initially known as computing devices, but later they are not just computing devices. Since initially computers were just meant for computing, algorithms (that are a necessary element in the process of computing) were used.

In websites and apps

Facebook and Instagram

Photo by Brett Jordan on Unsplash

If you have watched the documentary “The Social Dilemma”, you probably know how Facebook and Instagram use powerful tools and algorithms to study their user to make their user spend more time on their apps. They always keep track of your activities and store your activity log as information. Which they use to give you the ads and newsfeed that’s pleasurable for you.

Twitter

Twitter although is very different from Facebook and Instagram. It's comparatively authentic than Facebook or Instagram. It's because we have official accounts of politicians and the government. It is very much news-oriented, but it still uses users' information to make it more appealing. It shows you the trends that are related to your interests and location.

Google

Photo by Brett Jordan on Unsplash

If you do not have much information about a particular thing, what will you do? you will probably google it…. right? Why is it so? Obviously, it's because Google is the search engine that gives you the best information about a particular topic. It also uses algorithms to recommend the best-suited results related to your search. For example, if you always visit a particular website. You will see that whenever you search about anything Google tries to provide you with the information present on that website related to your search.

Whenever you search for anything on Google, it provides you with the best suitable search recommendation based on your previous searches.

Photo by Szabo Viktor on Unsplash

YouTube

YouTube is a video-sharing website. But now it has also become a search engine. It has left google behind in the field of search engines. Because it’s so convenient to look at a video related to a certain thing that you are looking for rather than reading articles, especially in the modern era of smartphones.

Every Youtuber, either famous or not. Says like share and comment on my videos. Why do they do this? It’s just because they want to increase engagement on their videos and channel ultimately. Which will recommend their videos to more viewers, because of “algorithms.”

Recently, YouTube has introduced tabs that are exclusively based on your interests. It always keeps you recommending videos and you cannot deny the fact that those are actually the best recommendations for you. Our apps in the modern-day know ourselves better than us. One of a friend used to say about the YouTube it can make the giraffe-neck evolution in humans.

Conclusion

We have seen that algorithms are not bound to a singular subject, and how they are used in different fields in different ways. Maybe you find it knowledgeable, if yes make sure to give it a like. LOL (Hope you get the sarcasm)",4428
"A computer system, which may consist of multiple individual systems and components, designed to provide mission critical services must be able to perform consistently and in a timely manner under a variety of operating conditions.

It must be able to meet its goals and objectives whether it is in normal operation or in a state of strife or in a hostile setting. A discussion about survivable computer systems can be very complex and wide-ranging. However, in this article, we will only cover the fundamentals.

Computer Security And Survivable Computer Systems

Survivable computer systems and computer security are related in many ways, but they are very different on a fundamental level. For example, hardening a specific system to be resistant to intelligent attacks could be a component of a survivable computer system.

It does not address a computer system’s ability to fulfill its purpose when it is impacted by an event such as a deliberate attack, natural disaster, accident, or widespread failure.

A survivable computer system must be able to adapt and perform its primary critical functions even in a hostile environment, even if various computer system components are incapacitated. Even if the entire “primary” system has been destroyed in some cases.

As an example, due to widespread communication failure, a system designed to provide real-time critical information regarding the analysis of specialized medications ceases to function for a few hours. It does, however, maintain the data’s validity when communication is restored and systems are brought back online. This computer system may be considered to have survived in circumstances beyond its control.

However, if the same system fails to provide continuous access to information under normal circumstances or operating conditions due to a localized failure, it may not be considered to have fulfilled its purpose or met its objective.

Computer Systems That Are Fault Tolerant And High Availability

Many computer systems are built with fault-tolerant components to ensure that they continue to function even when critical components fail. Multiple power supplies, redundant disk drives or arrays, and even multiple processors and system boards that can continue to function even if their peer component is destroyed or fails are examples of redundant components.

The likelihood of all redundant components failing at the same time is quite low. A malicious entity that knows how the redundant components are configured, on the other hand, may be able to engineer critical failures throughout the system, rendering the fault tolerant components ineffective.

Many computer systems are built with fault-tolerant components to ensure that they continue to function even when critical components fail. Multiple power supplies, redundant disk drives or arrays, and even multiple processors and system boards that can continue to function even if their peer component is destroyed or fails are examples of redundant components.

The likelihood of all redundant components failing at the same time is quite low. A malicious entity that knows how the redundant components are configured, on the other hand, may be able to engineer critical failures throughout the system, rendering the fault tolerant components ineffective.

A survivable computer system also has high availability. This design component, however, may not ensure computer system survival during certain events, such as various types of malicious attack.

A critical web service that has been duplicated, say, across multiple machines, to allow continuous functionality if one or more of the individual web servers fails is an example of this. The issue is that many high availability implementations use the same components and methodology on all of the individual systems.

If an intelligent attack or malicious event occurs that targets a specific set of vulnerabilities on one of the individual systems, it is reasonable to assume that the remaining computer systems participating in the highly available implementation are also vulnerable to the same or similar vulnerabilities. A certain degree of variation in how all systems participate in the highly available implementation must be achieved.

Click to continue reading.",4268
"WinRAR 6.0 Final + Crack Free Download

Winrar Crack is a powerful file archive extractor and file archive maker for both 32BIt and 64BIT architecture. whit this program you can easily create any type of archives such as ZIP and RAR and compress your file as much as is possible. you also be able to works with large files up to 8000 billion gigabytes to compress or extract with ease.

One of the essential features of WinRar software is the Self-Extract feature. It allows you to convert a compressed file into an executable file with an EXE extension so that you can use it on computers that do not have the WinRar program installed. Unzip them.

Winrar Crack can encrypt your files during compressing with a great 128BIT AES encryption algorithm to make it harder for hackers, crackers, to access your important file and crack them that may be caused to risk your security but don’t worry with that encryption algorithm you make sure that your files hold with a bulletproof encryption algorithm. Winrar Key also is available for other operating systems with the same abilities. below you can found more features of Winrar Crack.

WinRAR Crack Features:

– Tiny size compared to many capabilities

– Numerous and stunning skins in different sizes

– Support for additional languages ​​such as sweet Persian language

– Password on the file and the ability to compress and fast and secure files

– Ability to create and Implementing the well-known compressed formats RAR and ZIP

– Possibility of inserting passwords when compressing files

– Quickly extract files from compressed mode

– Rrecover and repair compressed files

– Create compressed files in EXE format

– Ability to repair files

– Scan capability Antivirus to detect malicious files

– Wizard to make it easier to create compressed files and also extract them

– Use the latest methods to reduce the volume and quality of compression

– Improve compression speed for multi-processor and multi-core systems

– Add the option to delete duplicate folders in program settings

– Compatibility with various versions of Windows including Windows 10, 7,8.

For Further Information You May Also Visit Here…

https://pcserialkeys.com/",2193
"My passion for learning the Computer technology grew when i was in 6th grade and started to learn computers in school, i was so intrigued by it and i remember the feeling when my parents bought me and my brothers our first PC to share i was in 7th grade, and then after i learned about the importance that the Computer technology holds, my dad sent me to get classes for Computers because it was new and i wanted to learn more, even though i was learning it in school, i took extra classes to learn more of it like learning Microsoft and basics of Command Line Interface, terminal , how to create directories and files. It was basic but i still remember it and how excited i was.

When i went to College i decided to take Computers major because that’s what i had always been interested in, and there i learned the programming languages like C, C++, Java, learning Operating systems, Computer Graphics, Digitals, Information technology, Computer Networking, ASP.net, databases like Oracle learned how to use MySQL, HTML and much more and that’s when i graduated with degree in Bachelor’s of Computer Applications in 2016 with University Merit from a Accredited University in India.

Just one month after i gave my last exam, i moved to United States in 2016 . I didn’t get to attend my Graduation Ceremony. When i moved here and wanted to continue my education so i decided to study Computer Networking i studied for the CCENT exam and passed it and got the Certification from CISCO. It didn’t end there, i started applying for Jobs, i had no luck getting even the interview, every job i had applied to wanted a job experience, which i had none since i moved here right after graduating and i didn’t have a job it was discouraging and i felt like i had to start over.

Since i was new here with no network of friends or people i know , i had to find a job to start somewhere when this didn’t work for me, i worked at Customer Service at Target as my first ever job when i was 21 and then fast forward i got the Job at a Bank and i have been in Banking for past 3 years worked for 2 different banks, which is an experience i am glad i have now because i learned a lot, it built my confidence in speaking with Clients and helping them and providing them the best service, i liked helping people but my dream of becoming a Software Engineer always stayed with me, i couldn’t ever stop thinking about it, every time i questioned my career choice.

i kept myself in touch with learning through Youtube or other educational websites until one day i came across Flatiron School, at first i didn’t know what to expect and i signed up for it and here i am today, super thrilled and excited to start this journey!! and can’t wait to build projects and learn as i go. This is i believe one of my best decisions in life i have made for myself to finally have the career i love and passionate about and

i will continue to share my journey and share the learnings through my blog. Stay tuned! :)",2982
"In this guide, I will be touching on what to look out for when searching for a student laptop. Personally, I’m a student and I know how it feels bad when you really can’t afford a gaming laptop as they are quite expensive at times. I will be making this guide short and clear as no one would like a lengthy guide. Without further ado, let's have a look at what to look out for when purchasing a student laptop that has the best bang for buck value:-

1)Price

First things let's talk about the price range. A typical student would be very tight on budget. As I’m based in Malaysia, all the prices that I will be stating would be in MYR (Malaysian ringgit). As a student in Malaysia, you want to look for something that’s priced around RM2000 to RM3000 for a mediocre household. This price range would get you a fairly decent laptop for a student but don’t expect too much performance from these machines in terms of gaming. If you are even tighter on budget, I would genuinely recommend the lowest price of RM1000 to RM1500. Now, do note that laptops from this price range will only allow you to do your daily work although it might be game possible but I genuinely don’t recommend to game on a laptop at this price point as it will ruin the overall experience of the game. In my other blogs I would be recommending my personal options for student laptops in these price range.

2) Weight

A student would clearly want a fairly lightweight laptop as you would be carrying it around wherever you go during projects and meet-ups with your project members. Try to get a laptop that is within 1.5 to 2.3 kg as anything more than that would be kind of a burden on the shoulder.

3) Battery Life

Generally, I would recommend searching up for laptops that have at least 4 hrs of battery life. Anything less than that would be kind of lacking and you would have to carry your charger wherever you go. The better battery life I recommend would be around 6 to 7 hrs as this will be sufficient for the time when you are usually outside as a student. A quick note though, try not to trust the battery life stated on the producers websites as they are not as accurate as doing a benchmark testing by youtubers themselves. So before buying, try to look out and do some research on the laptop you are getting before buying.

4) Monitor

If you are a normal student, any display literally should be fine but if you are a student that is pursuing a career in arts than the monitors comes into some serious plays. You would want something that is more colour accurate, vibrant and has better rating percentages in sRGB as well as Nits. These 2 things will clearly state how colour accurate your monitor actually is, but for the most of us just get the display that is bright enough for viewing outside and has a fairly decent viewing angle and you should be good to go. Another thing about the monitor that will effect your weight is the display size. Do you prefer 14″, 15.6″ or even bigger? Generally, a bigger screen would mean more weight to your computer so keep in mind of that but a bigger screen also means that it is easier to read texts on the screen.

5) Design and Layout

On the design part as usual can’t say much as everyone has their own preference. To me, you would want to look for a professional-looking chassis. Like smooth edges with clean corners that sort of thing for a better presentation during work. Now let's talk about the keyboard design, since we will be doing a lot of typing like thesis projects, we would want a fairly spacious keyboard for a better typing experience. You would also want to consider getting a backlit keyboard design as it will help with working long hours in your dome when your dome-mates are asleep and the lights are off so that you can see the keyboard. Talking about sleeping dome-mates, try getting a laptop that has relatively quiet fan noise for less distraction during class or literally anywhere you go.

6) Processor and GPU (Graphics Processing Unit) Selection // Upgrade Abilities

For processors, we would want something that is powerful enough for daily multitasking and some light gaming at the end of the day. Generally, I would recommend Ryzen processors as their integrated GPU’s are generally better than Intel’s one due to their APU (Accelerating Processing Unit) which takes advantage of your dual-channel RAM memory to provide better frames for some light gaming while consuming less power than a dedicated GPU. Besides the new Ryzen 5000 mobile processor series’s Integrated Graphics also outperforms entry-level dedicated graphics such as MX Nvidia series cards. On the other hand, do lookout for a laptop that has a fair amount of upgradeability such as the RAM slots and the SSD slots. These upgrades will be useful in the future when you need extra RAM or even more storage to allocate your files.

That’s all I have for you in this blog, hope it helps clear up your mind on selecting a laptop for your student routine, and good luck with your purchase! Do stay tune for my next blog where I will be recommending my personal laptop choices for students as well as for gaming. Did I miss out on anything? Did I make a mistake? Do give feedback as I will be more than happy to correct my mistakes, thanks!",5267
"The New Graphics Cards From Nvidia || RTX 3080Ti and RTX 3070Ti Tehnologijaviews Jun 1·2 min read

Nvidia has presented at the Computex 2021 computer fair in Taipei (Taiwan) its new flagship in its catalog of high-end graphics cards for gaming, GeForce RTX 3080 Ti, 50 percent faster than the generation above, which comes together with the new RTX 3070 Ti model.

Nvidia’s new RTX 30 Ti graphics cards are aimed at current video games that have raised the realism and demands for graphics cards, such as Cyberpunk 2077 and Watch Dogs: Legion, as reported by Nvidia in a statement.

RTX 3080 Ti is powered by Nvidia’s Ampere graphics architecture , which improves performance with features such as ray tracing, and also includes Nvidia DLSS , Nvidia’s AI supersampling technology that improves gaming graphics and shrink functionality of latency.

The company’s new flagship graphics improves rendering speed up to double on traditional tasks from the two-generation GTX 1080 Ti model, but the increase is even greater on modern aspects such as ray tracing.

Nvidia has also introduced the new GeForce RTX 3070 Ti model, a graphics that improves up to 50 percent the performance of the previous generation (RTX 2070 SUPER), while the number of frames per second is double compared to the cards of two ago generations (GTX 1070 Ti).

Nvidia’s new RTX 3080 Ti will be available globally from June 3 for a price of $ 1,199 (€ 980), while the RTX 3070 Ti model will be sold from June 10 for $ 599 (€ 490).

Do You Know What I Have Posted on",1536
"Change has been the only constant throughout our existence. As world gets more and more digitalised, everything that works in a physical world should also have a digital alternative. In simple words, Non-fungible tokens are certificates for the ownership of a digital asset without being aligned to any particular application. Fungibility implies the trait of any entity to not lose its value when divided into smaller entities. For example- two 50rs have the same value as a 100rs note but when a diamond is broken into two, it will not have the same monetary value. So, diamond becomes a non-fungible asset.

NFTs have been around for a few years now but they have recently gained some popularity due to the trading of digital artwork and it’s adoption by the gaming industry. The programming behind NFTs is similar to cryptocurrency but everything else is different. Each NFT possesses a unique digital signature. Hence, it can’t be exchanged for another as opposed to any cryptocurrency.

NFTs can be created for both tangible and non-tangible items. You can create a NFT for an an artwork you possess or the avatar you use to play in a video game or an image, video or music of yours. A prominent example of a NFT sale can be the first tweet of Twitter’s co-founder Jack dorsey which was sold for a whopping 2.9 million dollars.

NFT ownership is like owning an exclusive property which has a sole owner. Any individual NFT can only have one owner at a time which makes it easily tradable using blockchain technology.

Benefits of NFT-

NFTs enable people to be a collector of unique items digitally and also able to trade them without relying on any third party like auction galleries. An artist can sell their art and also decide their royalty. In the physical world, artists are often deprived of the royalty from the further sale of their work whereas NFT ensure to secure all royalties for them.

If you are an influencer or celebrity, you can release your content, as a unique piece of art. It is an interesting way for creators to monetize their content and it also provides their fans to own something related to the their favourite celebrity and claim ownership to it.

Everything in the digital world can easily be copied. Although some things are restricted by copyright laws, still it lacks the feeling of ownership. When you own an original painting in the real world, it has tremendous value compared to their copies. NFT makes this possible in the digital world for any item. You can sell or buy special moments, posts, images, videos etc of your choice using NFTs.

Future of NFTs-

As for any new product, future of NFT is still blurry yet with history as witness the most likely prospect for NFTs is only to grow far and beyond from its present state. Although value of NFT largely depends on what the other person is willing to pay but same is the case with any property.

Like any investment, there is also a downside if it does not have the same resale value or you struggle to find a buyer. Overall as for all things on the internet, it is highly unlikely to not get a buyer if you hold something of value. If you are personally attached to an artist, the property you hold also has a personal value to it as you are the sole owner of something related to them.

There are open marketplaces like Rarible or Opensea where you can buy and sell NFTs. So, come and explore. The opportunity to be a collector of the new era awaits you.",3457
"It’s no surprise that video conferencing platforms gained popularity during the pandemic. While COVID-19 changed the mode of living for many of us, the pace of life hasn’t really slowed down. Whether you are meeting a new client, holding a weekly check-in, or even adding new hires at your nonprofit, video conferencing likely helped you complete these tasks.

This post will touch on the psychology behind video conferencing and how we can better connect with our colleagues in a distant virtual space. We had the pleasure of speaking with Susan Comfort, who is the founder of Comfort Consulting. She also co-directs NonprofitWellness.org and believes we need to talk as much about “team-care” as about self-care. Comfort’s expertise spans wellness, the nervous system, and the digital workplace. Her interest in the brain science of taking breaks was spurred by leading Playworks DC, and 27 years overall in the nonprofit sector. Her background is in organizational development and wellness for team building.

The beauty of video conferencing is that it gives us the ability to connect with anyone, at any time, and any place. But if you’d like to replicate the feeling of an in-person workplace community online, read on. While we cannot make the pandemic go away with the snap of a finger, we can certainly provide some helpful tips for your next conference call.

The Brain Science Behind Video Conferencing

Understanding our central nervous system gives insight into our video conferencing experience. According to Comfort, when we are in a meeting, our brain feels that what we are experiencing is a regular conversation, but our nervous system is telling us that it is not. As a result, we may not feel safe or comfortable in that scenario.

Comfort tells us that the vagus nerve, which extends throughout the torso and feeds instant messages to the amygdala (our lizard brain), helps us judge “safety” or “danger.” However, we don’t get enough context clues during video conferencing to truly feel safe. Additionally, we have experiences, like seeing ourselves on camera, that are uncommon in live interactions. These can add to that subtle yet nagging feeling of discomfort. To summarize this concept, Comfort quotes tweets from Gianpiero Petriglieri, a psychiatrist who researches leadership development:

“I spoke to an old therapist friend today, and finally understood why everyone’s so exhausted after the video calls. It’s the plausible deniability of each other’s absence. Our minds are tricked into the idea of being together when our bodies feel we’re not. Dissonance is exhausting. …”

“It’s easier being in each other’s presence, or in each other’s absence, than in the constant presence of each other’s absence. …”

“Our bodies process so much context, so much information, in encounters, that meeting on video is being a weird kind of blindfolded. We sense too little and can’t imagine enough. That single deprivation requires a lot of conscious effort.”

Ultimately, our experience during a video call is impacted by our visual experience. Other mediums — phone calls for example — don’t trick our brains into thinking we will get sensory information that is not available. While we cannot solve the fundamental issue of our brains being tricked, we can aim to make our brains feel safer.

Strategies for Building Deeper Trust on a Video Call

Comfort’s first recommendation for fostering safety on a video call is to talk about wellness. Incorporating a wellness practice into a meeting might look like starting with a question that gives insight into your coworkers’ daily lives. For your team, it might be helpful to take 15 to 30 seconds to ask questions like what do you do to bolster your mental health? What’s one vegetable you cooked or ate over the weekend? How do you ensure a good night’s sleep? Most people divulge only what they feel is safe to share with the group, yet there is inherent vulnerability in talking about wellness.

Asking these types of questions focused on physical or mental health can enable your team to experience a deeper connection with one another, and as a result, build trust and empathy. These moments of interconnectedness are what teams need most, and they can help minimize the cognitive dissonance sometimes experienced on a video call.

Another helpful strategy is to incorporate a 15-second stretch or movement activity into the meeting. This stretch is a great refresher and forces people to be fully present and attuned to their senses. It’s also a great way to create cohesion and connectedness as each team member is partaking in an activity aside from work and collectively destressing. By including movement into an otherwise sedentary meeting, your team may also experience another layer of vulnerability as you practice an activity that may not be a part of normal meeting convention.

These strategies tie into gauging social cues online. It might be helpful to revisit your online group agreements. Your team’s list might include solutions for common video calling challenges like sitting a lot, a lack of group discussion, a lack of personal connection, and how to build in more and better time for breaks.

Additionally, consider alternative ways to take an already scheduled meeting or to diversify ways to experience a meeting. For example, you might call in to a meeting (when appropriate) instead of being on camera, or even participate while you’re taking a walk. You could also decide, as a group, to alternate the time you spend with your camera on and off. As Comfort points out, if the main reason to have cameras on is to ensure that workers are present, or paying attention, that might indicate a lack of trust. If allowing cameras off makes some people feel safer and more trusted, it’s worth trying.

These activities create opportunities to integrate wellness into your daily work life beyond breaks. Diversifying your practices and revisiting group agreements encourage a culture of mental preparation, resilience, and restoration for your team.

Self-Care vs. Team-Care

The era of video conferencing offers new opportunities to take care of ourselves and each other in intentional ways. One way to bolster your team’s wellness is to expand beyond self-care by focusing on team-care.

A key distinction between team-care and self-care is that the responsibility to take care of one’s mental and emotional state is not completely placed on the individual. Comfort points out that in the nonprofit world, “we are not so good at self-care.” Therefore, as a team, there can be a sense of accountability for one another. We can focus on the fundamental idea that we need to take care of each other as we share our lives in the workplace. Team-care is fueled by empathy, which is essential in any workplace and vital for an individual’s ability to be resilient and do well at their job.

According to Comfort, team-care speaks to the importance of wellness in the nonprofit workplace, where staff are oftentimes used to being “givers.” Since helping others comes more naturally than self-care, this means a culture of caring for others (team-care) can be easily nurtured. However, health solutions aren’t one-size-fits-all. Each individual has unique wellness needs (not to mention different norms about sharing intimate bodily details). Ultimately, it’s also up to leaders in an organization to model the shift to a wellness-focused mindset.

Understanding more about how our nervous system functions can help us build connections and trust in a world with COVID. While remote work has brought new challenges, it also presents an opportunity to build new practices and habits that deviate from workplace norms that we have had for years. This may very well create new and better workplace cultures that help employees manage stress and build deeper trust with one another in the years ahead.

Additional Resources

Top photo: Shutterstock",7933
"Use technology the right way to save lost time and reduce mental stress.

Hi everyone, in my last post I talked about how it is important to have a goal-oriented mindset to be able to make good use of your time and become focused and productive. If you didn't already read it, check it out here.

Are you motivated to go ahead and crush those goals? Yes? Great!

But achieving the flow state is very difficult with the sheer amount of noise and distractions we have in our lives. We are also overwhelmed with so much information, that we are not able to remember it when the time actually comes.

It is a quite well-known belief that if you are able to automate the boring or annoying parts of your life, you will be able to have more time for the core stuff and also the fun stuff.

To give your mind peace, it's important to cut out that noise and make your own life easier so that you can put all your attention to the task or goal that is most important to you. And lucky for us, technology today is at its peak to allow us to do that.

Here are some tools I live by that help me. Even if you don’t like these specific tools, there are many that have similar features, but the problem is the same, so find one that works for you and is able to make your life easier.

Notion

Notion is one of the most talked-about apps in the tech space right now, and for a valid reason. What started out as a new way for document collaboration has evolved into a tool that basically can manage your life.

You can use this app and its beautiful interface and UX to create pages/boards/timelines/to-do lists and so many more things and you can organize them all in one space.

I like to think of it as the power of Microsoft Office combined with the convenience and accessibility of Google Suite, sprinkled with the design and UX of Apple.

Taken from the Notion website

I personally use it for many different things. I even wrote the draft of this article on Notion.

Here are some of them

Taking notes for whatever I am studying or learning at the moment and organizing them into easily accessible pages

Planning and drafting my writing (I actually wrote this on Notion before I published it here)

Using it for budget planning, investments, and subscription tracking

Long and short term goal planning as well as journalling

Travel planner

Creating shopping lists

Personal website — yes I actually made my personal website using Notion and another cool app called super.so .

Subtle plug to check it out — www.tanmaygoel.com

And finally, I also use it for its original purpose, for collaborating with my team on side projects like Encore and mymizu

It can be used for anything you want to use it for, the possibilities are endless.

I think one of the best things about it is that I have everything I need within one app synced across all my devices. Their templates make it easy to stop wasting time on figuring out the style of the document and actually start doing it.

What’s more? It’s totally FREE and always being updated with new features, so definitely check it out!

Taken from the Notion website

https://www.notion.so/product

Todoist

If Notion is more for long-term, broader planning, then Todoist is my daily planner.

Todoist is not a planner, it’s the map of my day. I treat it like a communication device to my future self.

Many times during the day, I will randomly find myself remembering something I have to do, which could be something as simple as buying new toothpaste in a few days, or a complicated set of tasks that I have to do for my work or side projects.

Todoist’s quick add feature is perfect. I don’t need to even waste any time to set due dates or recurring dates, I naturally write it, and it will create all the settings in the background.

I just write all the tasks I need to do as a reminder for my future self, and my mind stays relaxed knowing that my past self has planned out my day for me already, and I don’t have to give any mind space to my pending tasks.

Additionally, I can also prioritize, tag, and organize them so that I am visually able to distinguish which ones are important to me, and which ones can be done later. Being a Product Manager, this makes me feel right at home.

Taken from the Todoist website

Whenever I am unsure what to do, I just open my Todoist and it always has some stuff that I could be doing that was written to me by my past self.

I also use it as a habit tracker, by putting in a daily recurring to-do, and I am reminded of these habits every day.

And again it is on all platforms, so I can view them from whatever device I want.

https://todoist.com/

Hey for Email

Hey was created by people frustrated by how little improvement there has been to email since forever.

We have so many emails coming our way every day and most of them are mixed in together. All the emails you actually want to read are mixed in with receipts and spam. Maybe important emails are auto-filed in a non-important folder or vice versa. You could create some workflow to fix that, but the makers of Hey took a big stand and decided to do it for you by re-thinking the whole experience from scratch.

Taken from the Hey website

Some of the features I really like about it -

Every time someone new sends you an email, Hey lets you decide whether it is important to you or not. If you say no, you will never see that sender again.

After that, you can decide where these emails belong. If they are important and require your attention they go to the Inbox if they are just some transactional emails that you need to have, but will never look at, they go into the Paper Trail, and if they are newsletters, they go into this beautiful feed where you can read them like a social media feed.

It has a ‘Reply Later’ bucket where you can put all the emails you want to reply to later (so you don't forget them), and you can reply to all of them in one go when you have the time and mind space.

You can also ‘Set Aside’ some important emails like travel bookings, meeting emails, tickets, etc that you might need to quickly access whenever needed.

Although it comes with a price tag of a 100$ per year, you get a 14 day trial period to check it out without putting in any credit card information. I forward email from all my multiple accounts into my Hey email and use it as a common source of all my email.

I really love their mission and bluntness, and now I cannot imagine email in any other way. I have not opened any of my Gmail accounts for months since I started using it.

https://hey.com

Lastpass (or any password manager)

This has seriously been a LIFESAVER. If you are like me and cannot remember passwords to save your life, you need to get a Password Manager.

If you have an iPhone, you can directly use Keychain, Google also has its dedicated password manager (synced across google chrome), however, if you are using multiple devices across multiple platforms, it might be a good idea to have it all in one roof.

Password Managers safely store all of your login information for different websites and apps and if you also download the chrome extension, it will also auto-fill it when you access the website. You can also simply use them to view your passwords and input them whenever needed.

Taken from the LastPass website

I use LastPass simply because they are a company built for this purpose, so I trust them to provide the best service and features for remembering passwords and other details.

Gone are the days when I have to go through ‘Forgot Password’ flows, or I get locked out of my account for inputting the wrong password multiple times. Also now I never repeat passwords, and I am able to input stronger passwords for all my accounts.

Also if you don’t already, whenever you get a chance, opt-in for 2FA. Trust me, it could make a huge difference to your privacy!

https://www.lastpass.com/how-lastpass-works

Alexa (or Siri, or Google Assistant)

I am an IoT NERD.

If I could, I would automate everything I own. (Maybe someday). But for now, I make my life easier by using whatever I have at my disposal.

First of all, I would recommend buying an Amazon Echo. It’s a really great investment that will slowly integrate into your life, in a very positive and productive way. Most importantly, it reduces my smartphone usage and always blows my mind how far technology has come that we are able to control things using our voices!

Here are some interesting ways I do that.

I use Alexa to control my music, and they also double up as good-quality speakers.

All my lights are Alexa enabled. You don’t need to buy expensive Phillips Hue lights, you can buy very cheap 10$ lights that work as well.

For example, I set different lighting combinations for different times of the day. If I say “Alexa, night lights’, all my lights will turn off, and my lamp turns on at the dimmest ambient colour to prepare me for my sleep.

I use Alexa for alarms, reminders, current time, weather, the status of my amazon deliveries and even to read out my calendar, all without the need to touch my phone at all, it does not even distract me from what I’m doing and keeps me in my flow state.

For example, creating a morning routine with Alexa, first, it rings the alarm, then tells me my calendar for the day, followed by the weather, and then plays my morning playlist from Spotify. By the time the songs start coming on, I am pretty much awake and ready for the day without even looking at my phone.

The Skills store has an enormous amount of different companies making apps that you can use for Alexa. So definitely experiment with it. It could help with meditation, learning languages, or even play your news or audiobooks.

Also, this recent ad for Alexa made me lol!

But the main takeaway from this is to use your voice assistants to the fullest. They can easily save you multiple steps on your phones or devices and reduce your screen usage.

You can buy the Echo on Amazon.",9965
"Innovation

hbr.org

HT @fgilbane Most companies don’t have a clear path to success. But that might change with a new, data-centric approach to AI. By @AndrewYNg #AI

Some K-pop stars are bypassing Twitter and Facebook to create platforms for fans, like Universe, which features AI-generated voice calls with the idols — www.theverge.com

Korea’s fancafes have given rise to new social platforms like LYSN, Weverse, and Universe, which serve as self-contained versions of Facebook or Twitter entirely for K-pop fans. #AI

Technology

Facebook Wants Us to Live in the Metaverse — www.newyorker.com

What does that even mean?

stratechery.com

Interesting Take on Metaverses via @stratechery #metaverse

Gaming braces for its Netflix and Spotify moment — www.protocol.com

A new wave of game subscriptions has only just begun emerging in the last few years. Now, as these subscriptions are married to nascent but fast-growing cloud gaming services that allow you to stream games to almost any screen, the industry is bracing itself for a potential paradigm shift akin to what happened to television, film and music.

mitsloan.mit.edu

Having built new norms during COVID-19, firms should now focus on operations, employee experience, customer experience, and organizational culture. Via @MITSloan

techcrunch.com

Twitter may have shut down its Stories features known as Fleets, but the Stories format will continue to invade other social platforms. TikTok today confirmed it’s piloting a new feature, TikTok Stories, which will allow it to explore additional ways for its community to bring their creative ideas to life.

Apple wins patent for in-screen Touch ID and Face ID — 9to5mac.com

#Apple #patents

www.adexchanger.com

#Amazon

How Amazon plans to cut waste after backlash over destruction of unused items — apple.news

Amazon has launched two programs as part of an effort to give products a second life when they get returned to businesses that sell items on its platform.

Amazon will pay you $10 in credit for your palm print biometrics — techcrunch.com

The retail giant has a spotty history with biometric data.

libn.com

The online shopping giant is pushing landlords to give its drivers the ability to unlock apartment-building doors themselves with a mobile device.

www.axios.com

A 9/11 documentary, “The Outsider,” will debut on Facebook Live for $3.99 on Aug. 19. #Facebook

www.bloomberg.com

Facebook Inc. has disabled the personal accounts of a group of New York University researchers studying political ads on the social network, claiming they are scraping data in violation of the company’s terms of service.

adage.com

RT @adage Alphabet Inc.’s Google was accused in an antitrust lawsuit of giving itself the edge in online advertising by cutting a cozy deal with Facebook that gives the social network an advantage in virtual auctions which determine whose ads appear where.

www.theregister.com

‘This is peak Chrome; a reasonably good idea hampered because it was pushed out thoughtlessly’

www.mediapost.com

HT @MediaPost

www.technologyreview.com

MT @michellemanafy Many TikTok users assumed that the text-to-speech voice they heard on the app wasn’t a real person. It was. A Canadian voice actor named Bev Standing had never given ByteDance, the company that owns TikTok, permission to use it. By @histoftech

Publishing & Media

publishingperspectives.com

A total 2,661 authors, illustrators, and translators question UK ‘reconsideration’ on post-Brexit copyright and parallel imports on books. #Copyright @Porter_Anderson @PublishersAssoc @Soc_of_Authors

www.bincfoundation.org

MT @BincFoundation Join us in celebrating Binc’s 25th Anniversary! Thursday, August 12, 2021, 8pm EST #ThinkBinc #Bincat25

www.publishersweekly.com

HT @BoSacks The International Publishers Association and PEN have documented hundreds of attacks on press freedom, many of them targeting LGBTQ and dissident groups.

Scribd Originals Issues Short “Semi-Autobiographical Tale” By Margaret Atwood — www.prnewswire.com

www.booknetcanada.ca

Pearson CEO predicts demise of physical textbooks as digital service launches — money.yahoo.com

The 177-year-old company debuted its Pearson+ subscription service in the U.S.

deadline.com

Private equity deals like the recent $900 million takeover of Hello Sunshine are poised to continue long into the future, dealmakers say.

www.niemanlab.org

“No matter the amount of tweeting or social media promotion that you might do, [the place] where you need to look to grow your audience is existing podcasts.” #podcasting

www.theverge.com

HBO Max is becoming more of a podcast app. This fall, it’ll be the exclusive home for Batman: The Audio Adventures, a scripted podcast starring Jeffrey Wright and Rosario Dawson.

www.adweek.com

The two companies have formed Condé Nast Certified Video Plus, providing advertisers access to both audiences through one offering.

www.adweek.com

This will be a year-long content-sharing collaboration to cross-pollinate the Bloomberg Equality and Ebony audiences through original content.

digiday.com

Good Housekeeping set a standard at Hearst UK that the rest of the portfolio wants to replicate.

wwd.com

wwd.com

In recent years, a number of media brands have been tapping into the beauty market. #magazines

apnews.com

Twitter signed a deal with The Associated Press and Reuters to help elevate accurate information on its platform. Twitter said the program will expand its existing work to help explain why certain subjects are trending on the site, to show information and news from trusted resources and to debunk misinformation.

www.reuters.com

AT&T Inc’s satellite television provider, DirecTV, will become a standalone video business as part of a deal between the wireless service provider and buyout firm TPG Capital.

www.nytimes.com

The company expects to end the year with about 8.5 million. In its quarterly results, it reported holding nearly $1 billion in cash.

variety.com

News Corp will pay $1.15 billion in cash for Oil Price Information Service (OPIS), a specialized, digital-centric info provider.

www.hollywoodreporter.com

HT @LWShanley @publishingtrend

LinkedIn Acquires Tutorial Video App Jumprope as it Looks to Expand its Creator Tools — www.socialmediatoday.com

Revel Acquires The Woolfer, the Leading Social Platform for Women Over 40; Announces $3.5M Seed Round — www.prnewswire.com

Resources & Opportunities

Scholarships — Hugo House — hugohouse.org

MT @diversebooks Writers: @HugoHouse is offering writing classes both in-person and online, along with scholarships for their Fall Quarter courses! Applications close on August 16th:.

New Visions Writing Contest for Writers of Color and Indigenous/Native Writers — www.leeandlow.com

RT @diversebooks Unpublished BIPOC writers: @LEEandLOW has extended the deadlines for both its annual writing contests to August 15th! Winners receive $2000 + a standard publication contract.

New Visions (MG/YA): http://ow.ly/kGM450FKw62

New Voices (Picture Books): http://ow.ly/B5Cw50FKw60

help.medium.com

RT @rgay Medium is hosting a writing challenge. Details here: https://help.medium.com/hc/en-us/articles/4405083394455-Medium-Writers-Challenge-Contest-Official-Rules… And there are some FAQs about your intellectual property and the rights Medium is seeking.

publishingperspectives.com

HT @elizabethscraig The new award, with its inaugural presentation set for next summer, is open to submissions both in fiction and nonfiction. @pubperspectives #wkb84

Join us for Penguin Teen Summer Festival! — Penguin Teen — www.penguinteen.com

#teamPRH

www.nyfa.org

RT @MadeinNY We invite NYC storytellers to apply for #NYCWomensFund for the opportunity to bring their films, docs, TV shows, music projects, and theater productions by, for, or about women to life on screen and stage. Apply now at @nyfacurrent: http://nyfa.org/NYCWomensFund.

www.cbsnews.com

MT @MarkLevineNYC Very cool: @Yelp now let’s you filter your search for restaurants, stores etc by whether all its staff are fully vaccinated, and whether they screen for vaccination.

www.microcovid.org

MT @Pistachio I’m planning to __________ but now I wonder if it’s safe given how Delta COVID spreads. How can i calculate the risks?",8282
"A look at the Bimble platform

UK: ShortTermRentalz speaks to Francesca Howland, co-founder of Bimble, a London-based app and platform which aims to provide an easy way to collect, remember and share all the places you love, to discuss her business, standing out in the guide market, consumer trust and how travel can come back from the global pandemic.

Please introduce yourself, Bimble and the role the app is playing in the short-term rental and wider travel industry.

“Of course, my name is Francesca Howland and I’m co-founder of Bimble. Bimble is the app for places. It’s like Spotify for placelists. It’s a free travel and lifestyle app for collecting and exchanging lists of great little places to go.

“With Bimble, anybody can collect and save the places they love, or would like to visit in the future, anywhere in the world — but it is also used by hosts and letting managers to create area guides for their properties to share with guests.

“People talk about places all the time, they read about them in articles on and off-line, they share them on social media — but there was no convenient way to collect and store that information before we launched Bimble, so people were resorting to screenshots and iOS notes in an effort to keep track. And scattered information is less likely to turn into a concrete plan.

“When a host makes a Bimble area guide and attaches it to their property listing, it gets people’s attention and attracts guests who can move straight from browsing into planning, by making their own Bimble lists — adding the B&B to their collection, along with places to eat, shop, walk and visit.”

How is it different from making area guides in other ways?

“Firstly, other systems offer a traditional one-way process: the host or manager creates an area guide for guests to view in the property.

“Bimble is digital and interactive

As a host or manager, you can easily create a Bimble guide that can be shared across all platforms. It’s beautifully designed.

Bimble offers an audience of its own — people collecting ideas of places to visit, so it is, in itself, a marketing tool for hosts. We see hundreds of guests clicking to and from individual property Bimbles every month.

Because Bimble guest guides are interactive, your guests can create their own lists of places to go in your area by picking some of your recommendations and adding some that they have discovered elsewhere. They can then add comments and share their list with their own friends, spreading the word about your property and your neighbourhood.

“Next, the information that is shared in other systems becomes out-dated. Whereas Bimble pulls information from the internet and stays up-to-date and useful for guests, giving you peace of mind. Guests can access your Bimbles when they are out and about and use the offline feature when connectivity is a problem.

“Finally, Bimble gives guests a much richer experience. When you list the places you recommend, your audience can see opening times for the places, see them on a map, navigate to them, collect them into their own lists and share them with friends.

“Bimble is the app for places, so it is the most natural and logical way for you to share local information. And, it’s free to use!”

How would you say your business is being affected by the Covid-19 pandemic?

“At first, we were worried. We had just launched our app in February, so we were banking on people using Bimble to prepare for the summer season and suddenly that seemed very problematic. But actually, within days of lockdown, we started to see usage growing ahead of expectations.

“Clearly, the industry is going through a really tough patch right now — but there are good reasons to feel optimistic. We have seen thousands of people on Bimble, collecting ideas of places to visit once lockdown is over. In a recent study, we found that 24% of adults were spending time thinking about future holidays during isolation. In particular, we can see that millennials are impatient to get out and about in the world. When we polled our community on Bimble, who are predominantly millennials, 59% of them said they would be travelling again as soon as they could.”

What was the inspiration for launching Bimble in the first place?

“We love exploring neighbourhoods, going out, travelling and finding the places that locals go to. Great little everyday places are where life happens and memories are made. There’s information about places everywhere: in social media, blogs, guidebooks, conversions with friends — but there was nowhere to collect and store it. There just wasn’t an app for places.

“Airbnb use the slogans “belong anywhere” and “live like a local” and we subscribe wholeheartedly to this approach to travel — but how can visitors belong if they don’t know where the locals go? The B&B gives them a home, a Bimble Area Guide gives them a neighbourhood.”

During this lockdown period, how is demand for your services being affected? Are you seeing any surprises / places where demand is spiking at the moment?

“We have seen people making a variety of lists for future holidays: dream lists with ideas for more adventurous and exotic destinations to go to as soon as air travel resumes, but also a proliferation of clear, planning lists for options closer to home for the coming months.

“People are listing independent businesses, little restaurants, bars and coffee shops, as they long for a change of scene and an opportunity to meet up with close friends and family members in less crowded spots. We think this crisis will reinforce the pre-existing trend towards holidaying like a local and away from more touristy type activities.”

How do you think community and the element of trust will be important in the growth of the short-term rental industry moving forward? How do apps like your own help to build that?

“A short-term rental property benefits enormously from having attractive little businesses nearby. Guests will actively look for properties which are within easy reach of a lovely place to eat, a great bakery, a vintage clothing store etc. The more places there are to explore, the longer they need to stay to make the most of their visit. So the community around your property is an essential part of your success. But people tend not to wander into local places unless they have been told about them. Your recommendations will be trusted, because you know the area. Therefore, by making a Bimble list, you are building a relationship with your local businesses by sending them customers, whilst giving your guests a much better experience.

“Bimble is not a review site and this is precisely because we know that trust is essential in an online landscape that has been swamped with fake reviews designed to damage competitors. So we designed Bimble as a positive platform: you list places that you like — we call them Bimbleworthy places — and you remove the ones you don’t like, without resorting to negativity. When you invite guests to look at your Bimble area guide, you are bringing them into a community environment built on trust.”

Are you seeing any trends for travellers and the length of stays they are looking to book for their dream holiday?

“We can’t directly see the time that people intend to spend away. However, we can assume that longer lists correspond to a longer stay. It looks as though people are looking at staying longer in less places when they go away, rather than skipping from place to place. So that corresponds to less geographical ground covered, but more in-depth exploration of a particular area.”

What challenges and opportunities do you see for the future of travel once the coronavirus starts to recede?

“I feel totally confident that the short lets market will bounce back and continue to grow. It fits with the way that people want to explore the world — living, briefly, in another place. Travel, for millennials and GenZ is much more about rich experience and other lifestyles than it is about visiting famous landmarks.

“The challenge, therefore, is to cope with the short term crisis. We may see a shift towards domestic travel in the immediate future but this could actually benefit the short term rentals market.

“Also, we are not alone in expecting that more people will continue to work virtually than before. This creates an interesting opportunity for more people to travel whilst working. It will make sense for hosts to advertise their remote working facilities such as the quality of their internet connection and a good desk and chair.”",8562
"People and Knowledge Management

Deb: Your experiences are a convergence of different perspectives. Where does the human perspective come in?

Annie: When you deal with artificial intelligence and knowledge management systems, every single different perspective counts, because we’re emulating man through digital representation.

When I first got into knowledge management systems, I took the position that operations run an organization. Operations consist of processes, which means that process engineering is key at the beginning of a digital transformation. Many leaders who I worked with early in my career would assure me that they knew what really matters in their organization. My feedback to them was that they were putting the cart before the horse. How could they manage what they did not yet know? They had not engineered knowledge, so how were they going to manage it?

Over time I began to realize the difference between process and knowledge. Process runs the operations of the organization. Knowledge is what people gain from operations so they can improve on it. Knowledge is what is shared with people. The dynamics of the input, output, people, and technology all need to be known so you can make the right decisions around how to improve your organization. Process uses knowledge to improve the competencies of people and the structure of the organization so that it is as efficient and effective as possible. It also helps identify innovations. Knowledge is above or wrapped around the organization. It is a separate process, fed from all the other processes within the organization. Knowledge is a people process.

Deb: What about the knowledge that’s inside individual people within an organization? Each person has inherent experiences and unique knowledge they have gained outside the organization. How is that useful inside an organization?

Annie: That’s part of the dynamics. Most organizations look at their employees as liabilities because they pay them, rather than looking at them as assets because of their contributions. Knowledge within individuals is an intangible asset that must be captured. It is important to somehow represent employees in a way that is valuable to the organization. That value might come from their personality, experience, or education. In any case, you must consider what the organization needs first. Then you ask what you need to know about each person, and then you can identify who has the right assets and begin to utilize them throughout the organization.

The process is knowledge management. Locating experts within a company is a good example. You may have people within the organization that have knowledge, experience, or credentials that may not be known but that can be of value to the organization. Losing one of these highly qualified people creates risk for the organization. Or it might create unnecessary costs because the company hires or contracts someone with the needed knowledge, now knowing that is already exists in-house.

Knowledge Management Technology

Deb: What’s one of the biggest things that businesses are missing when it comes to advanced technology implementation?

Annie: What I believe they are missing is knowledge, specifically, about the technology itself. It is an oxymoron. For some reason, many people do not feel they need to go back and refresh their skills or knowledge. Technology is changing rapidly. People need to keep up. This does not exclude leaders like CEOs. It is just like how doctors must refresh their skills and knowledge continually, taking certain tests or getting certified for specific procedures. Doctors understand a lot of the new procedures, tests, etc., within their discipline and their need to keep up with these changes. It is no different for business leaders. A lot of executives do not understand the technical terms or technical language necessary to develop a digital twin of their enterprise. They hire contractors because they believe they are experts and provide them the proper guidance or information to make decisions. How can you trust someone when you do not know what it is you’re looking for or understand what is necessary to effect the change within your organization?

Holding Knowledge and Managing Knowledge

Deb: Many of today’s leaders grew up believing they are supposed to know everything, without having to relearn or refresh. Obviously, that’s untrue. How do we get around this mental hurdle to help them to gain useful new knowledge?

Annie: I ponder this question quite a bit. It’s an uphill battle. If you’re unaware of what you need to be learning or unaware of what someone is telling you, you might succumb to certain influences that take you down a wrong path. I’m not certain how we fill this gap. It’s not going to be filled by bringing in the younger generation, which knows the technology but doesn’t have the experience to understand things that fall outside of normal organizational boundaries. It is a hard problem with potentially severe consequences if leaders aren’t willing to step out of their comfort zones to do what they need to do. I am concerned about our businesses because of this.

One way to gain knowledge is by supplementing existing capabilities. But before you do, you must make certain that you have selected the right person for the job. Many times, the person who comes in has the skills and experience but isn’t given the guidance, scope of requirements, or context of what is needed from executives. When this happens, the contractors have no other choice but to be driven down the path they know instead of implementing a solution for the unique organization. This gap places the leaders and support/development staff at a disadvantage.

We’ve got a human problem. And we’ve got a knowledge problem. I think those in the senior level ranks need to realize that they’re not going to be punished because they need to go back and gain or refresh certain skills. I don’t think that they should be sent back to get a degree because they lack a certain knowledge area. What I do think works is going to universities that have continuing studies or certificates that build on what you already have. You don’t have to spend ten years getting another degree. Sometimes even two weeks to go somewhere so you can gain a clearer understanding of technology will fill the gap. Once you have this understanding, a leader is better qualified and positioned to be able to talk to contractors about what they are really looking for from technology. If a leader does not have tentacles within and beyond their enterprise, it will be difficult to guide a digital transformation.

I was working with an organization developing scorecards and they asked me to assess them. After spending some time with what they were doing I went to the leaders and explained that the organization’s data was not coming from any system within the organization. And what data they had was being arbitrarily placed into spreadsheets. They were aware of this. There was no reason why the data wasn’t tied to systems except they didn’t know it needed to be.

Knowledge management ties to operations. Measures need to be established. Value areas must be defined. Data must be captured and rolled up for executives to have a scorecard that is functional against the organization. Many leaders still don’t understand this concept.

System of Systems Thinking in the Organization

Deb: How important is understanding the system of systems thinking within an organization?

Annie: The organization is complex, especially when considering that data represents an organization. Everything that is needed to be known must be digitally transformed and represented within the digital twin of the organization. Data must be accurate, mapped, and interdependencies identified to understand if things are going well or wrong within the organization. Think about our bodies when something isn’t right. The doctor starts by checking our vitals first to tell us what is happening with the major biological systems within the body. Then the doctor proceeds to drill down and check other important subsystems. In the same way, the organization needs to identify the vital components first. How do they do this? They need the ability to filter back down into the different data sets and individual components to trace the problem to its source. This process is complex, and systems thinking is required.

The Risks and Upsides of Technology

Deb: What is your thought about the ethics of technology?

Annie: I truly believe technology can improve us. But I think where we fail is we don’t insert it properly into the way we work. We don’t look at what is the responsibility of the computer and the responsibility of the individual. This consideration takes us right back to the importance of system and process thinking. If we decompose the way we work, it is easy to see the areas that best fit automation, as well as the areas that are best served by people. For some reason, people will listen to a computer and say that the computer is right, even though they don’t know the algorithms that have been designed or developed for them to follow. We don’t just need to augment. We need to train.

People are fearful of losing their jobs. The problem is not necessarily them losing their jobs, it’s that they are being displaced. People need supplemental skills so that they can transfer over into other areas of importance. A perfect example of this is secretaries and administrative people. They were wonderful. They did what they were trained to do. When computers came about, these positions were eliminated but many of the tasks they did were still of importance, just digitally represented. They used to maintain paper documents in folders in file cabinets, and with computers, the file cabinets became directories, the folders became digital folders within the directories and the paper documents became digital files.

The digital transformation of the filing system still required the knowledge of the secretary/administrative expert, who (by the way) could have contributed to the knowledge about who had access to the files — the security that should have been included in the digital representation of the filing system. Administrative personnel needed new skills to know how to set up those folders and how to put data into digital folders for use by people. Instead, they were replaced by technical administrators. The ethical approach is to recognize the contributions of the existing employees and systematically identify where the new value-added capability will emerge and train versus replace. This is just a single use case of ethics, there are so many more to be cognizant about.

Deb: You stated concern about the future of business. What are your concerns?

Annie: My concern is for their digital transformations. They need to represent their organizations using technology. And I’m not sure that they have the skills or the ability to do that. A lot of organizations are hiring data scientists to develop algorithms. But what happens is the data scientists aren’t familiar with the industry or the business and how it actually operates, and the organization ends up being misrepresented. Leaders need to step away from egos to realize that they need new skills and learning to make sure they incorporate technology into their organizations in a meaningful way. It was Albert Einstein who said you must first learn the rules of the game and then you must play the game better than anyone else. We are learning a model of the world where the rules of the game are unknown — this is where the problem lies. Many organizations haven’t changed processes in 15 or more years. Incorporating technology that represents the past is not going to help them be successful in the future. Leaders need to understand there is a new game emerging and they need to be a part of defining it, not just trying to play it.

One person can’t do it all. You need to structure your strategic team in a way that makes sure the specific areas within the organization are covered. Those areas should be aligned with the vital components of the organization. For example, you might have a chief human capital officer to oversee employees/staff. This chief human capital officer should be equipped with technology to answer questions — like why are valuable people leaving the organization. It’s the same consideration as if I’m sinking a lot of money into technology but performance is not improving. The chief technical officer, who oversees technology, should be equipped with technology to help strategic leaders understand why, so they can fix the problem.

Future Focus

Deb: What’s your greatest hope for the future?

Annie: My greatest hope is that we build cognition into an organization. That it extends from the internal to the external so we can have visibility into what we’re doing and the impact we’re making. I hope that we can know the rules of the knowledge game and when they’re being broken. I hope that we know things are fair and distributed evenly and that we’re leveraging the growth of people, as opposed to holding them back or not allowing them to bring all of their capabilities forward. I would like to see people educated about the potential of AI to support them so they understand the shifting dynamics and can move into the next position without losing their job and having to start over again. Leaders can help people understand how technology is going to change their jobs. They can explain to people, “you are still going to do this, but now we’re going to shift over and you’re going to be doing it this way.”

I want to see computers augment us — not supersede us. I do not want to see them being used in ways that we don’t need them. They are here to enable us. If that means that they can think faster than us, then let them think faster and provide information. But people are the ultimate decision-makers. I know many people are fearful of computers. I’m not fearful of technology. I’m fearful of the morality of man.",14091
"How To Install And Use Conky on Any Linux Distro. TheKernal Jun 17·2 min read

We all want to make our Linux desktops look better. You can install docks, change icons, switch window managers, but there is something else that it gaining more steam. Conky. Conky allows you to display system information on your desktop, making it look better. This information can be ram, cpu, or gpu usage. Even time or date. I am going to try to walk you through the steps of installing and using Conky.

To use Conky you are going to have to install it. Conky is in most package managers. It is available on RedHat, Arch, And Debain based distros. Here are the commands to install.

Debain/Ubuntu: sudo apt-get install conky

Redhat/Fedora: sudo dnf -y install conky

Arch: sudo pacman -s conky

You will realize that when you run it by typing “conky” in the terminal, it does not look good. So, you need to install some themes. Conky themes change the look of your Conky and make them look better. You can usually just look up “conky themes” to find them, but I think that (1) shows some of the best themes.

Now that you’ve got your Conky theme downloaded, how to you add it to your desktop. Most Conky themes are zipped. You should extract/unzip them. In that file, there should be a text file. Take that text file, and rename it “.conkyrc”. Then move it to your home directory. Simple as that. Now when you start Conky by typing “conky” in the terminal, you should see your Conky theme on the desktop.

Now all that you need to do is autostart it. Most DE’s have a application to autostart a program. Simply search “autostart:” in your desktop environment and add “conky”. If you want to see how to do this on i3, look at my previous guide.

If you have followed my instructions every time you boot in you should have the beautiful Conky widget of your choice. There are hundreds of Conky themes out there, so feel free to experiment. Eventually, you will find the perfect Conky for you.

1: https://www.ubuntupit.com/best-conky-themes-for-linux/",2035
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 4, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAElite CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/RnXpUrx

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9155
"As a rule, associated with new cutting-edge technology, disruption is not necessarily linked to a fantastic discovery. It actually defines a change in the way we do something or how we behave in a situation. And if there’s one thing the pandemic has done is to change our habits and behavior, isn’t it?

The spread of the covid-19 virus almost radically changed many of the things we did leading to new habits, such as social distancing, mask-wearing in the Western world, telemedicine consultations, and remote work. With the pandemic, and the need to preserve our health and that of others, we are constantly thinking about how to carry out our tasks without exposing ourselves to the risk of contagion. In other words, we started to have a disruptive mentality.

Disruption can come with the launch of a new product or service, which considerably changes the way you do, consume or solve something, but it can also happen with a relevant decrease in costs or price. If a service that used to be charged is now offered for free with the same efficiency, it causes disruption, because no one else will be able to charge for it. That is, we can shout “Eureka” without necessarily discovering a theory of physics, like the Greek mathematician Archimedes.

The covid-19 pandemic accelerated the digital transformation. Some trends that were already emerging were accelerated and amplified. We move forward in months what would likely take a few years if there was no global emergency. New solutions and services, digital platforms, startups have multiplied at an impressive speed.

Amid the complex challenges in very different areas, such as science, health, commerce, food, transport, the scenario designed by the new coronavirus has created countless opportunities. Apparently, the changes promoted by the pandemic should shape the main technological trends to be incorporated in the coming years.

We will reunite with friends, travel, and go out on the streets.

We are moving towards an era of XaaS (Everything as a Service), in which companies will offer products and services in increasingly decentralized technological environments, with on-demand and customized solutions. Remote work and virtual collaboration will become the norm (the so-called new normal) in many areas, even when we overcome the uncertainties arising from the pandemic.

Emerging technologies, such as the internet of behaviors (IoB), with increasingly mobile and virtual interactions, systems capable of ensuring privacy and data protection, public cloud services, smarter and more flexible business processes and hyper-automation will shape this new reality. These technologies added to the advancement of innovations such as machine learning, internet of things (IoT), cloud computing, artificial intelligence, 5G, digital twins, and the disruptive mindset we’ve developed will take us to the next stage, which some call the Fourth Industrial Revolution.

These changes will have a major impact on the corporate world, the economy, and social behavior. Materials science is one of the areas that must undergo a disruption, with the development of products that, in addition to being recyclable, can be reused, significantly extending their useful life and reducing waste generation. Likewise, the need to substantially reduce gas emissions will lead to significant changes in the areas of energy and transport.

The covid-19 saga will end, and we will certainly return to old habits, such as meeting friends, traveling and taking to the streets without fear, but even so, our reality will be very different from the one we lived until the appearance of the new coronavirus.",3655
"➕Official Partners “TVs” TV Shows & Movies

● Watch Elite Season 4 Episode 1 Eng Sub ●

Elite Season 4 Episode 1 : Full Series

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ



Elite — Season 4, Episode 1 || FULL EPISODES : When three working class kids enroll in the most exclusive school in Spain, the clash between the wealthy and the poor students leads to tragedy. .

Elite 4x1 > Elite S4xE1 > Elite S4E1 > Elite TVs > Elite Cast > Elite Online > Elite Eps.4 > Elite Season 4 > Elite Episode 1 > Elite Premiere > Elite New Season > Elite Full Episodes > Elite Season 4 Episode 1 > Watch Elite Season 4 Episode 1 Online

Streaming Elite Season 4 :: Episode 1 S4E1 ► ((Episode 1 : Full Series)) Full Episodes ●Exclusively● On TVs, Online Free TV Shows & TV Elite ➤ Let’s go to watch the latest episodes of your favourite Elite.

❖ P.L.A.Y ► https://cutt.ly/RnXpUrx

Elite 4x1

Elite S4E1

Elite TVs

Elite Cast

Elite Online

Elite Eps.4

Elite Season 4

Elite Episode 1

Elite Premiere

Elite New Season

Elite Full Episodes

Elite Watch Online

Elite Season 4 Episode 1

Watch Elite Season 4 Episode 1 Online

⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 4, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAElite CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/RnXpUrx

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",10301
"➕Official Partners “TVs” TV Shows & Movies

● Watch Hospital Playlist Season 2 Episode 1 Eng Sub ●

Hospital Playlist Season 2 Episode 1 : Full Series

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ

ஜ ۩۞۩ ஜ▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭▭ஜ ۩۞۩ ஜ



Hospital Playlist — Season 2, Episode 1 || FULL EPISODES : Every day is extraordinary for five doctors and their patients inside a hospital, where birth, death and everything in between coexist. .

Hospital Playlist 2x1 > Hospital Playlist S2xE1 > Hospital Playlist S2E1 > Hospital Playlist TVs > Hospital Playlist Cast > Hospital Playlist Online > Hospital Playlist Eps.2 > Hospital Playlist Season 2 > Hospital Playlist Episode 1 > Hospital Playlist Premiere > Hospital Playlist New Season > Hospital Playlist Full Episodes > Hospital Playlist Season 2 Episode 1 > Watch Hospital Playlist Season 2 Episode 1 Online

Streaming Hospital Playlist Season 2 :: Episode 1 S2E1 ► ((Episode 1 : Full Series)) Full Episodes ●Exclusively● On TVs, Online Free TV Shows & TV Hospital Playlist ➤ Let’s go to watch the latest episodes of your favourite Hospital Playlist.

❖ P.L.A.Y ► https://cutt.ly/rnZNRDX

Hospital Playlist 2x1

Hospital Playlist S2E1

Hospital Playlist TVs

Hospital Playlist Cast

Hospital Playlist Online

Hospital Playlist Eps.2

Hospital Playlist Season 2

Hospital Playlist Episode 1

Hospital Playlist Premiere

Hospital Playlist New Season

Hospital Playlist Full Episodes

Hospital Playlist Watch Online

Hospital Playlist Season 2 Episode 1

Watch Hospital Playlist Season 2 Episode 1 Online

⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 2, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRAHospital Playlist CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/rnZNRDX

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",10711
"Planet Earth is described as a rocky planet with a solid crusty surface. For the people who inhabit it, some of its most striking features consist of tall mountain ranges, deep canyons and vast coastlines. As we view these seemingly rocky, solid features, It is easy for many parts of the world to forget that the Earth is, in fact, alive and active with constant movement. The tectonic surface we stand on moves around 3 to 5 cm a year, and at the equator, the earth spins at around 1609km/h. In addition to this, coastal erosion, landslides, natural disasters, and volcanic eruptions all contribute to movement and a forever changing surface that we inhabit as guests on this plnet.

Satellite Imagery time-lapse of construction at the WestConnex St Peters Interchange.

As we continue to inhabit and expand the built environment we are increasingly subject to the movement of soil and earth which must be accounted for in construction. By not doing this, we expose construction projects to the risk of damage caused by the natural movements of the Earth’s surface.

One of the largest current construction projects in Australia is Westconnex. Westconnex is a $16 billion AUD, 33km motorway tunnelling project that was designed to keep up with Sydney’s growing population. The project is predicted to finish in 2023, however, the stress between the Earth and Westconnex has already begun. Many residents across several Sydney suburbs located above the WestConnex tunnelling project have reported subtle but notable damage to their properties, such as small cracks and jamming doors.

Due to the scale of the tunnelling, the only practical way to scale and accurately measure the impact it has had on the hundreds of thousands of homes across the city was to use satellite radar technology.

Krish Patel, Sat-Scan CEO saw an opportunity to utilise satellite technology to measure and monitor issues caused by large scale construction projects such as Westconnex. It was no coincidence that multiple residents above the WestConnex tunnelling all experienced building damage at the same time. To make matters worse for residents, the government weren’t willing to take responsibility for the damage to their homes. Sat-Scan was able to utilise satellite radar data to demonstrate that WestConnex tunnelling did cause measurable soil movement and significant damage to residential properties in surrounding areas.

Unfortunately until very recently, satellite imaging technology has been incredibly difficult to access. The high cost of satellite imagery along with the technical and legal hurdles associated with procuring it, has meant that it’s been difficult to utilise for monitoring critical infrastructure projects such as the Westconnex.

Sydney-based startup Arlula was able to support Sat-Scan in accessing the critical satellite data needed for its assessment process. By leveraging the Arlula API platform and satellite imaging partner network, Sat-Scan was able to easily capture 900sqkm of new radar imagery per month over Sydney. This service was critical in assessing the impact and scale of damage to properties across the city.

Try the Arlula catalogue browser now at https://api.arlula.com/catalog

Sat-Scan’s Earth movement monitoring service has since been purchased by local governments in Sydney to help residents fight back against the large corporations that have caused this damage in the first place. By combining satellite imaging technology with data analytics tools Sat-Scan was able to empower local residents and help save their properties from destruction.

There is an argument to be made for Government agencies undergoing multi-billion dollar construction projects to collaborate with innovative startups like Sat-Scan. By doing so, they will be able to harness new technologies that help limit or prevent damage to the Earth and the population that occupies it.

Satellite Imagery time-lapse of construction at the WestConnex Rozelle Interchange.

The next step to adopting this technology is awareness and education. There are so many unique ways satellite technology can be harnessed and the ability to monitor the earth will assist us in making more responsible choices. Arlula continues to improve access to satellite data that helps formulate these incredibly valuable insights.

To follow Sat-Scan’s journey you can visit them on their website, https://www.sat-scan.com/. And to easily access satellite imagery for your own project you can start by visiting https://api.arlula.com/catalog.",4531
"Glossary of Geospatial Terms

Do you find it hard to find one, cohesive place to understand the fundamentals of the geospatial domain? Well, you’re not alone. I am here to make it happen for you in very simple English — a glossary of some of the most complicated aspects of the geospatial genre.

Spatial vs Geospatial

Spatial means space, so it is related to space and the position, size, shape, etc. of a particular object (any object, not only on the Earth’s 🌍 surface).

Geospatial means the distribution of something in a geographic sense; it refers to entities that can be located by some coordinate system 🌐.

Geospatial is that type of spatial data which is related to the Earth’s surface and/or near Earth’s surface.

Vector data vs Raster data

Vector data is used to present generalizations of objects or features on the Earth’s surface. There are three main types of vector data: points, lines, and polygons.

Raster data is the data taken from satellite 🛰images or any other aerial device images.

In addition to raster and vector data, there is also LiDAR data (also known as 3D data). 3D data is data that extends the typical latitude and longitude 2-D coordinates and incorporates elevation and or depth into the data.

Vector geometry objects

a) Point:

It is the exact positioning of a real-world geographical area and is represented by a pair of longitude and latitude collectively known as coordinates. Example: [77.59199, 12.99864]

Point geometry object (Grey)

b) Multipoint:

Two or more points in space are called multipoint. Example:

Multipoint geometry object (Grey)

c) Line string:

Connecting points create lines that can be used to represent roads, railway tracks, etc. Example:

Line string geometry object (Pink)

d) Multiline string:

Two or more line string in a space is called Multiline string.

Multiline geometry object image (Black)

e) Polygon:

Connecting lines that create an enclosed area create a polygon. Example:

Polygon image

f) Multi-polygon:

Two or more polygons in space are called Multi Polygon.

Multi-polygon image

Point in Polygon analysis

Let us consider a point, p and a polygon, P. If the point p is inside polygon P then this type of analysis is called Point-in-Polygon analysis. This is how Footfall, Foot Traffic, or Visitors counts are done. Example:

Point in Polygon Image

Geofencing

It is the technique of creating a virtual boundary also called a Geoboundary on the perimeter of any real-world geographical place. For example, consider a racecourse in Bangalore, India that has been geofenced below:

Image representation before geofencing the racecourse

Image representation after geofencing the racecourse

Coordinate Reference System (CRS) or Spatial Reference System 🌐

CRS helps you to precisely relate 2D (coordinates) with real-world places on Earth. CRS is that reference system that helps you flatten the Earth’s surface (3D) to a 2-dimensional surface.

Horizontal accuracy

This type of accuracy represents the radius of the margin of error that the given coordinates’ measurement can be in meters. Let us say that the horizontal accuracy of a device ID is 500 meters. This means that the device is somewhere within a 500-meter radius from the stated location observations (latitude and longitude).

The smaller the horizontal accuracy, the more precise the observation 👍.

GeoJSON and Shapefile

GeoJSON is a wrapper over a JSON file, which includes spatial attributes as an add-on component to it.

You can try creating your own geoJSON file by geofencing any of your favorite locations at geojson.io.

Shapefile is a common format file used to store vector data. One cannot edit the shapefile directly as it is not a text file.

Mapshaper is an online free web tool to convert GeoJSON to shapefile and vice versa.

Global positioning system (GPS)

It is a satellite system that helps you get location and time observation of any place 🏨, mobile device 📱, or any GPS-enabled package. Example:

Data dictionary of GPS data

Topology

Topology expresses the spatial relationships between connecting or adjacent vector features (points, polylines, and polygons).",4149
"Household Lighting accounts for about 9 percent of a typical households energy use. This may not seem like a lot but considering that a typical incandescent light bulb uses approximately 0.2kWh of energy a day and averaging approximately 50 light bulbs per household this figure increases to 300kWh per month.

There are ways to minimize the amount of energy a household spends on powering lighting for the sake of the Environment.

One of these methods is to switch your to more eco-friendly LED lights. LED lights consume 80–90% less light that a typical incandescent light.

This energy consumption can be mitigated even more by using smart lighting technologies that are slowly gaining in popularity. One such smart lighting technology is motion sensing light technology.

Motion sensing lights are designed to to only be powered on and illuminated when the sensor in the light fixture is triggered by motion. This technology has two major benefits.

Lights will not be kept turned on accidentally by children or guests. The energy consumption is controlled automatically through sensing technology based on household traffic in certain areas of the house.

Smarter Crime deterrence

With motion sensing lighting it is also possible to consider the application of this sort of technology at entrances to your household. By maximizing the amount of light at potential household entrances a potential security breach of your household will be minimized and the sensing light will function as a deterrent to burglars.

Coming Home Late at Night

The sensing light technology also has applications to individuals coming home late at night . At certain points it may be difficult to find the keyhole to your door or navigating through your household in the dark. Smart sensing Technology eases the users household navigation and reduces the risk of:

falling down stairs

drooping objects

Bumping into furniture

Consider the Switch to Smart Sensing Light Technology

As we have outlined the many benefits of smart sensing light technology, home owners should consider the cost to benefits of acquiring and utilizing this form of technology.",2141
"Dyson Lightcycle Morph Review — An Enlightening Life Changer But Is It Seriously Worth Your Money? | Hitech Century Kevin Tan Just now·11 min read

We’ve gotten our hands on the Dyson Lightcycle Morph that has just arrived in Malaysia for testing to see if an LED lamp that costs as much as an upper-tier smartphone is worth the money. Here’s our Dyson Lightcycle Morph review after putting it through its paces.

What is the Dyson Lightcycle Morph task light?

To recap, the Dyson Lightcycle Morph is the second generation smart LED lamp that made its global debut last year in Europe and the US. It has just been launched in Malaysia several weeks ago at the time of this writing and we jumped at the chance to field test their latest offering. Its predecessor, the first Dyson Lightcycle was launched earlier in 2019 and arrived in the same year on our shores.

The older Lightcycle and the newer Lightcycle Morph are functionally similar in terms of the quality and customisability of their LED lighting but the newer Lightcycle Morph improves things with a more sophisticated 3-point revolve mechanism that lets you maneuver the light fixture around for more precise angling of the light beam and the option to use it in a host of different ways that include employing it as as ambient or indirect lighting.

It also has a unique feature where you can shunt the light beam into its stand, which has hundreds of tiny perforations to use it a soft ambient light with an appearance that’s a cross between a lava lamp and a lightsaber.

To offer direct illumination, the Lightcycle Morph incorporates six LEDs, three of them cool white and three of them warm yellow with the collective ability to serve up to 850 lumens of brightness using 11.5W with a Colour Rendering Index (CRI) of at least 90/100.

On average, 450 lumens are needed for a typical desk lamp tasked for reading and writing duties while precision tasks that require intricate work and more focus like building a model or fine needle work will need on average a much higher 800 to 1,000 lumens of brightness.

For the uninitiated, CRI is an indicator of how faithfully a light source is able to show colours that are true to life in comparison to natural daylight.

If you’re a traditional content creator like a painter, graphic artist or modeler working indoors, this is considered essential and with a CRI of 90/100, the Lightcycle Morph is able to illuminate colours close to what you see under natural sunlight.

For the average user who just needs enough light indoors so they read a newspaper or to stop them from stubbing a toe in the dark, this sounds like an insane amount of overengineering for a desk lamp but as with all things Dyson, there’s a reason for all this and the Lightcycle Morph has been built to last a lifetime.

Officially speaking, Dyson has rated the Lightcycle Morph to be able to last for over 60 years of usage which is astronomical by any measure of the word.

Unboxing the Dyson Lightcycle Morph

Getting the light out of the box is a relatively straightforward task. Our Dyson Lightcycle Morph review sample is the desktop version which means it has a shorter stand.

The floor standing version has a stand that stretches all the way up from the base all the way to about waist height while maintaining the same micro perforations all along its length for the Morph’s unique ambient lighting mode.

Functionally speaking, both the floor and desktop versions emit the same light intensity and quality so you’re primarily paying for the increased length of the light emitting stand. The flexible 3-point revolve mechanism for both models are identical.

The various components all arrive nestled in cardboard trays and assembling it isn’t very hard though the base is heavy which you’ll have to hold up one-handed halfway through assembly.

You essentially pop the light and stand which is one complete piece into the base, add in an additional plastic piece to lock the stand into place and then finally plug it in.

The plug point terminates in a UK-style 3-pin plug and sidles under the stand through a neat cutout to hook up directly underneath the light stand itself. Beyond these components, there aren’t any other optional bits and bobs in the box beyond the obligatory user and quick start manuals.

Dyson Lightcycle Morph Review — Specifications

The Dyson Lightcycle Morph comes with the following official specifications and prices for Malaysia. Bar the difference in height between the floor-standing and desktop versions, there are no other subvariants of the Lightcycle Morph.

Dyson Lightcycle Morph design and setup — Most enlightening

Our review unit comes in a stylish shade of matte black though you can optionally get it in white as well. The finish itself is fingerprint resistant which is a blessing as you’ll likely be moving the light armature itself quite a bit as you use it throughout the day.

To date, it’s taken Dyson close to 5 years and needed 90 engineers to create it and it shows in the intricacy and robustness of its design. The base itself is weighted and rather heavy at over 3kg; necessary as it acts as a counterweight for the whole light stand assembly.

Attached to the base is the ambient light stand itself which has hundreds of tiny holes — all 16,740 of them to emit a soft ambient light once you dock in the armature to the stand.

Interestingly enough, the base of the light stand has a USB-C port with 1.5A charging that lets you slow-charge a phone or laptop.

It’s a thoughtful feature to have though USB-C to USB-C charging cables aren’t common yet except on super flagship phones like the Galaxy S21 series. Having a USB-A port and wireless charging would have been fantastic but it doesn’t really affect its raison d’etre.

The 3-point revolve mechanism itself is arguably the most well crafted aspect of the Morph that consists of two 34cm long-interconnected pieces that let you rotate the whole affair almost 360-degree radius around the stand itself. The tip of the mechanism itself houses the LED lights along with a series of touch sensitive controls.

You can also rotate the light end aka the ‘Intelligent Optical Head’ as Dyson calls it almost 360-degrees as well so you can aim it straight up to the ceiling if you deign to do so. What really makes this a marvel of engineering is that the entire mechanism is so smooth and so well balanced that you can maneuver it around with just one finger.

In order to ensure that the LED’s last over six decades, Dyson has integrated a unique copper heat pipe into the mechanism that shunts heat away from the LED lights and ensures it continues working at top efficiency over its lifespan.

Two touch sensitive sliders at the top let you control the intensity and the warmth of the light from a deep yellow akin to sunset that won’t affect your night vision to an intense, cold white light akin to broad daylight for precision work.

The underside also hosts a series of sensors that toggle three important modes. The first is a ‘daylight resync’ button that lets you automatically change the light intensity and brightness based on where you are located on the planet and time of day.

The second button activates a motion sensor that turns the light off if it doesn’t detect motion for five minutes to save on power. Last but not least is an ‘auto brightness’ button that adjusts light output constantly so that if background light suddenly changes, the light output is adjusted so that it remains at a constant level.

Assembly aside, the toughest part to setting it up is perhaps pairing it with the Dyson Link app though in actual fact, it’s a relatively fast process.

Once you set up an account with Dyson, you then input the location where you’ll be using the Lightcycle Morph, select a few simple settings and you’re done.

Dyson Lightcycle Morph Malaysia: Performance — Let there be light!

Since the pandemic hit, I’ve been primarily working from home, repurposing an unused store room as a home office. Needless to say, the situation isn’t perfect and I’ve kludged together a modest setup with a laptop, a study desk with an accompanying chair and an old LED reading lamp.

There isn’t much natural light where my desk is located and even in the daytime, things can get pretty dim as the nearest window doesn’t even get direct sunlight except in the evening.

After sundown, the lamp becomes a necessity for visibility as there’s no overhead lighting at all. I deployed the Lightcycle Morph into this mess on my desk and over the course of a week, I noticed subtle but tangible improvements in my workflow. After two weeks, it proved its worth.

One of the interesting questions that the Dyson Link app asks you is your age on setup. This is on account of an interesting fact — the older you are, the more light you need. According to Dyson, a 60 year old needs 4 times more light to see versus a 20 year old.

The app is able to tell the Lightcycle Morph just how much light a user needs and in my case, I apparently didn’t have enough in my workspace and where I previously needed to squint with my dinky old lamp, the Lightcycle Morph was able to cast out an even flicker-free glow that made reading text both in hardcopy and onscreen a more relaxed experience.

The ability to angle the light fixture in any direction and also alter the intensity and colour of the lighting on demand made for a real game changer. There’s no flickering and the light even on the weakest settings is still bright enough to read and work by as a reading lamp which is its primary function.

With the ability to angle the light fixture any way I saw fit, I tested it by angling it upwards to the wall as ambient lighting and it worked marvelously, brightening up the room with an even light.

The ambient mode where you dock the light into the stand itself makes it emit a softer glow throughout the length of the stand without messing with my night vision as it filters out blue light, making it ideally suited as a night light.

Where the Lightcycle Morph earns its eco-friendly credentials is the fact that it’s not only Energy star compliant, it is also able to intelligently turn itself off if it doesn’t sense motion for five minutes.

Granted, it does happen when we’re fixated on reading a page or two of text when it inadvertently shuts off but a simple wave of the hand turns it back on. More importantly, this feature eliminates wastefulness especially if I forgot to turn it off when leaving the room.

Over the course of my Lightcycle Morph review, I found that its myriad features offered subtle quality of life improvements, gradually becoming the cornerstone of my home office setup which is somewhat poetic from a certain perspective seeing as light, or rather good quality lighting ought to be the cornerstone of any effective work setting.

While I didn’t quite appreciate it at first, thinking it as ostentatious frippery, I gradually found that its ability to deliver customisable lighting with the ability to swap colour warmth and intensity on the fly as well as its exceptionally flexible armature grew on me across the two plus weeks I was testing it.

Over time, my old desk lamp which had watched over me hammering out hundreds of pages of text since the MCO hit lies forlorn in a corner, the inevitable technological detritus of superior technology like how a shiny modern Tesla supplants a horse-drawn wagon.

Dyson Lightcycle Morph review — Is it worth buying?

As far as performance is concerned, the Dyson Lightcycle Morph does what it sets out to do — delivering superior, consistent light that is customised for your age and current geographic location. It won’t immediately improve your productivity overnight but its subtle array of quality of life improvements will contribute to it over time.

Of course, that still leaves the biggest problem — it’s price tag. The Dyson Lightcycle Morph costs quite a bit more than the average LED desk lamp with the desk version costing RM2,499 while the floor standing version clocks in at a substantial RM2,999. To date, this is so far one of the most expensive LED lamps in the market.

To put things into perspective, the average LED desk lamp on Shopee costs about RM150 or so with the cheapest costing about RM30 or so, coming in different form factors, light intensities and build quality.

The price seems to be a flying Godzilla kick to the wallet, but the fact remains that the Lightcycle Morph doesn’t cost that much if you take a long term perspective as its cooling system means that it will continue working for 60 years. If you divide the cost over the years, you’re literally paying cents per day — 0.11MYR to be exact — for bright, quality lighting for most of your life.

The Dyson Lightcycle Morph is indubitably an innovative and premium lighting solution that breaks new ground that no other brand has attempted before. It’s not for everyone but for those who appreciate and need quality lighting — all the more so seeing the fact that we’re mostly working from home these days — it’s less of an expense and more of an investment.

If you are looking to upgrade your home office to the next level and invest in the quality of your lighting , the Dyson Lightcycle Morph is the definitive choice.

Dyson Lightcycle Morph review unit courtesy of Dyson Malaysia. Available for purchase online at all Dyson experience stores nationwide and online at the official Dyson online store at https://www.dyson.my/dyson-lightcycle-morph-desk-light-white-silver Dyson Lightcycle Morph",13568
"The Thieaudio L2 comes in a black rectangular cardboard box with a black slipcover. The Thieaudio logo is embossed in silver on the front of the slipcover as well as the top lid of the box. In addition to the IEMs, the package includes a detachable 2-pin cable, a blue synthetic leather carry case, six conventional black silicone eartips (S, M, L), and 6 clear silicone eartips with black cores (S, M, L). The two-tone eartips are shorter and wider than the black eartips. The L2 also includes a quality control pass chit, a user manual written in English and Chinese, and a warranty card. The lid of the carry case is exterior embossed with the Thieaudio logo and has a magnetic fastener.

BUILD QUALITY AND DESIGN:

The Thieaudio L2 has translucent blue acrylic shells with pearlescent faceplates. The internal components are faintly visible through the shells. The shells have a pseudo-custom fit. The Thieaudio logo is set into the left faceplate in silver, while the right faceplate is unmarked. “Legacy 2” and the unit serial number are printed in white on the inner face of the shell. The IEMs are otherwise unmarked. The seam between the shell body and faceplate is invisible to the naked eye but can be faintly felt. The nozzles have two separate sound tubes and lack a nozzle cover. The nozzles have a lip for securing eartips. There are two circular vents just below the 2-pin connector, one with a metal rim, one without.

The included 2-pin cable is attractive and substantial. The right 2-pin connector is distinguished only by a red base. The Thieaudio logo is printed on the jack hardware. There is strain relief above the straight 3.5mm jack but none at the Y-split. The cable has pre-formed clear plastic earguides without memory wire. There is a chin adjustment slider, and the cable resists tangling. The cable is moderately microphonic.

COMFORT, FIT, AND ISOLATION:

The Thieaudio L2 is intended to be worn cable-up. The earpieces have a shallow-to-moderate insertion depth. The L2 is extremely comfortable. I did not experience driver flex with the L2. Isolation is above average.

MEASUREMENTS:

Measurements of the Thieaudio L2 can be found on my expanding squig.link database:

Thieaudio Legacy 2 — Squiglink by Bedrock Reviews

My measurements are conducted with a Dayton iMM-6 microphone using a vinyl tubing coupler and a calibrated USB sound interface. The measurements use a compensation file derived from relating my raw measurements to published measurements from Crinacle and Antdroid. There is a resonant peak at 8k. Measurements above 10 kHz are not reliable.

SOUND:

The Thieaudio L2 has a Harman-ish tuning, meaning an emphasis on sub-bass over mid-bass, a healthy but not overbearing amount of pinna gain, and a downward-sloping frequency response from 1–3 kHz down through the treble. The L2’s frequency response is very similar to that of the SeeAudio Yume. The biggest difference between the two IEMs is in the lower treble, which the Yume has more of. Like the Yume, the L2 is close to what I would call my ideal IEM tuning.

The L2’s bass is impactful, with excellent sub-bass extension and an organic-sounding transient response. The bass is cleanly articulated but one-dimensional, with limited texture.

The L2 has a clear, even midrange. The L2’s lower midrange presentation is noteworthy for avoiding the sense of clinical coolness found on many Harman-ish budget IEMs. There is a sense of midrange warmth with the L2 not found with the Moondrop Aria. The L2’s upper midrange, while elevated, drops off faster than many other Harman-inspired IEMs, avoiding any harshness in the presence region. Male and female vocals are roughly even in their emphasis, and male vocals have an equal richness to them as female vocals. Male and female vocal intelligibility are both excellent. The L2 has a natural timbre and avoids BA plasticity.

The L2’s treble presentation is exceptionally smooth and is weighted most towards the mid-treble. There is a pronounced dip in the lower treble which avoids the tinsel-like quality to cymbal hits exhibited by the SeeAudio Yume. The L2 is lacking in air and does not have the most spacious soundstage in its price bracket. Instrument separation is very good nonetheless.

Out of the box, the L2’s detail retrieval is a step behind what I would expect from a $100 IEM across the board, though I have found this is largely tip-dependent. With SednaEarfitLight eartips I felt the L2 was only slightly inferior to the $80 Aria in terms of resolution. I recommend using the L2 with the widest bore tip available. Overall, I prefer the L2’s tonality to the Aria’s.

ELEMENTS AND SOURCE PAIRING:

The Thieaudio L2 can be readily driven with modest sources. I did not notice hiss with any of my sources.

CLOSING WORDS:

The Thieaudio L2 has superb tonality and is a great option at the $100 price point, especially for IEM newcomers.

The Thieaudio L2 can be purchased below:

Linsoul Thieaudio Legacy 2 2 Beryllium DD + BA Hybrid In-Ear Monitor — Linsoul Audio",5036
"Although the headphones l ook good and feel fantastic on, I was very disappointed with the functionality . I loaded the Bose Music app on my iPhone and then followed the instructions to pair the headphones. The Bose Music app worked well until the firmware on headphones automatically updated and installed. After that, game over! The Bose Music app could not pair with the headphones … Went on the Bose website and tried to do a factory reset, but that was useless . Bose customer service was also useless .

Great audio, connectivity and comfort issues

These work great for this “new normal” of teleworking with multiple people in the house. Being able to turn the level of noise cancellation to three different levels … is great. Audio quality seems very good. I have some occasional connectivity issues. … What happens is when the phone is dialing out I hear the dialing etc… through the headphones, but as soon as someone picks up it flips over to regular phone audio unless I change it back to bluetooth manually. This happens maybe 10% of the time. I have a very large head … I have to have the headphones maximally extended to fit (which is OK) but after a couple of hours of continual use the cartilage around my ears hurt and I can get a bit of a headache … they are not at all comfortable to drop them down to wear them around your neck when not in use … the earpieces apply pressure to the side/front of my neck. Not comfortable and feels like they could fall off.

Rating: 4 / 5

By The Dude, sourced from BestBuy",1527
"How do I even start to share how awesome is the music coming out from my #bowerswilkins #px7? Definitely a legend on its own. My travel and audio editing buddy.

By @danieltaiwh, sourced from Instagram

It’s lighter than the PX and P7 thanks to the new carbon fiber design in the headband arms.

Battery life of the PX7 is up there with the best in this class. You can get up to 30 hours of playback with a 15-minute quick charge giving you 5 hours of playback. Also, the method of charging is via USB-C.

The PX7 has excellent noise cancelling which is about as good as that of the Bose NC 700 but not the Sony XM3.

The have excellent detail but there was something lacking in the mids (I suppose) where vocals seemed a bit washed out. Also, the sound quality took a dive when the ANC was turned on.

The ear cups fold in but they are not flat in relation to the headband so if the headband is flat on a surface then the ear cups are raised quite a bit. Carrying these on your commute will IMO be a burden and for me it is odd as the key use case for ANC headphones is surely ‘On the move’.",1092
"Most Honest & Helpful Reviews for Sony WH-1000XM4 Wireless Noise Cancelling Headphones — Curated by Rosi Rosi Reviews Sep 2·7 min read

Sourced from Sony

Not sure whether it’s worth buying the Sony WH-1000XM4 Headphones? Instead of spending hours scrolling through thousands of reviews, we curated the ten most trustworthy and helpful reviews so you could easily decide if this product is right for you.

Number of reviews and ratings from the most popular websites

Review #1: Helpful Review

Best yet. Sound Quality: Best Sound quality I’ve heard so far … However i did bump into some odd sound and connectivity issues between ps5 controller connected to the controller making the sound quality sound very muffled while pushing out 3D audio from the ps5 … Everything else sound quality is superb. Connectivity: This is where i had some issues, not with all devices but with the ps5 and a midrange Samsung Tv … Comfort: Comfortable enough up to 12hr of none stop playing video games, after that started to feel heavy on my ears causing some pain especially if you wear glasses. If you have some what warm place the do make you sweat … Battery life: Exeptional 30hrs of batterylife , 12hrs none stop video games while of line (no chat) and battery was at 45%. Rating: 4/5 By Memphisreins, sourced from Best Buy

Review #2: Critical Review

Not as good as the reviews/hype I think this is a lazy product from Sony. The noise canceling is not any better than cheap ones I used before. Sure the audio is good but so are less expensive headphones. I bought these for the dual device support, it works but is so annoying. I get a delay for the second device … I guess I have a small head too because the fit on these is kind of loose, when looking down or pick something up they almost fall off. I just think for the price I expected so much more refinement. I use them all day every day but overall disappointed by the lack of customization in the app and overall quality of day-to-day experience. Rating: 2/ 5 By Mrsexytime, sourced from official website

Review #3: Positive Review",2080
"Looking for a great pair of headphones at an affordable price? Check out these tech-heavy, budget-friendly headphones.

Want a new set of headphones without spending upwards of $200? Well, you don’t have to. Today, we’re highlighting the most affordable headphones you can buy in 2021, and you’re going to want to get your credit card ready because these headphones boast ANC, long-lasting batteries, cross-platform compatibility, and all the other specs you expect from more expensive models.

Related: 10 Tech gadgets to impress your friends and family

Below, you’ll find headphones that give you 44 hours of battery life-and that’s with the ANC on. There’s also a whole bunch of gaming headsets that work across multiple platforms, all while giving you a more immersive experience. Check out these headphones and more on the list below.

1. The Thermaltake ARGENT H5 boasts 50 mm drivers. This gaming headset completely immerses you in your environment.

For gaming headphones that won’t eat into your budget, go for the Thermaltake ARGENT H5 RGB 7.1. This gaming headset has spatial awareness, helping you detect your enemies’ location. The mic lets you communicate with other players.

Get it for $109.99 on the official website.

2. The Anker Soundcore Life Q30 has a 40-hour playtime and provides clear, crisp sound thanks to its 40 mm drivers.

Yes, you can get affordable headphones with a long battery life: the Anker Soundcore Life Q30. They play for 40 hours in Active Noise Cancelling mode or up to 60 hours in Standard mode. Then, a 5-minute charge provides an extra 4 hours of listening.

Get them for $79.99 on Amazon.

3. The Xbox Stereo Headset delivers spatial sound technologies and is lightweight. That way, you can game in comfort without any headaches.

Avoid sore ears while gaming with the Xbox Stereo Headset. It supports spatial sound technologies like Dolby Atmos, Windows Sonic, and more. Meanwhile, the large, soft earcups are comfy over your ears.

Preorder it for $59.99 on the official website.

4. Theprovides excellent immersion with its ENC microphone. That way, background noise won’t interfere with your communication.

The EKSA E910 is one of the most affordable set of headphones because it costs a wallet-friendly $89.99. But it gives you an immersive experience, too, thanks to its ENC microphone. It blocks out environmental noise while you talk for more seamless gaming.

Get it for $89.99 on the official website.

5. The JBL Tune 660NC help you tune out the outside world and concentrate on the music thanks to their active noise cancellation.

Focus on your audio with the JBL Tune 660NC. These wireless headphones have active noise cancellation and impressive sound. Also, you get up to 44 hours of playtime-even when you have the ANC on.

Get them for $99.95 on the official website.

6. The EKSA E900 Pro 7.1 provides virtual surround sound and works on your Xbox, Nintendo Switch, PC, PS4, and PS5.

The EKSA E900 Pro 7.1 is one versatile headset. It works with an array of gaming platforms as well as mobile gaming devices. It’s also quite comfortable to wear and comes with two cables that you can detach.

Get it for $39 on the official website.

7. The Razer Barracuda X works on multiple platforms. Even better, it’s light, ergonomic design lets you play comfortably.

Another of the most affordable headphones you can buy in 2021 is the Razer Barracuda X. It costs under $100 and weighs less than half a pound. Meanwhile, the TriForce 40 mm drivers deliver incredible sound.

Get it for $99.99 on the official website.

8. The JBL Tune 510BT lets you listen for as long as you wish. And with Bluetooth 5, it wirelessly streams JBL Pure Bass.

You get long battery life and full bass capabilities with the JBL Tune 510BT. Additionally, the speed charge function delivers 2 hours of battery with just a 5-minute charge. Best of all, it comes in 5 colors.

Get it for $49.95 on the official website.

9. The Logitech G335 distributes weight evenly with its suspension design. What’s more, it weighs only 240 grams for comfortable wear.

For a stylish yet comfortable set of must-have affordable headphones, check out the Logitech G335. They boast a comfortable fit and work on most platforms. Finally, their reversible elastic straps are washable.

Get it for $69.99 on the official website.

10. Theworks across gaming platforms. Meanwhile, the EPOS-developed sound delivers deep bass and premium clarity.

Game across platforms with ease when you have the EPOS H3. This gaming headset works on Xbox One, PS5, softphone, MacOS X, Nintendo Switch, and others. What’s more, you can lift the boom-arm mic to mute it.

Get it for $119 on the official website.

From gaming headsets to headphones for leisure listening, you can get a great product without blowing your budget. Which ones did you like the most? Let us know your thoughts in the comments.

Want more tech news, reviews, and guides from Gadget Flow? Follow us on Apple News, Google News, Feedly, and Flipboard. If you use Flipboard, you should definitely check out our Curated Stories. We publish three new stories every day, so make sure to follow us to stay updated!

The Gadget Flow Daily Digest highlights and explores the latest in tech trends to keep you informed. Want it straight to your inbox? Subscribe ➜",5329
"Comfortable and effortless, with minor issues.

These have proven to be some of the most convenient and comfortable headphones I’ve ever worn. The pads are made with a breathable mesh and my ears fit into them without touching the drivers themselves. The sound is also excellent, with a good dynamic range that highlights details and provides bass without overdoing either. I have a few minor but regularly annoying issues with these headphones. Connectivity: They lack a 3.5mm headphone out, there is no USB audio to use these wired to a device, and they charge with a Lightning cable … For latency-critical work on my MacBook, it is annoying I have to use this cable which does not charge the device unlike comparable headphones … These headphones lack a user-accessible power switch or toggle … They automatically pair with all of my Apple devices and switch between them depending on which device I’m actively using. The latency is good when watching YouTube or other content and they sound wonderful.

Rating: 4 /5

By Alex, sourced from Best Buy",1051
"What is Slogging?

Your Slack? Insightful Words every day by your highly intelligent people.

Your Company’s Blog? Not so much.

​Every day, amazing tech companies have tons of high-quality conversations. Conversations that could shape the future of the Internet. Conversations that prove the intelligence of your people and the values of your company. And yet, most tech companies are only publishing a blog post a week or less. Lots of remarkable content is lost to the ether forever, never marketing what you’re about. Slogging is about elevating the best conversations you’re already having.

Big Data Jobs

The future of tech publishing revolves around transparency and distribution. To up your rate of quality publishing, Slogging will empower you to curate and distribute your best organic internal discussions via Hacker Noon.

How Slogging works:

One command to curate your most marketable Slack discussions into Hacker Noon stories.

Step1: Have a Great Slack Conversation. If you’re a tech startup this is constantly already happening.

Step 2: On the Slack Thread, Admin commands “…” -> Slogging. You can also add a suggested headline after the command.

Trending AI Articles:

Step 3: The Thread Becomes a Beautifully Formatted Hacker Noon Draft. Via Hacker Noon, you can review/edit the story as much as you want before submitting, or automatically submit from Slack.

Step 4: Editors Review Your Tech Conversation for Publication in Hacker Noon. Your tech conversation has the opportunity to reach our 3,000,000 monthly readers. You own the content and can publish it anywhere else too

How to Start Slogging

Slogging is currently in beta and you can sign up via the waitlist here.

Don’t forget to give us your 👏 !",1732
"When I lived in Indonesia and the Philippines for close to a decade, I saw poverty, injustice, and inequality that many Westerners would not believe exists.

It was heartbreaking in the beginning. I used to well up when I saw women sleeping at the side of the road with their young children. I went into a Kampung (village) to teach English voluntarily and encountered a child covered head to toe in boils and infected spots for lack of a cream that costs $10. I have seen a teacher jailed and accused of sexual assault because some corrupt politicians wanted to shut down the school he worked at in order to seize the land to build on. I didn’t witness it myself, but I have heard the stories about Suharto and Marcos. Both men stole billions of dollars from their people.

This doesn’t happen because of a lack of resources. It happens because of corruption. Corruption kills, and there’s no way to say it any other way. It is cancer in the body of global society. It costs children their lives, forces women into sexual slavery, and with enough time, causes entire states to fail.

I never thought there was a cure for corruption other than some benevolent dictator coming along. Like Singapore, which was fortunate enough to have a Lee Kuan Yew. However, such men are few and far between, and even Lee Kuan Yew could not govern Indonesia or the Philippines the way he governed Singapore. They’re too big, with different islands run by almost entirely independent governments that resent and refuse to answer to central powers.

And then I discovered Bitcoin.

No, I’m not talking about the BTC pyramid scheme that allows corrupt criminals to move funds without being traced. I’m talking about the original Bitcoin (BSV) — the immutable global ledger where every transaction is traceable, trackable, and leaves an evidence trail.

I’m certain that Bitcoin is not the only solution to corruption. It needs strong laws and a willingness to enforce them, too. However, Bitcoin can play a role in that. Once the world wakes up to what this technology really is rather than simply focusing on the speculative price of tokens, it will change everything.

Imagine the following scenario as it will illustrate what I’m trying to say.

In the Philippines, PhilHealth (the country’s medical fund) has a known wallet or series of wallets. The taxes raised by the people are dished out by the government into said wallet(s). All wallets it interacts with are known and KYC checked, and since Bitcoin is a public blockchain, all of the funds can be watched, traced, and tracked by journalists, anti-corruption campaigners, and anyone else who cares. If funds are transferred to unknown wallets, it will be time-stamped and provable in court. NOBODY, no matter how powerful, can change the ledger. Investigations can then take place to bring the corrupt individuals to justice. Nodes can also be ordered by law to freeze and seize funds. That’s why Bitcoin nodes must be known.

Can you imagine what this would do for that one organization? Scandals like this would never happen again. Scandals like these aren’t just about stolen funds. They are about people being murdered by thieves. They cost children access to vital medicines. That’s what the corrupt thieves involved in the PhilHealth scandal are; murderers.

Now imagine this at scale. Not just one organization or corporation, but the entire world’s financial system. Everything can be private where necessary because not all wallets have to be known to everyone, but public bodies such as government departments can be known to all. This would allow for an unprecedented era of transparency and progress on eliminating corruption.

I am going to dedicate the rest of my life to bringing about a world built on Bitcoin. All I can offer is education about how it works and connections for those ready to build on it. It struck me like a lightning bolt from the sky when I first realized what it was. It may sound strange to those who don’t fully understand it yet, but I believe Bitcoin is the biggest and most world-changing invention we will witness in our lifetimes. It’s also currently the most misunderstood technology.

BTC and other ridiculous speculative bubbles will die. But Bitcoin as a global public ledger will live on. It can be one set of books for the world. No more Bernie Madoffs, and no more corrupt dictators siphoning off funds into bank accounts while children die on the streets of resource-rich countries. What’s a better thing to dedicate your work to than that?",4534
"Jeff Bezos Is the World’s Most Dangerous Politician

Six Ways to Get Rich

A few years ago, I read a book about how to become a billionaire. The author analyzed the Forbes 400 and discovered that there were essentially only a handful of ways that every single person on the list became extremely wealthy.

The first and most popular way is to inherit money and avoid taxation. Think: the Waltons, the Basses, the Koch brothers, the Barclays. Other ways included conglomerating an industry, monopolizing a technology, exploiting a natural resource or vulnerable population, and engaging in cronyism — manipulating government and the judiciary to extract advantageous personal systemic benefits.

If this list seems sad and depressing, you’re not alone. While ideally most of us would like to live in a compassionate meritocracy where the best and brightest reap the greatest rewards without leaving the masses to languish, the facts are incredibly clear: you have to leverage an advantage and hurt others in order to control extreme wealth. Extreme wealth is simply never merited.

And it’s always been this way: John D. Rockefeller’s Cleveland refinery was on its way to bankruptcy until he extracted his illegal railway kickbacks. Nikola Telsa got absolutely screwed by Thomas Edison. Andrew Carnegie literally murdered his employees for asking for a pay-raise.

So which strategy did Bezos use?

Jeff Bezos has used almost all of these wealth-amassing strategies to some extent, except inheritance, (His parents, who are still alive, gave him $245,573 to help start the company.)

Many fanboys believe Bezos is rich because of Amazon’s “innovative business model,” but they couldn’t be any further from the truth. In reality, Jeff Bezos has become one of the world’s largest hoarders of extreme wealth because he was the first person in online tech to fully weaponize financialization in order to destroy his competition and decimate democracy.

The cold hard fact is that Amazon wasn’t and isn’t significantly better than all the other online shops of the past and present, he just had access to more capital, and the discipline to use it to strangle them all to death and then leverage their position to extract additional gains from the commons.

It’s a model that many tech companies are now using to devastating effect.

How to monopolize an industry

Jeff Bezos was the first person to really perfect the insidious art of financialization. The lethal gambit is simple:

Find a way to “disrupt” an industry of real producers. (IE Amazon went after book retailers, Uber/Lyft went after taxi drivers, Airbnb went after house owners, Facebook went after content creators, etc.) Create an attractive site/app, drive a massive wedge between suppliers and customers, then charge a fat broker’s fee to re-introduce them. Build a huge amount of media hype to attract colossal amounts of debt and private equity across several rounds of funding. Rather than paying a dividend to shareholders, use their original investment and their annual profits to strengthen your team and strangle your competition. Make sure the story of your rapid expansion overshadows your mounting losses. Once you’ve destroyed the competition and have finally started to turn a profit, use the money to swallow up-and-coming competitors and coerce democracy to extract advantages that small companies can’t get.

This is exactly what Amazon did. For 58 quarters — 14 years from its IPO — Amazon lost money year-on-year in a bid to destroy real businesses.

Another example of an extremely predatory company is Airbnb. They lost $135 million in 2015, $136 million in 2016, $70 million in 2017, $16 million in 2018, $674 million in 2019, and $696 million in 2020, with total cumulative losses since 2008 totaling $2.8 billion.

In previous generations, such a dismal record would make a company worthless, with investors avoiding it at all costs. In today’s fairy tale/nightmare of destructive financialization — paired with huge amounts of hype and publicity — Airbnb is seen as a brilliant and important investment, with stock speculators assigning them an initial public market “value” of nearly $100 billion. In Amazon’s case, it’s now “worth” $1.5+ trillion.

Amazon and Airbnb aren’t anomalies. Lyft lost $1 billion the year before it went public and is now “worth” $15+ billion. According to one IPO specialist, “In 2018, 81% of U.S. companies were unprofitable in the year leading up to their public offerings.”

To be clear, these aren’t ground-breaking companies with brilliant original patents on world-changing inventions. They’re marginally better innovations with a metric ton of debt and private equity to crush all comers.

John D. Rockefeller was the first to do what Jeff Bezos has now perfected:

“One of his favored methods was to give competitors “a good sweating.” A good sweating consisted in lowering prices in selected markets to such an extent that all competitors either went broke, sold out to Standard Oil or otherwise agreed to abide by the rules set forth by Standard Oil, i.e. John D. Rockefeller.”

If this sounds like a giant, anti-meritocratic unethical fraud, it’s because it is.

And it only gets worse from there

Now that Amazon is a public company, Bezos doesn’t have as much access to an ongoing war chest of new funding with which to shatter lesser-capitalized companies. But his unethical options are many:

He can continue acquiring profitable companies like Whole Foods, IMDB, Zappos, Goodreads, Audible, and dozens of others and use their cash to fund his endless expansion. He can continue to degrade civil society by evading taxation and offshoring huge amounts of company revenue. (Amazon is now considered the worst in the world.) He can pull an Elon Musk and continually dilute shareholder stakes through multiple rounds of stock-splitting and secondary offerings.

But that’s not enough for Jeff Bezos.

Despite already controlling a vast amount of the online retail market, and the grocery world, and web services — Amazon has run the CIA’s servers for years — Bezos has shifted to a very old, but new-to-him strategy:

Cronyism.

Greatness

Ida Tarbell, the courageous female journalist in the early 1900s, was sick and tired of watching John D. Rockefeller’s Standard Oil monopoly shred apart thousands of small businesses, crush competitors with financialization, secure illegal advantages, evade taxation, amass immoral amounts of personal wealth at the expense of labor and the masses, and undermine democracy by lobbying politicians and running their own puppets in every judgeship, senate seat, and congressional position. Tarbell decided to drag his shadowy company into the light.

Rockefeller’s attack on democracy was legendary: senators beholden to Rockefeller lobbied the President to lease them drilling rights on Osage Indian lands. Rockefeller helped his business partner Johnson Camden get elected to the Senate; the man then quit the company but kept his stock. Rockefeller helped get Senator Payne elected, even though his son was Standard Oil’s treasurer.

In the end, it took Ida Tarbell scathing exposé and Teddy Roosevelt’s people-first administration to start the decade-long process of unwinding Rockefeller’s sprawling monopoly.

Like Rockefeller, Jeff Bezos knows how to use politics to destroy his competition, wrest power from the public, beat the courts, rent politicians, and slowly but surely destroy democracy on a scale Rockefeller never could have imagined:

And Bezos is just getting started. Amazon’s numbers are still growing exponentially.

Ida Tarbell, to her credit, acknowledged Rockefeller’s genius where it was due. She stated clearly that the Standard Oil Company was legitimately great, but that it was also illegitimately great.

The same can be said of Amazon.

Amazon is legitimately great. It is also illegitimately great.

What is Jeff Bezos?

At first, he was a brilliant innovator.

And then he became a successful capitalist.

And then he became a predator corporatist.

And now he has become an anti-democratic statesman.

Jeff Bezos is, undeniably, the world’s most dangerous politician. Why? Because, like John D. Rockefeller’s Standard Oil, Jeff Bezos’s Amazon has set a foul template for those who follow.

When you look at the big picture, you see that corporatism now runs the world. So-called “democracies” have been corporate-captured, the majority of legislatures and statehouses and court benched occupied with the well-funded puppet politicians of the corporate state. No one does it better than Big Tech, and Bezos is king of the sector. Bezos perfected the art of financialization as a way to ruin competition in the Internet age, and now he is doing to democracy what Rockefeller attempted in his day. And hundreds of other companies are happily following suit.

Jeff Bezos built an interesting company, but then he crossed a line — went over to the dark side if you will — when he decided to engage in menace economics and democratic destruction.

When Will Jeff Bezos Stop?

Saint Augustine said that the human heart can only fully be satisfied by one thing aside from God himself: everything. All the sex, all the money, all the power, all the glory, all the possessions. All of it. Nothing short of everything could ever fully satiate the human heart. We are wired for more.

Sadly, people like Jeff Bezos simply will not stop until they are stopped.

There are only three ways to break the power of politicians like Jeff Bezos

The first way is to legislate change. This was a nearly impossible task in Rockefeller’s day, and it took independent-minded Thor-like politicians like Roosevelt and Taft to shatter the monopoly. Let’s not pretend there’s a mainline leader in the Democrat or Republican party who cares half as much for democracy today. Teddy famously returned a check from Standard Oil for $100,000. By contrast, the Donald was backed by 99 billionaires, and Joe Biden is backed by 131. The partisan system simply will not save us this time around — Joe Biden seems like a very kind man, but he is clearly a corporate shill. Until America wakes up and finds a way to vote in a third party leader who will remove corporate money from politics, this avenue is effectively closed.

The second option — and it should never be an option — is violent insurrection. In Rockefeller’s day, out-of-work oilmen lit his refineries on fire, blew up his oil depots, derailed his oil cars, and attempted to bomb the gigantic mansions of his board members. Violence, of course, is not an option today — not only is it deeply immoral, but corporate elites control the police, military, and surveillance apparatus and will continue to use them to violent effect when uprisings start.

The third option — which is almost certain to fail — is to defund predator companies themselves. This is extremely hard to do when they’re the only option because they’ve bankrupted or bought out all the other brands. The reality is that most of us are co-conspirators in the rise of Jeff Bezos and oligarchs like him, in the same way that earlier Americans had to warm their houses with Standard Oil because Rockefeller controlled 90% of the market. This is why the future needs to be local. Perhaps the only thing that can depose the global corporatocracy is sustained radical mass economic resistance, including:

The widespread use of cryptocurrency.

The mass adoption of cooperative business models.

A global boycott of predator companies.

State and city-level laws that protect individuals, small businesses, and local democracy from the assaults of multinational conglomerates.

A mass tax strike.

In all likelihood, we may have already reached escape velocity — there may very well be no way to stop Jeff Bezos and the corporatocracy from destroying democracy and consuming the planet. In fact, that’s almost certainly part of Jeff Bezos’s long-term master plan.

But one thing is for certain: if companies like Amazon and politicians like Jeff Bezos use capital and technology to bankrupt billions of workers, there may be no force on earth that can protect them from the fury to come. Snakes tend to bite when there’s a boot on their neck.

Our first step is to help the world understand how dangerous Jeff Bezos and his class have become. Only when we’re aware of the problem can we change our global trajectory.

As Bezos’s own newspaper says: Democracy dies in darkness. This is true — and he should know it — because he’s the one with the knife to its throat.

But the opposite is also true…

Corruption dies in the light.",12618
"Sketch Versus Figma — Symbols & Components

Both Sketch and Figma use symbols as a way to repeat elements within a design across multiple pages so that you don’t have to go through each page and make changes to each component. As a designer, this helps to streamline things more efficiently.

In Sketch, these are called Symbols. When you create a Symbol, it puts them on a separate page. For instance, after you make a button, your original Symbol gets moved to the Symbols page, and a flattened version ends up left behind. If you need to edit or change the color or shape of that button, all you have to do is go to the Symbols page to adjust, and everything will get updated to that particular element.

The symbols in Figma are referred to as Components. The main difference between the two platforms is that Figma’s Components are saved where you originally created them and are not moved to a separate page. When you want to make a new instance, you simply make a copy of the master Component, then edit where needed, and it will update. Figma’s way of handling Components is not as organized as Sketch, but its one clear advantage is that it’s simpler to use.

Sketch Versus Figma — Comments

Figma has a built-in comment section, which is great when giving feedback or suggestions to a design when working in a team-based environment. If you need to add notes to a particular design regarding a modification, then you can easily do this. Basically, it works similarly to a Google doc. Once those edits have been reconciled, you can update the file to being resolved. If you work remotely, this added feature is definitely a bonus.

Sketch does not offer this in-house commenting system, and you have to download a plug-in that makes commenting on others’ work possible. Keep in mind, it does have a small one-time fee attached. If you want to go the free route, you can download Zeplin, and use that integrated feature to collaborate with your team.

Is it possible that Sketch will update its platform to include an integrated commenting section? As Figma becomes more popular, then yes, I do believe Sketch will seriously consider this integration.

Sketch Versus Figma — Desktop App & Browser

Another difference between the two platforms is that Sketch is a desktop app only available for Apple computers while Figma is a web-based app. If you need to collaborate with your team, you will have to make sure you’re connected to the internet.

Figma does offer a desktop app, so if you’re designing offline, then this is possible. However, one thing to be aware of is that if you are working offline, you won’t be able to open up a new file, only the ones you have currently.

If you are working where you don’t have secure internet access, using Sketch might be your best bet, so long as you are using an Apple computer.

Sketch Versus Figma — Borders & Strokes

Both Sketch and Figma have the option to add outlines or padding around various objects. Though, Figma calls these Strokes and Sketch refers to them as Borders. Both do exactly the same thing just different jargon is used. You can alter the thickness of each outline around any object using the right side panel box in both platforms.

Borders in Sketch:",3229
"Inside a DOMContentLoaded event listener, I declared the variables for my classes and ids: ‘like-btn’, ‘comment-form’, ‘ul’, “list” globally.

For the like button, I wanted to create a way for the user to interact with the page while still being able to stay on theme. Going back to our index.html, I used querySelector and getElementByID as a way to get the class for each of my declared variables. Then I made two main event listeners, one for the element id “like-btn” and one for the element id “comment-form”; identified here as “button” and “list” respectively.

Like Button

For the like button, I created an event listener that is attached to the “button” variable. Inside the event listener, I then created an ordered list item variable (“li”) and declared globally that “i = 0”. I then added the onclick event for when the user clicked the button, they are able to see how many people liked hating puppies. The reset button was added because I refused to have sprawling lists without clean-up.

Comment Section

For the comment section, I created an event listener that is attached to the “list” variable. Inside the event listener, I then created an ordered list item variable (“li”), added preventDefault, and created a delete button. For the delete button, I appended it to the “li” so the user could delete their comment.

Counter, Add button and Reset for Counter

It wouldn’t be a Cruella De Vil website without a little math right? I added a counter variable, along with an add button to increase the counter number and a reset button to restart the count.",1573
"The simple answer is, you are habituated to using it.

The elaborate answer is exceedingly interesting!

Stuck to my phone by Ajeetha

Habit creating products develop the bottom line of a company. Businesses who are trying to build products that are a better version of an existing app are not quite successful, the reason being, people lean back to an already existing app as they are used to it. In accounting it is called the LIFO- Last in first out, meaning the latest habits you learn are the first to go out. New behaviours have a short life that users usually revert back to their old ways pretty soon! This is also backed by a shred of impressive evidence, Two- thirds of alcoholics who go to rehabilitation pick up a bottle of alcohol within a year. Research also shows nearly everyone who loses weight by dieting puts on all those pounds back within two years. Hence the saying, old habits die hard!

You may ask why this happens? Your brain is an amazing machine that creates neural pathways when you’ve been doing an activity for over years, like remembering the route to your apartment or brushing your teeth every morning. It’s very instinctive and there is no pressure to remember these things as your brain has constructed pathways over time. It’s almost like a default setting. Isn’t that amazing?!

The more frequent a behaviour is repeated the stronger a habit becomes. It’s also one of the reasons why most of us just google to answer any question instead of taking some time out to think, go through the memories stored in our brain to answer it. We just GOOGLE! That there, is a habit creating a product.

Some of us want to have a digital scrapbook of our lives and not miss out on anything happening around the world, that’s why we use Instagram. Some of us are interested to see how Bitcoin is performing or live in fear of losing money and hence we keep checking apps like Coinbase.

CREATING A PRODUCT? ASK THESE QUESTIONS:

What problem am I trying to solve for the users with the products I’m creating?

How do the users currently solve that problem and why does it need a solution?

How frequently do you expect the users to engage with your product once they are habituated?

What user behaviours do you want to make into a habit?

It’s also important to know that if a product is just being created without answering the above questions and just improving the design of an already existing product, most likely they won’t sustain in the market. It’s about understanding what problem you want to solve for your users and these conversations should be the crux/ nucleus /heart of your product.

HOW TO TRIGGER YOUR PRODUCT INTO EXISTENCE?

A trigger is an external stimulus that prompts a user to be exposed to the product and its features. It is very similar to an oyster pearl. A pearl is formed by disturbance, by grit, sand particle or dust. External triggers give instructions or a nudge to the users by showing them the information to act on (Call to action). Like paid advertising, word of mouth, viral videos or also by push notifications only when enabled by a user. An external trigger is something similar to an alarm clock to which one wakes up.

An internal trigger however is something that is tied to an emotion or a pre-existing feeling to do something without any compulsion… the feeling of uploading a picture on Instagram because you feel good today.

Negative feelings can form a strong internal trigger, feelings like, I feel bored let me watch YouTube videos, I feel lonely let me text or video call my friends. The fear of missing something “FOMO”, as we call it is also an astounding internal trigger. When a product is able to find these needs and fill the gap; external prompts/triggers are no longer required. The usage of products almost becomes second nature as a bond of association is established between the user and the product.

HOW TO BUILD YOUR PEARL?

Jack Dorsey once said, he employs user narratives in building twitter and square. It’s unusual what storytelling does, it’s a crucial skill I believe should be used in every field of work as it helps one to shift from a product or feature building mindset to understanding user pain points and easing them. The reason why storytelling is powerful is because of the simplicity and emotion with which everyone in your business, right from the helping staff to a CEO can understand it.

HOW TO BUILD ON TRIGGERS? — An EXAMPLE

Empathy maps in design thinking are one such great tool to build a product.

My favourite would be the Toyotas 5-why method as it drills to the core of the pain /problem - With every “WHY” both the question and answer become so clear.

Example: Let’s say I am building the first-ever rickshaw service in London similar to Ola auto. This is how I’d go about my 5-WHY’s guiding the user narrative:

1) Why would Lilly travel in an Auto?

Ans: Because she wants to spend less time waiting for a cab.

2) Why would she spend less time?

Ans: Because she is travelling short distances & wants her journey to be hassle-free.

3) Why would she want it to be hassle-free?

Ans: Because in a city like London, many cabs drop her at the lane entrance instead of the doorstep.

4) Why would she not want to be dropped at the lane entrance?

Ans: She feels it’s an inconvenient travel experience for a short-distance trip.

5) Why would she feel inconvenient?

Ans: She fears she might be late for her meeting because of all the extra walking.

Asking these 5- why’s directs a product to reach the user's pain point or internal trigger, in this case, it is fear of not being on time.

And just like that, I have a product I’d build for users who’d want to have a faster short-distance trip and enable them to reach every doorstep they wish to set their foot on.

Source: Combined my ideas with snippets inspired by Hooked",5852
"We at Balderton are delighted to announce our investment in arive, backing two exceptional, free-spirited founders and Co-CEOs Max and Linus. arive is pioneering a new way for consumers to access the brands and products they love with near-instant delivery in select cities across Germany. Let everything you love arive.

Over the past 10 years, we at Balderton have seen a generation of consumer e-commerce success stories such as Depop and THG. Depop demonstrated that the connection between consumers and brands can become more intimate online, and THG demonstrated the importance of fulfilment execution. arive has both.

arive founders Linus and Max

We have always wondered what the next generation of internet-enabled consumption might look like. arive is a logistical play, but it’s also much more than that: it’s a home to lifestyle brands with a carefully curated assortment of lifestyle products that arrive at your doorstep in 30 minutes. Themselves late Millennials and Gen-Zs, the founders alongside the core team have built a customer-loved assortment rapidly, ranging from Aesop liquid soaps to Izipizi Sunglasses, Ava & May Scented Candles, Apple Air Tags and much more. The founders have executed exceptionally well over the last quarter, aided by their relentlessly practical — and yes, scrappy and rapid — approach. The entire team at Balderton is incredibly excited to be backing Max and Linus in building the next-generation consumer champion.",1465
"The Anti Selfie Selfie Club

Today we’re excited to announce the launch of Poparazzi, a new photo sharing app focused on preserving authentic moments with friends.

Over the last decade our feeds have become increasingly filled with edited, seemingly effortless perfection. When we post about ourselves, we naturally gravitate towards sharing only the most exciting moments in our life. We over-edit photos and write witty captions in an attempt to portray our best selves. The result: a competition for attention where nobody wins. The more we scroll, the more we feel like we don’t stack up. And so the cycle continues…

But life is made up of many unperfect, perfect moments that are worth capturing and sharing. Like watching your friend stuff their face with a burrito, or catching your significant other looking like a snack. Unfortunately, the pressure to appear as though our lives are perfect prevents us from sharing these moments with the people we love.

Poparazzi has entered the chat…

Leave your selfies at the door.

We built Poparazzi to take away the pressure to be perfect. We did this by not allowing you to post photos of yourself, putting the emphasis where it should’ve been all along: on the people you’re with. On Poparazzi, you are your friend’s paparazzi, and they are yours.

Your profile on Poparazzi is created by your friends when they take photos of you. On the flip side, you create your friends’ profiles when you take photos of them. The person(s) in the photo is always in control so you can remove any photo from your profile that you don’t like. You can also toggle your account to private, which means only people you approve of can take photos of you.

Poparazzi also helps you stay connected with your friends when you follow them. Throughout the day, you’ll see photos your friends are taking of each other (you might even see photos of yourself!). You’ll feel like you’re with them, even when you’re not physically together.

When we launched our beta a few months ago, it instantly resonated with our community. We hit 10,000 beta testers in just a few days and our AppStore page quickly racked up over 500,000 pre-installs. We realized that by making social media less about promoting ourselves and more about sharing moments with friends, people actually ended up sharing a lot more — and the content shared was refreshingly authentic. Over 100,000 photos have been shared on Poparazzi so far, and we’re just getting started.

Our community showed us the potential Poparazzi has to be a special place on the internet for authentic expression — and we’re so excited to finally make it available to everyone on the AppStore.

You can download it here.

Poparazzi is all about capturing life’s most authentic moments, instead of the staged ones. It’s about hyping up your friends, instead of competing against them. After all, life is a team sport and we’re all just hurling through space at an unimaginable speed. Let’s take the edge off.

Let’s get it poppin’",3004
"After the deep impact of the coronavirus pandemic, different companies are boomeranging and trying to pick up the same pace. Crypto enthusiasts are seeking to spend their digital coins traveling abroad.

The high volatility in cryptocurrencies and the recent market crash have raised alarming concerns among blockchain and crypto investors and industries.

“The world’s leading cryptocurrency-friendly online travel agency (OTA) found that 38% of Americans plan to travel in the summer of 2021. And found an increased desire among people to use cryptocurrencies to pay for their vacations” — Report

Crypto Tourism

In 2018–19, several tourism companies arranged special travel programs for cryptocurrency and blockchain enthusiasts. Such types of travel programs include luxurious cruises, which offer marvelous service to cryptocurrency holders.

For Example Scotland-based crypto wallet provider coins bank arranged 4 big crypto tourism cruises. Blockchain cruises, arranged by the tourism of Edinburgh.

Before the pandemic, the recent tourism program was in Europe in 2019. It was five days, estimated 2,500 crypto participants were there and sailed in the Mediterranean territory.

After this Covid-19 pandemic happened and all the travel industries including crypto tourism ceased in 2020.

In a simple sense, I explain that crypto tourists are travelers who use crypto assets to pay for their holidays. Like companies in New Zealand offers travel program where tourists could use crypto coins to see adventurous places.

It is basically a replacement for fiat currency.

Additionally, countries like the Bahamas, Greece, and Cyprus are crypto-friendly and accept digital money from their tourists.

Recently, El Salvador becomes the first country to adopt bitcoin as a legal tender. If more adoption happens in near future, it will be a historic event for cryptocurrency tourists.",1888
"Many of you might have come across the functionality to export data to a CSV/JSON in your day-to-day applications. It is a very common utility, and it’s a better practice to handle the complex logic on the Backend and render the data as desired to the client.

It’s straightforward to build when the dataset to be exported is small and static, i.e. you need to render the data from one source to another.

Scaling for larger data-sets

The complexity increases significantly when dealing with a large dataset especially when it needs to be processed on the fly to generate the final output.

For eg, passing your raw data to an ML algorithm/ or validating it against some predefined rules to get some insights. These algorithms may change with time or are dependent on some real-time attributes, making it impossible to pre-compute and serve saved data.

I faced this situation in one of my projects. The project was about processing health insurance claims and involved a huge amount of data (related to Patient Claims). The dataset includes a large number of attributes (almost 200) in our database models with millions of rows.

What was the product requirement?

Usually, the application dealt with one claim in a request when the User was working with the frontend.

But then the product team asked us to build a feature that would enable a user to export the output as a CSV. The CSV could potentially include hundreds of thousands of records. The CSV had to be generated once the user requested it through a download button on the application frontend. Generating a large CSV in response to an API request is bound to run into several issues.

Issues we faced during development

One of the issues was naturally the memory consumption on the backend and database server in handling even one of these requests. And moreover, loading the dataset and doing computations in memory wasn’t going to scale well.

Additionally, HTTP requests have a default timeout limit of 60 seconds. Yes, increasing the limit on Nginx/ Gunicorn is an option, but that isn’t an elegant solution and doesn’t solve the memory consumption of the backend servers, and may also result in hanging connections.

There were two main approaches to consider:

Go Async:

The server stores the request to generate the CSV by the user in a message broker like Redis or Rabbitmq, and a Celery worker actually performs the task in the background.

Here we dedicated a server (which we can enable only for a specific time during the day) for this purpose. Once we had the final output in the desired format, we shared a relevant link to the file with the user.

This will be an ideal solution if you’re dealing with audit logs/ secondary data, which might not be an immediate need from a business perspective.

But if the requirement states you to render the response immediately, which was our use case as our data is time-critical, I had to choose an alternative approach.

Streaming the response in real-time:

Streaming HTTP Response not only helps in avoiding in-memory data caching but also reduces the TTFB (Time to First Byte). This usually happens when we download movies, some static file, etc.

Our Implementation:

As our dataset was huge (at least a few hundred MBs), we needed to ensure that fetching the data from the database didn’t spike memory consumption on the backend server as we continued querying and processing the raw data.

Solution: We used Iterators (like Generators in Python) to process and fetch the data incrementally that do not allocate memory to all the results simultaneously.

2. The huge dataset could also drag the database server if all data is fetched at once.

Solution: To avoid load on the database server, we used pagination and kept querying the database in batches.

Here’s a sample code that fetches user data in batches using Python’s Generator.

from math import ceil

from django.core.paginator import Paginator

from api.serializers import UserSerializer





def get_user_data(user_query):

batch_size = 1000

total_users = user_query.count()

paginator = Paginator(user_query, batch_size)



for page in range(ceil(total_users / batch_size)):

paginated_users = paginator.get_page(page)

data = UserSerializer(paginated_users, many=True).data

for item in data:

yield item

3. Few data points were a bit intensive to calculate (and were dependent on static data points), so we invested our time on the feasibility of pre-processing data and saving it in the database, thus avoiding an intensive task at run time.

A classic example of this would be a banking application, which provides User’s spending on various sectors (Health, Entertainment, Fuel, etc.). Here if all the transactions are pre-populated with a category to which it belongs, it makes the calculation quite simpler than evaluating the category for each transaction at run-time.

4. We scaled up our servers just a little and through the measures above, we were able to stream a data request of 4000 rows in a few seconds. But the response time increased as the data under consideration increased and response time took 4–5 minutes if a user requested a large number of records(50k rows).

Solution: We limited the number of records that a user can request at a time for run-time delivery, and the Async approach otherwise.

Even though we still had to scale our servers and put a few restrictions, the implementation served our goals for the time being and gave us time to look for better solutions. The different approaches we experimented with along the way gave us a clearer understanding of modern scaling techniques and their particular use-cases.",5644
"(This is Part 8 of a multi-part series by Passage Technology: Reinventing Your Business, Reimagining Your Salesforce — see Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, and 7. In other entries for this series, we’ll be going in-depth on topics such as how Salesforce Admins can employ new approaches and empower their business partners. In addition, how businesses can streamline operations and objectively evolve strategic decision-making, ensuring group buy-in and productive innovation. And finally how to execute efficient project planning in the Salesforce environment, to be agile while achieving your goals for reinvention.)

Project management touches just about every aspect of how organizations achieve their goals, and it can have a huge impact on costs. The State of Project Management 2020 reports that project management challenges cost businesses $109 million for every $1 billion invested in a project (roughly 11% of all project costs), and one in six IT projects has a cost overage of 200%.

One of the major factors impacting project management is communication. The report shows that, each year, poor communication can cost businesses with up to 100 employees approximately $420,000, and businesses with a staff of 100,000+ employees more than $62 million. And the possibility for costly miscommunications is increasing as the remote workforce expands.

The good news is that you can improve communication by establishing project timelines, goals, and responsibilities with project management team collaboration software. A Pulse of the Profession study from the Project Management Institute found businesses that have implemented a project management structure experience 38% more success with projects than those without a structure in place.*

Managing Projects in Salesforce

Are you onboarding new employees? Integrating systems? Launching a new website or opening a new office? Salesforce can track all of the individual inputs and data points that go into your project, and AppExchange apps can help extend Salesforce’s project management capabilities. Project management apps like Milestones PM+, available on the AppExchange, can help you transform your Salesforce org into a project execution platform.

As internal technology teams implement new business processes and enhance or integrate systems, they need to document their work and stay on track in terms of budgeting, scheduling, and transferring knowledge.

Has the desired functionality been documented? Do you have a testing plan to make sure that users don’t experience bugs or usability problems? Whether a technology project involves Salesforce project management, mobile app development project management, or creating a new customer community, each project will have milestones and tasks within them that need to be completed and tracked.

By creating a project in Salesforce for each IT project type using Milestones PM+ templates, you can set your team up for success to meet their goals while facilitating efficiency, collaboration, and ensuring a smooth transition for user adoption. Following are examples of how organizations used Milestones PM+ from Passage Technology to achieve their goals with technical projects.

Project challenge #1: integrating core business systems and processes on the Salesforce platform

One of Britain’s leading energy companies, nPower needed a tool for managing projects across multiple work streams to integrate all of their internal sales processes. They were also implementing their credit system in Salesforce, which included an integration with Experian and Attradius.

The configurability of Milestones PM+ allowed them to take a controlled, iterative approach to improving program governance. By helping them apply structure to their Salesforce development processes, the app enabled them to adjust their project management processes to accommodate output growth without the loss of performance or quality.

Project challenge #2: historical data and systems from multiple companies are integrated into one Salesforce org

Uniti Fiber is the fiber infrastructure segment of Uniti, a publicly traded Real Estate Investment Trust (REIT). As a result of expansion, the Systems Operations team at Uniti Fiber was balancing the challenge of handling an influx of feature requests from all departments while integrating new companies with multiple other systems. They had also tripled their number of Salesforce users in just three years to 600 and were in the process of converting to Lightning in their Salesforce org.

Understanding costs is critical when you build out fiber optics and Milestones PM+ helped Uniti Fiber track and aggregate costs for management. They also used Rollup Helper from Passage Technology, which is free on the AppExchange, to aggregate dollar amounts. With Rollup Helper and Milestones PM+, they were able to track costs from multiple departments, and understand the monthly recurring cost savings.

Conclusion

Using a Salesforce project management app like Milestones PM+ gives you the power to transform your Salesforce org into a project execution platform, whether you’re managing large-scale IT projects, opening new locations, onboarding new employees, or managing marketing projects.

To learn more about Passage Technology’s apps and services, Contact Us or visit Milestones PM+ and Rollup Helper overview pages.

Copyright 2021 — Passage Technology LLC — All Rights Reserved — Not for Distribution without Prior Written Approval by Passage Technology

Source: *

The State of Project Management in 2020 [42 Statistics], Saaslist, Business Software Advisor",5630
"Look- I know that this one might be coming out of nowhere. Even I feel a little nervous about putting the T word, which we all agreed should only be ghost pronounced in polite company until 2028, and Mass Production, which the whole of economics will chase you down the street with flags to tell you how good it is, in a cause and effect relationship.

However, as a maker of healing pants, I feel it my duty to try and get to the bottom of the great global north malaise. For a culture that has supposedly subjugated scarcity with industrialisation and enjoys creating museum exhibits about living on Mars soon, we really seem to be a lot more miserable and a lot more miserly than our ancestors might have hoped. A miriad of solutions exist to the climate predicament, from the charmingly simple economic restructuring, to the technocrazed gas bombs. We should have sorted this by now. WE ARE ADVANCED.

But instead we continue on the ultimate lemming mission. Buying and buying and buying totally suicidal amounts of gear, that eclipse every advancement in sustainable production with a curve of consumption that continues to reach for the stars; Perhaps there’s something deeper going on. Perhaps an endless conveyor belt of things that are impressively the same, isn’t actually proof of being totally, completely, brilliant. Maybe, its a symbol of our misunderstanding of the world, and holds one of the keys to sorting this mess out.

The inception of Mass Production was undoubtedly traumatic.

Its your classic, history is written by the victors situation; some capitalists made everyone miserable, and earnt a shit tonne of profit, some of which they devoted to telling people that it wasn’t them who made everyone miserable.

Now look, I’m obviously not in the majority of historians (I’m actually not allowed in the historians club at all), when I tell you that the peasants were about 1 trillion times more happy making stuff themselves, than buying things in a shop. But, honestly, that is at least incredibly likely to be true.* Have you ever knitted something? Which do you prefer- knitting something and either keeping or selling it, or being forced to make your children crawl underneath a moving freight trainesque piece of machinery for a couple of years so you can buy something that a robot knitted (and is not your favourite colour)?

There’s a risk with that analogy in that not many people nowadays have knitted something and know how great it is (I make things for my real job, so this is pure bias talking here) and not many people have that many children nowadays and know that they make you feel guilty about even the slightest inconvenience.

Here’s another one just in case-

Do you prefer playing a computer game, or watching a computer play its own computer game (for a year) so that (at the end of the year) the computer will give you a medium high score?

I’m (attempting to) making multiple points here-

Humans like doing stuff for themselves… EVEN IF IT IS HARD. We like learning things, and doing better next time, and seeing how much we’ve improved, and having something to show for it. We don’t necessarily like doing the same stuff, but some people stopping other people doing the stuff that they like doing is mean. Traumatic even. And that’s what the capitalists (circa industrial revolution) were about- they reaaaaally liked doing stuff like organising and being in charge and making loads of money and they decided to deliberately prevent other people from spinning wool or weaving. The lives of the peasants didn’t immediately go from knitting your own jumper to getting an iced latte and buying a knock off Ralph Lauren cashmere from TK Maxx (TBH knitting is just a metaphor, knitted jumpers weren’t the fashion back then, and the peasants made very complex and technical clothing that is difficult for our tiny 21st century brains to imagine). After the capitalists invented robots that wielded knitting needles (again thats a metaphor), the peasants got REALLY, INCREDIBLY poor. Like, literally sell your own children poor. And the horrible truth is, that mass production is still making metaphorical jumper knitters REALLY INCREDIBLY poor, in most of the world, with no intention of redeeming them with iced lattes and TK maxxs before we all die of climate change. Mass Production just gives you back stuff that you could have made yourself, or swapped with someone, and TBH the peasants were all over that and firmly set out their boundaries; i.e. that they didn’t in fact want to watch a computer playing the computer game. But those dang, capitalists were one step ahead of them. If you’ll revisit the computer game analogy with me- lets say you prefer option a) and would like to carry on playing the computer game yourself, well the capitalist turns around and says- that in fact it’s his computer, and he was only lennnnnnnding it to you and now he wants it back so you’ll have to watch the computer playing it anyway, (except for this analogy to work imagine that computers just occur naturally in the landscape and you can just find one and take it home). Which was known as ‘The Enclosure of the Commons’, the retracting of land rights, or the blatant imperialist take over of Britain, that prevented the peasants from peacefully knitting their jumpers and having a few home brews at the end of a hard weeks super fun knitting.

Generational Trauma with a dram of Personal Trauma to wash it down.

Now look. I’m not a professor of Generational Trauma, but I was a kid once, and once one of my siblings wanted something that they BLATANTLY did not deserve, and my parent said I had to give it to them by invoking the ultimate repudiation ‘LIFES NOT FAIR’ and I still have a tiny bit of trauma from realising, in that painful moment, that life is not fair.

If that thing, that gotten taken away, had been the actual land that I grazed my fave cow on, (and thus provided me and my beloveds with delicious sustenance)… well… that would be traumatic. And it turns out ‘LIFE’S NOT FAIR’ is not a scientific fact like Einstein or something, it was invented by bullies when they did some land enclosing, and forced you to work in a factory (and your children to join a gang of choral pickpockets). And the very fact that the phrase LIFES NOT FAIR, has become part of child rearing vernacular is probably a symptom of Generational Trauma.

And all of this for the ability to mass produce. ‘The ends justify the means’- you cry. ‘Look how much stuff we can make, thanks to the sacrifice of those peasants, who eventually gave up their knitting needles and knuckled down to starving and watching computers having all the fun.’

Way Too Much Stuff

Have you actually looked at how much stuff we make? Have you though? Have you really looked at one of those mountains of clothes that just one street in one town throws away, barely worn, in one year. Do you know whats bigger than that mountain? The mountain of clothes that Burberry made and never even sold in one year. Do you know whats bigger than that? -the mountain of plastic hangers that ‘insert a brand that mass produces’ uses JUST TO TRANSPORT CLOTHES BETWEEN FACTORIES AND WAREHOUSES, every year. (Do you know whats bigger than that? the literal mountain that ‘insert distant relative of the royal family’ bought with the money they got from using slaves to farm cotton). Looking at any one of those mountains makes me want to gouge my own eyes out with a handwhittled spoon, and sacrifice them to a grain of pure sand.

We’re are literally living out the folk story where the magical pot makes porridge and then everyone drowns in porridge because the person who knows the magic words that turn the magic porridge off, well he died. (I’m tempted to put a Karl Marx joke in here, LOL, but I’m an anarcho-syndicalist). “Bankruptcy, Bankruptcy, Bankruptcy”. In a debt culture you can’t turn off the porridge pot, without going bust, and as for your fall back plan of jumping off the hamster wheel and just growing your own vegetables and foraging your own fire twigs… see above under ‘Enclosure of the Commons’.

And like, that’s just the traumatic history, the trauma of being born without the right to eat, the trauma of buying and selling people to grow enough cotton to feed a minotaur of a cotton spinner that can spin cotton 24 hours a day, the trauma of buying that minotaur on credit with compound interest, the trauma of bullies taking all your stuff, and taking your time, and taking your children’s time and not even caring that you’re actually damn good at knitting.

There’s more!!!!-

There’s the trauma of earning your £8 an hour (which was not an easy amount of hourly money to prize from the hands of our good old friends the capitalists TBF), and going down that high street shop, and picking out a pair of jeans that are not your favourite colour, but they’re vogue’s favourite colour so maybe you like them too, and putting them on your perfect bum (that grew all on its own from two cells using codes that it made up itself in the middle of a star millenias ago)- and your perfect bum doesn’t fit in those jeans, even though your thighs do and your waist does. And now you’re being forced to question if your bum actually is perfect (stardust and millenias and the improbability of amino acids forming genetic material aside).

When the reality is- those jeans don’t fit anyone, they fit a made up picture on a computer, and then a robot made probably 300 of them, despite them not fitting anyone (except in a few random examples) especially for them to end up, discarded after two uncomfortable wears, in an embarrassing mountain of clothes next to a disgusted George Monbiot. I don’t want to be a total leftist floozy over here (particularly as I am an anarcho-syndicalist) but that experience, over and over again- the overwhelming evidence that despite it all- all the boredom, and the disenfranchisement, and the vulnerability to shocks in the housing market, despite all of that, those jeans aren’t for you. That is rubbing salt in a centuries old wound.

One Size To Fit Them All (after they replace us with robots)

Mass Production doesn’t even grant you a pair of jeans that fit (its not its fault, it just literally can’t because it has to make everything the same, and everyone isn’t the same). And yet, those magazines and billboards and philosophers and historians (#notallphilosophersandhistorians) will gaslight you to kingdom come. They’ll insist that you are lucky that you don’t have to knit a jumper, in your favourite colour, to the measurements of your own unique and perfect body. (Or as a gift for a favourite Aunt in exchange for a beautiful chair that is also your favourite colour and fits your delectable ass just spot-oningly). Whilst you’re looking in bewilderment at your ass, they’re already drinking in that sadness, like crazed self-esteem vampires (Selfestampires), repackaging it and selling it back to you as another pair of jeans, or a diet, ideally both. LOOLLLLOLLLLOLLLLLMFAO ALL. THE. WAY. TO. THE. BANK.

Lucky you that you don’t have to knit your own jumper. Luckily you don’t have time to anyway.

*(P.S Peasants didn’t spend a lot of time writing their feelings down, but they did spend a lot of time fighting to protect their way of life… )",11315
"By Joshua Tan and Luke Miller

Agreements and contracts are everywhere, but they are built on layers and layers of legal and social institutions. Software is slowly entering into this stack. In this post, we introduce the Agreement Engine, an open-source software service for stitching together systems of modular agreement systems, as well as some of the social and legal theory on why and how agreements work.

Agreements are the building blocks of modern societies. Whenever you want to work with other people — whether it’s starting a business, building open-source software, or slaying a (virtual) dragon — you will need to first form an agreement with those people to communicate expectations, safeguard rights, and divvy up rewards. When we change the agreements that people can form, for example through policies that affect communication technology, social trust, or the rule of law, we change the kinds of economies and politics that people produce.

But what determines the agreements that people can form? Or, to put it another way: for a given kind of agreement (legal, informal, computational, etc.), how can we build systems for authoring and enforcing such agreements? Such systems range from a state’s business registration process, to the small claims court system, to the Ethereum smart contract system, to the guild systems of online games like World of Warcraft or EVE Online. In this article we’ll cover the art and science of how to build these systems as well as sticky questions about the user experience (UX) of legal contracts versus smart contracts versus informal agreements. This matters because:

If you’re a lawyer, legal contract systems are important, but legal contract systems are evolving to become more computational.

If you’re a blockchain enthusiast, smart contract systems are important, but the smart contract systems are evolving to become more “legal” — and not just because governments are getting involved.

If you design or operate online platforms of any sort (blockchains, MMOs, and beyond), your platform’s virtual economy will never take off if your users can’t coordinate and collaborate with each other.

If you’re a normal person, neither legal nor smart contract systems are particularly user-friendly or cheap. And that has significant consequences for a just internet.

At the end of this post, we’ll share a preview of Agreement Engine, a software service that converts any web resource into an enforceable contract, and place it within the larger project of building web-native “legal” systems. Agreement Engine is part of Metagov, a cross-platform toolkit for online governance.

What is an agreement, really?

What do the following have in common?

A signed rental contract A one-line email: “cool, meet you at 9pm outside the bar” A (multi-party) smart contract, e.g. this DAO A bid on eBay A gif of two men exchanging a knowing look

Figure 1. Walter and Jessie exchange a knowing look (from Breaking Bad).

Answer: they are all manifestations of mutual assent, a.k.a. agreements. Mutual assent is just a piece of jargon that means “we agree” — it’s an abstract relation or meeting of the minds between two or more subjects where the subjects share a common intention or a common belief. Agreements are manifestations of mutual assent, meaning that they bear witness to the claim that a common intention or belief exists. To put it another way: every agreement constitutes evidence of mutual assent.

Many agreements also incur obligations, where an obligation is an action that one has to do. These obligations usually come with mechanisms for enforcement, where enforcement is the act of compelling compliance with an obligation. For example: an email thread between two friends agreeing to meet at a bar (1) is explicit evidence of mutual assent and (2) creates a social obligation to meet at said bar where (3) the obligation to show up is enforced by an implied social norm: don’t break promises to friends or you’ll lose your friends and/or be reputed as a flake. A bid on eBay, on the other hand, (1) is evidence of the bidder’s agreement to buy the item at that price and (2) creates an obligation to pay the seller at that price if they win the auction where (3) that obligation is either enforced automatically through eBay’s escrow system or through eBay’s unpaid item policy.

Figure 2. A glossary of the many terms in this post.

Introducing the Agreement Engine

The (1)-(2)-(3) pattern of agreement-obligation-enforcement is extremely common in society and in the law. We call this pattern a contract system, and we call agreements that fit this pattern contracts. The Agreement Engine is a tool for building contract systems on the internet. For testing purposes, we’ve used Agreement Engine to build some actual contract systems (including the @AgreementEngine bot on Twitter) that end-users can use to produce contracts. [3]

As a tool for building contract systems, Agreement Engine is designed for two kinds of users: (1) platform operators that want to build up their communities and economies and (2) online communities that want a low-effort, minimal governance system that naturally evolves as the community grows.

Technically speaking, Agreement Engine is a software service that wraps a digital resource (usually text representing an agreement) and connects it to a governance service (usually an enforcement mechanism). We call these wrappers joint accounts, to distinguish them from agreements, and we call the process of wrapping an agreement with a joint account registration. Once wrapped or registered, a joint account can be validated and delivered to an enforcement mechanism through a process that we call authentication. [4]

To be clear, Agreement Engine itself is neither a system for authoring agreements nor a system for enforcing them. However, it has built-in interfaces with both authoring systems and enforcement systems (see Figure 3). In that sense, it serves a broadly similar role to both traditional legal notaries, which are trained to witness paper signatures for legal enforcement, as well as online services like DocuSign or HelloSign, which are designed to record digital signatures for legal enforcement. Unlike either legal notaries or digital signature software, which primarily take written or digital text and prepare the document for legal consumption, Agreement Engine is designed to take written or digital content of any sort and prepare that content for consumption by a range of online authorities including community administrators, digital juries, and automated platforms. In that sense, it is similar to other “web-native” arbitration services like Kleros, Aragon Agreements, or Celeste in that it is designed to operate independently of the traditional legal system. Unlike those services, however, it does not specify a deliberation mechanism for judging contract fulfillment or package an enforcement mechanism such as escrow. Agreement Engine is designed to be a lightweight, configurable component within a modular governance architecture.",7077
"BLOCKCAP PLANS TO HAVE 50K BITCOIN MINERS OPERATIONAL BY 2023

Executive chair and founder Darin Feinstein said the firm plans to make the U.S. a global leader in making blockchain technology mainstream.

One of the largest crypto mining firms in North America said that it has acquired an additional 8,000 rigs and has begun mining other cryptocurrencies.

In an announcement from Blockcap on Monday, the mining firm said it planned to have more than 50,000 mining rigs online by the end of 2022. Blockcap now controls more than 12,000 mining rigs, generating more than 6.6 Bitcoin (BTC) daily — roughly $380,000 at the time of publication. The firm added that it was accelerating its move to mine other cryptocurrencies including Ether (ETH).

According to the firm, the addition of the ETH mining rigs in operation would account for 1.229 terahashes per second, or roughly 0.21% of that of the Ethereum network. The more than 50,000 Bitcoin miners, when fully operational, are projected to control roughly 2% of BTC’s current combined hash rate.

“Blockcap’s growth strategy is focused on bringing various peer-to-peer digital assets directly to the people who will utilize them to improve their lives,” said Blockcap executive chair and founder Darin Feinstein. “We strive to contribute to the critical infrastructure necessary for mass adoption of these digital asset technologies so people can participate more fully in the global financial system.”

Both Blockcap and Riot Blockchain — two of the largest crypto mining operations in the United States — have announced plans to set up shop in Texas for their corporate offices and their mining facilities, respectively. Blockcap raised more than $75 million in two funding rounds led by Off The Chain Capital and Foundry Digital.",1785
"Sponsored Post:

Rare minerals are crucial to the manufacture of mobile phones, electronics and medical equipment, and a shortage could upset the global marketplace and spark wide ranging financial instability, a study warned Friday. The analysis of international commodity exchange networks and how they could be roiled by shortages of minerals was published in the journal science advances.(1) Besides, let’s face it. The new way discoveries are found, how resource targets are proposed, and even how mine development and operations are run could be outdated and virtually unsustainable in today’s world. To see this fascinating news about mineral resources. Explore this one!

Mining could yield a range of benefits to societies, but it could also cause conflict, not least in relation to above-ground and sub-surface land use. Similarly, mining could alter environments, but remediation and mitigation could restore systems. Boreal and Arctic regions are sensitive to impacts from development, both on social and environmental systems.(2) These other sectors claim that it is the only pure generator harnessing AI horsepower to discover what geologists couldn’t with their own eyes. Only about 1 out of approximately 3,333 potential mining average have been exploited in the last few years. Want to dig more information? Read more of it here.

These elements are said to be necessary components of about 200 products across a wide range of applications, especially advanced tech consumer products, such as cellular telephones, electric vehicles, solar panels, wind turbines and computers. Several other public sectors possibly engaged in mineral exploration and development. Could these potential elements affect the economy? Could this indulge people in need? More useful information here. Check disclaimer on my profile.

Source 1: https://www.industryweek.com/the-economy/article/21966311/rare-mineral-shortage-could-upset-global-markets

Source 2: https://environmentalevidencejournal.biomedcentral.com/articles/10.1186/s13750-019-0152-8",2045
"Mineral exploration is the process of searching for evidence of any mineralization hosted in the surrounding rocks. The general principle works by extracting pieces of geological information from several places, and extrapolating this over the larger area to develop a geological picture. Exploration works in stages of increasing sophistication, with cheap, cruder methods implemented at the start, and if the resultant information is economically interesting, this warrants the next, more advanced (and expensive) techniques. sponsored post. However, it is very rare to find sufficiently enriched ore bodies, and so most exploration campaigns stop after the first/couple of stages. To successfully do the work, these mining companies should be helped by an experienced mining team in order to locate enough enriched minerals. (1) In addition, several technologies on mining exploration we’re being studied to aid this problem. Will this make current mining explorations more efficient? Let’s have a peak on this article.

Many countries depend on mineral mining to increase their people’s resources, and their economies are based on finding minerals. Iron, copper, gold, silver, molybdenum, zinc, coal, plutonium, sulfide, tin, chromite, potash, and other minerals were discovered. Geophysical approaches play an important role in mineral discovery, groundwater investigation, and hydrocarbon exploration, according to geophysicists. Owing to mineral exploration activity in increasingly deeper wells, durable sensors that can work in high-temperature and hostile environments are in high demand. Check disclaimer on my profile and landing page. Fiber-optic sensors meet most of requirements of these demanding applications with their high temperature capacity, multiplexed and distributed sensing and small space placement capabilities. (2) Mining explorations may be more systematic than ever before with the aid of these technologies and several mining companies. This technology, as well as the developers who created it, are doing an excellent job!

Minerals and their derivatives are essential for modern life. Minerals are used as raw materials for almost all of the products we use. They embrace the way we live now and how we want to live in the future as we face the many problems that society faces. But we shouldn’t be worry; with today’s technologies and current mineral discovery companies, the mining industry will be in good shape! Let’s keep an eye on it!

Source 1: https://www.intechopen.com/books/minerals/introductory-chapter-mineral-exploration-from-the-point-of-view-of-geophysicists

Source 2: https://www.gsi.ie/en-ie/programmes-and-projects/minerals/Pages/default.aspx",2696
"The rare earths are a group of 17 metallic elements in the periodic table that are found in the center. These metals have peculiar fluorescent, conductive, and magnetic properties, making them extremely useful when alloyed (or mixed) with more common metals like iron in small amounts. The rare earth elements are not very rare geologically. These metals are present in abundance in many areas around the world, with certain elements in the earth’s crust in about the same abundance as copper or tin. Rare earths, on the other hand, are rarely present in high amounts and are commonly found mixed in with other elements or with radioactive elements like uranium and thorium. sponsored post. Moreover, it was said that the global community is in desperate need of rare earth supply, and with these other companies it was said that they might have the solution for that. (1) Do you want to know how could they able to supply rare earth elements? Find out how to do it here.

Rare earth elements are difficult to distinguish from surrounding materials and from one another due to their chemical properties. They are also difficult to purify due to these characteristics. To extract small amounts of rare earth metals, current processing methods need a lot of ore and produce a lot of harmful waste. Rare earth elements are being used more often in modern culture as we gain a better understanding of their special properties. Many popular technologies, such as smartphones, LED lights, and electric cars, contain rare earths. Rare earth elements are used in oil refining and nuclear power, while others are important in wind turbines and electric vehicles, as well as in medicine and manufacturing. Rare earths have become critical to modern life, but most Americans are unaware of our reliance on them. On the other hand, I came across with these interesting companies, it was said that they are an advanced mineral exploration companies focused on metals and elements commonly used in the EV market, military, national security and green technologies. (2) This is something you should be aware of right now. Investigate and learn more about what these other corporations might do with our rare earth elements.

Since rare earth elements are widely used in many of these industries, demand for them is expected to remain high. Check the disclaimer on my profile. Rare earth element-based inventions and technologies are on the way! While we wait for it, we need to learn more about this REE.

Source 1: https://www.sciencehistory.org/learn/science-matters/case-of-rare-earth-elements-history-future",2596
"Mineral reserves are naturally occurring accumulations or quantities of metals or minerals of adequate size and concentration that may have commercial significance under the right circumstances. Ore is a term used to describe economic quantities of metals or other mineral resources. Mineral products are naturally occurring mineral quantities that may be economically extracted. The distribution of mineral deposits is determined by the geological processes that formed them. Mineral deposits are therefore commonly clustered in geological provinces (mineral provinces or mineral districts) with some provinces being strongly endowed in particular mineral commodities. Check disclaimer on my profile. Global demand for metals has improved considerably over the past decade. Geologists are developing new approaches for studying ore deposits and discovering new sources. (1) What should be the effect of these actions taken by the mining industry on mining explorations? Let’s take a look at this article.

Mineral exploration and development are investigative activities prior to mining. The rewards of successful exploration and development can be large, if a mineral deposit is discovered, evaluated, and developed into a mine. For a mining company, successful exploration and development lead to increase that value of the company. sponsored post. For a local community or nation, successful mineral exploration and development can lead to jobs — often well paying — that otherwise would not exist; to new infrastructure, such as roads and electric power supplies, that are catalysts for broader, regional economic development; and to increased government revenues that, in turn, can be invested in social priorities such as education, health care, and poverty alleviation. In order to satisfy the global need for mineral reserves, mining companies are coordinating to each another to improve the mining industry. (2) This is outstanding work from the mining industry! As a result, it is reasonable to anticipate that the mining industry will be in fine condition in the future! Let’s keep an eye on this company!

Exploration and discovery of minerals are data-gathering practices. Mineral discovery and production, in this context, refers to a series of practices that gather data needed to locate mineral deposits and then determine whether or not they can be built into mines. The mining industry and the companies involved are just continuing to raise the standard! I’m excited to see how much this industry will progressed!

Source 1: http://www.ga.gov.au/scientific-topics/minerals/mineral-exploration/deposits-events

Source 2: https://www.miningnorth.com/_rsc/site-content/library/education/Mineral_Exploration_&_Development_Roderick_Eggert_Eng.pdf",2761
"Quantum Computing — Our Optimized Future

In 1994, Peter Shor, a mathematician, developed a quantum algorithm to find prime factors of large numbers so efficiently that it is beyond the best classical algorithm present today.

Essentially both quantum and classical computers try to solve problems, but they use a different way to get the answers by manipulating the data. A standard computer uses digital bits of 0s and 1s, while quantum computers use qubits. A qubit can take any value between 0 and 1. A quantum computer has the power to solve problems that classical computers are not capable of by entangling the qubits. For example, a quantum computer can crack today’s most common security systems — such as 128-bit AES encryption — in seconds. It would take millions of years for a supercomputer to perform the same task.

The US NIST(National Institute of Standards and Technology) has claimed that it will be able to crack existing public-key infrastructure like 128-bit AES encryption by 2029. If this happens then it will make the businesses and governments improve their conventional network security using quantum-key cryptography. With every new innovation, the opportunity to make money is always there. According to HPCwire, the new market for quantum computing could be worth $830mn+ by 2024.

Google last year claimed that it has 53 superconducting qubits Sycamore processor that was able to verify a set of numbers was randomly distributed in just 200 seconds. It also claimed that if the processing would have been performed on IBM’s Summit machine, the most powerful supercomputer, it would have taken 10,000 years. This is a remarkable breakthrough in the quantum computing industry and paves the way forward. We can say that Google had reached a significant milestone towards realizing the immense promise of quantum computers.

If we take a look back in 1945, ENIAC — the first general-purpose digital computer — was released and it could do a task in 30 seconds that a human could do in 20 hours. We have come so far today that our super-advanced classical devices are no longer with vacuum tubes and vast sizes. The devices are compact, portable, and easy to use as today’s quantum computers will be from those in 50 years’ time.

I can’t wait to see the future of quantum computing. Are you excited?",2327
"Learn more about the top universities that specialize in Quantum Computing Research

Quantum Computing has the potential to transform computation by solving certain types of obstinate issues. Only a few companies and start-ups have access to quantum computers that are composed of many tens of thousands of qubits. Some of these computers can be accessed through the cloud.

Quantum computers use quantum to process information in a completely different way from traditional computers. Quantum computers transmute information using qubits (quantum bits), which can be either one or zero, unlike traditional computers that work on binary bits.

In just a few short years, quantum information theory was combined with quantum computing they have evolved from a partial subject into a full class with well-funded programs, which takes place at quantum centers and institutes at top universities.

Google, Microsoft, and IBM continue to invest heavily in quantum computing because they believe it will transform our world by solving problems that traditional computers cannot.

10 Best leading Universities for Quantum Computing Research

1. The Institute for Quantum Computing- University of Waterloo-

Canadian University, University of Waterloo started offering quantum computing classes such as cat adoption agencies and adoption applications. Mike Lazaridis (creator of BlackBerry), funded the institute.

The quantum computing powerhouse employed 296 researchers. It has published over 1500 research papers since its inception. This institute’s strength is its ability to combine academic excellence with entrepreneurial innovation to bring about technology.

2. The University of Oxford-

This university says they are focusing their research on quantum computing because of its huge potential. It has the potential for transforming different areas such as finance, security, and healthcare.

David Deutsch, the university’s founder, first described universal quantum back in 1985. Oxford and the University of York were the first to demonstrate NMR quantum computers. The university remains one of the most prestigious universities in quantum computing.

Also read: Top 10 Best Artificial Intelligence Software

3. Harvard University — Harvard Quantum Initiative-

The mission of Harvard University is to help architects and researchers to investigate and recognize better approaches to change quantum hypothesis into a valuable framework and gadget.

The people group of analysts in the university has a distinct fascination for propelling designing and study of quantum frameworks and applications.

4. MIT — Centre for Theoretical Physics-

MIT is an exploration leviathan establishment. It has a high expanded arrive at that centers profoundly around quantum computing and quantum data. The strength of the university’s hypothetical material science is presently applied to quantum data and quantum computing.

MIT specialists additionally investigate regions like quantum calculations, Quantum data hypothesis, estimation and control, applications, and associations.

Also read: The Top 10 Digital Process Automation (DPA) Tools

5. The National University of Singapore and Nanyang Technological University — Centre for Quantum Technologies-

The university community was established to connect together physicists, specialists, and PC researchers to explore on fundamentals of quantum physical science and to develop gadgets dependent on quantum marvels.

The scientists and specialists of quantum innovation are inferring their revelations in detecting, correspondences, and computing.

6. The University of California Berkeley-

The Berkeley Center for Quantum Computation and Information includes analysts from the schools of Engineering, Chemistry, and Physical Sciences.

These analysts work on head and central issues in quantum gadgets, quantum cryptography, quantum data hypothesis, quantum calculations, quantum control, and the trial acknowledgment of quantum PCs.

7. University of Maryland — Joint Quantum Institute-

The Joint Quantum Institute includes quantum researchers from the National Institute of Standards and Technology (NIST), the Department of Physics of the University of Maryland (UMD), and the Laboratory for Physical Sciences (LPS). Every one of these organizations brings major hypothetical and exploratory examination programs with a devotion to control and send the quantum framework.

Also read: Top 6 Tips to Stay Focused on Your Financial Goals

8. University of Science and Technology of China (USTC) — Division of Quantum Physics and Quantum Information-

The Division of Quantum Physics and Quantum Information centers around the field of quantum data and quantum optics. The division for the most part coordinates towards fiber-based quantum correspondence, quantum memory, and quantum repeater, free-space quantum correspondence, quantum establishment, superconducting quantum computing, optical quantum computing, quantum reenactment with ultracold molecules, quantum metrology, and related hypotheses. Progressed test stages are worked by analysts to lead state-of-the-art research.

9. The University of Chicago — Chicago Quantum Exchange (CQE)-

The Chicago Quantum Exchange is a focal point of analysts showing distinct fascination for modern endeavors and propelling scholastics in the designing and study of quantum data and computing. The goal of the middle is to advance the ID and investigation of quantum data and computing advances and furthermore the improvement of new applications.

The prevalent objective is to apply research advancements to foster drastically new sorts of gadgets, materials, and computing strategies.

Also read: How to Calculate Your Body Temperature with an iPhone Using Smart Thermometer

10. University of Sydney — Australia-

The University of Sydney quantum science bunch accentuates coordinating to the most difficult issues of quantum computing and applying these bits of knowledge to develop new advancements. The scientific searches of the university are locked in with profound modern and enterprising exercises.

The exercises associated with the university range from quantum data and quantum computing, major physical science to innovation advancement.

Originally published at https://www.the-next-tech.com on July 7, 2021.",6324
"So i feel learning quantum technology can become progressively vital as time goes on, thus quantum physics, quantum computation, loads of these problems have traditionally been the type of factor that’s loads of elementary study, it exists inside the workplace but not very ample else outside of that. As years of gone on it is obtaining progressively near to being one thing that really starts to make impact within the globe and because it will, you are going to own to begin group action it with actual systems that exist already. Stuff that is historically lived in additional of the engineering type of apply things like telecommunications, applied science, machine learning, all that sort of stuff and then the quantum school are regarding delivery that existing type of basic base technology. Group action it with stuff that is already reasonably around in engineering and learning the way to use them each along. You use physical science each single day as you approach your life one thing you most likely do not realize as a result of once folks observe physical science together with American state, we tend to tend to emphasize a lot of crazy aspects of it, like particles that may be separated by the scale of the universe and still communicate with one another instantly or once particles may be in multiple places directly or multiple positions, and this tends to form physical science. appear to be this very strange abstract crazy reasonably physics that does not have a lot of to try and do with our everyday lives however this could not be beyond the reality thus this can be an inventory of 5 ways that, we tend to use physical science each single day?

Number one on the list is that the complete basis of the modern world it’s pcs and computers wouldn’t exist if It wasn’t for our understanding of quantum physics? The essential component of computer is that the electronic transistor that could be a little very little switch. it is very helpful as a result of it does not have any moving elements in it that is allowed America to miniaturize it and have billions of them on one chip all change a billion times per second. Transistors use the special electronic properties of semiconductor that will be a semiconductor and so the principles of semiconductors square measure settled by natural science and by the quantum technology. Now electrons in a very semiconductor will solely be in sure energy states and you’ll use these energy states to make on on-off switches. currently you cannot do an entire ton with simply pure atomic number 14, you wish to engineer these materials by adding in additional components like element or atomic number 5 to create materials with totally different energy levels associated by combining those materials you’ll be able to make a junction wherever commonly electrons do not flow till you apply a voltage then electrons can flow and that is however you create an on and off switch.

For more read Please Click on the link

https://besttechnologysite.com/quantum-physics-and-technology-is-that-the-new-world-to-maneuver/",3079
"A game of “Quantum” Tic-tac-toe.

In 1950, Bertie the Brain delighted crowds at the Canadian National Exhibition. It was the first known computer game with a visual display. It was a customed-made electronic version of Tic-tac-toe.

In 1952, Alexander Douglas created the first video game, called OXO, which simulated Tic-tac-toe on a general-purpose computer. It wasn’t likely that anyone outside of Cambridge played it. Ten years later, programmers at MIT created Spacewar!, which was installed on many PDP-1 Computers, including the one pictured here that I played with my daughter at the old STARTUP computer history exhibit in Albuquerque.

Playing Spacewar! in 2012.

Now, in 2021, the students in my class Introduction to Quantum Computing each created their own version of “Quantum” Tic-tac-toe, which can be played on a quantum computer. (We had to keep the Tic-tac-toe legacy alive! But, also, creating games is the best way to learn computational concepts.)

I created my own version of Quantum Tic-tac-toe alongside the students with the help of some UTS Software Engineering interns. My version is pretty simplistic, but it’s meant to be modular so you can easily modify and test out new quantum moves.

Each move adds a gate to the game circuit. The basic moves are as follows:

Measure ends the round and executes the game circuit on the quantum device. The win conditions will be counted and displayed.

ends the round and executes the game circuit on the quantum device. The win conditions will be counted and displayed. Not flips an “owned” tile to the other player. If the tile is not currently owned, this does nothing.

flips an “owned” tile to the other player. If the tile is not currently owned, this does nothing. O turns the initial tile toward being owned by “O.”

turns the initial tile toward being owned by “O.” X turns the initial tile toward being owned by “X.”

turns the initial tile toward being owned by “X.” SWAP swaps the location of two tiles.

As you play, you will see the game board and the circuit. The board shows the sequence of moves but is only indicative. The game circuit is the true state of the game — however, you need to understand a bit about quantum computing to appreciate what’s happening there.

To play the game requires “executing” it on a quantum computer (or simulation).

As of 1 Sep 2021, quantum computers are pretty small and with public access today you can play 2 x 2 Tic-tac-toe. The result of the above game took 10 seconds to run on a quantum device.

Details on the game that was played above and ran on an IBMQ quantum computer.

Okay, Tic-tac-toe, so what? Well, computer games have a long history of development alongside technology. As a teaching tool, games really are amazing. You can play Quantum Tic-tac-toe yourself on a quantum computer using the code below. You could even have a crack at modifying it to create your own set of rules. Enjoy!",2924
"Enter the realm of quantum computing

“ If quantum mechanics hasn’t profoundly shocked you, you haven’t understood it yet” — Niels Bohr

What sets quantum computing apart from the computational herd is, in fact, the three fundamental properties of quantum physics: superposition, interference, and entanglement.

It’s all about Quantum Superposition!

Superposition: (also known as coherence) expresses the ability of the quantum system to exist in all of its theoretically possible states at once. In other words — as long as no measurement is performed, there is a certain probability to have one state AND a certain probability to have another state concurrently. But once we observe, only one result is obtained. Here is an example:

Say we roll a dice with sides numbered from one to six. As long as it doesn’t stop rolling, it takes all the six values — 1,2,3,4,5, and 6. But once landed, it becomes a normal dice with only one value — 1 or 2 or 3 or 4 or 5 or 6.

One other famous example is Erwin Schrödinger’s cat.

According to what we have seen so far; when a qubit is in a state of superposition, it forms a linear combination of an infinite number of states between 1 and 0. However, you will never know which state it will take until you actually look at it, which brings up our next phenomenon, quantum measurement.

Quantum measurement: (also known as decoherence) refers to the act of observing or measuring a quantum particle. This latter collapses or ruins the superposition state and the particle takes on a classical binary state of either 1 or 0 — nothing in-between.

Entanglement — The most romantic notion in physics.

“Spukhafte Fernwirkung” or ”Spooky action at a distance”

That’s how Albert Einstein described quantum entanglement. Weird huh?

Intrigued by the mind-bending phenomenon, he derided the notion, citing it as proof that quantum theory was flawed and incomplete. Is entanglement that baffling?!

While we don’t really want to go down the rabbit hole of entanglement, here is a simple explanation:

Quantum entanglement

Unlike in classical physics, pairs of particles are said to be coupled or entangled with each other, when they form a system such that the state of one particle is dependent on the state of the other particle. Simply put, whatsoever process the particle experiences, it correlates to the other particle as well.

Not to mention that even if separated, across insanely large distances of space and time — say light-years. These entangled particles, however, can remain perfectly connected.

What if a measurement is applied?

Since there is a correlation between the entangled qubits, measuring a state of one qubit provides the information of the other qubit such that if one of them collapses — due to measurement, the other one collapses too.

Now let’s go back to Einstein’s spukhafte Fernwirkung thing.

He believed there must have been something wrong with quantum mechanics. But thanks to scientists, this phenomenon has been proved with countless quantum physics experiments.

Sorry, Einstein, you missed it.

Remember, in the quantum theory, once two particles undergo a shared state, they are no longer separate entities but exist as one. Thus if entanglement reminds you of something, it won’t be other than the experience of falling in love. Does entanglement make sense now?

Quantum entanglement is awesome. Think about it. Entangled particles are bound to each other in a way they aren’t to any other particle in the universe. And when something happens to one, it immediately affects the other.

How beautiful!

Interference: is another incredible property of quantum mechanics, one that makes you think what in the world is going on behind the scenes of our reality. The great physicist Richard Feynman once said about interference:

“The essentials of quantum mechanics could be grasped from an exploration of interference and the double-slit experiment.”

Microsoft researchers defined interference as the intrinsic behavior of a qubit, due to superposition, to influence the probability of it collapsing one way or another.

It’s considered as one of the most challenging principles of quantum theory which states that the building particles cannot only be in more than one place at any given time (due to superposition), however, that an individual particle, such as a photon (light particles) can cross its trajectory and interfere with the direction of its path.",4445
"The stupidest, most technologically and resource inefficient way to multiply by two

Photo by Drew Graham on Unsplash

What you will read here is a bad idea. Don’t try this at home. You’ve been warned. I’ll be using a metaphorical sledgehammer to crack a hypothetical nut. I am trying to do a much, much lamer version of using a drone to change a light bulb.

Using a quantum computer to multiply by 2, morally — Photo by unknown author on Reddit

What I want is quite simple — I want to calculate 2ⁿ, 2 to the power of n, for any integer n. For example, 2² = 4. That is not a particularly difficult problem. You can work it out pretty reliably by hand using techniques learned in grade school. Or, you can find a table online.

This really isn’t that hard—source: Wikipedia

Suppose I asked my digital assistant for the answer, though. I think you will agree that this might be a bit overkill given how much energy and sophistication is necessary to make the cloud work. But, that is not enough. We want the “killing a mosquito with a bazooka” method. For that, we need to move beyond digital technology and consult a quantum computer! That’s right, and I’m going to use a goddamn quantum computer to multiply by 2.

Get Quantum

Great, so for that, we need a quantum algorithm. Let’s get a bit technical for a second. Below is an algorithm that can compute any eight consecutive powers of 2. (Why eight? Well, quantum computers are kind of small right now.) It looks like a standard electronic circuit, with wires and components and such, but it’s quantum. That means, instead of bits of information, it uses quantum bits (qubits). The first wire is a scratch qubit — it’s just there to help with the calculation. The next three are the binary expansion of n — that’s the input data. Below I have input “111”, so I am asking for 2⁸. (The answer is 256, by the way.)

Circuit for raising 2 to some power. Made using the IBMQ Composer.

Cool, cool. So, that looks pretty complicated. We’re on the right track. Now, we just need some magic techno machine to run the algorithm on. Luckily, I have access to a quantum computer. (Don’t be too impressed — you do too.) I will use one of IBM’s cloud quantum computers. In particular, I will use the quantum processor called “ibmq_16_melbourne,” which is a quantum chip made of superconducting material sitting in a laboratory device that is one hundred times colder than outer space.

Specs for “ibmq_16_melbourne” on 6 June 2021. You can find specs for your IBMQ services here.

I ran the circuit above on IBM’s quantum computer 1,024 times. Unfortunately, the desired outcome showed up only 8 times — that’s less than 1% accuracy. The most common answer, showing up about 1% of the time, was 160. The correct answer, recall, is 256. So… this is not good.

Actually, using a quantum computer to multiply by 2. Photo by unknown author on gifs.com.

The quantum Rube Goldberg solution

Alright, so what gives? At the moment, quantum computers are too noisy to perform arithmetic, but that will not deter Progress™. We need to hack this. How can we get this device to calculate 2⁸? First, we need to ask what a quantum computer is really good at (today). That’s easy! So (today), a quantum computer is pretty good at one thing: making random bit strings. And, how many bit strings can n qubits generate? 2ⁿ!

I like this idea. It’s so simple. It presents a little conundrum, though. Imagine we start performing random experiments and recording outcomes with the intent of estimating the total number of possible outcomes, including those we haven’t even seen yet. Seems almost impossible, right? Well, this is the kind of statistics problem Alan Turing and teams of WWII cryptographers faced when trying to crack ciphers. And, when you stop for a moment, you realize that scientists and statisticians estimate the total number of things they haven’t actually counted all the time — the total number of species on Earth, the total number of stars in the galaxy, the total number of lies in a political speech, and so on.

The core idea in estimating totals from incomplete observations is that repeated events give you a lot of information. For example, flip a coin 10 times. Suppose you see H, H, T, H, T, H, H, T, T, T. Without making assumptions, you can’t know for sure that this coin will result in only these two possible outcomes indefinitely into the future. But, the fact that 8 of the 10 events were repeated events gives you a lot of confidence that’s the case. This can all be formalized with some fancy statistics I won’t bore you with.

So, I fired up Old Faithful (that’s what I’m calling it since “ibmq_16_melbourne” doesn’t quite roll off the tongue) and performed the following procedure:

Select a random instruction. Prepare a canonical state of the device. Apply the randomly chosen instruction from Step 1. Measure the system. Repeat 1–4 until r repeated outcomes are seen. Estimate the quantum dimension to be roughly k²/(2r), where k was the number of experiments needed.

Since the quantum dimension is 2ⁿ, where n is the number of qubits used, I would have my answer.

Quantum level accuracy

For my first experiment, I used 8 qubits to calculate 2⁸. The number of repeated outcomes I was looking for was 599. This is the number that would guarantee (with 95% probability) that the answer would be within 10% of the correct value. It took 849 repetitions of the experiment to reach the desired number of 599 repetitions. The answer? 259 (within 10% error of the true value, 19 times out of 20). Not bad! That’s within 2% of the correct answer!

More repetitions are obviously better. So, I waited to see 1,116 repetitions, which took 1,372 experiments. The answer then was 257. Ooo, so close! Then I was intrigued and performed a bunch more experiments with differing numbers of sought-after repetitions. The results are below.

Now, since Old Faithful has 15 qubits, the final step was obviously to calculate 2¹⁵. Again, going for 599 repetitions (which took 6,215 experiments), the answer would be accurate within 10% (with 95% confidence). And… drumroll… 2¹⁵ ≈ 32,242. (It’s actually 32,768 as per the table above, but not too shabby!) The full set of results are below.

Is there a point to all this?

No, not really. But, maybe? I was actually surprised by how accurate it was. This procedure is obviously useless for estimating 2ⁿ. But, it might be useful for quickly estimating the dimension of quantum devices. All that is necessary is to perform completely randomized experiments and look for repeated outcomes. With 95% confidence, one can estimate any dimension within 10% error by finding 599 repetitions. It takes about square-root the actual dimension to find them all (for a fixed number of sough-after repetitions), which is the best one could hope for.",6836
"The greater the functionality of a tool, the less efficient it will be for any given task. Take, for example, The Giant, which earns the title of the world’s most multifunctional penknife. It is a Swiss Army knife with 87 tools. It is over 8 inches long and weighs 3 pounds.

So multi-purpose, it has no purpose. Photo by Slartibartfass.

Even if I had a use for every one of those tools, I still would not buy that knife. Now, if it were only the size of a smartphone, that would be a product worth carrying around. The point this knife illustrates is that specialized tasks are best carried out using specialized tools. If you open a hundred bottles of wine per day, for example, this Swiss Army knife has a corkscrew — but you are far better off buying a machine optimized for opening bottles of wine.

Swiss Army computer

A computer is like a Swiss Army knife, but for calculations. A computer can solve all sorts of mathematical problems. And that’s extremely useful because many everyday problems can be phrased as math problems. Obvious examples are determining how much tip to leave at a restaurant, figuring out what time to catch the bus to arrive early for that meeting, adding up the values in a spreadsheet, and so on. Less obvious examples that are really just hidden math problems are recognizing faces in a digital photo, formatting words in a document, and seamlessly showing two people’s faces to each other on other sides of the world in real-time.

The central processing unit, or CPU, inside your tablet, smartphone, or laptop is tasked with carrying out any possible set of instructions thrown at it. But, because it can do anything, it’s not the best at doing specific things. This is where the other PUs come in. Probably the most famous is the GPU, or graphics processing unit.

Maybe graphics aren’t something you think about a lot. But, even to display the text you are reading now on your screen requires coordination of the brightness and color of millions of pixels. That’s not an easy calculation for a CPU. So, GPUs were made as special-purpose electronic devices which do the calculations required to display images really well, and not much else. The CPU outsources those difficult calculations to the GPU, and video gamers rejoice!

The QPU

There’s another kind of calculation involving the multiplication and addition of lots of numbers which is very time consuming for a CPU. This kind of calculation is essential for solving problems in quantum physics, including simulating chemical reactions and other microscopic phenomena. It would be convenient for these kinds of calculations if a quantum processing unit (QPU) were available. And indeed they are! These are confusingly called quantum computers, even though they are chips sent very specific calculations by a CPU.

You won’t find a QPU inside your computer today. This is a technology that is currently being developed by many companies and academic researchers around the world. The prototypes that exist today require a lot of supporting technology, such as refrigerators cooled using liquid helium. So, while QPUs are “small,” the pictures of them you will see show large laboratory equipment surrounding them. (Scroll back up to cover photo for a reminder.)

What will the future QPU in your computer do? First of all, we could not have guessed even 10 years ago what we’d be doing today with the supercomputers we all carry around in our pockets. (Mostly, we are applying digital filters to pictures of ourselves, as it turns out.) So, we probably can’t even conceive of what QPUs will be used for 10 years from now. However, we do have some clues as to industrial and scientific applications.

At the Quantum Algorithm Zoo, 65 problems are currently listed that a QPU could solve more efficiently than a CPU alone. Admittedly, those problems are abstract, but so are the detailed calculations that any processor carries out. The trick is in translating real-world problems into the math problems we know a QPU could be useful for. Not much effort has been put into this challenge simply because QPU didn’t exist until recently, so the incentive wasn’t there. As QPUs start to come online, though, new applications will come swiftly.

Simulate all the things

My favorite and inevitable application of QPUs is the simulation of physics. Physics simulations are ubiquitous. Gamers will know this well. When you think of video games, you should think of virtual worlds. These worlds have physical laws, and the motion of the objects and characters in the world need to be calculated — this is a simulation. Physics needs to be simulated when designing aircraft, bridges, and any other engineered system. Physics is simulated in science, too — entire galaxies have been simulated to understand their formation. But quantum physics has resisted simulation because CPUs are really bad at it.

Once we can simulate quantum physics on QPUs, we’ll be able to simulate chemical interactions to rapidly design new materials and medicines. We might also be able to simulate the physics at the creation of the universe or the center of a black hole, and who knows what we will find there.

The take-away

Now, you may have come here thinking you were supposed to walk away with an understanding of qubits, superposition, entanglement, parallelism, and other quantum magic you’ve read elsewhere. Those are not useful ways for thinking about QPUs unless you plan on studying for several more years to become a quantum scientist or engineer (and even then you shouldn’t be getting your information from blog posts). The basic thing you need to know about QPUs is the same thing you know about GPUs — they are special-purpose calculators which are good at solving a particular kind of mathematical problem.

If at some point you end up with a job title that has the word quantum in it, it will probably be a software job (much like there are 20 software engineers for every 1 computer hardware engineer today). The most challenging problem a Quantum Solutions Engineer might face is in translating the calculations their business currently performs into problems that can be outsourced to a QPU—and quantum entanglement, for example, won’t be relevant for that.",6247
"In a recent Quanta article, What Makes Quantum Computing So Hard to Explain?, Scott Aaronson cautions,

“To understand what quantum computers can do — and what they can’t — avoid falling for overly simple explanations.”

That’s great advice, but it is also kind of vacuous since you could say the same thing about digital computers, or a toaster for that matter. The problem with the explanations of quantum computing as they exist today is not that they are overly simple — it is that they are overly detailed. Google “how does a quantum computer work,” and you are met right out of the gate with qubits and parallel universes, spooky action at a distance, exponential growth, and — wow — no wonder people are confused.

The entire premise of the statement above is that someone wants to know what quantum computers can do — for them. Yet, quantum computer scientists feel compelled to talk about superposition and entanglement. While it is fine to talk about superposition and entanglement — I’ve stopped people on the street to talk about quantum physics — that’s not what people need to hear about quantum computing through casual Google searches. Aaronson himself makes an attempt in the article to explain quantum computing that avoids his own criticism:

“So a qubit is a bit that has a complex number called an amplitude attached to the possibility that it’s 0, and a different amplitude attached to the possibility that it’s 1.”

This is absolutely 100% correct , but I’m going to wager that this means nothing to someone who didn’t already know that. What is going on here? Why do we try so hard to explain every detail of quantum physics as if it is the only path to understanding quantum computation? The answer partially lies in the illusion of explanatory depth. We have this illusion that we understand things we know how to use. But we don’t. Think about it. Do you know how a computer works? A toaster? A doorknob? If you think you do, try to explain it. Try to explain how you would build it. Use pictures if you like, but I think you will quickly change your mind about how much you thought you understood about even simple technology.

We don’t use quantum computers, so we don’t have the illusion we understand how they work. This has two side effects. The first is that we (the quantum scientists) think conventional computing is generally well-understood or needs no explanation. The second is that we (the quantum scientists again) accept the idea that quantum computing is hard to explain. In turn, this causes us to try way too hard at explaining it, hoping the listener will feel as comfortable with quantum computing as they do with their smartphones.

To see why the “try too hard” approach is a problem, consider an analogy. Imagine our curious friend wants to know what a digital computer can do. Apparently, what the quantum computer scientist would do is start talking about bits of information, logical operators, stored-program architectures, and so on, expecting that the listener would easily connect these concepts together and deduce that the UberEats app is possible. But this is, of course, silly. Instead, what you would want to do is say, “Have you ever ordered food using your smartphone? OK. Let’s explore how your intention to get a nice kale salad gets interpreted by the computer on your phone…”

An even better analogy is the other hot deep-tech topic of artificial intelligence. Search for “what is AI,” and most legitimate explainers will state a generically vague answer and then spend most of the words detailing the existing and future applications. The vague answer given is usually something along the following lines — an AI is an autonomous machine that can learn from known examples and makes generalizations that work for unfamiliar examples. Then, the article will go on to say that AI is used in your digital assistant, to recognize faces in photos, to detect spam, and so on. The reader comes away happy that they know — insomuch as anyone with eight minutes of reading can know — what AI can do.

(By the way, if all someone came here for is an eight-minute read about quantum computers, try this instead.)

I suppose, at this point, the current reader is wondering what the current writer’s grand plan is for solving the world’s current quantum education and literacy problems. I’m glad you asked, as it is innovation myself and other colleagues worldwide are in the midst of creating. For me, it all starts with a change in perspective. When I look into the not-too-distant future, I see quantum software developers who have never heard of the words “superposition” and “entanglement” (much like someone writing code today for the next food delivery app doesn’t use the words “transistor” or “NAND gate”). So with that future quantum software developer in mind, I ask myself what their quantum education looks like and, more importantly, how do we get there from here? (No, not wormholes or flux capacitors.)

I would be remiss to exclude the pun of quantum baby steps. But there are also leaps. Quantum Computing for Babies and Quantum Leaps were an attempt to bring quantum computing to ever-younger audiences.

Others have brought new innovations for introducing quantum computing to general adult audiences. For example, Andy Matuschak and Michael Nielsen have created Quantum Country, which is best described as an introductory textbook with interspersed questions that will automatically be reasked based on how often you answer them correctly (spaced repetition for the cognitive science aficionados).

The first set of quiz cards in Quantum Country.

Brilliant — an app that teaches topics through active problem solving — has a Quantum Computing course. Note that it requires a premium membership to fully enjoy.

BLACK OPAL is an app from Q-CTRL that is currently in private beta which includes highly interactive exercises for learning quantum computing.

In-app with BLACK OPAL courtesy of Q-CTRL.

Quantum Atlas is a multimedia encyclopedia hosted at the Joint Quantum Institute, which is maintained by a large National Science Foundation-funded group of scientists and science journalists.

Seemingly orthogonal to all of that is quantum games, many of which are designed for the purpose of teaching quantum computing. The best-produced example is the somewhat unimaginatively named Quantum Game.

Demo screen from Quantum Game.

Speaking of games, I recently wrote about how I’ve changed the way I teach quantum computing to undergraduate students through game development.

I would have agreed with almost everything Aaronson said several years ago. (In fact, he and others have been saying the same thing for ten years.) But I’m kind of bored of that narrative. In fact, I would argue that quantum computing is not hard to explain. All we need is a different perspective.",6854
"Yes, here I go again with the bugs. Bugs are still one of the most frustrating things to encounter when you are coding. I know I find them frustrating to some extent. All you coders out there will agree that there is nothing quite like building something that you think will work only to have it break in front of your face.

But, do not get me wrong: bugs are not entirely terrible. You can always learn a thing or two when trying to debug any program. Some of the most rewarding experiences I have had involve fixing a nasty bug. That feeling you get when you watch it melt away at your fingertips is priceless. There is a famous phrase that goes like this:

“If debugging is the process of removing software bugs, then programming must be the process of putting them in. — Edsger W. Dijkstra”

It is pretty snarky, but I agree with the fact that most programs are not without bugs.

It is tough to know every scenario and state that your code will be running when you are writing it. Because of this, bugs are a natural side effect of coding. Unfortunately, that does not make it any less frustrating to deal with said bugs, especially if they are responsible for completely breaking your program.

With that said, there are skills and methods in debugging that help make the process go a little smoother. In this piece, I will share five of my tips on how to debug easily. Most of these tips come from David J. Agans’ textbook “Debugging: The 9 Indispensable Rules for Finding Even the Most Elusive Software and Hardware Problems.” Some of them are from my experience dealing with bugs. Let’s get into it.

1. Reproduce the Behavior Consistently

Reliably reproducing bugs is half the battle. You have a much better chance of identifying and fixing a bug if you can reproduce it the same way every time. When you identify a bug, do not even think about the code that might be having the issue. Just think of the code as a black box and figure out what inputs may have caused the bug. Once you can reproduce it, shorten the steps it takes to reproduce it. Think of the simplest thing that you could do to make the bug appear. This helps reduce the number of variables when you are trying to fix the bugs Simplify, simplify, simplify. Once you can reproduce it, write down the steps on how you did that, especially if there are a lot of them. You will be glad that you did, in case you forget.

2. Divide and Conquer

Given that you can reproduce the bug consistently, you now have to figure out which part of the code causes the error. This is where you have to try to go deeper into a specific area of the code. One to five lines of code is about the level of accuracy that you should be trying to achieve. So, where do you start? Well, depending on the codebase and your level of fluency with it, you might know right away.

In case you do not, I have some steps that may help:

First, find any strings that you can search for. For instance, there might be a string literal in the UI, or maybe the error message is unique. Search that entire codebase with that unique term. You will get a sense of what neighborhood or part to focus on.

Second, ask your team members where to look, in case you are working with other people on the same codebase. They will not think less of you, I promise. Trust me, doing this will save a lot of time.

Lastly, set breakpoints and print statements. This is especially helpful if you have already narrowed it down to a general area of the codebase. Set breakpoints and place print statements in different parts of that area to get a snapshot of the values and states that could give you a better idea of where the bug could be.

3. Survey the Landscape

After narrowing down a general area of the codebase, go through the code within that area. Getting a thorough understanding of what the code is doing allows you to make a well-informed change so that you don’t cause more bugs as you try and fix one bug. So, make sure to read the code carefully and figure out answers to questions that may pop in your head. Why was it written like this? When does a specific line of code run? Why is that variable or function necessary in that implementation? If there was an error message, then read that very carefully too. Question your assumptions — because you may be making some assumptions about the code that may not be true.

Another trick is to pin the timeline by looking at various paper trails. What were the changes made since the last time it worked? Take a look at your version control history. At this point, you are acting as a detective for what went wrong. Do a role play if you will. Anything that makes you understand the code is worth the time. Logical thinking is one of the skills that you need to hone for this part of problem-solving.

4. Make Changes One Step at a Time

Speed and efficiency are good qualities to have as a coder. When it comes to dealing with bugs, however, you need to be thorough.

“Bugs are delicate.”

Frustrating but delicate nonetheless. Changing too many things at a time won’t help you pinpoint the issue. You can even decide to change just one thing, then run the program to see if the bug behaves the same way. Change something else and rerun the program. Rinse, lather, repeat.

If it is possible, create a code sandbox. See if you can replicate the same bug by pulling that part of the code into something uncomplicated. It could be a standalone program, an app, a class, or whatever makes sense for your environment. That gets rid of all of the other variables. Doing so will help you concentrate on that part of the code, and it makes it easier to change only one thing at a time by getting rid of all of the other distractions.

5. Get a Fresh Perspective

Have you ever been stuck for hours or even days trying to fix the same thing that you can no longer tell if you are hungry or thirsty? It is crucial to know how to recognize when you are in this state because, after a certain period, it is actually harmful to hammer at the same thing with no results.

Albert Einstein said: “The definition of insanity is doing the same thing over and over again and expecting different results.” With that said, take some time off and get fresh eyes on the bug. If you are working on the same bug for longer than a day, it is time to take a break and look at it from a different viewpoint.



You can even try and get input from your friends or colleagues. Your friends can introduce you to new perspectives or new problem-solving methods that you may not have considered before. I know it is tough, but do not be afraid to ask for help. Teams are better when everyone can work to fix problems together. Whatever lessons you take away from this bug can be shared with somebody else. Sure, it might take more time to fix the bug, but it will be OK. If you are an introverted person, go the rubber ducky way. I do this all the time! Thinking out loud helps a lot and clears your train of thought.",6974
"Do Not Use Print For Debugging In Python Anymore

What is the most frequently used function in Python? Well, probably in most of the programming languages, it has to be the print() function. I believe most of the developers like me, would use it to print messages into the console many times during the development.

Of course, there is no alternative that can completely replace the print() function. However, when we want to output something for debugging purposes, there are definitely better ways of doing so. In this article, I’m going to introduce a very interesting 3rd party library in Python called “Ice Cream”. It could create lots of conveniences for quick and easy debugging.

A Bad Example

Image by Krzysztof Pluta from Pixabay

Let’s start with a relatively bad example. Suppose we have defined a function and we want to see whether it works as expected.

def square_of(num):

return num*num

This function simply returns the square of the number passed in as an argument. We may want to test it multiple times as follows.

print(square_of(2))

print(square_of(3))

print(square_of(4))

This is OK for now. However, we will have much more lines of code in practice. Also, there could be many print() functions that print different things into the output area. In this case, sometimes we may be confused about which output is generated by which print() function.

Therefore, it is a good manner to add some brief description to the content of the print() function to remind us what it is about.

print('square of 2:', square_of(2))

print('square of 3:', square_of(3))

print('square of 4:', square_of(4))

It is much better now, but it is too tiring to do this every time. Also, when we finish the development, very likely have to remove most of the debugging prints.

Basic Usage — Inspect Variables

Image by StockSnap from Pixabay

Let’s have a look at the Ice Cream library. How it solves the problems that were mentioned above?

First of all, we need to install it from the PyPI repository simply using pip .

pip install icecream

Then, let’s import the library as follows.

from icecream import ic

Now, we can use it for everything we want to print as debug information.

Call a Function

We can directly use ice cream to print a function just like what we have done using the print() function previously.

ic(square_of(2))

ic(square_of(3))

ic(square_of(4))

Great! We never specify anything in the ic() function, but it automatically outputs the function name and argument together with the outcome. So, we don’t have to manually add the “brief description” anymore.

Access a Dictionary

Not only calling a function, but Ice Cream can also output everything verbose that is convenient for debugging purpose, such as accessing a key-value pair of a dictionary.

my_dict = {

'name': 'Chris',

'age': 33

} ic(my_dict['name'])

In this example, I have defined a dictionary and try to access a value in it from its key. The Ice Cream output both the variable name of the dictionary and the key that I was accessing.

Access Attributes of an Object

One more example, let’s define a class and instantiate an object from it.

class Dog():

num_legs = 4

tail = True dog = Dog()

Now, let’s use Ice Cream to output an attribute of it.

ic(dog.tail)

Debug in If-Condition

Image by silviarita from Pixabay

The Ice Cream library is not only useful for inspecting a variable, but also in a control statement such as an if-condition. For example, let’s write a simple if-else condition as follows.

input = 'Chris' if input == 'Chris':

ic()

else:

ic()

We just put the Ice Cream function in the if and else blocks, see what happen.

Although the if-else statement does nothing at the moment, the ic() function still tells us where and when it has been called, as well as the line number.

BTW, I’m using Python Notebooks for this demo. If this is running in a “.py” file, it will also tell us the file name that it was called from.

Let’s consider a more practical usage as follows.

def check_user(username):

if username == 'Chris':

# do something

ic()

else:

# do something else

ic() check_user('Chris')

check_user('Jade')

The function will do something for different user. For debugging purposes, we always want to know which is the current user. Then, the ic() function will always tell us that.

Insert Into Existing Code

Image by StockSnap from Pixabay

This cool feature of the Ice Cream library needs to be highlighted in my opinion. That is, the ic() function will not only output the verbose information but also pass the value through so that it can be a wrap of anything. In other words, we can put the ic() function to anything in our code without affecting it.

Let’s keep using the sqaure_of() function that we defined in the previous section.

num = 2 square_of_num = square_of(ic(num))

In this example, suppose we have a variable num and we want to calculate its square. Instead of square_of(num) , I put the ic() function out of the variable num . Therefore, the value of the variable num is printed, and the result assigned to square_of_num will not be affected.

We can test the result as follows.

if ic(square_of_num) == pow(num, 2):

ic('Correct!')

Therefore, square_of_num equals to the square of the variable num . Also, in this if-condition, I also used the ic() function without affecting the purpose, but the variable square_of_num is printed for debugging!

Disable Ice Cream

Image by Free-Photos from Pixabay

One of the biggest issue when using the print() function for debugging is that there are too many of them. It is very common that we have them everywhere when we finished the development. Then, it would be a disaster if we want to clean our code to remove them.

If we’re using the Ice Cream library for debugging, what we need to do is simply disable it.

ic.disable()

After that, all the ic() function will stop output anything. For example, the code below will output nothing.

You may ask that how about the variable square_of_num ? Will it still be passed through if we disabled the Ice Cream function? Don’t worry, the disabling feature will only disable the output, we don’t need to worry about any other features.

if ic(square_of_num) == pow(num, 2):

print('Correct!')

If we change the output back to the print() function, it still can be output. That means the ic(square_of_num) still equivalent to square_of_num .

Of course, if we want to go back to the debug mode, the Ice Cream can be re-enabled.

ic.enable()

Customising Ice Creame Output

Image by Jan Vašek from Pixabay

The Ice Cream can also be customised for its output. The most commonly used customisation would be changing the prefix. You may have noticed that the default output always has the prefix ic | . Yes, we can customise it.

For example, we can change it to Debug | which makes more sense for its debugging purpose.

ic.configureOutput(prefix='Debug | ')

ic('test')

In fact, rather than a static string, the prefix can also be set to a function. For example, let’s define a function that returns the current timestamp in a formatted string.

from datetime import datetime def now():

return f'[{datetime.now()}] '

Then, we can set that function as the Ice Cream prefix.

ic.configureOutput(prefix=now)

ic('test')

Summary

Image by Seksak Kerdkanno from Pixabay

In this article, I have introduced an awesome 3rd party library for Python called “Ice Cream”. It enhanced the regular print() function of Python with verbose output. Therefore, it makes debugging very convenient.

The Ice Cream library will never replace the print() function, because it is designed for debugging purposes. Also, it does not mean to replace the logging system as well. In my opinion, it is in between these two. Check out and try it out!

If you feel my articles are helpful, please consider joining Medium Membership to support me and thousands of other writers! <- Click",7918
"I was sure about what I thought. Then a teacher gave me a lesson.

Our son was visiting from the East Coast, and one of his old college friends, having settled in San Francisco, had come by to see him. Mindful of the surge, they sat outside on the back deck, drinking, eating and chatting. We geezers stayed out of the way.

But on his way out after dinner, the friend stopped in the kitchen to talk with us. He’d become a teacher, and with headlines about San Francisco school controversies (mural destruction, removal of school names tainted by charges of racism, enforcement of race ratios in enrollment and more than everything, closings and reopenings) in the news all year, there was lots to say about how he was.

I was certain of my own opinions, I thought. A few days before, I’d signed a petition to recall the San Francisco School Board, which seemed to me more concerned with arcane issues of political correctness than it was with making a plan to get the kids back to school. I launched into my by-now burnished rant about getting the kids back into school, complete with speculation on the many kids who’d never return and criticism of the teachers’ union for finding reasons to stay away.

Politely but with certainty, our guest said he had another point of view. He leaned against the stove and talked about it. One of the places in which he teaches is a middle school with a tough reputation.

“I think the opportunity to not be on campus all the time has been beneficial for some students,” he said. “Because not all students at every school site experience a safe environment, being away from constant bullying.” His remarks made me flash back to the misery of middle school. Even in the tame bobby-soxer ’50s, I and everyone else I knew worried: Did we look okay? Were we popular? Were our friends the right ones?

The teacher settled into his stance, as though he were in front of a classroom.

At the time the pandemic started, he said, one of his students “ was an eighth-grader. I had him for 7th grade and for that year. He was one of those children just a little bit on the prickly side; he took things wrong.

“You would try to be encouraging, say some things that would give him some hope. But there were things that would trigger him. He would be standoffish, mean.

“A lot of times you chalk it up to just being young. As the year went by, my second year of having him, I was watching him with the (other) students, I witnessed a couple of fights that occurred. It was definitely a situation where I witnessed him being bullied, kids bringing up things about his past, whatever he’d done in the past.

“We tell those kids to ignore those things, not to say anything. But it’s just that environment that he’s around so often that doesn’t help. In the classroom, I saw him fight.

At one point, for example, “He came into the classroom. One student who was the school-wide bully, she saw him come in, started calling him names. Very awful mean things were being thrown his way. And he responded in kind, with as much verbal force as he could muster, and there was no backing down from either side.”

Always, “There is an uncontrollable escalation. It is quick. When it is ‘go time,’ it’s go time. Words become actions very quickly. And I just saw that he had to be that way, he had to respond so forcefully just to survive, not to be a further target. He already was a target, but ignoring it wouldn’t make it go away. I saw a couple of altercations like that.”

Then, in the Spring of 2020, schools closed and on-line learning started. “During on-line learning last year, I saw him on my class roster and I was a little scared. I was unsure how he’d be in that environment.” The teacher, still standing by the stove, uncrossed his legs and stood a little straighter, speaking with an almost fierce emphasis..

“And he was incredible. He was an angel. He was always asking for permission, permission for things he didn’t need to ask permission for. … In this environment, he was allowed to be a kid again, without threat from someone saying something to him that would trigger him. It was a controlled environment that let him be young again.”

The student, aggressive and defensive when physically surrounded by classmates, wound up on-line doing many projects on his own initiative, proudly showing them off to the teacher. “The niceness was shocking; he was so open and not defensive.”

During Zoomed sessions, “he was one of the few who always had his camera on, always engaging. A lot of young women would not turn on their cameras because they were afraid that other students would take screenshots and show them online. So many students didn’t want to be seen. They had their cameras off, and that was okay with me.” This boy was eager to be seen, protected by the limits of the computer screen.

What struck the teacher was the difference between on-site middle school learning, where the kids have their guards up because “bullying is problematic,” and virtual learning, where a kid could have the luxury of relaxing. This particular student “would engage with other kids; curious about what the other kids were doing. He would collaborate with other students. It was fun, he was fun. It was a beautiful time.”

In general, said the teacher, students whisper threats and insults to each other, and those “micro-aggressions” hit their targets and evaporate before the person in charge can turn from the blackboard. In an on-line chat during a Zoom, “If they say something that’s inappropriate, I immediately screen-shot it and send it to the parents. In a classroom setting, live, you don’t have that control of kids. You don’t have that impulse-control tool.” A girl might say something in class, “and a boy would say, ‘oh, that’s stupid.’ Every young girl would remember that. On line, you don’t hear anything like that.”

Out from under a cloud of threats, the kid the teacher was describing, liked “being around other people.” But in that bullying situation, physical response is “all he knows. ‘Someone steps to me, then I step to them. So if no one steps to me, then I’m good.’ There’s no part of his consciousness that says this is a world where I don’t have to experience this.” Fighting is “what you do when someone gets in your face. You push back harder. That’s how you survive.

“I know there was a lot of suffering for a lot of people,” he said, “but I think no kid should ever experience the bullying that my student experienced on campus, and he experienced it every day. No parent would want that. The fact that we tolerate it is something that gnaws at me. The impact for kids is really not understood. We are so numb to it, we don’t have the tools to deal with it.”

Is there any way for educators to take advantage of the safe haven virtual learning gives students? “I have no idea,” he said, but maybe some kids would benefit from the option.",6899
"When the coronavirus had first hit the U.S. and other countries early last year, clobbering the economy and businesses, many technology investors could not have predicted the future which is today. With numerous job cuts, broken supply chains, and profit warnings, business models were flipped on their heads. However, with the ongoing vaccine deployment by nations throwing the worst behind us, companies have now started forming digital transformation strategies to recuperate from the crisis.

Predictions for 2021 are catalyzed by the disruptive forces of the COVID-19 pandemic, which are expected to alter the fate of the global business environment for the near future.

“For future-fit IT leaders, the risks aren’t limited to the data center or network outages. Today’s risks include rapidly changing consumer trends that require digital pivots, increasingly complex security concerns, the ethical use of AI”, according to Brian Hopkins, an analyst at Forrester research.

While many businesses have been quick to realize the need for digitizing business processes their business models to survive, some are still making their way through. The rapid digitalization driven by the pandemic will continue into the recovery. The pandemic has been a testament to the fact that the ability to adapt and respond to any sudden shortcomings is a strong determinant to success in a highly digitalized economy. In order to reorganize business operations to align with the conditions the pandemic has left us with, organizations are re-strategizing their IT spending plans.

According to IDC, by the end of 2021, based on lessons learned, 80% of enterprises will put a mechanism in place to shift to cloud-centric infrastructure and applications twice as fast as before the pandemic. The pandemic-driven remote working environment has accelerated demand for cloud modernization and IT automation, leading companies to further investments in the space.

“We’ve seen two years of digital transformation in two months”, Microsoft CEO Satya Nadella.

The remote-work fashion has also handed fraudsters a new and very tempting field of play, leading to a surge in phishing and ransomware attacks. This has accelerated the need for the already booming industry of cybersecurity. According to Forbes.com, 55% of enterprise executives plan to increase their cybersecurity budgets in 2021 and 51% are adding full-time cyber staff in 2021, as companies invest to support remote working and a rapid move into cloud-based software services. The response to the crisis continues to press department budgets and limit resources for other less essential functions — a situation that we believe will direct investments in 2021.

The pandemic-led lockdowns have also affected consumer behavior in ways that have and will continue to spur growth in the field of Artificial Intelligence. As consumers buy more online to avoid the new risks of shopping in stores, they are giving sellers more data on preferences and shopping habits. With the post-Covid 19 resurgences of investment in these sectors, many firms are seeking to significantly step up investments in AI and Data. International Business Machines Corp estimates that only about 20% of companies use such AI-powered technology today, according to a report by Economic Times. But they predict that almost all enterprises will adopt it in the coming years.

The crisis led to many people working from their homes, which companies such as Zoom, DocuSign have benefitted from. Forrester expects remote work to will rise to 300% of pre-COVID-19 levels. The circumstances, however, worked wonders for some technologies and helped create a frothy market for initial public offerings and technology M&A that should extend well into this year. Riding on the remote working environment, Salesforce agreed to buy fellow software company Slack for an eye-popping $27.7 billion, highlighting how important Slack’s workplace collaboration technology has become. With consumers staying indoors, the demand for food delivery services has also exploded. DoorDash, which popped more than 80% in its IPO, valuing the food delivery company at $71.3 billion, was one of the biggest food delivery market debuts. A number of big Silicon Valley companies, including Palantir Technologies Inc and Snowflake Inc, also had blockbuster IPOs, riding on a stock market rally in the second half of the year that was fueled by stimulus money and hopes of an effective COVID-19 vaccine.

Actions adopted to drive the digital journey by the adoption of technologies due to the pandemic are expected to further spur M&A in the field of cloud, AI, cybersecurity, data analytics.

With 52% of executives who pursued digital technologies via M&A saying that the approach exceeded expectations and 45% reported similarly for digital partnerships, 2021 is set to see an increase in deals, corporate venture capital, and partnership investments, according to a report by EY.

As always, entrepreneurs who lead their companies to make the right investments and build strategic partnerships during times of crisis have emerged stronger, through increased access to capital, large global clients, and delivering digital products and services that disrupt the market in a still-uncertain economy.",5287
"It has been nearly a year since Team Luna introduced and then launched Amazon Luna and the feedback we’ve received in early access has been invaluable. We’ve learned so much from the customers who have joined us on the journey thus far and we are committed to continue to make Luna even better. Today, we are excited to announce availability on more devices, an expanded lineup of new channels and games, and new ways to play together with Luna Couch. Luna offers access to play high-quality, immersive games on supported PCs, Macs, Fire TVs, iOS phones and tablets, Android phones, and starting today, Fire tablets and Chromebooks.

Play Luna on Fire tablets and Chromebooks

With Luna, it’s easy to start gaming without the typical lengthy game downloads, software or game updates — just pick up and play. Luna offers some of the biggest games and most-loved franchises like Assassin’s Creed, Rainbow Six, and the newly added Mega Man 11 from Capcom and Hi-Rez Studio’s SMITE, plus new games coming later this fall like Far Cry 6 and Riders Republic from Ubisoft. Fire TV customers can play games on Luna without an early access invitation, and we’re excited to share that this same access will now roll out to Fire tablet customers in the U.S. Simply open the Luna app on a compatible Fire tablet, and start playing. With a touch controller, customers have the chance to try out games on their Fire tablet before upgrading their gaming experience with a Luna Controller or compatible controller.

Play SMITE and Mega Man 11 now on the Luna+ Channel

Exclusive Luna Game Channels

Luna Game Channels combine unique collections of games and unlimited hours at HD resolution on one or more supported devices simultaneously, offering a simple way to discover your next favorite game.

The Luna+ Channel, Amazon’s flagship curated channel, includes more than 95 games for every type of gamer, including Hi-Rez Studio’s popular multiplayer online battle arena title, SMITE, and Capcom’s powered up action-platformer, Mega Man 11, launching today. Customers will find a wide selection of genres as well as favorites like Control, DICE Legacy, DiRT 5, Saints Row the Third Remastered, Yakuza 0, and many more. The Luna+ Channel is available for $5.99 per month (early access pricing). Check out the full library of games here.

Launching today, the Family Channel offers subscribers a carefully curated collection of more than 35 games that are appropriate, approachable, and fun for the whole family, including SpongeBob SquarePants: Battle for Bikini Bottom — Rehydrated, Garfield Kart — Furious Racing, Space Otter Charlie, DreamWorks Dragons: Dawn of New Riders, Transformers: Battlegrounds, SkateBIRD (launching later this month), and with new games added over time. Customers can play Family Channel games, many of which include local cooperative play at HD resolution on up to two devices simultaneously. The Family Channel is available for $2.99 per month. Read more about it here.

Play the Family Channel on Luna today

The Ubisoft+ Channel now offers unlimited access to 30 of Ubisoft’s popular new games, including Assassins Creed: Valhalla, Immortals Fenyx Rising, Watch Dogs: Legion, and Rainbow Six Siege along with franchise favorites like Ghost Recon and The Division. The Ubisoft+ Channel also offers future day and date releases like the upcoming October blockbusters Far Cry 6 (launching October 7) and Rider’s Republic (October 28), and Rainbow Six Extraction early next year. Luna customers that subscribe to Ubisoft+ by September 29 can lock in the current $14.99 a month beta price as part of the Ubisoft+ Founder’s pricing promotion (unless customer cancels or changes subscription). On September 30, the price will increase to $17.99 per month for new subscriptions.

Luna will add a new retro-gaming focused channel soon featuring beloved classic, arcade-style games from publishers like Atari, SNK, and more. Channel subscribers will get to relive classics like Another World, Dragon’s Lair, King of Fighters, and many more in beautiful HD resolution.

A New Way to Play Together

We know gaming is a great social activity, so we’re making it even easier to play and engage with friends and family with Luna Couch. With Luna Couch, you can play Team Sonic Racing and Overcooked 2 — and other local cooperative and multiplayer games — without needing to be physically located together for game night. You can invite friends to Luna Couch and play any of the local multiplayer games across our Luna Channels and get right into the action — plus they don’t even need to be a Luna subscriber to join the fun. Hosting a Luna Couch session is easy. Just start a Luna Couch session, create a session code and share with other players and they’re in. To enter a Luna Couch code, open Luna on a supported device, enter the code, and join. You can also visit https://www.amazon.com/lunacouch to enter the code on PC or Mac on a Chrome browser.

Luna Couch, play together even when you’re apart

More Ways to Get Into the Game

We’re all about making it even easier to game, so for the next seven days (9/9–9/15) Amazon Prime members in the mainland U.S. can play four games on Luna for free, including Resident Evil 7 biohazard Gold Edition, Metro Exodus, Katamari Damacy REROLL, and Monster Boy and the Cursed Kingdom. Prime members will not need to request an Amazon Luna early access invitation to participate and can instantly start their free seven-day trial to the Luna+ channel after trying out the games. Prime members, who are not subscribed to Luna+ can play the four free Luna games within Luna or by visiting amazon.com/luna/ starting today.

Prime members play these games on Luna free for limited time

We’re also celebrating the expansion of Luna to new devices with more games by offering a new gaming bundle for Fire TV. The bundle features the brand new Fire TV Stick 4K Max and the Luna Controller with Cloud Direct technology for the limited time discounted price of $99. The new Fire TV Stick 4K Max is 40% more powerful than the previous generation Fire TV stick and now supports Wi-Fi 6, offering an auto low latency mode to support better cloud gaming experiences.

It’s easy to start gaming on Luna immediately, but to play on more Amazon screens you know and love, Luna now offers Fire tablet gaming bundles, which include the Amazon Fire HD 10 tablet and Luna Controller for the limited time discounted price of $156 or the Amazon Fire HD 8 tablet and Luna Controller for the limited time discounted price of $116. Luna Controllers are also available at a limited time discounted price at amazon.com/LunaController. We are committed to making gaming easier, more fun, more affordable, and available to as many gamers as possible and hope you’ll join us to shape the future of cloud gaming together.

We look forward to adding more channels, games, and features over time, so stay tuned for updates in the future, follow us on Twitter @AmazonLuna, read our AmazonLuna.blog, and play now or request early access at http://amazon.com/Luna/.",7077
"Photo by cottonbro from Pexels

TV as you know it will change in a big way. TV providers right now aren’t targeting their consumers the right way.

Same Ole Same

Streaming services today still use the same methods over-the-air/cable broadcasts used for advertising. They’re putting their offers in front of general audiences. In turn, their marketing is ignored.

We’ve all been watching TV and an advertisement comes on. It’s for a product you’ll never buy. Your attention leaves the screen in an instant. Here are some examples that might sound familiar. You’re a man and you have no use for feminine products. Or as a woman and you don’t need aftershave.

The Best Way to Do It

The best marketing is targeted. Companies like Google and Facebook rake in money because they cornered the market. They get you in front of the right offers. They know what you want and how you want it.

Social platforms and search engines track your use. This helps them understand your intentions. In turn, algorithms are used to serve the content they know you’ll love. They’re able to take in more advertising dollars because of this.

Moving Forward

Streaming services could put the same technology social platforms and search engines use to work for them. This would help to increase revenues. Then they’d be able to market based on race, sexual orientation, gender, location, and age.

It’s shocking streaming services haven’t tapped into targeted advertising more. They’re sitting on a gold mine and I’m not sure they realize it yet. Maybe this will happen once the platforms become more mature.

The rise of streaming services will make future TV more like social media platforms. Serving your recommended list and advertising based on algorithms.

More about the author.

Chad writes a weekly newsletter called Technically Underwood about community building. You can find out more at chadunderwood.com.",1896
"Be ready for the next power outage with the Boyone Wombat 600 Wh portable power station. It powers large appliances and recharges in three ways.

Prepare for long power outages, camping and hunting trips, and more with the Boyone Wombat 600 Wh portable power station. With its large, 162,000 mAh battery, it can charge up to seven devices at once. Plus, it recharges in three ways-solar, car outlet, and EV charger-so you literally never have to be without power.

If you live in an area that experiences extreme weather, one ice storm could leave you without electricity for days or even weeks. Luckily, the Boyone Wombat keeps you ready. This practical power station lets you run power-hungry devices-including refrigerators, CPAP/BI-PAP machines, computers, and more-when the power grid fails. Let’s check it out.

Get all the power you need

The Boyone Wombat can run power-hungry devices. That’s thanks to its 600 Wh capacity. In fact, this survival gadget can power your smartphone, DC refrigerator, drone, electric fan, coffee pot, and more. If it’s an appliance you need and requires 1,000 W or fewer, this 600 Wh portable power station can power it.

Use this power device for a range of appliances

Another great feature of the Boyone Wombat is its 7 ports. This 600 Wh portable power station has 3 USB ports, 1 12 V car charging port, 1 Type-C port, and 2 pure sine wave AL power sockets. These let the power station provide power to a range of appliances and charge 7 devices simultaneously.

Charge this portable power station in 3 ways

Even better, you can be sure that the Boyone Wombat never runs out of power itself thanks to its three charging options: solar, EV charger, and car outlet. EV chargers take 6.5 hours to bring the device up to a full charge, while car outlets take 7 hours.

If you’d rather go the solar-powered route, you’ll need a 150-, 300-, or 500-watt solar panel. These panels charge the Boyone Wombat in 5, 2.5, and 1.5 hours, respectively. So they’re a pretty quick way to run your devices using only the sun’s energy.

Rely on 5 types of light output

When the power’s out, the last thing you want to worry about is finding a flashlight so that you can see what you’re doing with your power station. Thankfully, the creators of this 600 Wh portable power station included 5 types of light output for the Boyone Wombat.

The Slightly Bright and Bright outputs help you add ambient light to whatever environment you’re in. Meanwhile, the SOS, Flash, and Pulse Light can alert others to your location in an emergency.

Choose this backup power device for camping, RVing, and more

Emergencies aren’t the only time you need an extra power source. Camping, RV, and hunting trips become much more comfortable-and safe-with the Boyone Wombat. Imagine camping for two weeks while still being able to charge your MacBook as many times as you want. And it’s the ideal solution for keeping your RV fridge running.

It’s even great for outdoor events. Because, since the pandemic, you’re probably going to and hosting a lot more of those. This 600 Wh portable power station can power the music at your child’s outdoor birthday party or run the warming plate for your buffet at a family picnic.

Store power ahead of time

The best way to get through a power outage is to prepare for it. And, with the Boyone Wombat, you can actually store your power ahead of time.

In fact, the company says that when the device is charged above 50%, it can keep its supply for 6–12 months. So you could totally charge this portable power supply in the fall, before the cold weather, and not have to worry about power for an entire year. Best of all, you’ll breathe a little easier all winter long knowing you’ve got backup power ready.

Enjoy helpful power-saving features

When you’re using a generator for power, you don’t want to waste any of it. For that reason, this 600 Wh portable power station automatically shuts down after a few hours when nothing is attached to it or if it’s running a minimal load. So this useful gadget does some of the conserving for you.

If you’re concerned about losing power during emergency weather or just want to make your time in the great outdoors more convenient, consider the Boyone Wombat. It has enough battery capacity to power large appliances, and, with three charging options, you can always give it power. It also boasts a range of ports, making it suitable for a wide range of devices. Go for this gadget to make sure you’re never without electricity.

The Boyone Wombat 600 Wh portable power station typically costs $629. You can get it for $499 on the official website. What sort of gadgets do you have in your emergency preparedness arsenal? Tell us about your favorites in the comments.",4755
"Calculating Mill Level Deforestation & Carbon Risk Scores Across the Palm Supply Chain

Illustrating the power of the Descartes Labs Platform to explore agricultural traceability challenges Descartes Labs Follow Sep 8 · 5 min read

Zoomed out view of mill-level carbon scores. Lower scores mean less nearby deforested area and estimated forest carbon loss.

According to the World Wildlife Federation, palm oil can be found in almost 50% of all packaged supermarket goods. As demand continues to increase, global agriculture and manufacturing companies are searching for ways to determine risk factors for deforestation and carbon emissions in the palm supply chain. Adding risk factors and associated traceability can help verify sustainable sourcing commitments and provide tools to help non-compliant participants to adopt more sustainable practices.

While there is some sourcing transparency from mills to manufacturers, a high number of third-party growers supply to these mills, comprising a challenge for NDPE compliance. Therefore, companies need to understand probable source locations, land-use histories, and aggregate sourcing practices for each mill in their supply chain. They also need to attribute ongoing deforestation activity to a given mill and supplier.

However, there is currently no widely available information that traces palm throughout the supply chain. And there is no direct way of attributing palm harvested within a plantation to the mill that eventually processes it. Further, concession ownership and permits have little transparency, making it challenging to attribute deforestation to specific growers. Still, there are a few factors that bound the sourcing region for a given palm mill (e.g. distance, transportation network, mill capacity, nearby mill capacity, etc.), that could allow us to estimate the probability and risk of a given mill processing palm from a given plantation.

Palm concessions and mills over a subset of the island of Sumatra on top of a Sentinel-1 radar composite image

Application of Deforestation & Forest Carbon Loss Risk Scores

This post explores a relatively simple method to attribute risk from deforestation and forest carbon loss to individual mills throughout the palm supply chain in Southeast Asia. It applies deforestation and carbon loss risk scores to the entire Universal Mill List maintained by the World Resources Institute (WRI), Rainforest Alliance (RA), Proforest, and Daemeter Consulting.

Our analysis leverages some key datasets and the compute power of the Descartes Labs Platform. The creation of these datasets in a singular development environment natively allows for the prediction of carbon equivalent of deforestation events. Each has been built with the other in mind.

Sentinel-2 and GEDI derived forest carbon composite

Sentinel-2 derived palm area growing mask

Sentinel-1 InSAR derived deforestation detections

Forest carbon predicted using an allometric equation from GEDI TCH at RH 95

Methodology and Replication

First, we show how the methodology works for a given mill. Then we replicate the methodology over every mill in the Universal Mill List. We start by drawing a radius around each mill, quantifying the deforested area and forest carbon lost within the region over a set period of time, and then normalize by the total forest area and forest carbon.

Creating a radius around each mill and displaying deforestation and forest carbon

Total area deforested from July 2020 — December 2020: 3,355 ha

Total forest carbon from July 2020 — December 2020: 575,478 Mg C

Weighted Average Using Inverse Distance

We also create a weighted average over the mill area using inverse distance such that deforested pixels closer to the mill are weighed more than those found farther from the mill.

Creating a simple distance-weighted risk score for deforestation and forest carbon loss.

Calculating Deforested Area Score & Carbon Loss Score

Once we have our totals and our distance weights, we can calculate a deforested area score and carbon loss score for the mill in question. Scores generally range between 0 and 4, with higher scores indicating higher risk

Deforested area score = 2.91

Distance-weighted average of deforested area ÷ maximum weighted average if the entire area was deforested

Carbon loss score = 3.27

Distance-weighted average of carbon lost ÷ maximum weighted average if the entire area was deforested

Applying the Descartes Labs Platform to the Full Universal Mill List

Next, we use the Descartes Labs Platform to apply the same methodology to the Universal Mill List within Indonesia and Malaysia.

Visual Results Using Descartes Labs Workflows API

Finally, we can visualize it all together using Descartes Labs Workflows API.

Zoomed out view of mill-level carbon scores. Lower scores mean less nearby deforested area and estimated forest carbon loss.

Histograms showing the frequency distribution of total deforested area, total carbon lost, and the overall area & carbon scores for each mill

When we zoom in on a particular location, we see the layers for each dataset along with the deforested area and carbon scores for nearby mills. Note that the size of the buffer around each mill has been reduced to prevent overlap during visualization.

Using the Dataset to Inform Supply Chain Risk Assessment

Now that the dataset has been created, it can be used to inform supply chain risk assessment for any company that sources from the Universal Mill List. To summarize, we built a mill-level deforestation & carbon emissions risk score across Malaysia and Indonesia. To do this, we leveraged proprietary Descartes Labs datasets to determine historical deforestation activity and carbon loss potential based on a distance-weighted formula to each mill. This produces a dataset that allows mills to be ranked by deforested area score and carbon loss score to determine focus areas for ongoing sustainability decisions.

Click here to download a datasheet with more details about our Tropical Deforestation Monitoring Package.",6065
"Flying at high speeds is a dream that millions of people around the world share.

Not only is it a thrill to imagine traveling at supersonic speeds or even hypersonic speeds, it’s also tempting to imagine arriving at your destination faster. How fast can you fly — exactly? Here’s a look at how the aerospace industry categorizes the speeds of flight and the type of aircraft capable of achieving those speeds.

Typically, we measure the speed of an aircraft by its Mach number, which is a velocity relative to the speed of sound (approximately 770 mph or 1,239 kmh at sea level). Mach 1 is the speed of sound.

Four general categories define the speed of flight: subsonic, transonic, supersonic and hypersonic. Each is relative to a Mach number.

Subsonic

At this speed, an aircraft is traveling slower than the speed of sound — less than about Mach 0.8. Subsonic aircraft include everything that flies slowly, including all general aviation aircraft, such as the Cessna 172, ultralights, and even paragliders.

Commercial aircraft, such as the Boeing 777 and Airbus 330, and smaller regional jets that have less than 100 seats, are subsonic as well. Most older military jets also fall into the subsonic category. Examples include the F-100 Super Sabre, which was developed in the 1950s and flown by the U.S. Air Force for 25 years.

Transonic

At this speed, an aircraft is approaching the speed of sound but hasn’t yet reached and surpassed Mach 1. At some places on the aircraft the speed will exceed Mach 1, while at others it will be less than Mach 1. There are a handful of aircraft that fly deep in the transonic regime, including the Cessna Citation X and the Gulfstream G650.

The line between subsonic and transonic is blurry. There are even transonic flows on both of the subsonic commercial airliner examples mentioned above. In some cases, you can even see the shadow of the shocks on the upper wing. Click here to watch the shock wave formation on a Boeing 737 in transonic flight.

Supersonic

At this speed, the entire aircraft is experiencing supersonic airflow and traveling at speeds faster than Mach 1. Generally, supersonic speeds range from Mach 1.2 to Mach 5. Boom’s Overture will fly comfortably in the supersonic regime at Mach 1.7.

Rockets, such as the Space Shuttle, fly at supersonic speeds immediately after liftoff and for about 45 seconds until about two minutes after launch. During this time, the shuttle accelerates from Mach 1 to Mach 5.

Many types of military aircraft are also capable of supersonic flight. For example, the F-4 Phantom II first took to the skies in 1960 and exceeded Mach 2. Nicknamed the “Flying Footlocker,” it was retired in 2013.

Hypersonic

At this speed, an aircraft is traveling faster than Mach 5.

The hypersonic X-15, a joint venture that NASA conducted with the Air Force, the Navy, and North American Aviation, Inc., flew at Mach 6.7. The X-15 flew from 1958 to 1969 and provided insights that later contributed to the Mercury, Gemini, and Apollo piloted spaceflight programs. It also helped inform the Space Shuttle, which flies at hypersonic speeds while in the earth’s upper atmosphere. (It slows down to supersonic speeds as it re-enters the lower part of the earth’s atmosphere.) Most recently, China launched its experimental “waverider” Starry Sky-2 hypersonic aircraft, which soared at about Mach-5.5 for 400 seconds (after being carried by a rocket to an altitude of 18 miles or 30km).

The reason for the distinction between supersonic and hypersonic is due to temperature changes. At speeds above Mach 5, most metals will melt or become so soft that they can’t be used for any type of structure. As a result, hypersonic aircraft must go to extreme measures for heat protection (such as the tiles and blankets protecting the space shuttle). While it’s challenging to imagine what happens to an aircraft at such temperatures, here are two examples: aluminum will melt at approximately 1,200° F or 648° C and steel will melt at approximately 2,500° F or 1,371° C.

To learn more about speed versus temperature, click here for a great blog about why air is hot when you fly fast and why there’s no such thing as “cooling air” once you’ve achieved Mach 1.",4232
"ABSTRACT

There have been countless innovations in the field of aerospace military application as nations battle in the air and undoubtedly stealth technology has been the most compelling of all of them making fighter jets “invisible” and hence invincible. This article presents an overview into the implementation of the scientific knowledge of it including the history, basics of the radar technology and how the materials work and are made from. Not to mention, unorthodox detection methods of stealth jets are also explored in addition to investigate the other end of the spectrum and how it can be overcomed.

INTRODUCTION

Ever since the inception of wars and fighting, military personnel and people have wanted to hide themselves from their enemies to stay away from danger. An excellent way of doing so would be camouflaging oneself by evading predation. War planes, though fast and agile in the 1940s, were still detected by the nemesis through simple radar technology, making their moves predictable and therefore ineffective. With the advancement of stealth technology, fighters became more agile and untraceable to even the latest radar detection posing a huge threat to the camp on the other side.

The history of stealth technology could be traced back to 1958 where the Cold War was taking place. The United States researched stealth technology for its U-2 spy plane over the Soviet Union, but it was unsuccessful, but it was a good first step in the right direction.

Various kinds of shapes, sizes and materials are utilized to operate in stealth mode and give no notification to the adversaries [1]. To become invincible, there is not one but multiple technology that comes together allowing commanders to move with impunity. The exponential rise in stealth innovation could be thought to have culminated in the manufacture of the modern F-35 and other similar aircraft making it a huge threat in its time.

Figure 1: Stealth jet, Lockheed Martin F-35 Lightning II

RADAR TECHNOLOGY

To understand how stealth fighters were masked from the radar systems, it is essential to know the nuts and bolts of them to have a deeper comprehension of them. Radar simply uses electromagnetic sensors which recognizes things at substantial distances. As it is an active sensing apparatus, it sends out its own radiance when pinpointing desired objects, by sending out EM waves and analyzing the echoes of the reflected energy imaging it on the display. Incidentally, it was also the time of the World Wars when radar technology picked up its speed and a lot of countries experimented with it to take advantage of it knowing where their enemies are aerially and even on the sea.

As time went by, many countries had gained enough knowledge to operate radar technologies, the enemies moves and projectiles became well-known and could easily be dodged using straightforward predictions. Thus, this required the use of stealth technology that could make the jets not completely invincible but make it hard for an enemy to track the jet.

RADAR CROSS-SECTION

The physics of radar technology became more apparent as when the concept of the Radar Cross-Section (RCS) was discovered which was essentially an EM signature of the traced object on the radar. This made stealth more accessible as it was a quantifiable measure of how detectable an object is. The goal for stealth aircrafts was to keep the RCS as low as possible, giving the operators impunity to attack others. There are numerous factors that affect the RCS of an object, some of which are the material, absolute size, directivity, and the incident angle of the transmitted beam [2]. It is noteworthy that as stealth aircraft tend for lower RCS values, commercial aircraft need to have a higher value as they need to be always on the radar.

Figure 2: Typical Radar Cross-Section

In terms of shape, the aircraft should be fabricated in a way to reduce its RCS. Taking for instance Lockheed’s F-117 Nighthawk has a smooth surface which is very angled and flat. The purpose of it being angled is such that when an EM wave is hit on the surface of the aircraft, the sharp angled surface bounces back the wave into a forward scattered direction not letting the signal to be transmitted back [3]. The bottom line is that the incident radar angle would hit the skin of the aircraft and reflect at a higher angle than to the normal angle away from the source. In contrast, commercial airplanes have curvy surfaces ensuring that at least some of the signal will be returned through the normal direction.

MATERIALS SCIENCE

No matter the use of a vehicle, the material it is made up of has significant implications on its application and operation and it is no exception to stealth technology. Apart from directing the EM signal from radar sources, the Radiation-Absorbent Material (RAM) as the name suggests absorbs the wave energy preventing reflection of it in any way. It is an innovative material that soaks up the radio frequency radiation as much effectively as possible, lowering the power of the reflected radiation and hence lowering the RCS. The performance of the material is subject to its composition as the absorbency level of different frequencies vary for distinct materials [4]. It is significant that the coating of materials on the surface of the aircraft would not make it completely invisible for any frequency but does lessen the RCS.

One of the most common RAM in the stealth technology sphere is Iron ball paint. It is well-known that energy cannot be created or destroyed and thus is the case with the principles of the Iron ball paint. The Iron ball coated will convert the radar energy received from various sources to heat energy which is then dissipated from the aircraft. It is possible because of the paint which contains carbonyl iron. The radar waves bring about molecular oscillations because of the inherent alternating magnetic field in the iron paint. There have been multiple variations of this type although the core principles of it remain the same.

For instance, the infamous F-117 Nighthawk employs a similar technique. It is important to say the least to understand the electrochemical fabrication process that enables this radar evasion. The carbonyl iron balls are individually suspended in commonly used epoxy paint [5]. A layer of silicon dioxide is coated and its purpose is to act as an insulator to negate any electrical reactions given out. While the paint is still liquid, a magnetic field is applied such that it penetrates the liquid balls and thus creating a magnetic field pattern inside the balls. As the paint hardens and solidifies, the magnetic field is held in its place and locks them in the magnetic pattern. Once the paint is made as it needs to be coated, it says without going the application of it on the aircraft is as crucial. Remarkably the paint job is done by industrial robots which layers the paint on the F-117 with known and particular thickness and liquid properties.

Figure 3: Iron ball paint coating on the F-35

The iron ball paint method as a RAM has been used by several air forces such as Taiwan and the Mainland. Noticeably, the Taiwanese RAM stealth aircrafts is in response to their opponent China who were the first to employ stealth fighters [6]. This shows how keen countries are striving to take a stance on becoming military heavyweights by using state-of-the-art technology. There are various other materials that are embedded in the fighter aircrafts such as the foam absorber, split-ring absorber, etc. Most of which use the physical and chemical properties of the materials with nuances that differentiates the efficiency of evading the radar.

DETECTING STEALTH AIRCRAFTS

The realm of stealth technology is certainly vast and rapidly improving with improving sciences but on the other end of the spectrum, what could be done to detect these “invisible” aircrafts that could be literally fatal for some states.

There are also a number of spotting stealth aircrafts tricks but one the most promising technology is that of the usage of quantum radar [7]. Though there have been sizable organizations and regions in the field of it, Canada’s University of Waterloo has invested millions to their design of the quantum radar. The radar is expected to pinpoint aircrafts with higher accuracy than conventional ones and nevertheless perceive sly aircrafts. It is well known that the quantum world has given a large boost to existing technology in many fields such as computing and telecommunications and it is certainly the case for radar applications.

Quantum illumination works by the principle of quantum entanglement which is a physical feature of quantum particles. The entanglement entails that when the photon particles are connected, no matter the displacement, the state of each of one of them is known to the other photon and continues to affect it. This phenomenon of decoherence is the basis of detecting stealth aircrafts.

Under traditional circumstances, the photons emitted by the radar will get bounced off the surface of the stealth aircraft due to its sharp angles as aforementioned making it difficult to see on radars. However, using the quantum radar

would yield a different result when in contact with the object [8]. As it bounces off the surface of the aircraft, as the photons are entangled, the detecting source could trace back where the photons were fired at and keep shooting more of them until a picture of it is built up making it known to the radar.

Figure 4: Diagram depiction of photon entanglement

CHALLENGES IN STEALTH TECHNOLOGY

Although as seen there are rapid innovations in the domain of stealth innovations, there are several setbacks that have to be dealt with in order to bring about fully-fledged stealth mode. In terms of the Radio Absorbent Materials mentioned, it is critical to note that not all of the types of materials used work at all radio frequencies but are designed for specific radar frequency. With regards to the application of it on the aircrafts, it is not easy to be coated on moving or intricate surfaces which are usually the case in stealth aircrafts.

Moreover, the quantum radar used to detect these aircrafts are very much in their preliminary works and would need substantial design work to implement it. Primarily, the engineering challenge in the quantum radar would be the miniaturization of the electrical components in the system. Without saying, this would require more toil to integrate these complex photon detectors into smaller conglomerates which can make the detection more effective.

CONCLUSION

The advent of special materials and quantum technology has fostered the development of stealth technology. Even though there might be several drawbacks on either side of the spectrum the benefits most definitely outweigh them and with reassuring research and development, it is only going to get better with time.

All in all, in this brief article, stealth innovation has been looked at and how it operates from the contemporary radar technology and the radar cross-section to the fascinating stealth materials used and not to mention the revitalized modern quantum radar. The huge potential in stealth has provoked many countries to take part in it and it is bringing about a mini-arms race within the military forces making the future prospects of it truly riveting.

REFERENCES",11415
"Humanitarian engineer Buckminster Fuller once described an approach to building economic systems which he referred to as ephemeralization, defining it as our ability to “do more and more with less and less until eventually we can do everything with nothing.”

Now, such a thing seems impossible with the numerous technological and social constraints that come to mind, but the idea of “doing more with less” as a design principle — if followed responsibly — could eventually lead us to a future where atoms and bits dance in perfect synchrony, benefiting everyone at the expense of no one.

With that north star, we can start prototyping the systems of tomorrow with the tools of today.

With that in mind, let’s explore a potential techno-social medium for building this future: smart property.

What is Smart Property?

In the online age, people are used to summoning entities around the world to meet their everyday needs. A few taps on the scrying mirror of a smartphone can marshal human and autonomous agents to provide all manner of services. Transportation. Goods. Housing. Entertainment. Education. Nearly anything.

This ability to program value flows and social interactions around goods and services presents an incredibly powerful design space which we can refer to as smart property.

In practice, smart property is property that can be bought, sold, collateralized, and accessed via software APIs and search engines. While we already have this in a sense with housing (AirBnB), transportation (Uber), distribution (Amazon), and other areas, these systems are often controlled by centralized corporations operating under the illusory banner of “platform,” resulting in ongoing concerns around privacy, security, and labor practices.

At Mattereum, we believe smart property is an inevitable evolution of commerce, but how this system is implemented and the motivations behind it are paramount. Smart property can be how we achieve equilibrium with the planet and with each other, or it could be co-opted by the incumbent powers-that-be with results that can be read in dystopian science fiction.

Smart Property Can Fix Society’s Inventory Problem

The world is suffering from a severe misallocation of resources, especially on a long-term time horizon. At the core, this is a system design problem. Between planned obsolescence and trade globalization in the pursuit of corporate profit, the things we produce are not actually built to last and rely on an unimaginably complex, global infrastructure powered by distant (or not so distant) horrific labor practices and environmental costs.

The IPCW industrial cycle, designed by Mattereum CEO Vinay Gupta and a team at the Rocky Mountain Institute, provides an almost mandala-like mapping of industry. As you can see, the four main areas are Investment (of different capital types), Production, Consumption, and Waste.

One way to explain this odd paradox of gluttony and scarcity in the world today is to map where in this cycle we have innovated over the last half century, and where we have not.

Investment and Production witnessed a transformative leap in operational efficiency and scale in the post-WWII era, yet similar progress has not been made in optimizing Consumption and managing Waste.

Tightening these feedback loops will require better policy, technology, and social awareness.

We don’t know what we don’t know about our stuff and figuring it out is critical to achieving sustainability and improving quality of life for all.

Smart property as a design framework can help us generate, access, and act upon the necessary product information to build secondhand markets of durable quality goods and facilitate truly efficient p2p commerce. Imagine a digital wallet interface akin to an MMORPG’s character inventory in which the assets are actual objects which can be owned in common, shared, leased, bought, or sold with a few taps on a screen.

This is possible.

Smart Property Streamlines Complexity in Commerce

It is important to emphasize that smart property is not about embedding sensor devices in everything under the sun (Internet of Things, smart cities) and more about designing the social protocols around goods and services enabled by a tight integration between law and software to streamline, structure, and secure the numerous interactions and transactions around a good or service.

Oddly enough, with smart property it is the space and social interaction around an object which is where the magic happens. The ‘ground’ is the key design space rather than the ‘figure’ of an object.

On the first episode of The Future of Stuff podcast by Mattereum, Vinay illustrated the potential of smart property in multiple examples, including one that would seem random at first glance, but is profound in its implications.

The Logistical Nightmare of Wedding Planning

If the idea of smart property is to automate and streamline transactions and interactions between various parties around a good or service, reducing complexity and costs to those involved, then wedding planning makes for a surprisingly fitting use case.

Venue. Catering. Travel. Entertainment. Fashion. So many moving pieces in constant motion. What if there were a system which allowed one to aggregate these pieces via software protocols, creating order in the chaos and vastly reducing mental and transaction costs?

As Vinay explains, someone in charge of planning the event could use a smart property system to “build a multi-path matrix of possible solutions,” securing various options leading up to the day of the event as a safeguard for any failure or setback in the chain.

“It’s this ability to take processes where there are tons and tons of dependencies, turn the dependencies into option contracts, group the option contracts together until you have a solution, and then once you have a solution, execute all of the options. Bringing that kind of capability out of heavy engineering and finance into ordinary everyday people’s lives will be completely revolutionary.”

Of course, the application of smart property in event planning is not limited to weddings. Think of all the events we attend throughout our lives which involve networks of contracted parties which do the work of creating these dynamic social experiences. Concerts. Reward ceremonies. Conferences. Imagine booking and planning these events as easy as using AirBnB.

Smart Property is a Critical Use Case of Blockchain

It is imperative that a smart property system be decentralized and operated on a peer-to-peer basis. There’s a real risk of smart property systems as envisioned by Mattereum being co-opted by the incumbent Big Tech companies and being absorbed into monopolies and oligopolies, resulting in economic centralization with severe consequences for society (price distortions, resource misallocation, etc).

We do not want a world in which everybody owns nothing and Big Tech companies lease the majority of material goods to the populace, under contractual agreements which favor them at the expense of the user at every turn. No recourse. No agency.

If antitrust legislation and governmental efforts to prevent monopolies or keep them in check fail (and such legislation tends to wax and wane across administrations), then people would be at the mercy of market forces (such mercy does not exist).

Building smart property systems with p2p protocols is critical to ensuring that the already terrible distribution of wealth and power is not made even worse for future generations.

Blockchains provide cryptographically secure and verifiable transaction networks for the seamless and borderless exchange of value. A global, programmable medium of value transfer open to all is a design space we are still trying to wrap our heads around. And it is currently the most compelling technological solution for smart property.

Traditional client-server databases are silos which lack the interoperability necessary for coordinating and streamlining complex business transactions across multiple contexts. Having a shared financial infrastructure instead of a patchwork of protocols that struggle to maintain consensus can vastly reduce complexity and cost of trade.

However, neither blockchain nor any other computer system for that matter can power smart property alone. There needs to be a way of “programming” the material world and the social agreements between people.

Bringing the programmable value of blockchain into the real world requires a tight integration between legal systems that enforce property rights and the software systems which facilitate commerce.

Enter Mattereum.

Prototyping Smart Property with the Mattereum Protocol

Mattereum is building the legal-transactional framework that secures trust in digital commerce. Bridging the programmable value of blockchain-based digital assets with the programmable agreements of contract law with the affordances of both presents a compelling design framework for smart property.

The Mattereum Protocol leverages an integrated legal and software system consisting of three main components: Mattereum Asset Passports, Real-World Asset NFTs, and Trust Communities. Here’s how the protocol works, in brief:

(For a more detailed guide of the protocol, we highly recommend referring to the Mattereum Product Walkthrough.)

Asset Passports are a bundle of legal warranties tied to a particular object. Essential warranties include identification, confirmation of the presence and capabilities of verification technologies such as NFC tags, vault status, carbon offsets, anti-slavery certification, and tokenization. The heart of the Mattereum Protocol, Asset Passports are powerful, composable tools for securing property rights in virtual and physical space.

are a bundle of legal warranties tied to a particular object. Essential warranties include identification, confirmation of the presence and capabilities of verification technologies such as NFC tags, vault status, carbon offsets, anti-slavery certification, and tokenization. The heart of the Mattereum Protocol, Asset Passports are powerful, composable tools for securing property rights in virtual and physical space. Real-World Asset NFTs are unique digital tokens that denote the right to take physical custody of a particular object. While sharing features with the NFTs now gaining popularity in the cultural mainstream in the creative industries, these tokens are different in that they are backed by an underlying physical asset, complete with warranties, insurance, and legal enforceability to create trust in trade. They are paired with objects via a warranty in the Asset Passport.

are unique digital tokens that denote the right to take physical custody of a particular object. While sharing features with the NFTs now gaining popularity in the cultural mainstream in the creative industries, these tokens are different in that they are backed by an underlying physical asset, complete with warranties, insurance, and legal enforceability to create trust in trade. They are paired with objects via a warranty in the Asset Passport. Trust Communities are the networks of expert certifiers that accrue around each object (artwork, memorabilia, precious metals) throughout its lifetime. All expert certification of an object is backed by a legal warranty as well as financial stake, adding weight to an expert’s assessment. Built upon the composable, legal-tech framework of the Asset Passport, Trust Communities restore value, increase liquidity, and bring peace of mind to digital commerce.

So far, the Mattereum Protocol has tokenized a wide range of objects, ranging from gold-backed NFTs, to pop culture memorabilia, to historical/cultural artifacts.

Currently, these assets and arrangements are only a simple display of smart property. An asset is secured in a vault and can be physically delivered to the NFT bearer upon burning the token, with all the necessary legal warranties attached to the property. A transferable, cryptographic voucher of sorts.

Mattereum is researching and developing tools and methodologies to build upon this groundwork, streamlining the passporting and tokenization process for a wider range of asset classes and exploring new opportunities in this exciting design space.

In the future, there will be smart legal templates which create a streamlined user experience around generating the Asset Passport and minting digital assets, automated custodians that reduce the risk and complexity around custody of physical goods (and even provide corporate legal status of certain objects to empower collective ownership, asset governance, amongst other possibilities), advanced search capabilities with robust metadata schemas of all goods and services on the protocol, and APIs that allow others to build and expand on these foundations.",12861
"Smart contracts refer to computer protocols that digitally facilitate the verification, control, or execution of an agreement. Smart contracts run on the blockchain platform, which will process all the transactions in a contract; hence, middlemen are not required for executing the transactions.

Similar to traditional contracts, smart contracts define rules and penalties around an agreement and automatically enforce those obligations. While they can work independently, many smart contracts can also be implemented together.

The integral components of a smart contract are termed as objects. There are essentially three objects in a smart contract — the signatories, who are the parties involved in the smart contracts that use digital signatures to approve or disapprove the contractual terms; the subject of agreement or contract; and the specific terms.

Uses of Smart Contracts

Smart contracts can be used in a variety of fields, from healthcare to supply chain to financial services. Some examples are as follows:

1. Government voting system

Smart contracts provide a secure environment making the voting system less susceptible to manipulation. Votes using smart contracts would be ledger-protected, which is extremely difficult to decode.

Moreover, smart contracts could increase the turnover of voters, which is historically low due to the inefficient system that requires voters to line up, show identity, and complete forms. Voting, when transferred online using smart contracts, can increase the number of participants in a voting system.

2. Healthcare

Blockchain can store the encoded health records of patients with a private key. Only specific individuals would be granted access to the records for privacy concerns. Similarly, research can be conducted confidentially and securely using smart contracts.

All hospital receipts of patients can be stored on the blockchain and automatically shared with insurance companies as proof of service. Moreover, the ledger can be used for different activities, such as managing supplies, supervising drugs, and regulation compliance.

3. Supply chain

Traditionally, supply chains suffer due to paper-based systems where forms pass through multiple channels to get approvals. The laborious process increases the risk of fraud and loss.

Blockchain can nullify such risks by delivering an accessible and secure digital version to parties involved in the chain. Smart contracts can be used for inventory management and the automation of payments and tasks.

4. Financial services

Smart contracts help in transforming traditional financial services in multiple ways. In the case of insurance claims, they perform error checking, routing, and transfer payments to the user if everything is found appropriate.

Smart contracts incorporate critical tools for bookkeeping and eliminate the possibility of infiltration of accounting records. They also enable shareholders to take part in decision making in a transparent way. Also, they help in trade clearing, where the funds are transferred once the amounts of trade settlements are calculated.

Benefits of Smart Contracts

1. Autonomy and savings

Smart contracts do not need brokers or other intermediaries to confirm the agreement; thus, they eliminate the risk of manipulation by third parties. Moreover, the absence of intermediary in smart contracts results in cost savings.

2. Backup

All the documents stored on blockchain are duplicated multiple times; thus, originals can be restored in the event of any data loss.

3. Safety

Smart contracts are encrypted, and cryptography keeps all the documents safe from infiltration.

4. Speed

Smart contracts automate tasks by using computer protocols, saving hours of various business processes.

5. Accuracy

Using smart contracts results in the elimination of errors that occur due to the manual filling of numerous forms.

Limitations of Smart Contracts

1. Difficult to change

Changing smart contract processes is almost impossible, any error in the code can be time-consuming and expensive to correct.

2. Possibility of loopholes

According to the concept of good faith, parties will deal fairly and not get benefits unethically from a contract. However, using smart contracts makes it difficult to ensure that the terms are met according to what was agreed upon.

3. Third party

Although smart contracts seek to eliminate third-party involvement, it is not possible to eliminate them. Third parties assume different roles from the ones they take in traditional contracts. For example, lawyers will not be needed to prepare individual contracts; however, they will be needed by developers to understand the terms to create codes for smart contracts.

4. Vague terms

Since contracts include terms that are not always understood, smart contracts are not always able to handle terms and conditions that are vague.",4884
"A “smart contract” is simply a program that runs on the Ethereum blockchain. It’s a collection of code (its functions) and data (its state) that resides at a specific address on the Ethereum blockchain.

Photo by vjkombajn from pixabay

A. Terminology

smart contract: is a computer program or a transaction protocol which is intended to automatically execute, control or document legally relevant events and actions according to the terms of a contract or an agreement. The objectives of smart contracts are the reduction of need in trusted intermediators, arbitrations and enforcement costs, fraud losses, as well as the reduction of malicious and accidental exceptions.

White paper: is a report or guide that informs readers concisely about a complex issue and presents the issuing body’s philosophy on the matter. It is meant to help readers understand an issue, solve a problem, or make a decision.

Blockchain: is a growing list of records, called blocks, that are linked together using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data. The timestamp proves that the transaction data existed when the block was published in order to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent block

Ethereum: is a decentralized, open-source blockchain with smart contract functionality. Ether is the native cryptocurrency of the platform; among cryptocurrencies, it is second only to Bitcoin in market capitalization.

B. What is Smart Contracts?

The phrase and concept of “smart contracts” was developed by Szabo (computer scientist, lawyer and cryptographer) with the goal of bringing what he calls the “highly evolved” practices of contract law and practice to the design of electronic commerce protocols between strangers on the Internet

Smart contracts are a type of Ethereum account (if you want to know more you can read their white paper here. This means they have a balance and they can send transactions over the network. However they’re not controlled by a user, instead they are deployed to the network and run as programmed. User accounts can then interact with a smart contract by submitting transactions that execute a function defined on the smart contract. Smart contracts can define rules, like a regular contract, and automatically enforce them via the code. Smart contracts cannot be deleted by default, and interactions with them are irreversible.

C. Smart Contract Architecture

Within a smart contract, there can be as many stipulations as needed to satisfy the participants that the task will be completed satisfactorily. To establish the terms, participants must determine how transactions and their data are represented on the blockchain, agree on the if/when…then…” rules that govern those transactions, explore all possible exceptions, and define a framework for resolving disputes.

Smart Contract Schematic

Then the smart contract can be programmed by a developer — although increasingly, organizations that use blockchain for business provide templates, web interfaces, and other online tools to simplify structuring smart contracts. The basic architecture of the EVM (ethereum virtual machine) that runs smart contracts is that all calls to the contract are executed as a transaction where the ether required for a contract method executed is transferred from the calling account address to the contract account address. The contract code resides on the contract address on the blockchain and expects the calls to come in as transactions carrying the method parameter data along with the transaction as “input”. To enable a standard format for all clients, the method name, and parameters need to be marshaled in a recommended format.

D. Pros and Cons

Why we should use smart contracts?

1-Speed, efficiency and accuracy

Once a condition is met, the contract is executed immediately. Because smart contracts are digital and automated, there’s no paperwork to process and no time spent reconciling errors that often result from manually filling in documents.

2- Trust and transparency

Because there’s no third party involved, and because encrypted records of transactions are shared across participants, there’s no need to question whether information has been altered for personal benefit.

3- Security

Blockchain transaction records are encrypted, which makes them very hard to hack. Moreover, because each record is connected to the previous and subsequent records on a distributed ledger, hackers would have to alter the entire chain to change a single record.

4-Savings

Smart contracts remove the need for intermediaries to handle transactions and, by extension, their associated time delays and fees.

Why we should AVOID using smart contract?!!!

Smart contracts introduce an additional risk that does not exist in most text-based contractual relationships — the possibility that the contract will be hacked or that the code or protocol simply contains an unintended programming error. Given the relative security of blockchains, these concepts are closely aligned; namely, most “hacks” associated with blockchain technology are really exploitation of an unintended coding error. As with many bugs in computer code, these errors are not glaring, but rather become obvious only once they have been exploited. For example, in 2017 an attacker was able to drain several multi-signature wallets offered by Parity of $31 million in ether. Multi-signature wallets add a layer of security because they require more than one private key to access the wallet. However, in the Parity attack, the attacker was able to exploit a flaw in the Parity code by re-initializing the smart contract and making himself or herself the sole owner of the multi-signature wallets. Parties to a smart contract will need to consider how risk and liability for unintended coding errors and resulting exploitation are allocated between the parties, and possibly with any third party developers or insurers of the smart contract.

A blockchain-based smart contract is visible to all users of said blockchain. However, this leads to a situation where bugs, including security holes, are visible to all yet may not be quickly fixed. Such an attack, difficult to fix quickly, was successfully executed on The DAO in June 2016, draining approximately US$50 million worth of Ether at the time, while developers attempted to come to a solution that would gain consensus. The DAO program had a time delay in place before the hacker could remove the funds; a hard fork of the Ethereum software was done to claw back the funds from the attacker before the time limit expired. Other high-profile attacks include the Parity multi-signature wallet attacks, and an integer underflow/overflow attack (2018), totaling over US$184 million. Issues in Ethereum smart contracts, in particular, include ambiguities and easy-but-insecure constructs in its contract language Solidity, compiler bugs, Ethereum Virtual Machine bugs, attacks on the blockchain network, the immutability of bugs and that there is no central source documenting known vulnerabilities, attacks and problematic constructs.

References:

https://www.ibm.com/topics/smart-contracts

https://corpgov.law.harvard.edu/2018/05/26/an-introduction-to-smart-contracts-and-their-potential-and-inherent-limitations/

https://ethereum.org/en/developers/docs/smart-contracts/

https://en.wikipedia.org/wiki/Smart_contract

https://blockgeeks.com/ethereum-smart-contract-clients/

https://web.archive.org/web/20170730133911/http://iqdupont.com/assets/documents/DUPONT%2D2017%2DPreprint%2DAlgorithmic%2DGovernance.pdf

https://www.freecodecamp.org/news/a-hacker-stole-31m-of-ether-how-it-happened-and-what-it-means-for-ethereum-9e5dc29e33ce/",8056
"Smart Contracts are virtual contracts cryptography in Blockchain that can automatically record the terms of an agreement and enable every crucial step towards its achievement. SARA Technologies can help any business that wishes to extend its boundaries and make its business procedures simple or easy by developing a smart contract. The Smart Contract is also known as a Virtual Contract organized digitally — it establishes consensus while transferring digital currencies or assets between parties monitored by a computer program.

Smart Contract’s role is not limited to just defining the difficulties and rules around the consensus like the customary Contract but also automatically enforce those liabilities.

Nick Szabo invented the Smart Contracts in 1994 to make business activities simpler by developing a Smart Contract. During that period, there was no interest in using smart contracts because the subscription for the digital program was not so high. Still, today various businesses are acquiring the bitcoin and supporting blockchain technologies for their enhancement which will trigger their future growth and use of digital currencies.

As we all know, blockchain is a secured technology; therefore, a smart contract can be very much secure compared to a traditional contract. Since the blockchain cuts the need for a middleman, the same is done with a smart contract. Moreover, with smart contracts, businesses can also divert governmental and tax regulations’ imperfection without any problem.

Smart Contract Development Services

Architecture: Our developers can work on any standard software that includes pre-alpha, alpha, beta, and release candidates. After completing these steps, the Smart Contract is released to the market.

Accuracy: These virtual contracts are anti-human errors and provide faster business operations. As a smart contract development company, SARA offers customized services to users worldwide as per their business needs.

Audit: We are not just delivering smart contract services but also guaranteed to make the clients available with highly reliable, stable, and authentic audit services on how to use smart contracts to better their businesses.

Reduced Costs: Smart Contracts could be the right choice because it is a decentralized process that eliminates the risk of fraud and with a minimum third-party intervention that diminishes processing and standard contract cost.

Robotic Monitoring: If you face challenges while validating authenticating numerous transactions taking place simultaneously into your system, then Smart Contract could be a perfect solution to your problem. It simplifies the entire process with its automated consensus mechanism where you need not worry about the transaction processing and risks of failure. It makes your business more productive and secure with enhanced efficiency.

Private Blockchain: Our developers are efficient enough to build contracts in your private blockchain network as per your specific business need and ensure quality during the smart contract provision into the central system.

What Makes SARA a Perfect Smart Contract Development Partner

Team of Experts

As an enterprise blockchain development company, SARA deploys dedicated developers who are masters in providing robust smart contracts solutions to our customers.

Strong Portfolio

We are a team of professionals with years of expertise in creating and implementing smart contracts blockchain into various industry domains. We have hundreds of smart contracts running successfully and satisfactorily that inspire us to keep working and produce more advanced blockchain solutions to support technology.

One-Stop Solution

With SARA, your journey ends if you are looking for a partner to collaborate with you to drive your smart contract development idea on a path of success and achievement. Our services go through the span you use our products, and we ensure you to assist with utmost satisfaction and perfection in our services.

Other Services That We Offer",4028
"Introduction: Top Selling Sony Home Theater System USA 2021

Nowadays it is easier to find high-quality and affordable options for the home theater. All of today’s picks are from a trusted brand SONY. So all the Sony home theater system will guarantee to give you big end offerings.

It doesn’t matter how big your tv screen is if it is lacking in sound quality. Which means you are only getting half of the experience. Trust us your tv speaker won’t be doing much. Fortunately, you can upgrade to a home theater system regardless of budget.

We have basically focused today on high audio and high video quality. The best home theater system provides an excellent balance of good quality sound and easy installation. And well many people prefer that.

In 2021 a sony home theater speaker can give you a great audio experience. And for that, you don’t even need a complicated setup. So do read further and find the best Sony home theater for yourself.

Top Selling Sony Home Theater System USA 2021: List

The quality of the product is excellent, and it is easy to use. The compact contemporary design of this system fits anywhere in your home. The built-in power can easily fill a bedroom, kitchen, or office space with its great sound.

Feature a tiny powerful device that converts any aux speaker into a Bluetooth speaker. So that you can stream your music or take phone calls. You will enjoy the convenient Bluetooth connectivity with compatible Bluetooth devices.

And you will be able to stream music without wires. The near field connections technology takes Bluetooth connectivity to a next level. Allowing users to simply align their enabled device. And tap them together to pair and activate the connection. Well, you can also use the integrated AM/FM tuner to receive the local broadcast signals.

You can also play your CDs or your personal recorded CD-R by using the integrated motorized slot CD player. Well, you can also play MP3 files that have been recorded to CD discs.

Striking the Pros of using Sony Compact Stereo Sound System for House with Bluetooth Wireless Streaming

The classic three-box design makes a statement in any room.

Allows a separate place for the placement of the speakers for a wider stereo effect.

Has a built-in CD/DVD player for your disc collection.

Striking the Cons of using Sony Compact Stereo Sound System for House with Bluetooth Wireless Streaming

The build and control are average.

Equipped with 4 woofers and 1 tweeter, the Sony SS-CS8 2-Way 3-Driver Center Channel Speaker handles up to 145 watts. The woofer of the speaker uses a mic-reinforced diaphragm. The upper surface of which is fashioned to provide faithful sound.

While the lower layer is designed to provide a powerful bass response. The cabinet is built up of wood. Which is designed to provide a natural resonance. The network crossover of the speaker is mounted directly to the cabinet. So that it becomes vibration isolated. The foot of the speaker has rubber pads to avoid shelf vibration.

The crossover network in the SS-CS8 is intended to assure minimal signal loss. For an energetic vocal response with even the most delicate nuisance. It is mounted directly above the cabinet to avoid vibration.

Striking the Pros of using Sony 5.1-Channel Surround Sound Multimedia Home Theater Speaker Bundle

Has a powerful bass response.

The rubber pads make it vibration isolated.

Has a powered subwoofer.

Striking the Cons of using Sony 5.1-Channel Surround Sound Multimedia Home Theater Speaker Bundle

A lit bit costlier.

Enjoy the wireless audio streaming with the Sony 7.2-Channel Wireless Bluetooth 4K 3D HD Blu-ray A/V Surround Sound Home Theater System. Features Bluetooth with NFC connectivity. And also have four HDMI inputs with one HDMI output. All support 4k resolution.

It has a 7.2 channel that surrounds sounds and a two-channel stereo. Everything is huge. All speakers and receivers are huge. Whenever you play the music the sound quality would be great. The sound is crystal clear and with so many options music and movies are awesome.

Striking the Pros of using Sony 7.2-Channel Wireless Bluetooth 4K 3D HD Blu-ray A/V Surround Sound Home Theater System

It is very versatile.

The sound is crystal clear.

Offers dramatic and cinematic sound.

It is compatible with blu ray 3D movies.

The setup of the speaker is very easy.

Striking the Cons of using Sony 7.2-Channel Wireless Bluetooth 4K 3D HD Blu-ray A/V Surround Sound Home Theater System

There is a lack of sound adjustment.

Enjoy the clear mid and high frequencies from the soundbar. Brings every music and movie to life. In the volume and clarity, with a total 320watt power output. The contours of the soundbars fit perfectly with the design of your tv.

And it is also very simple to connect it. The seven sound modes enhance your entertainment experience. The Cinema mode is for movies, game studio mode is developed by PlayStation developers. Music mode helps you to listen to every detail clear. And the news mode is designed for clear dialogue.

Hear the sound that will be coming from all around. The virtual sound surround technology puts you right in the heart of movies. That is done by emulating the wide stage of cinema-style surround sound. Even without the need for additional rear speakers.

Striking the Pros of using Sony HT-S350 Soundbar with Wireless Subwoofer

The sound is very powerful.

Solid Bass.

Supports Dolby digital.

Bluetooth supported.

HDMI and ARC capable.

It is easy to set up.

Quite affordable.

Striking the Cons of using Sony HT-S350 Soundbar with Wireless Subwoofer

Has no Dolby Atmos, but it features S-PRO front surround instead.

There is a need for an HDMI splitter for multiple connections.

This Sony home theater system gives your favorite shows and movies the sound they deserve with a 2.1inch soundbar. This space-saving solution is designed to match the decor of your home. The compact one-bar design with a built-in woofer completely matches your room.

There is no need for another box and extra cables around your room. With HDMI, ARC, one cable can give an easy connection for all your tv audio. You can also connect it Wireless to your tv via Bluetooth. And can control tv and soundbar with the help of only one remote.

Virtual sound technology just puts you right at the heart of movies and music. The low profile design of the soundbar does not obstruct the view of your tv. The voice enhancement features strengthen the listening experience.

Striking the Pros of Sony S200F 2.1ch Soundbar with built-in Subwoofer and Bluetooth Home Theater

It is great for dialogue content.

Performs well even on high volume.

Striking the Cons of Sony S200F 2.1ch Soundbar with built-in Subwoofer and Bluetooth Home Theater

Doesn’t get too loud.

It does not support DTS.

Always stays on sound surround feature.

Well, that sums up our list for Sony home theater. I hope it would help you out to find “which Sony home theater is best?”

Acknowledging Questions

How To Setup Sony Home Theater System?

The two most common connections used to hear TV sound from the A/V receiver or from the home theater system is:

Option 1: HDMI connection using the ARC feature.

Option 2: Connection with the help of an HDMI cable, coaxial digital cable, or audio cable.

Which option you will be going to use depends upon the ports of your products. Suppose if your tv and audio system both support the ARC feature. I will recommend then using option 1 to connect your products. Otherwise, you can use option 2.

Originally published at https://shoppingpossible.com/ on August 16, 2021",7628
"Get a great home theater without the shocking credit card bill when you go for these affordable home cinema gadgets. Check out these reasonably priced TVs, soundbars, and projectors.

Want a quality home theater without spending all your hard-earned cash? It’s possible, and that’s why today we’re rounding up the gadgets you need to build an affordable home cinema setup. You deserve great home entertainment without depleting your budget. Luckily, there are some great TVs under $1,000 as well as affordably priced projectors and soundbars.

Related: 10 Best tech gadgets you need in your life

For under $1,000, you can get a TV that adjusts its brightness to your room’s lighting. Or, you can purchase a TV that looks like a piece of artwork. And, when it comes to sound, some of the soundbars below actually enhance dialogue and let you stream in 4K. Are you ready to go shopping? Then check out the items below.

1. The TCL 6-Series Roku TV provides 4K resolution and is powered by QLED technology for impressive detail.

Get the TCL 6-Series Roku TV for an all-around great viewing experience. The 4K resolution and QLED technology give you an impressively detailed picture. You’ll love the mini LED backlight, and it’s easy to use with Roku TV.

Get it for $949.99 on the official website.

2. The LG CineBeam PH30N LED Projector gives you a 25″-100″ screen. Plus, it’s battery powered so you won’t have to deal with any cords.

Another gadget that can help you build an affordable home cinema setup is the LG CineBeam PH30N LED Projector. It adds that impressive cinema quality to your movies and comes in a portable size.

Get it for $369.99 on the official website.

3. The Roku All-in-One Streambar Pro delivers room-filling sound and 4K streaming. Because you’ll want top-quality video to go with your cinematic sound.

The Roku All-in-One Streambar Pro is an excellent deal. It delivers not just dynamic sound but also 4K streaming. So you can expect vivid hues and sharp resolution in addition to dynamic bass. It also works with popular voice assistants.

Get it for $179.99 on the official website.

4. The Samsung Q70T 4K UHD Smart TV adjusts its lighting to fit your room. Plus, it even upgrades your old content, automatically.

You won’t have to worry about your lighting with the Samsung Q70T 4K UHD Smart TV. It completes your affordable home cinema setup because the 4K Quantum processor changes its brightness based on the lighting in your room.

Get it for $899.99 on the official website.

5. The Polk Audio React home theater soundbar has Alexa built in. And it lets you control your smart home gadgets, stream music, and more.

Add movie-theater quality audio to your Netflix binges with the Polk Audio React home theater soundbar. It provides larger-than-life sound. Plus, the built-in Alexa is so convenient and controlling the volume is just a command away.

Get it for $249 on the official website.

6. The Anker Nebula Mars II Pro portable projector lets you watch your favorite content anywhere since it projects 500-lumen images.

Another cool gadget an affordable home cinema setup needs is the Anker Nebula Mars II Pro portable projector. In fact, you can even use this projector outdoors, thanks to its super-bright projections. It even connects to Android 7.1, so you can watch Netflix, YouTube, and more.

Get it for $499.99 on the official website.

7. The TCL Alto 6+ 2.1 Roku TV soundbar has sound modes that make what you watch even better. It also comes with a subwoofer.

Take your movies and audio to the next level, affordably, with the TCL Alto 6+ 2.1 Roku soundbar. It boasts specialized sound modes and its own subwoofer. It’s also easy to install and works wonderfully with your Roku TV.

Get it for $99.99.

8. The Samsung The Frame 2021 lifestyle TV looks like framed artwork in your living room. And its stands look like display easels.

Add some style to your affordable home cinema setup with the Samsung The Frame 2021 lifestyle TV. It’s made to look like framed artwork thanks to the easel-like stand options. Subscribe to the art store to display curated pieces.

Get it for $529.99 on the official website.

9. The Bose Smart Soundbar 300 sleek speaker has a modern look and impressive connectivity. Get it for great dialogue, music, and more.

The Bose Smart Soundbar 300 adds great audio to pretty much all of your content. With it, you can expect enhanced dialogue and heart-thumping audio for your music and games. It also features Wi-Fi, AirPlay 2, Spotify Connect, and Bluetooth connectivity.

Get it refurbished for $359 on eBay.

10. The LG 2020 NanoCell LCD TV Collection boasts an elegantly thin design. The TVs have an impressive quality to immerse you in your shows.

The LG 2020 NanoCell LCD TV Collection comes in 4 different series. They boast a range of sizes from 49–86 inches. Each has features that make your viewing more immersive. With their sleek design, they look like artwork on your wall.

Get them starting at $899.99 on the official website.

You don’t need to spend a ton of money to have a great home theater setup. These TVs, soundbars, and projectors prove that you can find great tech at reasonable prices. Which items did you like the most? Let us know about them in the comments.

Want more tech news, reviews, and guides from Gadget Flow? Follow us on Apple News, Google News, Feedly, and Flipboard. If you use Flipboard, you should definitely check out our Curated Stories. We publish three new stories every day, so make sure to follow us to stay updated!

The Gadget Flow Daily Digest highlights and explores the latest in tech trends to keep you informed. Want it straight to your inbox? Subscribe ➜",5700
"Akasa’s flagship model for Mini-ITX motherboards, the Maxwell Pro

If you are looking to upgrade your home theatre system or even set it up from scratch, you would want to make use of the best technology to ensure a thrilling experience. However, getting your home theatre system together is a little different than setting up a regular PC — it needs to be optimised for entertainment. A great start would be opting for a fanless cooling system.

What is Fanless Cooling?

Similar to normal CPU coolers, fanless PCs are cooled via a heatsink. It takes care of PC cooling without producing the noise associated with fans. The difference however, comes in the size.

In PCs, heat sinks are generally placed on heat generating components such as CPUs. For maximum heat transfer from the PC components to the heatsinks, they are typically made from aluminium alloys. A heat sink then uses its surrounding medium — typically air — to cool itself down through its surface area. In order to optimise the cooling process, heatsinks are made up of a large number of fins through which more air can pass, thus increasing surface area and heat dissipation.

The extruded fins that come with Akasa’s fanless cases help cool down the CPU without the need for fans.

In standard PC setups, heatsinks need to be small enough to fit into a case and employ the use of fans to ensure there is enough air flow within the enclosed case to cool down the components. In order to eliminate the need for fans, fanless PCs don’t enclose their heatsinks in a case but rather have it become the case. This maximises the surface area and exposes the heat sink to airflow naturally while still maintaining excellent cooling performance.

The Best Choice for a Home Theatre PC

A home theatre system is all about the blend of functionality and comfortable entertainment. Fanless cooling is the best option for a Home Theatre PC (HTPC). Here is why:

Zero Noise Production

Fanless PCs run silent. This is great for a home theatre system, as you can enjoy an immersive entertainment experience without the interruption of fan noise. Instead of fans, the heat will rely on natural air flow, allowing silent heat dissipation from the heat-generating components.

This is great for any noise-sensitive environment not just HTPCs but also libraries, theatre rooms, music studios, voice-acting booths and so on.

Low Power Consumption

Fanless PCs are highly energy efficient. With significantly lower power consumption and even lower heat generation, fanless PCs generate optimal performance. The lack of moving fans also reduces the chance of dust accumulation. Thus, there is a lower chance of sudden damage. You can save on maintenance and repair costs, as well as any possible downtime.

PC Cooling will not interrupt your home theatre if you use a fanless PC. You can optimise the thermal energy pulled from the components that generate heat. This makes for a highly efficient HTPC system — no noise and no dust. It’s perfect for anyone looking for an impressive entertainment dock with minimal maintenance necessary.

Which is the best build for you?

Akasa’s wide selection of fanless cases has every occasion and taste covered. Our Turing cases are made for those who like to flaunt their Intel or Ryzen NUC HTPC, balancing contemporary design with functional fanless cooling. If you rather prefer to keep the NUC and case discreet behind the TV, the Newton range is the perfect pick for you with its you small footprint and volume. And if you need a case that can cater to larger and more powerful Mini-ITX boards we recommend one of our newest additions: the Maxwell Pro. With our selection of fanless Raspberry Pi 4 cases even your Raspberry Pi boards can provide excellent HTPC experiences, whether it uses the discreet Pi-4 Pro or the stylish Gem and Maze Pro.

Some of Akasa’s newest fanless additions: The Turing A50 MKII, the Newton TN and Maxwell Pro.

All of our cases are equipped with extruded fins to maximise their cooling capabilities and are made of the most heat conductive metals — aluminium for the body and thermal module, and in some cases (no pun intended) copper for heat pipes — to transfer the heat away from your CPU and other important PC components. To eliminate as much of the air between the components and the heat sink as possible, and thus increase heat transfer, Akasa uses its specialised Pro-grade+ 5026 thermal paste for excellent thermal performance, even at low pressures. Paired with the extra care that goes into the design of each of our cases, there is a fanless option for everyone.

Team Akasa — marketing@akasa.co.uk",4635
"Prominently depicted in science fiction movies as the benchmark of futuristic worlds, artificial intelligence has today already proved its worth in our reliability on high-tech functionality for modern societies. The basic outline of artificial intelligence is comprised of the ability to solve complex cognitive tasks at levels comparable to human intelligence. Artificial intelligence has also been known to recognize logical problems and come up with benefitting solutions. However, the original concept behind artificial intelligence included the upraise of machines to facilitate an easy fulfillment of common human tasks. Nowadays, artificial intelligence is being increasingly ubiquitous in various global industries with immense potential to impact economies and societies around the world.

In today’s age of sustainable development, artificial intelligence can have a profound influence on the Sustainable Development Goals set to define the development agenda for countries. In 2015, an initiative of the United Nations called The 2030 Agenda for Sustainable Development put forward a blueprint in front of the world. The blueprint is based around 17 crucial Sustainable Development Goals (SDGs) that the United Nations hopes to achieve by 2030 as the result of a global partnership. The SDGs are aimed at ensuring prosperity and peace for people around the globe to subsequently form a sustainable future. Artificial intelligence can have a significant influence on devising technological strategies for solving global issues like poverty, education, and climatic change and stimulate world economic growth. It is rapidly giving rise to a new frontier with revolutionary changes in governmental policy-making and corporate practices. The endless capabilities of artificially intelligent robotics and machines involve problem-solving cognizance regarding sustainability at surprising understanding levels of human intelligence.

In recent years, many countries have realized that there is a severe need to push their systems towards sustainable development and address the most intractable problems through contributions and collaborations. For this, artificial intelligence has largely been prioritized to manage sustainable development through good governance, social development, economic development, and environmental protection. Artificial intelligence can be integrated into essential initiatives that independently and mutually reinforce the fundamentals of sustainable development. Artificial intelligence induces a novel and dynamic aspect towards defining problems and proposing their solutions with the involvement of a phenomenal amount of data related to global sustainability.

Since its inception, artificial intelligence has come a long way in the race of achieving sustainability. It has also led to the emergence of various research innovators in academic frameworks and industrial trades through the incorporation of experiments with theories and thought processes to address problems and create their practical solutions. Therefore, it is no surprise that the visionary standpoints of artificial intelligence have brought in investments, by governments and private stakeholders, for various sustainability projects particularly surrounding the SDGs. Hailing it as a revolutionary breakthrough, many have projected the utilization of artificial intelligence as an ineludible means for attaining elevated economic growth and global prosperity.

However, there is a considerable portion of individuals that have raised their concerns over artificial intelligence and portrayed a more dismal image of a future affiliated with it. In contrast to its advocators, these people believe that artificial intelligence will actually hinder sustainable development by yielding exacerbation and inequity in global economics that can potentially befall humanity towards devastating existential crises. There are currently two categorizations of artificial intelligence that include narrow artificial intelligence (NAI) and artificial general intelligence (AGI). Including all of its modern modes, NAI possesses a weak framework of artificial intelligence. Whereas, AGI is proving to be feasible due to its proliferating applications despite its theoretical principles. As a subset of AGI, human-level machine intelligence (HLMI) is being deemed as an effective benchmark to solve complex tasks at the real-life human intellectual level. HLMI is the cause of concerns for people who fear that, through HLMI, artificial intelligence could take up human jobs and potentially replace humankind as the apex species on Earth.

While developments in AGI have mainstreamed bio-humanoid robots, there have been rising concerns over the future of artificial intelligence which have downgraded its prospects for sustainable development. There have been further attempts towards advancements in the accumulation and distribution of data that can massively displace jobs in the wake of improved and effective production. These impactful advancements can be too much for the current frameworks and therefore come in conflict with the 1st SDG (No Poverty) and 9th SDG (Industry, Innovation, and Infrastructure). This would be particularly devastating for developing countries where the degradation of laborer’s rights and unemployment are prevalent. Furthermore, critics of artificial intelligence comment on a growing naivety that the future owners of artificial intelligence would widely ensure just distribution of its rewards. They believe that the ownership of artificially intelligent technologies would concentrate wealth and widen the gap between current global systems and sustainably developed systems. In this way, countries can face widespread disparity with artificial intelligence coming in conflict with the 10th SDG (Reduced Inequalities). With a constant increase in systems performing faster and better by artificial intelligence, there can be an inevitably growing pressure on human psychology due to the calamitous future of employment.

Artificial intelligence has the potential to revolutionize multiple aspects of the world for the pursuit of sustainable development. It can enable businesses towards achieving the SDGs by promoting economic growth and addressing the impact of our activities. The utilization of artificially intelligent applications can place innovators at the forefront of sustainable development with enhanced efficiencies in technology and resources due to multi-sector partnerships between countries. However, artificial intelligence can be termed as a double-edged sword due to various potential risks affiliated with it. There would be a rigorous need of managing its limits to contain unintended consequences. Any illicit usage can potentially worsen processes like global warming and pollution and facilitate the capitalistic global economic order rather than addressing them.

The prevalence of partnerships and incentives between governments and corporations is bound to extravagate the transformative applications of artificial intelligence to the point of no return. The wise usage of artificial intelligence for sustainability projects only can yield sustainable business opportunities with effective connections and access between people. However, there is a severe need to keep the usage in check which is why the academic sector has a huge part to play in the induction of artificial intelligence in daily life. They must address the potential drawbacks as well as the beneficial opportunities of artificial intelligence through specifically developed curriculum and policies. In this way, future generations can prepare themselves for what artificial intelligence holds for them in the face of sustainable development.",7778
"THE ORIGIN OF PERFECT JAPANESE ARCHITECTURE:THE SHOJI DOORS!!

Japanese architecture is considered one of the finest project in the world while the use of natural wood in making the Shoji doors, window,room divider have been much explored when the tourist from all around the world used to visit Kyoto,Japan where they live in those traditional houses to feel the perfect balance between the calmness and the peace in those tourist destination. The Shoji doors are often made with the use of natural wood and transparent sheet of paper but now a days,a glass sliding doors in the verandah are very often used with the Shoji door from inside to promote the insulation in the homes especially in the cold and harsh climates.

While these Japanese traditional homes in Kyoto, Japan have been on the verge of extinction and if the proper plans & research of re-developing these traditional homes in the form of world class apartments, resorts with the sea view with the help of new technology around the world which could benefit millions of people living all around the world through branding if the proper care are not taken of by the architects in the Japan and around the world,then this could lead to a greater damage and could be on the verge of the extinction to one of the finest architecture around the world which have generated revenue through the tourism industry and increase in the employment in the region!!",1418
"Technology is amazing but different chargers for different devices irritate the hell out of me. Despite multiple efforts of drilling tables and fixing wires at a particular spot, My gadgets are usually out of battery whenever they are needed the most.

There is chaos in my mind almost all the time. The train of thought traverses from a wide range of how to reform board exams to why do I keep throwing full tosses when I bowl leg-spin. This attention deficit disorder is further fueled by the presence of electronics on and around my table. The ease with which I swiped right while writing this and scrolled Spotify, only to come back to scan Twitter is fascinating. Most of the gadgets have a designated spot and time to be used, like kindle before bed, Laptop for work, earplugs, and Garmin for running/biking, and iPad for reading articles or watching New Girl while eating meals. The other gadgets have to also compete for attention against a formidable opponent, The smartphone ( It’s winning these days).

While I went overboard with the context, This piece is not about attention disorder or how gadgets shape our lives. This is about an extremely inconsequential problem on the mess of different chargers in my life.

Technology is amazing but different chargers for different devices irritate the hell out of me. Despite multiple efforts of drilling tables and fixing wires at a particular spot, My gadgets are usually out of battery whenever they are needed the most. An almost certain visual is of a half-trimmed beard face of mine when unfailingly the power runs out of trimmer. As I struggle to find the right charger for my bike’s front headlight, I scan my entire house only to discover that we have 23 USB Cables. Add to that a charger for Garmin watch, Bike Light, Kindle and speaker use the same USB, Macbook and earphones use Type C USB cable, iPad and iPhones use the lightning cable and a regular headphone attached to the desk. Double them from previous generation products or lost but later found. This totals to a staggering fifteen cables, out of which 7–8 are a regular feature on the desk. Juicing each product separately is operationally difficult and invariably leads to a great deal of electronic waste.

So what must be done? An exasperated person like me which Kelkar and Shah mention in their authoritative book, In service of the Republic might demand, “Can’t we have a government that standardizes this?.” At first blush, This may seem like a very tempting idea. A single charger for all gadgets will fix all the trouble and most importantly the second argument of looking up to a muscular and under-resourced government for frivolous tasks like these is a regular moment of societal euphoria.

For any populist government that likes to ban and centralize, the proposition of coming up with a law that requires all electronics to be sold in India to conform to one single charging port is not entirely unthinkable.

But Government intervention, while being the most reflexive option, when given considerable thought comes with its own set of challenges.

Assuming the Government decides to select a certain type of charger as a national charger, Let us look at the probable questions that arise.

Who will be the deciding authority? What are the odds that the decision will not be influenced by intense lobbying from Multinational Corporations? In the case of one nation one charger, Will the Government ban devices and chargers of the nonstandard kind? If they ban, then who will enforce the ban? Will the cops come to our house search in order to catch the culprit ( Leaving essential Policing tasks, not uncommon plus at least this will not require additional training as they have similar experience in imposing alcohol and vaping pen and other such bans)

Or will there be a challan that can be surpassed by using the same device to call the MLA henchmen and be let go?

What will the state do about foreigners and NRI bringing different pin chargers and devices with them, there would also be a need for a specialized team at customs to prohibit entry of any restricted items.

An invariable fallout of such a law would be a thriving black market for devices and chargers other than the National kind. This will also lead to an elaborate network of rent-seeking, Black marketing, and scams that will make the curse worse than the disease itself. All this and we still haven’t discussed the environmental and economic cost of devices and chargers that completely lose their relevance.

Not to forget all this mess will be achieved at the cost of an incredible amount from the public exchequer.

This is an illustration of how a small Government intervention can quickly turn into a mess. There is an almost inevitable certainty of unintended consequences that will follow up with any government involvement.

So what should we do?

We as a society should restrain ourselves and maybe think through, that do we really need the iron fist of the government in problems that can be self-adjusted by the markets.

The state with its limited resources should restrict itself to playing the role of an effective regulator, channelizing its energies weighing the cost and benefits to the most essential things. We need a state that does very few things in a very good way instead of a state that does a lot of things in a very poor way.

We need a state that focuses on capacity building, making good roads, strengthening public education and healthcare and other essential public services like defence and policing.

For most other things, like this highly annoying and inconsequential problem of so many cables, the market will come up with solutions. The apple charging pad,magsafe, wireless charging, wire bin, and cable management boxes or whatever you find on amazon shall do the job.

And for the Government, maybe strengthening the e-waste recycling ecosystem sounds like a good option.",5928
"Ahh, 5G: the next-generation mobile broadband tech that’s got so many of us talking.

When we dissected the consumer demand trends in 2021, we mentioned briefly that 5G represents the beginning of the future for smartphones; with that said, we don’t know a great deal about 5G. In fact, like any huge transition, the move to 5G has been plagued with confusion and a series of conspiracy theories. Some of these theories are really creative and might we sound absurd — like how 5G is linked to the spread of the coronavirus pandemic.

Today, we’ll take a closer look at this fascinating development to answer some of the most burning questions that people often have about it. So, if you’ve been curious to learn more about the shift from 4G to 5G wireless connectivity, keep reading!

1. How is 5G different from 4G?

5G vs 4G: Speed

We all want quicker connection speeds and the switch to 5G will see just that. For one, 5G is believed to be at least 20 times faster than 4G. With 4G you get somewhere between 10Mbps and 50Mbps. 5G, on the other hand, could get you up to 50Mbps on average.

5G vs 4G: Latency

For the uninitiated, latency refers to the time it takes for data from your device to be uploaded and reach its target. It measures the time it takes for data to go from source to destination in milliseconds (ms). Latency is particularly important to avid gamers because where games are involved, response time can greatly impact the outcome.

With existing 4G networks, the average latency you’ll expect to see is around 50ms. With 5G networks, you can expect an average of 10ms. 5G technology may even drop that down to 1ms.

More for you:

2. Does it mean that 4G will become obsolete?

Many of us are still using 4G networks. In fact, many of us still rely on a 3G network when a 4G network isn’t available. In the same vein, 4G isn’t going to disappear overnight with the arrival of 5G. In fact, when 4G and 5G work hand in hand, consumers benefit from getting a decent connection speed on their mobile devices wherever they are.

Furthermore, as 5G infrastructure improves, 4G networks will, too, resulting in faster speed for all.

3. What are some changes and innovations that 5G will power?

5G networks will help us work more efficiently, boosting productivity | Photo on Freepik

The 5G technology is triggering and will continue to trigger unthinkable possibilities. We’ll look at just two of them here.

Autonomous vehicles

Driverless cars must collect a large quantity of data, process it locally, and then transfer it to the cloud. This information is then sent back to the car, allowing it to make safe judgments. Vehicles will receive real-time updates on hazards that develop beyond the line of sight, allowing vehicles to respond safely and immediately. This technology will be enabled by 5G networks.

Changing the way we work

Many of us are used to working from home since the start of the COVID pandemic. With 5G technology, we’re going to see even more changes in the way we work. Businesses can take advantage of the technology to have better phone calls, higher-quality video meetings, VR meetings, or even deploy AI-enabled tools via cloud-based apps to accelerate workflow.

Now, even tasks can be accomplished remotely because 5G’s low-latency and high-frequency data transfers will make engineering and many other types of highly skilled work possible from anywhere as long as there is a decent connection.

4. Will 5G have adverse health effects?

We mentioned earlier that 5G has been linked to the coronavirus, but that’s not all. 5G has been rumored to be associated with a slew of adverse health effects. For example, some people claim that 5G causes cancer or brain tumor. But we’re saying: it’s all fake news.

There’s no evidence that 5G is unsafe for use, and the myths of the adverse health effects of 5G have been debunked. In fact, it is quite the reverse. 5G technology actually enables remote surgery, which will have massive implications for the healthcare industry. We don’t know about you, but that doesn’t sound bad to us at all.

5. What 5G devices are available today?

If you’re looking to get your hands on a 5G smartphone, here are some you could consider:

iPhone 12 Pro Max

Featuring the most 5G bands on any smartphone, iPhone 12 Pro models offer the broadest 5G coverage worldwide. Furthermore, with a huge battery, you’re going to have enough juice to enjoy your 5G network thoroughly. The phone also has a massive 6.7-inch screen for you to Netflix and chill to.

Samsung Galaxy S21 Ultra

The Galaxy S21 Ultra shows us what we get when powerful camera performance meets epic 5G entertainment. With a large 6.8-inch AMOLED display and S Pen support, you can fully make use of this phone’s 5G support.

Google Pixel 4a 5G

If you’re willing to forgo the 90Hz refresh rate found on the Pixel 5’s display, Google Pixel 4a 5G is a great 5G phone with top-notch features. You’ll get a 2MP main shooter and 16MP ultrawide angle lens, and AI-powered software features. The Pixel 4a 5G works with every 5G network.

More for you:

Conclusion

The advancement of 5G technology will have an impact on more than just the devices we presently use. Mobile phones, gaming, and other apps will all use 5G. Things that were previously only imaginable in science fiction and fantasy will become possible with 5G.",5369
"Think about the first thing you pick up in the morning and the last thing you put down at night. For most of us, that’s our smartphone. Even though our phones are such a big part of our lives, we don’t know a great deal about them. To be fair, tech junkies who live and breathe tech love to engage in conversations about the processing power and chipsets of computers, gaming consoles, as well as smartphones. But for most of us who would identify as the average consumer, these topics are often jargon-filled and downright confusing.

If you belong to the latter group, this post will help. We’ll answer three big questions that you likely have regarding smartphone chipsets, such that you’ll no longer appear clueless in conversations with your tech-savvy friends anymore.

1. What’s an SoC?

Tech enthusiasts mention SoC in conversations so frequently, but few take the time to explain what it actually means. SoC stands for system-on-chip. Most SoCs today come inside mobile devices like smartphones and tablets. It’s essentially the processor or the “brain” of smartphones.

However, unlike a computer’s single processor chip package widely known as the Central Processing Unit (CPU), an SoC is a complete processing system that includes both hardware and software in one single package. In other words, smartphone SoCs contain a CPU, too, but that’s just one of the many components within an SoC.

By combining the required electronic circuits of various components onto a single, integrated chip, it is more energy-efficient, accords greater performance, takes up less space, and is more reliable than multi-chip systems.

2. What essential components make up an SoC?

Typically, a smartphone SoC integrates these core components:

Central Processing Unit (CPU) : If an SoC is the brain of your smartphone, the CPU is the brain behind the SoC. A modern smartphone CPU consists of multiple cores, which is why you’ll hear of Dual-core, Quad-core, and Octa-core. Each core represents an individual processing unit and they handle different tasks. The power and performance of a CPU are measured in GigaHertz (GHz).

: If an SoC is the brain of your smartphone, the CPU is the brain behind the SoC. A modern smartphone CPU consists of multiple cores, which is why you’ll hear of Dual-core, Quad-core, and Octa-core. Each core represents an individual processing unit and they handle different tasks. The power and performance of a CPU are measured in GigaHertz (GHz). Graphics Processing Unit (GPU) : A GPU is designed specifically to handle graphics-related tasks. This is the component that is responsible for processing the vivid and true-to-life games that you love playing.

: A GPU is designed specifically to handle graphics-related tasks. This is the component that is responsible for processing the vivid and true-to-life games that you love playing. Image Processing Unit (ISP) : An ISP helps to translate data from an image sensor to a photo or video that you can share as a file with your family and friends.

: An ISP helps to translate data from an image sensor to a photo or video that you can share as a file with your family and friends. Digital Signal Processor (DSP) : DSPs mathematically manipulate real-world signals like voice, audio, video, temperature, pressure, or position.

: DSPs mathematically manipulate real-world signals like voice, audio, video, temperature, pressure, or position. Neural Processing Unit (NPU) : An NPU helps to accelerate machine learning tasks. It’s often associated with Artificial Intelligence (AI) tasks such as voice recognition and camera processing.

: An NPU helps to accelerate machine learning tasks. It’s often associated with Artificial Intelligence (AI) tasks such as voice recognition and camera processing. Modems : A modem converts wireless signals into data that your smartphone can comprehend. Most smartphone modems integrate 5G, 4G, 3G, and Wi-Fi into a single system.

: A modem converts wireless signals into data that your smartphone can comprehend. Most smartphone modems integrate 5G, 4G, 3G, and Wi-Fi into a single system. Video encoder/decoder: This converts video files and formats efficiently.

3. Who are the major players in the smartphone SoC market?

Tear apart any modern smartphone and you’ll find a chipset from either Apple, Qualcomm, Samsung, MediaTek, or Huawei. These companies are major players in the smartphone SoC market. We’ll take a closer look at each of them.

Apple SoC

Before 2010, Apple sourced its SoCs from Samsung to power devices like the iPhone and iPad. The Apple A4, which was launched in 2010, stands today as the first Apple-designed SoC. Fast forward 11 years and we see today that the Apple A14 Bionic, seen on the iPhone 12 series, is Apple’s latest smartphone SoC that is designed exclusively by and for the brand.

Qualcomm SoC

You’ve probably heard of Qualcomm, an American company that sells smartphone SoCs under the Snapdragon brand. The chipset manufacturer produces SoCs for budget, mid-range, and high-end smartphones.

The Qualcomm Snapdragon 800 series is known for having some of the most powerful chipsets available for Android smartphones. Currently, the Snapdragon 888 holds the title for the top-notch SoC by Qualcomm.

As you go down the range, you’ll find the Qualcomm Snapdragon 700 series, used on upper mid-range smartphones; the Qualcomm Snapdragon 600 series, found in popular budget smartphones that offer a bang for your buck; and the Qualcomm Snapdragon 400 and 200 series, for basic and entry-level phones often found in emerging markets.

Samsung SoCs

Samsung designs its own lineup of smartphone SoCs and brands it as Exynos. In the latter half of 2020, the popular Korean brand broke the news of the Exynos 1080, a new high-end SoC that will be used in affordable flagship devices in 2021. Soon after, the company announced the powerful Exynos 2100, which is used in the Samsung Galaxy S21 series.

MediaTek SoCs

MediaTek is a Taiwanese firm that designs smartphone SoCs. The company’s 5G-centric smartphone SoC is dubbed the Dimensity series. Announced in early 2021, the Dimensity 1200 is the company’s new flagship-grade SoC.

Beyond that, MediaTek offers gaming-focused chipsets (Helio G series), budget SoCs (Helio P series), and SoCs for low-end Android phones (Helio A series)

Huawei SoCs

Huawei devices come with Kirin chipsets which are developed by HiSilicon, a Huawei-owned semiconductor company. The brand’s current flagship SoC belongs to the Kirin 9000 series, which powers the Huawei Mate 40 series.

Predecessors of the Kirin 9000 series include Kirin 990 (found in the Huawei P40 series and the Honor 30 Pro Plus), as well as the Kirin 800 (for upper mid-range handsets), 700, and 600 series (for budget handsets).

Unfortunately, with the US-China trade war, Huawei’s segment of the SoC market isn’t exactly stable. In fact, HiSilicon confirmed that it has stopped making Kirin SoCs. The Huawei Mate 40 may very well be the last phone to come with the Kirin 9000 SoC. What comes next in terms of Kirin SoCs is uncertain.

Conclusion: The future of smartphone SoCs

Just a few years ago, you’d find basic components like CPUs and GPUs on an SoC. Today, with advancements in security, machine learning, and 5G capabilities, modern smartphone SoCs have evolved to include components like DSPs, ISPs, modems, and NPUs.

As our needs and wants grow, this rapid development will only continue. We’d expect our smartphones to evolve to be able to handle more complex tasks than before. That said, we’ll be seeing more and more dedicated processors on a single SoC and smartphones that are more powerful than ever before, and we can’t wait to see that happen.",7724
"When you hear biometrics, it’s easy for you to think of fingerprint scanners and selfies that unlock your smartphones.

But did you know… the use of biometrics dates as far back as the Babylonian era and has been so, in various sectors like law enforcement and government agencies; and it wasn’t until the 2000s that the technology got implemented in mobile phones, the Pantech GI100 being the first phone to have a fingerprint scanner.

Now, before biometric technologies like fingerprint scanning and voice recognition came to smartphones, we relied on passwords and PINs to keep our data safe–and we still do; though with the smartphone industry fastly evolving, it’s very possible we’ll move into a passwordless era soon.

And to buttress, a recent survey among smartphone users — aged 18–30 — shows over 70% of them agreeing that biometric technologies are crucial features of their devices.

Source: Eze Wholesale

For sure, biometric technologies in smartphones are here to stay and will only get better.

How do biometrics work?

If you’ve scanned your fingerprint, or face, to unlock your phone, you have a fair idea of how biometrics work.

Let’s take fingerprint scanning as an example.

When you record your fingerprints, the data is stored; and when you intend to unlock your device in the present, that data is compared with the ‘live’ information you’re inputting. From this, you can guess what is needed for a biometric system to work: a device to record your data, where the recorded data would be stored, and some software that converts the scanned biometric data to a digitalized format, comparing your live data with the stored one.

So, in simple terms, a biometric system consists of

Sensor : That part of the biometric system that you interact with. It records your information and reads it when biometric data needs to be recognized. No matter the form by which data capture occurs, sensors must be involved.

: That part of the biometric system that you interact with. It records your information and reads it when biometric data needs to be recognized. No matter the form by which data capture occurs, sensors must be involved. Computer : For storing your biometric information. Whether you’re trying to access a device or use your biometric information for something else, you need a computer. The computer (in this case your smartphone) has a database that collects and stores your information.

: For storing your biometric information. Whether you’re trying to access a device or use your biometric information for something else, you need a computer. The computer (in this case your smartphone) has a database that collects and stores your information. Software: This converts the scanned biometric data into a digitalized format that will be accessed for comparison when needed. The software is what connects the computer to the sensor.

These components give users a seamless experience when using biometric features.

Ready to learn about the different biometric data types used in smartphones? Let’s get started.

Fingerprint scanning

Fingerprint recognition is the most commonly used biometric feature in smartphones.

It works by looking for specific characteristics in the line pattern on the surface of the finger. This line pattern is stored in the form of an image.

While the Pantech GI100, launched in 2004, is credited as the first mobile phone to have a fingerprint scanner, the Toshiba G500 is considered the first mobile to take mobile fingerprint scanning to the mainstream. HTC, Acer, and LG followed suit, soon after — as more mobile phones adapted the fingerprint scanning feature.

In 2013, Apple’s iPhone 5s was released and the modern-day fingerprint scanning revolution began.

Samsung’s Galaxy Note4 was the company’s first smartphone with the feature.

Today, smartphones like the Vivo X20 Plus UD have in-display fingerprint scanners, while some like the Xiaomi Mi 10I have fingerprint sensors mounted on the side of the phones.

Although some smartphone companies like to add a touch of their own to their fingerprint scanners like Apple’s TouchID, the idea behind fingerprint scanning fairly remains the same.

As of 2021, there are three main types of fingerprint scanners: Optical, Capacitive, and Ultrasonic.

Source: Eze Wholesale

More for you:

Facial recognition

From the phrase ‘facial recognition,’ you can guess what this biometric data type is about. It’s a system that analyses the shape and position of different parts of the face to determine a match.

In 2011, the Samsung Galaxy Nexus featured a facial recognition system — Face Unlock. But not long after, the difficulty in using the feature made the buzz die down. Subsequently, the Galaxy S8 was released with a facial recognition feature; and its cool new way of unlocking devices gained popularity among users.

Apple’s FaceID takes facial recognition up a notch with a 3D scanning feature, designed in such a way that even with cosmetic makeup or wearing hats, users can still securely unlock their devices. The iPhone X is the first iPhone to have this feature.

Other smartphone companies like HUAWEI, OnePlus, and Xiaomi have all joined in adopting facial recognition technologies for their devices.

Iris scanning

The patterns in eye ligaments are unique to each individual, which makes iris scanning a viable option for biometric identification. It’s a more secure method when compared to facial recognition and fingerprint scanning, because, unlike other biometric modalities, the iris is not affected by external conditions. The Arrows NX F-04G debuted in 2015 as the first smartphone to have the iris scanning feature. In 2017, Samsung introduced this feature on its Galaxy S8 and Galaxy Note 8 smartphones.

Voice recognition

With digital assistants like Google Assistant, Siri, and Bixby, speech recognition is something that is used by many smartphone users today. However, in terms of using this feature for security, smartphone companies have not fully embraced this feature.

To match a voiceprint, you’re required to match the tone and the normal voice that was recorded; this can be difficult when you have a cold or if you’re not in the right emotional state. The voice recognition is deemed less secure when compared to other identification types because your phone can be unlocked by someone else that says the correct voice password. This, however, did not deter LG as the LG V30 unlocks with voice command.

Biometrics over passwords?

Biometrics offers several advantages over passwords, a reason many users prefer it to passwords.

The main benefit of biometrics is that they cannot be replicated (unlike passwords, which could get hacked or stolen).

Another benefit is simplicity. Users, like myself, are tired of having to memorize passwords upon passwords. So the convenience that comes with biometrics gives it an edge over passwords and PINs. And while many argue that biometrics should be used as complements and not replacements for passwords, the tendency for a passwordless era has only kept growing.

Source: Eze Wholesale

Also, biometric payments are now implemented in banking apps and various online apps, as they provide ease of payment and greater security.

More for you:

Looking to the future

We’ve looked at the various biometric identification types, a bit of the history of each, and current developments.

While the advantages of biometrics over passwords are glaring and attractive, there are also risks involved with mobile biometric usages, such as those associated with the storage of biometric data, imposter attacks, and spoof attacks.

As we mitigate these risks with more research and development, it’s certain that there are more biometric solutions the consumer electronics industry can offer. And yes, we can’t wait to see them arrive.",7831
"Your home should be a safe haven. But if you sometimes feel fearful alone indoors or skeptical about leaving your property unattended, a smart home security camera can offer repose. Discover our top picks in today’s Daily Digest.

That feeling of safety is priceless, and we have just the gadgets that can combat feelings of fear at home: outdoor smart home security cameras. Designed to monitor happenings surrounding your property, smart cameras detect suspicious movement and alert you of the activity. It’s like having a personal security guard keep tabs on your land 24/7, and we guarantee that you’ll sleep easier at night with one installed.

Related: The best Alexa gadgets to buy in 2021

But these products’ purpose isn’t solely to combat fear; they’re also convenient for communicating with guests at the door without in-person interaction. Some luxury models include Alexa Greetings to leave a scheduled message for a delivery person when you’re not home. Feel more settled in your home with our selection of smart home security cameras to act as your eyes and ears.

1. The Nooie Smart Cam Doorbell features intelligent human detection and an antitheft locking mechanism. It’s one of our favorite outdoor smart home security cameras for peace of mind indoors.

Upgrade your home’s security with the Nooie Smart Cam Doorbell. Use it for both convenience and peace of mind at home. It provides two-way audio to communicate with guests and delivery people. It also includes Live View, allowing you to see who’s at your door in real time. Moreover, it uses smart technology to detect humans from objects, minimizing unwanted notifications to your phone.

Order this smart doorbell for $149.99.

2. Weatherproof and wire-free, the Swann Xtreem is a smart home camera with a 6-month battery life and a 110-degree FOV.

Allow the Swann Xtreem to be another pair of eyes watching over your property when you’re on vacation. It boasts a generous 1080p resolution camera to produce high-quality footage. In fact, you can even see footage clearly in the dark up to 26 feet with the powerful infrared night vision.

Purchase this smart security camera for $179.99.

3. Designed with a 180-degree FOV, the Arlo Essential gives full coverage. It even works with Amazon Alexa and Google Assistant.

See more at your front door with the Arlo Essential. Featuring Object Detection, it recognizes the difference between a person and an object to reduce nuisance notifications. Additionally, with Intelligent Alerts, you can quickly contact emergency services via the app if you encounter suspicious or dangerous activity.

Purchase this smart camera for $179.99.

4. The WUUK Smart Antitheft Doorbell features a voice gender modifier to make children and women feel more secure when home alone.

Don’t fear communicating with someone at the door when you have the WUUK Smart Antitheft Doorbell. Its unique voice modifier changes the tone of your voice to hide your identity. Moreover, its adjustable motion sensors alert you when it detects motion, helping to protect your home right from your smartphone.

Order this gadget from Amazon for $89.99.

5. Featuring ultrabright LEDs, the Ring Floodlight Cam Wired Pro emits 2,000 lumens of brightness, so you can see anyone outside.

Additionally, the Ring Floodlight Cam Wired Pro features 1,080p HDR video resolution and color night vision for superb quality day and night. It also includes 3D Motion Detection and Bird’s Eye View features for maximum protection and convenience. And, with dual-band Wi-Fi connectivity and a weather-resistant design, it gives you years of use.

Order this smart security camera for $249.99.

6. Warn off intruders with the Wyze Cam v3. It features a siren and an IP65 waterproof rating for security and durability.

Capture moments outdoors and indoors with the Wyze Cam v3. This security camera features 1,080p resolution to provide clear footage during the day and night. In fact, it delivers colored night vision. Additionally, the Starlight ISP reduces noise from low-light conditions for clear visibility at all times.

Purchase this outdoor camera for $35.98.

7. Keep tabs on all your doors, windows, and hallways with the Kangaroo Motion + Entry Sensor. It’s a security device that provides flexible and customizable home monitoring for peace of mind.

Choose to monitor just motion or opening and closing of doors and windows with the Kangaroo Motion + Entry Sensor. It’s also designed to prevent false alarms when you’re at home. Moreover, this device detects motion up to 20 feet away and offers 110-degree FOV motion sensing. It even includes a built-in pet rejection feature for animals.

Purchase this home security device for $29.99.

8. The Ring Video Doorbell Pro 2 provides Head-to-Toe HD+ Video, 3D Motion Detection, and built-in Alexa Greetings.

Keep all the convenience and safety features you need with the Ring Video Doorbell Pro 2. This smart home gadget ensures you never miss an important moment with Head-to-Toe HD+ Video in 1,536p. Also, you can receive an aerial view of your yard to observe motion from a new perspective.

Order this smart doorbell for $249.99.

9. Answer the door from anywhere with the Ring Video Doorbell Wired. It includes the brand’s HD Video and Two-Way Talk features.

Take some of the stress out of your day with the Ring Video Doorbell Wired. This smart doorbell provides advanced motion detection and sends real-time alerts to your smartphone, Alexa device, or Ring Charm. Moreover, you can adjust motion settings to filter out motion on a busy street.

Purchase this Ring doorbell for $59.99.

10. Experience high-quality 4K video with the Arlo Ultra 2 Spotlight Security Camera. It lets you zoom in on objects with immense clarity.

Finally, the Arlo Ultra 2 Spotlight Security Camera delivers an ultra-wide viewing angle, six-month battery life, and noise-canceling two-way audio. You can even see clearly in the dark with color night vision. Furthermore, this smart camera works with Apple HomeKit, Amazon Alexa, and Google Assistant for convenient operation.

Order this Arlo camera for a reduced price of $249.99.

Make a house feel like a home with these outdoor smart home security cameras. They’re invaluable to you and your family’s safety and mental wellbeing. What are your go-to security gadgets? Let us know in the comments.

Want more tech news, reviews, and guides from Gadget Flow? Follow us on Apple News, Google News, Feedly, and Flipboard. If you use Flipboard, you should definitely check out our Curated Stories. We publish three new stories every day, so make sure to follow us to stay updated!

The Gadget Flow Daily Digest highlights and explores the latest in tech trends to keep you informed. Want it straight to your inbox? Subscribe ➜",6799
"Blink Camera Red Light signifies that your internet has stopped working. This is why your Blink Camera Blinking Red stopped working. Hence, you need to resolve the wifi issue as soon as possible.

Just not only this, there could be more other reasons that are responsible for the offline issue in a Blink security camera. We will discuss all of them in the guide.

We are going to arrange all the troubleshooting methods to resolve the red light issue on the Blink camera. If you need any more help then dial the Blink Camera Customer Service Phone Number now.

Possible Reasons Behind the Blink Camera Red Light

It can’t be said clearly why the blink camera flashing red. Sometimes, it’s the faulty internet connection while other times wrong security camera setup could be responsible.

A slow or faulty internet connection is one of the main reasons behind the Blink Camera Red Light issue. The red on the Blink camera represents the weak wifi connection. If you have just purchased the Blink security camera, the Blink camera flashing red light at the initial stage of its setup. So, you don’t need to be worried as this light will disappear after a few seconds.

How to Fix Blink Camera Red Light Issue?

If you have found that Blink Camera Not Detecting Motion then follow the steps that have been mentioned below.

Check Your Wifi Connection

As we have already mentioned, Blink Camera Red Light means that your camera has not been connected to the internet connection. So, try to resolve the issue by restarting the router. We are going to tell you how to restart the router in the next step.

2. Restart Your Wifi Device

In order to tackle the technical issues, restarting is one of the best ways. So, remove the power plug from the power outlet and then wait for 10–20 seconds. Now, reconnect the power cable again. Turn ON the router and check it has started working or still the Blink camera blinking red light.

3. Check the Batteries

The batteries that you have not inserted inside the camera are not properly inserted. Or else, the batteries get damaged, hence replace them with new and original ones.

4. Move The Camera Near The Router

If you have placed the base station too far from the router then minimize the distance. This could be one of the main reasons why Blink Camera Video Stopped Unexpectedly. For more help, dial the given Blink Camera Customer Service Phone Number.

5. Power Recycle The Blink Camera

After following all the steps if still, you are facing the Blink Camera Red Light issue then power recycles the Blink camera. To complete this process, turn off the camera by removing the power plug from the wall outlet and the batteries by removing the back cover.

After waiting for 10–30 seconds, reconnect all the cables again. Turn ON the Blink camera and check if it has started recording or not.

6. Reset the Blink Security Camera

To complete the reset process, you just have to press the reset button with the help of a thin pin-like object. Hold the reset button until the Blink camera will not start again. Once your Blink security camera will be reset successfully, turn it ON and check if it has started recording or not.

Let us tell you that, resetting the camera will delete all the save settings. Hence, you need to do the security camera setup again.

Conclusion

Blink Camera Red Light means that something is wrong with the internet connection. In this guide, we have arranged all the troubleshooting methods to make your security camera start recording again. If you need some more help then, dial the toll-free Blink Camera Customer Service Number now.",3613
"Blink Camera Blinking Red | Blink Camera Offline | Blink Helpline +1–800–983–7116 Lilyvictoria Follow Jul 20 · 3 min read

If your Blink Camera Blinking Red then it means that your camera is not connected to the internet. The Blink Camera Red Light also appears during the setup process. But, don’t worry, this red light will start blinking green and then solid blue light.

In this guide, we are going to discuss all the reasons and the troubleshooting methods to fix the red light issue. For more issues, dial the given toll-free Blink Camera Customer Service Phone Number. You can dial this toll-free number anytime you want.

Why Is My Blink Camera Blinking Red Light?

We have already mentioned above that there could be two basic reasons why Blink Camera Blinking Red. It may happen because of a weak internet connection or the batteries of the camera getting damaged. You can easily fix the problem in some easy steps that are mentioned below.

Weak Internet Connection

At the initial stage of Blink Security Camera Setup, it is a must to connect the camera to wifi. Until and unless you will not connect the Blink camera to the internet, it will not record anything. Also, you will not be able to see any video on your mobile application.

While doing the Blink camera setup, you will see that Blink camera Blinking Red light 3–4 times. It will continue to do the soft red blinks until your internet connection is not stable. Hence, keep an eye on the blinking pattern.

In this case, your camera has been connecting and disconnecting continuously then checking the internet. Contact your internet service providers if you find anything wrong with wifi.

2. Reconnect Blink Camera to the Internet

It could be possible that you may have not completed the setup properly. This is why the Blink Camera Not Working Red Light issue occurs. Hence, reconnect the camera again with the help of the below-given steps:

First, connect the security camera to the wifi network of Sync Module

Now, you will be asked to add the personal wifi connection(choose the name of your wifi from the given networks).

It is necessary to check whether you have entered the right password or not.

In a short time, the security camera will be connected successfully to the internet. This will definitely resolve the Blink Camera Blinking Red issue easily.

If Blink Camera Connected but Not Working then dial the given toll-free Blink Camera Customer Service Phone Number now.

3. Low Camera Battery

Apart from the weak internet connection, if your Blink Camera blinking red light, it also indicates a low battery level. If you have found that your camera is flashing the red light 5 or 6 times and suddenly all the light goes out. It is an indication of a battery failure problem.

Follow the below-given steps to avoid battery failure in your Blink camera:

It is necessary to replace the batteries with the new ones after a regular interval of time.

Make sure that you are using the AA Lithium non-rechargeable batteries in your Blink camera.

Install the batteries in the right manner.

4. Restart the Blink Security Camera

Restarting is one of the best ways to resolve technical issues. If your blink camera flashing red light then restart it immediately. To do so, turn OFF the camera, remove the power plug and wait for 2–3 minutes. Now, turn ON the security camera and check if it has started recording or not.

Conclusion

This article can be concluded on the point that the red light issue on Blink camera represents the internet-related problem. In this guide, we have mentioned all the steps to troubleshoot Blink Camera Blinking Red light problem. For more help, dial the given toll-free Blink Camera Customer Service Phone Number now.",3737
"These days, most modern web browsers include a lot of great functionalities that anyone can easily access. All the features in any browser work to provide you with the fast and best web surfing experience. And among all other internet browsers, Chrome, Edge, and Firefox are the most widely used web browsers, and they are quite popular among millions of users all over the world. Chrome, Edge, and Firefox can be used on various platforms, including Windows computers, Android, and iOS devices. All of these three mentioned browsers are packed with many great settings and options. And it is possible that you might not be familiar with all of their options. Each browser has some hidden features that you can use to change the advanced settings. For instance, whether you use the Chrome browser, Firefox, or Edge browser, they all include a home button.

Whenever you launch any web browser on your device, the homepage or the start page is the first thing that appears on your screen. And Mozilla Firefox, Google Chrome, and Microsoft Edge come with a dedicated home button that you will find in the toolbar. One can use this home button to get to the browser’s homepage. And if you want, you can even hide or show the home button. Do you want to know how you can do so? Do you also use Chrome, Edge, or the Firefox browser on your device? Well, if you want to know how to show or hide the home button in the Chrome, Edge, and Firefox browser, you must read this entire post as here, we have mentioned the proper procedures for this.

Techniques to Hide or Show the Home Button in Chrome, Edge, and Firefox

All of the three mentioned browsers, namely, Chrome, Edge, and Firefox, enable the users to hide or show the home button whenever they want. And no matter which browser we use, we all love to customize our browser’s settings to make our browsing experience more personalized. Every web browser comes with a standard settings page where you can find all the stable settings. And using the settings page, you can do whatever you want, like adding or removing the home button. Here, we have mentioned the procedure to hide and unhide the home button for all the mentioned web browsers, namely Chrome, Edge, and Microsoft. So, without delaying any more, let’s get to the techniques now.

Procedure to Hide or Show the Home Button in the Google Chrome Browser

If you are a user of the Google Chrome web browser and want to hide or show the home button, you must go through the below-mentioned instructions:

To get started with the procedure, First of all, open the Google Chrome browser on your computer and then click on the three-dotted menu icon placed at the top-right corner of your computer screen.

After clicking on the icon, you will see a list of the options appearing on your computer screen. You will need to click on the option of “Settings.” Alternatively, you can also get to the settings page by typing “chrome://settings/ into the search box and then use the enter key.

And when you finally get to the next page, scroll down and locate the “Appearance” section. Here, you will see a toggle button placed next to the option of “Show Home Button.”You will need to turn on the toggle button to enable the option. And after turning on the option, the home button will appear on the top-left corner of your computer screen. You can click on the icon to reach the browser’s home page. To hide the home button, turn off the toggle switch, after which you will not see the button on the webpage.

The Process to Hide or Show the Home Button on the Microsoft Edge Browser

Here are the steps that you will have to follow to hide or show the home button on the Microsoft Edge browser:

Firstly, you will have to launch the Edge browser on your computer system by clicking on its icon from the taskbar.

After opening the browser, go to the top-right corner of your computer screen and click the ellipsis button.

Now, a list of the options will appear on your screen. You will have to click on the “Settings” option.

And then, click on the option of “View Advanced Settings” to proceed further.

After this, scroll down through the screen and look for the “Show the Home Button” option. Turn on the toggle button given beside the option to enable the home button. And that’s it. You can hide the home button by turning off the toggle switch.

Steps to Hide or Show the Home Button in Firefox

Follow the below-mentioned steps to hide or show the home button in the Firefox browser:

To show or hide the home button in the Firefox browser, firstly, open the browser on your computer system.

After this, go to the three horizontal lines icon at the top of your screen and click on it.

From the menu that appears on your screen, click on the option of ‘Customize.”

To make the home button visible, drag the home button icon into the Firefox toolbar.

To hide the button, you will need to drag and drop the home button from the Firefox toolbar to the screen. And that’s it.

So, this is how you can hide or show the Home button in the Chrome, Edge, and Firefox browser. Try the procedures now, and to know more, please follow our official website.

Source : https://blogsoftweb.com",5197
"Is Metal Cutting Possible Only With Air?

Taiwan Plasma Corporation’s PLACUT series has well experienced exclusive feature of Metal Cutting Only With Air. Our professional Plasma machines of 80 to 200 Amps are available with Air cooled & water cooled torches. Extremely reliable Power Source & Robust air plasma torch for cutting, gouging in 2D & 3D. It cuts Mild steel, Stainless steel, copper, Aluminium with high speed, ease & good quality.

Features & Benefits:

Built with highly energy efficient IGBT digital inverters saves upto 40% power compared to conventional plasma.

Easily readable digital display of operation parameter & Error codes helps low skill user to monitor the machines

Highly engineered ergonomic torch with latest consumable technology helps cut, gouge most economically yet clean and fast

Just plug in 3 phase grid, connect compressor, you are ready to cut, any new comer can use it.

High portability, single knob controls and a stable cutting arc makes it popular with user in no time

Comes with Manual & CNC functions as standard.

Visit our website to know more-https://taiwan-plasma.com/product/placut-80p100p120p-manual-mechanised/",1167
"Mobility as a Service Market Size | Share | Analysis | Trends — Global Forecast to 2027 | MarketResearchFuture Ketan Wagh May 5·4 min read

Overview

The mobility as a service market is anticipated to grow by US$484.20 Billion by 2024 rapidly growing at a CAGR of 38.9% during the forecast period. The global mobility as a service market is driven by rapid expansion of public infrastructure and favorable government policies. These factors have helped shape the Mobility as a Service Market and are expected to boost the growth. Companies in the mobility as a service market could also face challenges such as concerns regarding the safety of MaaS systems. The details covered in the mobility as a service market report cover all the aspects of the industry. Mobility as a service market analysts have also shared growth projections in the report and have suggested mobility as a service market player to plan growth strategies accordingly.

Market Segmentation

The global mobility as a service market has been segmented based on type, service type, business model, and application platform. On the basis of type, the market for mobility as a service is segmented based on public and private. Additionally, the market on the basis of service type, is segmented into car, bus, ride. The global market for mobility as a service is also covered based on business model segment which is further split into business to business, business to customer and peer to peer. On the basis of application platforms, the market for mobility as a service is segmented based on android, ios and others.

Factors like growing demand for mobile applications and low development cost support the mobility as a service market growth. The performance of mobility as a service market has also been studied for the past and current years. Additionally, the mobility as a service market report provides analysis of these segments. The mobility as a service market segmental analysis provided in the report offers major details about the mobility as a service market performance and future.

Get a Free Sample @ https://www.marketresearchfuture.com/sample_request/3109

Regional Overview

Companies in the mobility as a service market are spread across the world. The mobility as a service market report provides major information about regional markets of North America, Europe, Asia-Pacific (APAC), and the rest of the world. The North American mobility as a service market has many companies across the United States, Canada, and Mexico. Europe has companies in the mobility as a service market across Germany, France, Spain, Italy, and the United Kingdom. A detailed analysis of the mobility as a service market across India, China, and Japan in the Asia-Pacific region is also presented in the report. The mobility as a service market of the Middle East, Africa, and other regions has also been studied by analysts. The regional analysis of mobility as a service market can be found in the market research report.

Competitive Landscape

The mobility as a service market is supported by widespread use of social media and easy availability of funds. The population growth around the world and increasing demand of mobility as a service market based services and products also support the market growth. However, the mobility as a service market growth can be affected due to a poor operational environment. The report covers all such details which will help companies in the mobility as a service market to strengthen their business plan and improve their product portfolio. The mobility as a service market research report also provides company profiles of major companies. The company profile of organizations operating in the mobility as a service market discusses strategies, growth plans, size, revenue, and mergers and acquisition details of top companies in the mobility as a service market. New entrants and established players can plan their strategies based on this data provided in the mobility as a service market research report.

Industry News

The purchase of the leading mobile vehicle repairs and servicing company extends Cox Automobile Mobility Fleet Optimization and marketplace services capabilities and global scope. The mobile repair services specialist at Dickinson will be the gateway to the future success direction of Pivet. Dickinson Fleet Services’ technical learning can be extended to benefit Cox Automotive partners and consumers learning, growing and implementing in the space of mobile fleet services.

Browse Complete Report @ https://www.marketresearchfuture.com/reports/mobility-as-a-service-market-3109

About Market Research Future:

At Market Research Future (MRFR), we enable our customers to unravel the complexity of various industries through our Cooked Research Report (CRR), Half-Cooked Research Reports (HCRR), Raw Research Reports (3R), Continuous-Feed Research (CFR), and Market Research & Consulting Services.

MRFR team have supreme objective to provide the optimum quality market research and intelligence services to our clients. Our market research studies by products, services, technologies, applications, end users, and market players for global, regional, and country level market segments, enable our clients to see more, know more, and do more, which help to answer all their most important questions.

Contact:

Market Research Future

+1 646 845 9312

Email: sales@marketresearchfuture.com",5432
"We are incredibly excited to announce that we are collaborating with Festo and the Manufacturing Analytics Group at the University of Cambridge, Institute of Manufacturing (IfM), to provide research and recommendations to successfully develop a multi-agent system architecture for distributed manufacturing. With the use of our Fetch.ai technology stack, including the Autonomous Economic Agents framework and blockchain in synchronized harmony, our goal is to transform the existing manufacturing control systems, delivering a scalable solution for the 21st century and beyond.

Despite advancements in technology, the manufacturing industry remains rife with challenges and inefficiencies, lowering productivity, utilization, production variety. Distributed Manufacturing is a relatively new paradigm proposed to overcome some of these challenges. In Distributed Manufacturing, producers lease excess capacity for customized, low volume high variety orders. Whilst a promising approach to improve productivity and reduce wasted capacity, the take up of Distributed Manufacturing itself has been difficult. One of the issues is a lack of automated mechanisms to match suppliers and buyers. Firms need to spend manual effort to orchestrate matches, which are unlikely to outweigh the cost benefits obtained from a Distributed Manufacturing approach. Another issue has been the monopolization of economic transactions by platform providers, which results in suppliers having to succumb to pressure for reducing prices.

For years, multi-agent systems (MAS) architecture has been considered a possible solution to reducing the above issues associated with the conventional, centralized manufacturing orchestration. MAS offers a way to automatically allocate suppliers of services to buyers, without the associated manual transaction costs. It also allows for decentralized matchmaking, reducing the power of platform providers in suppliers. MAS take up has been slow due to a lack of suitable infrastructure. Ultimately, the missing link has been the application of cutting-edge research in AI and the connection with the blockchain technology that helps us understand the benefits that multi-agent systems can provide within the distributed manufacturing sector.

This collaboration will bridge these gaps, shedding light on the lack of current industry applications available to act as benchmarks to capitalize on the solutions multi-agent systems can provide to the distributing manufacturing sector.

Multi-agent systems to achieve distributed manufacturing

Utilizing multi-agent systems as a solution to distributed manufacturing will address the following:

Improve flexibility and computational efficiency, thus enhancing overall system performance. Create distributed platform economies by providing a more natural way of representing task allocation, team planning, user preferences, open environments, and so on, through autonomous agent interaction. Alleviate concerns over the handling of confidential information and creation of bottlenecks.

Our findings so far include

Our test results show that using multi-criteria decision-making allows order agents, with varying preferences, to select the best manufacturer for their needs.

The simulation testbed can be used by Festo to run additional tests and find problems with new features before they are deployed in the real marketplace.

The research has identified the need to reduce the number of bottlenecks and advance the optimization functionality of agents. This will be a trade-off between added complexity to allow more flexibility for manufacturers to use the method that is best suited to their needs.

The Fetch.ai-Festo-University of Cambridge collaboration is set to continue to develop these approaches for further cases and suggest guidelines for future development directions to develop multi-agent systems for distributed manufacturing. Stay tuned for more information and updates on our blog.",3973
"What is the role of parents in determining the screen (time) habits of children? Close Relationships May 5·2 min read

As Meninas — Karyna Iglesias (2015)

In the modern world, screen access has become easier for children as well as for adults. The more time parents spend in front of the screen, the more their children follow suit. Instead of complaining about kids’ screen time, parents need to find ways to make a change in their own behaviors and reduce their own screen times.

Parents’ manners and inhibiting attitudes are effective in determining the screen time of children. However, when children take a participatory role in determining screen time, they can define their relationship with the screen more easily. The World Health Organization also states that children under the age of 2 should not be exposed to the screen and that children aged 2–4 should be limited to a maximum screen time of 1 hour per day. In addition, it is emphasized that it is paramount for children aged 1–5 to spend at least 3 hours a day physically active, away from the screen, and in nature.

On the other hand, technology/screen use can take forms such as video chatting with friends, continuing distance education, and playing online games. These forms benefit children in developing their creativity and learning to act collaboratively. However, excessive screen use harms children’s sensory systems and ability to focus. Spending excessive time in front of the screen can even cause obesity due to reduced physical activity.

Here is a song and a movie that handles the impact of technology in our lives:

The movie: Disconnect (2012)

Disconnect, a movie about how the internet changes the daily life of people from multiple storylines that become intertwined. From an underage online stripper whose life changes when being contacted by a newscaster for an interview to a socially excluded high schooler who gets cyberbullied with a catfishing scam, the movie reveals many ways people can get disconnected from the real world. In the rest of the movie, we are shown that despite the unresolved disconnectedness of the virtual world, we can find connections in the real one.

The song: Father John Misty — Total Entertainment Forever

Pure Comedy, the 3rd album released by American musician Josh Tillman, under the name of Father John Misty, differs from his other albums with its baroque pop sound and a more political lyric content. In Total Entertainment Forever, the artist draws a dark picture about how putting technology before human life negatively affects society. The song shames us on our daily use of technology, telling us to cut back for the sake of future generations.

Source: Nwankwo, F., Shin, H. D., Al-Habaibeh, A., & Massoud, H. (2019). Evaluation of Children’s Screen Viewing Time and Parental Role in Household Context. Global Pediatric Health, 6.",2869
"The world’s first blockchain-based crowdfunding social network dedicated to data rights management & licensing for film, TV & video content announces an integration with Moonbeam.

Livetree — a video based social network dedicated to crowdfunding video stories based on the United Nations’ 17 Sustainable Development Goals (SDG) is pleased to announce an integration with Moonbeam. Livetree is a video streaming platform with a web 3.0 decentralised infrastructure and video data protocol for crowdfunding, data rights and video licensing.

Moonbeam, the Ethereum-compatible smart contract platform on Polkadot, provides a developer platform that enables Solidity contracts to easily be deployed with limited changes required. The Livetree team has completed the first phase of integration on Moonbase Alpha TestNet with a successful deployment of Livetree’s existing Ethereum based contracts to Moonbeam.

The full integration will involve importing all of Livetree’s Ethereum based crowdfund and data governance smart contracts to Moonbeam as well as creating a bridge from Ethereum MainNet. The integration aims to address Ethereum’s restrictive gas prices which prevent true on-chain governance, all while facilitating a bridge to the Polkadot ecosystem. Once complete, this will enable users to create videos via Livetree’s new app and use the related video protocol data to generate rewards for those who crowdfund the stories that matter to them, via Livetree’s video protocol data tokens. The resulting data tokens can then be staked and acquired in the Polkadot ecosystem via Moonbeam in an open, secure fashion.

The integration with Moonbeam offers several additional benefits. By integrating to Moonbeam, it opens up the Polkadot ecosystem to be leveraged by Livetree in a variety of ways. For example, the possibility to utilise a specialised parachain that could convert Livetree’s video data protocol tokens to NFTs in an environmentally friendly fashion. This could also make the data protocol tokens available via Polkadot based marketplaces specific for royalty management and programmatic advertising.

“It’s been a pleasure working with the Moonbeam team. We have long envisioned being able to create our own blockchain that has been tailored to our requirements. The costs and overhead with building and maintaining a blockchain has always been a drawback, especially with Ethereum’s popularity we needed to ensure there was compatibility. It is rare to find technical support in the blockchain ecosystem and Moonbeam is setting a precedent which I think surpasses anything I have seen in the space.” Said Ashley Turing, Livetree CEO.

“The values and goals of the Livetree platform are ones that the Moonbeam ecosystem would be proud to support. Providing ways for users to interact with their end users through a sustainable platform really underscores the value of proof-of-stake blockchains.” Says Nate Hamilton, Director of Business Development for Moonbeam. “We’re happy to have the Livetree team building on Moonbeam and look forward to their continued success.”

Why Livetree can revolutionise video online

Given the global environmental, biodiversity and human crisis we currently face it is clear governments, corporates and social media have failed to provide the fair playing field of media and use of data online. Dominated by often biased, opaque recommendation algorithms, we have seen these entities create dangerous echo chambers which only go on to exacerbate existing world issues. Livetree has embraced Web 3.0 technologies, and with the advent of Polkadot, now has the scalability to deliver its promise of decentralisation. The revolutionary video data protocol makes the use of data online more transparent and available for new data markets. Livetree also offers the possibility to crowdfund and create direct action for the stories that matter to you.

About Livetree

Livetree is a media technology company headquartered in London, UK. Founded in 2015, Livetree champions sustainability as one of their core values, and in turn, has helped them forge a community of brands, thought-leaders and experts, who all have the same shared values but most importantly a shared goal: to help save the planet.

Through positive stories, Livetree brings together the highest ethical, social, and governance standards to create and connect communities through stories that matter. Livetree have already invested significantly into the technology of over-the-top (OTT) video streaming, crowdfunding and video machine learning while having developed a sizable network of leading content providers (BBC, Channel 4 + 30+ others). Livetree is the next generation of open and transparent social network that is set to reimagine how you watch, interact and share content.

Livetree was founded by CEO and Founder Ashley Turing, a serial entrepreneur with more than 20 years of experience in technology innovation.

About Moonbeam

Moonbeam is an Ethereum-compatible smart contract platform on the Polkadot network that makes it easy to build natively interoperable applications. This Ethereum compatibility allows developers to deploy existing Solidity smart contracts and DApp frontends to Moonbeam with minimal changes. As a parachain on the Polkadot network, Moonbeam will benefit from the shared security of the Polkadot relay chain and integrations with other chains that are connected to Polkadot. Currently, in active development by PureStake, Moonbeam is expected to reach MainNet by Q3, 2021. Learn more: https://moonbeam.network/.

About Ashley Turing, founder and CEO of Livetree

Livetree CEO and founder Ashley Turing is an entrepreneur with more than 20 years of experience in technology innovation. After graduating in computer science, he worked with a team from the Massachusetts Institute of Technology (MIT) before moving to Microsoft. Turing has also worked on technology projects in areas including online gaming and investment banking. Based on his first-hand experience of the global financial crisis and the near collapse of the world’s financial system, Turing launched Livetree as an alternative sustainable model for economic development.

For press enquiries contact Winifred Hewitt-Wright E: connect@livetree.co.uk

Discover & Join Livetree here",6303
"While Long Covid brings about a lot of bad news, the good news is that there is not a single type of organic damage found across all Long Covid sufferers which could explain even a fraction of the complex and evolving symptoms. This indicates Long Covid is due to dysfunction and not damage, making it easily treatable with the right understanding. While there is organic damage found, there is nothing causative and it only seems to be a side product of another process.

The Dysfunction, Not Damage Theory is that the acute phase of COVID19 causes a number of metabolic shifts, pushing the body into metabolic states which are stable but cause dysfunction. Combined with a small amount of (reversible) damage to the mitochondrial membranes, dysfunction of iron homeostasis and an unbalanced microbiome — this seems it can explain all of the complex and evolving symptoms of Long Covid.

Dysfunction, Not Damage Theory:

An acute oxidative event causes iron mobilization into the brain and other tissues, causing auto-oxidation, which produces massive oxidative stress and destroys cellular lipids, antioxidants and DNA. The bodies DNA repair system, PARP activates but due to oxidative conditions depletes NAD+and its building blocks, this triggers what I refer to as the “Niacin Sink Trap”.

At the same time, the production of vitamin B12 is interrupted, because the cobalamine building block is oxidized, triggering the “Methyl Trap”. Both the Niacin Sink Trap and Methyl Trap are stable metabolic dysfunction which cause a variety of evolving and complex symptoms.

The iron which was mobilized causes oxidative damage to cell membranes and DNA, since the body doesn’t have a mechanism to regulate extra iron well and is bound in the form of ferritin and possibly other compounds. These leak iron due to the bodies pro-inflammatory state and causes evolving and complex symptoms.

The mitochondrial membranes that got oxidized stick around as they get complexed with proteins, causing fatigue and lower bioenergetic functioning as they can not function properly.

The microbiome gets remodeled due to iron absorption, triggering microbial community changes, growth of more pathogenic microbes and disruption of gut/brain axis.

In some people, the oxidative stress/damage will reactivation dormant neurotrophic viruses like EBV and oxidized cell membranes will harbor any dormant Lyme disease pathogens. Parasites may overgrow due to the iron mobilization and oxidative stress as well as the microbiome remodeling.

Any one or more of these things happen together, many of them playing off each-other in feedback loops, with the resulting symptoms depending on the deficiencies in the vitamin co-factors and genetic predisposition at the time of oxidative assault, as well as any dormant infections.

Analysis based on symptom time progression research:

A recent study which surveyed thousands of participants all over the world regarding their disease progression and symptoms, then used statistical analysis and natural language processing found that there are three clusters of symptom progression over time which are highly correlated. This indicates that there are three main mechanisms in long covid and an analysis of the three graphs below indicates the mechanisms are coupled together.

Looking at the clusters below, it appears the three clusters correlate to:

Cluster 1 — Dysbiosis of the Microbiome

Cluster 2 — Niacin Sink Trap and Methyl Trap, Metabolic Dysfunction

Cluster 3 — Iron Mobilization and Dysfunction

This validates the “Dysfunction not Damage” theory of Long Covid and a further analysis of the symptoms and how they correlate to each of these mechanisms paints a very strong picture of the driving mechanisms behind Long Covid.

Analysis of Disease Mechanisms Based on Time Course of Symptoms:

Clusters of symptoms reported over time, from the paper, “Characterizing long COVID in an international cohort: 7 months of symptoms and their impact”

Cluster 1 — Dysbiosis of the Microbiome

Symptoms from the research paper:

Diarrhea, Loss of Appetite, Vomiting, Runny Nose, Sore Throat, Dry Cough, Rattling of Breath, Elevated temperature and Fever

Analysis:

The early peak and rapid drops in symptom prevalence over time in this cluster indicates it is due to an initial assault, this looks like microbiome disruption and bacterial mobilization and then re-balancing over time. The symptoms of GI and respiratory issues as well as fever are a strong indicator that is is microbial related. Iron mobilization will cause disruption of the microbiome, systematic mobilization of bacteria and a switch to pathogenic activity. This microbiome remodeling appears to be a compensation mechanism for the burst of free iron.

Lab Tests:

Microbiome analysis, organic acids panel

Cluster 2 — Niacin Sink and Methyl Trap

Symptoms from the research paper:

Fainting, Pain/Burning in Chest, Tachycardia, COVID toe, Abdominal Pain, Nausea, Bone Aches or Burning, Muscle Aches, Tightness in Chest, Acute/Sudden Confusion, Changes to Smell and Taste, Dizziness/Balance Issues, Hallucinations, Headaches, Insomnia, Other Sleeping Problems, Sleep Apnea, Slurring Words/Speech, Breathing Difficulty with Normal Oxygen Stats, Cough with Mucus Production, Coughing up Blood, Respiratory and Sinus Issues, Shortness of Breath, Sneezing, Chills/Flushing/Sweats, Fatigue, Low Temperature

Analysis:

The symptoms look like the combined symptoms of the two metabolic traps (niacin sink and methyl trap), showing a small peak on the front-end (most likely due to early pro-oxidative state) coupled to the other two charts, and then generally flat line activity indicative of a stable metabolic trap.

Drivers of Symptoms:

The Niacin Metabolic Trap will create symptoms of pellegra such as the well known “COVID Toe”, abdominal pain and others. This metabolic trap hijacks tryptophan to produce NAD+ and will prevent the production of circulatory serotonin and melatonin, cause issues with mood and sleeping/dreaming including inability to enter REM sleep and strange dreams. This will also cause release of neurotoxic quinoloic acid type compounds causing issues with mood, irritability, seizures and headaches. The core issue of NAD+ depletion will cause low energy, fatigue, post exercise malaise and many other issues. This state will lead to immune dysfunction as well as a lack of stimulation of bone marrow, leading in some cases to tooth loss, fragile bones and other issues.

The Methyl Metabolic Trap will create symptoms depending on the genetic makeup of the persons methylation system. Most common would be the symptoms of vitamin B12 deficiency including: anemia, “COVID tongue”, shortness of breath, feeling cold, neurological damage dizziness, irregular heartbeats, weight loss, numbness or tingling in your hands and feet, muscle weakness, personality changes, unsteady movements, mental confusion or forgetfulness.

Many people will have symptoms of hypohomocysteinemia as homocystein builds up due to the broken methyl cycle, these include: seizures, psychiatric issues, eye abnormalities, vascular issues. Many people will also have symptoms of glutathione deficiency including slowing down of physical reactions, speech (psychomotor retardation), intellectual disability and a loss of coordination (ataxia). The methyl trap will also cause issues with gene expression, epigenetic changes and demyelination of nerves.

The broken methylation cycle will present with histadelia symptoms (high histamine) such as: muscle pain, headaches, fatigue, insomnia, irritability, anxiety, paranoia, being suspicious, and hallucinations. This can also cause issues with sulfur metabolism, leading to the “COVID smell”.

Lab Tests:

Niacin Sink Trap — Organic Acids Panel (tryptophan metabolites high)

Methyl Trap — Organic Acids Panel (methylmalonic acid high, uracil high), Low SAM/SAH ratio, Homocystein high, Histamine high, General Methylation Status Test

Cluster 3 — Iron Mobilization and Dysfunction

Symptoms from the research paper:

Bradycardia, Palpitations, Visibly Inflamed/Bulging Veins, Dermatographia, Other Skin and Allergy, Peeling Skin, Petechia, Skin Rashes, Constipation, Gastroesophageal Reflux, Hearing Loss, Other Ear/Hearing Symptoms, Tinnitus, Vision Symptoms, New Allergies, New Anaphylaxis Reaction, Joint Pain, Muscle Spasms, All Sensorimotor Symptoms, Brain Fog, Memory Issues, Nerve Pain, Speech/Language Issues, Tremors, Vibrating Sensations, All Menstrual/Period Issues, Bladder Control Issues, Other Temperature Issues, Post Exertional Malaise

Analysis:

If you overlap cluster 1 and cluster 3, they have a strong anti-correlation. Bacteria in the body will absorb excess iron, then they will mobilize and go out of balance and not be able to absorb it anymore as the microbiome is remodeled. This explains why cluster 3 (iron based) ramps up as cluster 1 (microbiome based) ramps down.

Drivers of Symptoms:

Iron mobilization and dysfunction will cause symptoms of hyperferritinemia/neuroferritinopathy such as: joint pain, abdominal pain, lack of sex drive, anti-phospholipid syndrome (sticky blood syndrome), auto-immune conditions such as lupus/rheumatoid arthritis, macrophage activation syndrome, eye damage, cardiac abnormalities, neurodegeneration, micro-clotting and vascular dysfunction, rhythmic shaking, internal tremors, involuntary muscle tensing, issues with swallowing or vocal cords, neurological symptoms which are only on one side of the body, general neuropathy. It will also cause symptoms which mimick dysautonomia. Iron is a driver of chronic diseases such as cardiac disease, cancer proliferation and secondary infections. This class of symptoms are MUCH more common in long haulers who got sick around March 2020, particularly those with internal tremors/vibrations.

Lab Tests:

Iron panels and ferritin do not have correlation to iron deposited in tissues, but may be dramatically high or low, indicating disruption of iron homeostasis.

8-OHdG, a DNA damage biomarker, may be most relevant and will be elevated if you have iron releasing from tissues causing oxidation.

High homocystein has been correlated to iron overload disorders although it is also expected to be elevated due to the methyl trap.

MRI with an educated technician can determine brain iron accumulation based on specific signal characteristics.

Observation of ferritin based Fleischer Rings in the eyes, in some visible under blacklight.

Transcranial sonography can be used to detect changes in some parts of the brain in brain iron accumulation disorders.

Other — Oxidative Damage to Membranes

Analysis:

There are other potential mechanisms which are not as pronounced such as oxidative damage to the mitochondria. There are very few long term tissue changes due to an intensive oxidative event, but this is one of them. secondary infections in terms of bacteria, neurotrophic viruses (EBV) and parasites. Specifically Lyme disease pathogens are known to thrive in an environment with oxidized cell membranes. These are secondary to the main metabolic traps and iron dysfunction and are also treatable/reversible.

Symptoms:

chronic fatigue, post exercise malaise and Lyme disease reactivation

Lab Tests:

Cellular respiration test

Other — Reactivation of Secondary Infections

Analysis:

Secondary infections in terms of bacteria, neurotrophic viruses (EBV) and parasites seem to be common in those suffering from Long Covid, as well as triggering of CIRS, or Chronic Inflammatory Response Syndrome in those that have historical exposure to biotoxins such as mold exposure. Specifically Lyme disease pathogens are known to thrive in an environment with oxidized cell membranes. These are secondary to the main metabolic traps and iron dysfunction and are also treatable/reversible. While Epstein-Bar virus is often reactivated in acute COVID19, anecdotally it does not seem to be a main driver of issues and blood brain crossing antivirals do not seem to be a magic bullet for long covid, EBV reactivation seems to be a consequence of an acute oxidative event and glutathione depletion.

Symptoms:

Diverse, complex and evolving

Lab Tests:

Testing for Lyme and associated pathogens is incredibly complex and misleading, its important to work with a doctor trained in Shoemaker or Klinghardt methods.

A lyme co-infection, Bartonella, often leaves visible “Bartonella Striae” which look similar to vertical parallel stretch marks, if those recently appeared it is a good indicator to get tested for Lyme disease.

A diagnosis of CIRS can be made from specific immune panels in the Shoemaker tradition.

Epstein-Bar and other viral re-activations can be detected using viral antibody tests.

Key Research Papers:

If I had to hand researchers a number of papers to connect the dots around “long covid”, I would pick:

Niacin Sink Trap:

- The COVID-19 Burden or Tryptophan Syndrome: Autoimmunity, Immunoparalysis and Tolerance in a Tumorigenic Environment

- The oxidative stress-induced niacin sink (OSINS) model for HIV pathogenesis

Methyl Trap:

- COVID-19: A methyl-group assault?

Iron redistribution:

- The Hyperferritinemic Syndrome: macrophage activation syndrome, Still’s disease, septic shock and catastrophic antiphospholipid syndrome

Detailed Mechanisms

Detailed Mechanism — Niacin Sink Metabolic Trap

The Niacin Sink Trap has been documented and well researched, initially during the study of HIV by Dr. Ethan Taylor and recently in the context of COVID19, the downstream effects on tryptophan and other systems has been researched in a groundbreaking paper regarding Long Covid. The Open Medicine Foundation, a leader in Chronic Fatigue Research, supports a number of research initiatives into this mechanism which they generally call the “Metabolic Trap Theory of CFS”. A quality blog post goes into detail in the various aspects of OMF promoted metabolic trap theory. According to this theory, cofactors coupled to the oxidation/NAD+ cascade are critical to outcomes and this is seen in the observation that low selenium levels correlated to poor outcomes in China. Observations of high blood sugar being a predictor of death in COVID19 patients, regardless of diebetic status validate metabolic changes being a major risk factor of COVID19 according to NAD+ theory. Activation of the same kynurenine pathways predicted in NAD+ theory of COVID19 correlate to blood sugar deregulation. I wrote a dedicated blog post on the subject called “The Team of Doctors and Biohackers Who Seem to Be Successfully Treating “Long Covid””.

The theory is that COVID19 causes NAD+ depletion, some people with vitamin deficiencies, existing NAD+ depletion or genetic deficits in energy metabolism have long term NAD+ dysfunction. The body tries to make up for low NAD+ by feeding in tryptophan, which is the precursor for serotonin and melatonin, causing poor mood and sleep. Serotonin is also a master regulator in circulation and having low levels causes a number of issues including with the gut/brain axis. Every time tryptophan is fed into the cycle, large amounts of neurotoxic Kynurenine metabolites are produced. Furthermore, energy disruptions are specific to high metabolically active tissues such as the heart and brain. A number of recent metabolomics papers have found massive disruptions in tryptophan and kynurenine markers.

Biochemical cascades in a vicious feedback cycle based on the original work of Dr. Taylor, “The oxidative stress-induced niacin sink (OSINS) model for HIV pathogenesis”

Unrelated to the core mechanism, a recent publication in the journal Nature, stated: “vitamin B3 (niacin or nicotinamide) is highly effective in preventing lung tissue damage. It might be a wise approach to supply this food supplement to the COVID-19 patients.” A recent bioinformatics analysis also indicated niacin should be studied as a treatment for COVID19. A forward looking pre-print indicates nicotinic acid may act as a one of a kind bioenergetic “pump” of inflammatory molecules out of cells, critical for COVID19. Niacin has been seen to easily cure systematic NAD+ deficiency in clinical research. An informal survey of over 200 long covid sufferers, showed a high correlation between niacin intake and improvement in a recent video by a patient advocate journalist. This same patient advocate journalist interviewed both myself and Dr. Ade Wentzel regarding biochemical aspects of this theory. In terms of doing the most good for the most number of people, repleating NAD+ using nicotinic acid/niacin is incredibly helpful in Long Covid. Researchers have found that melatonin helps express GPR109A, the receptor that many of niacins therapeutic effects seem to come from and anecdotally the use of melatonin before niacin seems to signifigantly increase the beneficial effects. Melatonin is also an iron binder specific to the nervous system and is neuroprotective against metal auto-oxidative damage.

Detailed Mechanism — Methyl Metabolic Trap

While it has been understood since the 1980’s, the methyl metabolic trap is very well explained in the context of COVID19 in the very important paper, “COVID-19: A methyl-group assault?”. One group of researchers points out the genetic complexities in this methyl trap in the context of COVID19, as well as the resulting homocystein overload that can be present.

One piece of clinical research points out that there is a link between biomarkers of NAD+, oxidative stress, methylation, folate and B12 — this is interesting as it bridges the gap between the two metabolic traps. In a pro-oxidative environment, the unstable cob(I)alamin gets oxidized to cob(II)alamin. The paper states, “MS inactivation occurs when free radicals oxidise cob(I)alamin to a cob(II)alamin species. Re-activation requires methyl group donation by SAM” Other than oxidative stress, another mechanistic bridge seems to be the enzyme “Cob(II)alamin_reductase”, which is an NAD+ dependent enzyme that converts cob(II)alamin back to cob(I)alamin. A third possibility is the direct disruption of methionone synthase or catalysis of homocystein production by iron .

Proponents of copper/iron bio-energetic balancing point out that methionone synthase may very well be copper dependent and copper deficiency is driving the unbalancing of iron, oxidative stress and metabolic traps. Recent studies on rats being fed a copper deficient diet seem to validate this concept. There is a known “homocysteine paradox” where high homocystein correlates to cardiac and vascular disease yet interventions that aim to lower homocystein are not effective at lowering morbidity. Some researchers believe the homocysteine paradox is due to a lack of copper, (and iron homeostasis disruption) therefore homocystein is not the driver of disease but a biomarker. Multiple papers show excess dietary iron is known to disrupt copper homeostasis, which is required for proper regulation of iron transport. Whatever the mechanism, its clear the methylation/folate cycle is disrupted at the B12 dependant methionine synthase step.

Disruption of methionine synthase due to oxidative stress destroying cob(I)alamin

The Methyl Metabolic Trap will create symptoms depending on the genetic makeup of the persons methylation system. Most common would be the symptoms of vitamin B12 deficiency including: anemia, “COVID tongue”, shortness of breath, feeling cold, neurological damage dizziness, irregular heartbeats, weight loss, numbness or tingling in your hands and feet, muscle weakness, personality changes, unsteady movements, mental confusion or forgetfulness.

Many people will have symptoms of hypohomocysteinemia as homocystein builds up due to the broken methyl cycle, these include: seizures, psychiatric issues, eye abnormalities, vascular issues. Many people will also have symptoms of glutathione deficiency including slowing down of physical reactions, speech (psychomotor retardation), intellectual disability and a loss of coordination (ataxia). The methyl trap will also cause issues with gene expression, epigenetic changes and demyelination of nerves.

The broken methylation cycle can present with histadelia symptoms (high histamine) such as: muscle pain, headaches, fatigue, insomnia, irritability, anxiety, paranoia, being suspicious, and hallucinations. This can also cause issues with sulfur metabolism, leading to the “COVID smell”.

A mixture of hydroxycobalamine, folinic acid and TMG was used for treating functional methionine synthase deficiency which probably most mimics the Methyl Trap in this case. In general, TMG also known as Betaine, is a successful treatment for the high homocystein levels expected in this situation.

The T2-Hyperintensities which are the most common issue found on MRI’s after COVID19 and the rarer cerebral atropy have been linked to vitamin B12 deficiency. Vitamin B12 is known to be correlated to leukoencephalopathy and ataxia and in combination with potential brain iron overload, could explain the neurological findings in COVID19.

Detailed Mechanism — Iron Redistribution

Iron and COVID19:

There is strong indications of disruption of iron homo stasis in COVID19 — the paper “Iron: Innocent bystander or vicious culprit in COVID-19 pathogenesis?” reviews mechanisms and findings connected to iron in COVID19. Multiple papers have found that large amounts of free iron in the body are correlated to fatality, blood types with higher iron binding capacities correlate to fatality and iron dynamics are significantly interrupted in COVID19 patients. Iron is also a critical mechanism to mediate bacterial infections and forward looking research shows anomalous presence of bacterial DNA reads and biomarkers in COVID19 clinical data indicating attack of iron utilizing anaerobic Prevotella sp. bacteria on the blood.

COVID19 may most similarly mimic cadmium poisoning, where a catalytically active metal is causing systematic oxidative stress. It is relevant to note that in cadmium poisoning, the symptoms overlap perfectly with COVID19, including “cytokine storm”, anosmia and ground glass lung opacities. The cadmium poisoning mechanisms have been well studied and an analysis will show its close relationship. Combined with the many papers linking COVID19 and systematic oxidative stress, it paints a picture which looks more like a metal catalyzed oxidative disease than a respiratory illness. In fact, the whole “cytokine storm” concept in COVID19 seems to be pop-science, a recent paper indicates that the cytokine storm component of COVID19 is minimal, it appears to be an “oxidative storm”.

A paper titled, “COVID-19 gone bad: A new character in the spectrum of the hyper-ferritinemic syndrome?” goes into detailed and compares COVID19 biomarkers to hyper-ferritinimia biomarkers in the acute phase, but does not discuss the long term issues. Another paper links issues with the blood to ferritin biomarkers and states, “Strongly associated with the COVID-19 coagulopathies is the presence of hyperferritinemia”. Another paper discusses the role of iron chelators and COVID19 being a hyperferritinemic syndrome. A paper discussing iron nano-particle pollution and COVID19 mechanisms states, “altered iron balance favoring excess reactive or catalytic iron may be the single most important underlying pathological process predisposing to severe COVID”.

Some manifestations of Long Covid strongly mimic the unique symptoms of hyperferritinemia and other iron overload syndromes such as Neurodegeneration with Brain Iron Accumulation (NBIA): High glucose/glucose intolerance, joint pain, stomach pain, fatigue, cardiac abnormalities, changes in menstration are the common symptoms of NBIA.

Iron and oxidative stress:

Iron causes rampant oxidation in the body through energetic oxygen radical production, including via the extremely powerful Fenton reaction. Iron will oxidize any materials it comes into contact with, specifically cellular membrane lipids (such as the mitochondrial membrane) and DNA. Recently, a paper was published which reviewed biomarkers of oxidative DNA damage and found a strong correlation to COVID19 severity. Another paper which looked at Ascorbate levels which would be reduced during oxidative stress, states: “Our study revealed that vitamin C levels are undetectable in more than 90% of the patients included. The mechanisms of this significant reduction in vitamin C are uncertain.” A study of antioxidants as well as biomarkers of oxidative stress in COVID19 patients shows dramatic patterns of low antioxidants and high oxidative stress. Thiol levels, a common biological group very sensitive to oxidation is highly correlated to COVID19 disease progression in a recent paper. The serum thiol biomarker levels tested are considered a direct measure of redox status because of their rapid reaction with oxygen radicals.

A recent publication reviewed free DNA in the body and its correlation to disease state and found some astounding correlations, not only did COVID19 highly involve DNA fragments in circulation, but patients treated with experimental antivirals, experimental immunsupressives, hydroxychloroquine and/or the standard of care did not have changes in free DNA, as if the viral load was not correlated to free DNA. Its relevant to point out that free DNA itself can trigger cytokine storm and tissue damage.

Even more interesting was that a large percentage of this free DNA came from erythroblasts, the red blood cell precursors found in the bone marrow. This is a shocking discovery as damage this aggressive to the red blood cell precursors in the bone marrow can not be easily accounted for by any current viral mechanism. Interestingly, erythroblasts contain a large amount of iron (main site of iron utilization) and a structure that is not stable and resistant to auto-oxidation like mature red blood cells.

An interesting paper about COVID19 liver damage stated, “None of the patients suffered underlying liver pathologies. Histopathological and ultrastructural analyses were performed. The most striking finding we

demonstrated in all patients was iron accumulation into hepatocytes, associated with degenerative changes. Abundant ferritin particles were found enclosed in siderosomes, and large aggregates of hemosiderin were found, often in close contact with damaged mitochondria. Iron-caused oxidative

stress may be responsible for mitochondria metabolic dysfunction.”

Iron and the Microbiome:

A detailed review of microbiome changes due to COVID19 show an increase in the abundance of four genera, Streptococcus, Clostridium, Lactobacillus, and Bifidobacterium with a decrease in Bacteroides, Roseburia, Faecalibacterium,

Coprococcus, and Parabacteroides. There was an increase in opportunistic pathogens Streptococcus, Rothia, Veillonella, and Actinomyces.

Iron release into the microbiome can disrupt biofilms and cause the mobilization of bacteria, it can also convert these bacteria to “pathobionts” or potentially harmful bacteria. In general, most pathogenic bacteria possess more efficient pathways to acquire free iron than beneficial bacteria as it is essential for virulence expression and replication.

Iron overload is known to alter the microbiome, particularly the tryptophan metabolism, which is already known to be causing issues in the niacin sink trap. This is another example of multiple mechanisms creating feedback loops and catalyzing each-other in Long Covid. Some researchers speculate that altered tryptophan metabolism could account for all long covid symptoms.

Interestingly, this is the exact mechanism proposed to contribute to Alzheimer’s Disease, known as the “Iron Dysregulation and Dormant Microbes (IDDM) hypothesis”. A recent paper states, “The simultaneous iron dysregulation and microbial aberrations affect the hematological system, promoting fibrin amylodiogenesis, and pathological clotting. Systemic inflammation and oxidative stress can contribute to blood brain barrier permeability and the ensuing neuro-inflammation, characteristic of Alzheimer’s type dementia's.”

Iron and the Brain:

Excess iron in the brain can cause a number of issues, including the micro-hemorrhaging seen on COVID19 MRI studies. This disease state is known as neuroferritinopathy or the general term, Neurodegeneration with Brain Iron Accumulation (NBIA). This could explain some of the strange symptoms of “long haulers” such as neurological changes on just one side of the body. Research around iron dysfunction in the brain and nervous system is one of the most exciting areas of medicine, with a recent paper stating: “A major feature of virtually all neurodegenerative diseases is the accumulation of excess iron.” In general, excess iron levels are associated with Alzheimer’s disease, Parkinson’s disease, Huntington’s disease, Friedreich’s ataxia and other neurological disorders, cancer, Fanconi anemia, stroke, heart disease, diabetes and ageing. A fascinating video discussions how iron accumulation and dysregulation may be a core mechanism of aging and age related diseases.

Iron dysregulation in the brain may be responsible for the loss of smell in acute COVID19, with the olfactory bulb being particularly sensitive to iron dysregulation and a “canary in the coal mine” of neurological iron dysregulation. This could indicate why the same anosmia is seen in cadmium toxicity.

Ferritin deposited in the brain will show up on MRI according to a review, “On brain MRI in neuroferritinopathy, iron deposits are observed as low-intensity areas on T2WI and as signal loss on T2(∗)WI. On T2WI, hyperintense abnormalities reflecting tissue edema and gliosis are also seen. Another characteristic finding is the presence of symmetrical cystic changes in the basal ganglia, which are seen in the advanced stages of this disorder. Atrophy is sometimes noted in the cerebellar and cerebral cortices.”.

T2 hyperintensities of white matter is one of the most common reported MRI findings in those that are suffering from long covid and a paper on iron storage disease states, “T2 hyperintensities in white matter have been reported in most NBIA (Neurodegeneration with Brain Iron Accumulation) subtypes”.

A number of long haulers are having vision degeneration, while changes in vision such as a loss of color or partial field of vision could be caused by the methyl trap, issues with the lens is a common manifestation of hyperferritinimia called Hyperferritinemia Cataract Syndrome. Ferritin accumulates in the eye causing a progressive distortion of the lens.

This seems to indicate why many people talk about “COVID eyes”, zombie/glassy looking eyes that seem to have been common in the acute phase in March 2020. Long haulers which have noticed changes in the color of their eyes or are having vision issues should ask their doctor to check for “Fleischer’s Ring”, an indication of ferritin in the eye that is easily seen. Interestingly, a number of long haulers with symptoms of vibrations and internal buzzing noticed that under a blacklight they had fluorescent Fleischer Rings (different than the copper containing Kayser-Fleischer Rings, even 18 months after infection.

Fleischer Rings fluorescing under blacklight in multiple long haulers with buzzing/vibrating feelings and electro-sensitivity

Another diagnostic sign which can indicate iron overload is the “Iron Fist”, anecdotally many long haulers with Cluster 3 symptoms notice this. The method is to try and form a fist and see if your first two fingers have trouble closing or are painful at the top two joints, this is called the “iron salute” sign of iron overload. In general joint pain is a strong indication of iron overload.

Its not clear which form iron is taking during COVID19 tissue accumulation as MRI research had found that iron exists in the form of the mineral magnetite in human brains and this may be contributing the brains electromagnetic interactions. In fact, a paper titled “Iron Oxides in the Human Brain” states “There is evidence for the formation of six of the recognized iron oxides in human brain tissue, namely, ferrihydrite, goethite, hematite, magnetite, maghemite, and wüstite”. It goes on to discuss the organic and other forms that are present in the brain, “Brain iron can be classified as heme or nonheme iron, where the latter takes many distinct forms within the human brain, including labile (unbound, ionic) iron and low molecular weight complexes and metalloproteins with one or more individual iron ion binding sites. Dense clusters of hundreds or thousands of iron ions can occur where iron is sequestered or deposited in iron storage proteins such as ferritin, in storage protein degradation products such as hemosiderin, or in pathological intra- and extracellular deposits and inclusions. In the pigment lipofuscin, iron is loosely bound along with a range of other metal elements . This contrasts with iron bound in the pigment neuromelanin and in ferritin and hemosiderin, where iron oxides or hydroxides (referred to collectively here as oxides) are formed with structures ranging from amorphous to highly organized crystalline structures.” Iron oxide compounds in the nervous system are known to create a feedback loop of oxidative stress, more iron accumulation and protein aggregation leading to neurodegeneration.

Iron dysfunction and the blood:

A recent paper discusses the unique aspects of iron induced fibrogen clots, they are not only remarkably resistant to degradation but they have an ability to “capture” red blood cells and cause further damage. The reason they are resistant to healthy degradation is due to the iron causing active oxidation to the enzymes trying to break them down. The study found tested multiple stabilization agents and found that sodium salicylate (aspirin) was effective against these very specific types of iron induced fibrogen clots while biologics were not able to break them down. A study of patients 60 days after COVID19 stated, “60 days after disease onset, 30% of subjects still presented with iron deficiency and 9% had anemia, mostly categorized as anemia of inflammation. Anemic patients had increased levels of inflammation markers such as interleukin-6 and C-reactive protein and survived a more severe course of COVID-19. Hyperferritinemia was still present in 38% of all individuals and was more frequent in subjects with preceding severe or critical COVID-19.”

This strange decomposition resistant fibirin-amyloid type microclotting is the exact type seen in recent research from one of the worlds experts on spike protein and microclots and her research indicates that they are persistent after the acute COVID19 phase. Another study shows the connection between iron overload diseases and the strange blood clotting and red blood cell shape changes seen in COVID19.

Long haulers often suffer from “Sticky Blood Syndrome” where it is incredibly difficult to draw blood from them, this is a known effect of anti-phospholipid syndrome, an autoimmune disease commonly seen after COVDI19. While multiple studies show that COVID19 causes auto-antibodies targeting the blood, the root cause has not been identified. These exact same autoimmune diseases are seen in hyperferritinemia, triggered by free iron releasing in the blood. In fact, iron effecting the blood in this way can lead to “Catastrophic Anti-phospholipid Syndrome”, which one paper theorizes could be a mechanism of COVID19 fatality. A meta-analysis found almost half of patients hospitalized with COVID19 developed the same auto-antibodies that are known to be induced by iron. This same paper discusses iron causing macrophage activation syndrome. Its interesting to point out that in ferritin triggered macrophage activation syndrome, the general treatment is corticosteroids, the same treatment which is used in acute COVID19 cases and seems to prevent the manifestation of some long term side effects, but coming with its own issues.

This macrophage activation comes from their inability to process iron properly and get “fat with iron”, which some researcher believe is due to issues with copper/ceruloplasmin enzyme and a driver of auto-immune disease. A study correlates the lung damage seen in COVID19 with iron levels, indicating a mechanisms of lung damage similar to the ground glass opacities seen in cadmium toxicity- metal catalyzed auto-oxidation. Tissue analysis shows iron filled macrophages in lung tissue from COVID19 patients. Another tissue study shows a number of iron abnormalities, including iron laden macrophages and iron dysregulation in the bone marrow.

Iron storage issues:

Ferritin can “leak” iron in a number of situations, most often due to the modification of the ferritin enzyme structure by degredation, but also through reductive mobilization means. NAD+ dependent oxidoreductase enzymes have been speculated to be the mechanism where free iron is released from ferritin, linking NAD+ depletion to iron homeostasis. The pro-oxidative, pro-inflammatory environment in long covid with depleted NAD+ will dramatically lower the stability and iron holding abilities of ferritin, causing a feedback loop. Dysfunction in redox functioning may be the most likely cause of leaking ferritin, as the Methyl Trap will deplete glutathione and with NAD+ depleted, there is a very critical unbalancing of the redox chemistry in the body. Glutathinone is highly involved in redox state but also directly in iron trafficking. Another effect of the Methyl Trap is potentially causing changes which destabilize ferritin, as homocystein is highly correlated to iron release.

A paper reviewing methods of iron release from ferritin states the two main degradation mechanisms are “Lysosomal proteolysis following autophagy from the cytosol into lysosomes, and by the proteasome in the cytosol following tagging by ubiquitin.” Interestingly, Chloroquine (and Hydroxychloroquine) are known to prevent ferritin iron release by ferritin protein degradation in enterocyte, hepatocyte and reticulocyte cell culture models. There are other less understood mechanisms of iron release from ferritin including contact with lysosomes containing reducing agents.

Its not clear if the effects of long covid are due to over production of ferritin and normal leaking processes, or a targeted process that leaches iron from ferritin. As a side-note, a paper has recently been published which shows that there is a very interesting sequence similarity between hepcidin, the iron transport enzyme and the SARS-CoV-2 genetic sequence. This is relevant to research because as one paper states, genetic malfunctions affecting the hepcidin-ferroportin axis are a main cause of iron overload disorder.

Some researchers specializing in iron indicate that all iron lab tests do not represent iron deposited in the tissues and low iron is almost never due to lack of iron in the diet. In fact, there are many researchers who believe that the laws regulating that iron needs to be added to grains and other staple foods, are causing much more harm than good. They state there is evidence that iron is correlated to a number of modern disease states and combined with the environmental toxins like glyphosate disrupting iron homeostasis. This may be a case of pop-science driving public policy, as anemia seems to be caused by disruption of iron transport and not unavailability of iron in the diet. In fact, Morley Robbins states that the majority of people he sees in his nutritional practice with iron overload are actually vegetarians who eat more grains and other food artificially supplemented with iron. Groups like the Gates Foundation heavily promote iron supplementation in the developing world in an attempt to stamp out anemia, but on the population level they may be doing much more harm than good, particularly in the context of a disease like COVID19 which mobilizes and redistributes iron. It does seem that anemia is an issue with iron transport, not dietary iron, in fact the role of copper in anemia should be promoted instead of iron according to a study cited by researchers in this area.

Treatment Theory*

*Talk to your doctor before any changes in supplementation or lifestyle, this is research analysis and not medical advice.

Treating iron dysfunction:

The paradoxical thing is that some long haulers have low iron/ferritin but seem to also have symptoms of iron buildup in their nervous system and other tissues, other have very high levels of ferritin —depending on transporter issues. Iron lab results do not correlate to iron deposited in tissues, but they may be out of range regardless due to disruption of metal homeostasis.

Its very important to note that ferritin is an iron storage enzyme and it is measured in circulating serum for lab tests, but its possible that ferritin or other iron forms and complexes could accumulate in organs/tissues and not be accurately represented by the serum ferritin test. Particularly ferritin in the brain and central nervous system. Iron can also collect in skeletal muscle, connective tissues and weight gain is common when the body tries to create fats to dilute and counteract the iron and its oxidation. Iron can be problematically deposited in tissues without high iron biomarkers and the most relevant test may be 8-OHdG, a DNA damage biomarker, or other oxidative stress based tests. High homocystein has been correlated to iron overload and can also correlate with the methyl trap, indicating that a urinary organic acids test can open some windows into understanding the metabolic disruption.

While MRI can be used with a skilled technician to analyze the iron present in the brain, not all types of iron can be properly visualized. According to research on brain iron oxides, “An alternative clinical technique for the detection of brain iron is transcranial ultrasound or transcranial sonography (TCS). TCS has been developed as a cost-effective and portable method to detect changes in certain brain regions exhibiting iron accumulation”.

In terms of treating iron in the nervous system, there are a number of approaches, including new classes of compounds mainly investigated for Alzheimer's. A drug called Defiriprone is on the market for iron overload disorders and trialed for Alzheimers patients, while it has its own issues and side effects it does seem to be effective at removing unwanted iron from the brain and nervous system.

What I consider one of the greatest advances in preventing disease in my lifetime is the compound NMBI, developed as a method for safely removing mercury and other metals from the brain and other organs. It binds free metals in the body, without binding metals that are being used in biological functions and unlike other chelators for removing heavy metals, it binds them irreversibly and does not cause redistribution. The inventor points out that it is also one of the strongest anti-oxidants and enormous amounts have been given to animals in studies with no side effects. Amazingly, animals given NMBI can handle injections of toxic amounts of mercury that would be fatal under any other conditions.

NMBI was sold as a supplement under the label OSR in the early 2000’s, but the FDA simultaneously changed their supplement laws and shut down OSR. This was due to the fact that OSR is essentially a synthetic combination of two natural compounds. It currently has orphan drug status and the inventor is working through clinical trials via the company EmeraMed. The effectiveness of NMBI for mercury and other metal toxicity is so profound and painless that people are successfully coordinating on Facebook groups to order the powdered raw material from custom synthesis labs, have it tested and work together to dose themselves with their own protocols.

There are naturalistic ways to get rid of excess iron. Compounds like green tea extract, curcumin and IP-6 are used by functional medicine practitioners, with IP-6 being the most aggressive. Donating blood is also used in some cases of genetic iron overload and can drop ferritin levels as much as 20 points in one donation. In the case of COVID19 recovery, its very important to have a full blood panel done as well as copper and ceruloplasmin enzyme to determine if you need to supplement copper/iron. There does not seem to be any test for showing iron accumulated in the nervous system and other tissues but the general DNA oxidation marker 8-OHdG can shed a light on if you have a source of metal catalyzed auto-oxidation in the body. High homocystein, also a product of the metabolic trap, is a marker of iron overload according to one study and is found on the common urinary organic acids panel. Certain methods using MRI can indicate levels of iron, but require a trained and dynamic MRI technition to understand the complexities of visualizing iron deposits. A paper explains the patterns in iron integration with the brain in neuro-degenerative disease and may be relevant for signs of COVID19 iron integration with the nervous system and brain.

Its important to make sure to not only bind iron, but support the iron transporting mechanisms by having enough copper, magnesium and other co-factors. Copper is highly involved in proper iron transport and a disruption of iron homeostasis is known to effect copper status and absorption, leading to more disruption of iron transport. Zinc, Copper and Selenium deficiencies can lead to iron accumulation, which is interesting as these particularly metals correlate to COVID19 outcomes. A paper found that higher levels of copper and the important iron shuttling copper based enzyme ceruloplasmin, was correlated with better outcomes in acute COVID19 cases. Issues with ceruoplasmin have been known to lead to iron accumulation in the retina, liver, pancreas and brain. Ceruoplasmin is one of the most complex enzymes in our body, with many functions and can be easily disrupted, particularly through environmental toxicity, one of the strongest disruptors being the industrial pesticide, Glyphosate.

Morley Robbins, author of the Root Cause protocol, has dedicated his life to understanding iron dysfunction points out that anemia is often not a dietary deficiency of iron, but an issue with Ceruoplasmin or other iron transporters and just dumping in more iron is not a sustainable solution. He states that we need more bioavaliable copper, more magnesium and less iron to regulate copper more efficiently. Vitamin A is also another compound involved in Ceruplasmin that contributes to copper regulating iron homeostasis.

One company has made a product which is copper bound to nicotinic acid, for bio-available delivery, while expensive, it may be an important part of recovering from iron dysregulation in the nervous system. The MitoSynergy product line is one of the more expensive supplements, but seems to be a very new and unique approach for supporting proper iron homeostasis.

Another method is to trigger autophagy, which is the bodies cellular clearing mechanism. Usually it is done through intermittant fasting supplements such as high dose nicotinic acid will also trigger it. A group on facebook with over 6000 long haulers is having success in dramatically lowering their symptoms by triggering autophagy on the weekend. Through trial and error the group has developed a protocol for diet and supplementation which supports autophagy. While it does take a number of weeks, the results from the patient surveys speak for themselves — in my opinion this validates the theory that iron is driving most of the issues with long covid as autophagy will lower stored iron even in areas such as the brain and CNS.

Research shows that iron overload impairs normal autophagy which may be a general problem with long haulers. There is a phenomenon of ferritinophagy and iron triggered cell death which makes sense why the long haulers triggering autophagy can “feel” autophagy starting which is normally not possible. They feel aches and pains in different places on their body, indicating free iron may being released during the cleanup process and causing inflammation and oxidative stress.

Data analysis of over 1000 patient surveys, using autophagy according to the “Recover from Long Covid” facebook group method

A study in aging monkeys shows that a calorie-restricted diet reduces brain iron and preserves motor performance, in fact, triggering autophagy via fasting and the release of iron may be one of the main drivers of the many known health benefits of fasting.

While the mechanism is not clear, anecdotally, high dose nicotinic acid (500mg+) seems to have a purging effect on iron materials accumulated in the body (and will trigger autophagy if pulsed on and off during the week), potentially through rebalancing low level redox chemistry in the cells which may have lead to iron “leaking” in the first place. In the context of redox balancing it would make sense to supplement proper forms of copper and magnesium to support this mechanism.

There are methods of detoxification that were promoted in the past, such as the famous niacin/sauna protocol. One clinic promotes the use of high dose nicotinic acid and sauna for detoxing heavy metals, with a famous case being a cohort of 9/11 firefighters with metal overload/neurodegeneration from suspected magnesium metal contamination. In the photo below you can see the heavy metals that were removed from the body and collected on his towel during a series of sessions. This is something to explore as metals in the nervous system will lead to horrible neurodegeneration in the long run.

Stains from suspected magnesium salts collected on a towel after a series of niacin/sauna detox

Treating Niacin Sink Metabolic Trap:

The Niacin Sink Trap was identified early on in the pandemic by a number of patient-centric researchers and is one of the easier aspects to deal with. In November, I had written the blog post, The Team of Doctors and Biohackers Who Seem to Be Successfully Treating “Long Covid” about a group in South Africa who were able to treat Long Covid based on an understanding of the Niacin Sink Trap. While I had hundreds of people reach out and say it helped them get their lives back, and it did the most good for the most number of people with no adverse issues, it was not the full story for everyone.

Based on my anecdotal observations, taking nicotinic acid or niacin (vitamin B3) at high doses works miracles for long covid. There does seem to be a dose-response curve and some people only get benefits at the level of 500mg+ a day, which is much larger than the daily recommended allowance and causes a “niacin flush” effect until tolerance builds. The niacin flush is similar to feeling like you have sunburn and tingling on your skin for an hour or so. I would suggest to take 100mg of instant release niacin to calibrate your response. The average person will have a flushing reaction visible on their skin. If you are an under-methylator, you may not have a flush at all, this can also happen if the butyrate producing bacteria in your gut are not doing their job ( very common after COVID19) and you may need to take a butyrate supplement for a few days before. Some people can have a very disruptive flushing effect at this amount that makes them feel anxious, this is an indication that you are part of the 9% or so of the population who is an over-methylator.

It seems ideal to work your way up to 500mg of nicotinic acid, matched with 500mg of nicotinamide, the other form of vitamin B3. While this seems excessive, these vitamins are water soluble and doses well beyond 3000mg are often given for lipid metabolism issues. In fact there is an entire field of health called Orthomolecular medicine which advocates for very large, multigram daily doses of niacin and vitamin C. This seems best if matched with NAC, around 1500mg a day, which will replenish glutathione and the cellular stores of cystein. There are a number of other important co-factors such as zinc, vitamin C, selenium, quercetin and vitamin D. This metabolic trap should be dealt with before anything else as it will support the body in handling oxidative stress.

Using an organic acids panel test will give you the best insight into your metabolic situation and should be used by your doctor before any changes in diet or supplementation. In the case of the Niacin Sink Trap, you would expect to see tryptophan metabolites high, there are multiple of these on the organic acids panel and this would be a good indicator.

Treating Methyl Metabolic Trap:

The methyl metabolic trap is complex because there are many different types of genetic pre-dispositions to an altered methylation status and it involves a number of feedback loops. Therefore, its best to consult a functional medicine doctor who is confident in testing methylation status. For the average person the best way to push out of the methyl metabolic trap is to supplement with methyl folate and a methyl donor such as TMG for a few days, and then start taking vitamin B12 at high dose. If you only take vitamin B12, for example, by injection and are in the methyl trap, it can make you feel worse due to a buildup of homocystein, you need to support the methylation cycle first. B6 supplementation may be important as this is coupled to the same system but should come in the later stages. Using an organic acids panel test will give you the best insight into your metabolic situation and should be used by your doctor before any changes in diet or supplementation. methylmalonic acid and uracil would be expected to be high on an organic acids panel, often methylmalonic acid is used as a proxy for vitmain B12 and many doctors have told their patients their B12 is through the roof when it is infact a proxy indicator. Another issues is that due to the methyl trap, B12 building blocks can be oxidized and trigger false positives on a proper B12 test. Homocystein and histamine are two other panels that may be expected to be high due to the methyl trap. Experts on the methylation cycle use a low SAM/SAH ratio to determine methylation dysfunction. There is involvement from vitamins such as B6, B2 and B1 that could also be tested for and supplemented, it might make sense to take B6 right off the bat as its directly involved in this cycle.

Treating Oxidized Mitochondrial Membranes:

This may be the last step in recovering from Long Covid, many find that after other protocols they feel well but still get exhausted easily and in general have fatigue. A solution to this is called “Lipid Replacement Therapy” where phospholipids are taken to replace the burned membrane lipids. NTFactor is a purpose made supplement for this purpose and has a number of clinical trials showing impressive results for diseases involving fatigue such as Chronic Fatigue Syndrome. Otherwise, Soy Lecithin is a great source and taking somewhere around 5 grams a day will accomplish the same thing over time, butyrate will help burn off the existing membranes and compounds like acetyl-carnitine and ALA will help shuttle the lipids and burned protein/lipid rafts. This process can take many months but in the clinical trials there were signifigant improvements in fatigue using this method. This will also lower the potential for Lyme disease and associated pathogens to thrive by feeding on the oxidized membranes. Triggering autophagy will not only assist with removing iron based materials but also help repair oxidized mitochondrial membranes.

Treating Microbial Remodeling:

In terms of repairing the microbiome, as long as your diet is healthy, it will remodel and repair itself when the other issues have been taken care of. If you have high ferritin values or symptoms of iron accumulation in your body it would make sense to deal with that first. An organic acids panel test would also give insight into the amount of microbiome disruption, as well as a window into the methyl and niacin sink metabolic traps. Microbiome tests exist that can tell you the makeup of your microbiome, and while interesting and a powerful tool, can not contribute much past showing a dysbiosis.

A common type of bacteria to become scarce after COVID19 is butyrate producing bacteria, while there are no commercial probiotics that contain this species, research shows that taking butyrate supplements actually increase the butyrate producing bacteria. Butyrate supplementation is known to rebalance the microbiome through other mechanisms also. Butyrate is also a co-factor for a receptor (GPR109A) that nicotinic acid acts on and is required for some of its therapeutic action, as well as having a positive effect on the lipid replacement therapy process. It would also be important to take fiber also as butyrate is responsible for fiber metabolism. Taking probiotics and pre-biotics would be important as well as biotoxin binders such as zeolite, activated charcoal, chlorella and bentonite.

Other Treatments:

There are other groups and methodology for treating long covid but none of them seem to work as well nicotinic acid and other naturalistic methods. One group is IncelDX, run by Dr. Bruce Patterson. They developed the first biomarker system which showed long covid inflammatory dysfunction. Using their existing biotechnology platform, they identified fragments of of non-replication competent viral material were found in monocytes and state that all long covid symptoms are a result of over-activation of monocytes.

They treat long covid with a mix and match Ivermectin, Statins, SSRI’s and steroids and seem to have a reduction in brain fog and inflammation, but in my observation nobody I know has made a full recovery using their methods. Ivermectin may have benefit for acute COVID19 but it does not seem to be a magic bullet for long covid, with a high potential for neurotoxicity in people who have a disrupted blood/brain barrier. While there can be a normalization of inflammatory biomarkers using these methods, I have talked to some patients who had a normalization of biomarkers but no change in symptoms during treatment, while others did not have treatment and had normalization of biomarkers. Interestingly they promote the use of niacin in some of their webinars. While I commend them for focusing on long covid, they should be more patient centric and more responsive to symptoms, they have their hands full with running a biotechnology company and with multiple self-promotional appearances on Dr. Drew. They are not rushing to understand why many of their patients arent getting better regardless of using their treatments and biomarkers stabilizing. In a recent webinar, Dr. Patterson blamed “de-conditioning” for the lack of his patients fully recovering. I would challenge IncelDX to compare their treatments with even a simple nicotinic acid/NAC/IP-6 stack.

Dr. Adrian Wentzel, Robert Miller and Guy Richards saw an association with COVID19 outcomes and bioenergetic functioning at an early stage in the pandemic and have done a large amount of high quality patient-centric work. I wrote about this group in a previous article and their niacin based stack has helped tens if not hundreds of thousands of people get their lives back.

Dr. Tina Peers promotes anti-histamines on-top of a similar stack but from my observations anti-histamines only work in the short run before they become ineffective and these methods do not address methylation issues or iron dysfunction properly. Anti-histamines may in fact be a good way of determining if you have a histamine issues/methylation issues, specifically if they lower brain fog and other systematic issues.

Leronlimab, is a monoclonal antibody by CytoDyn which has just released preliminary results and will most likely be the FDA approved drug for Long Covid in the future. It is injection only, incredibly expensive and the clinical trial patients flocking to Facebook groups state that while it did make them feel better, the effects seemed to wear off after a few weeks. Regardless, as usual in the pharmaceutical approach, it has hit the carefully designed clinical trial endpoints, has a long patent life and investor interest to push it through the FDA process, so it will most likely be “the solution” in the future.

The pharmaceutical treatments like Leronlimab and the ones promoted by Dr. Bruce Patterson seem to only “turn off the fire alarms” but not “put out the fire” and seem to be the usual manifestation of a scientific/medical system that does not really understand the human body and is disconnected from human needs.

Example Long Covid Treatment Protocol*:

*Talk to your doctor before any changes in supplementation or lifestyle, this is research analysis and not medical advice.

Before starting:

Nicotinic acid “challenge”, 100mg flush niacin on an empty stomach before moving on to higher doses, should cause a temporary feeling like sunburn but no inflammatory or cardiac reactions.

Tests:

Urinary Organic Acids Panel

Histamine

Supplementation:

Oxidative stress support (if recently sick):

500mg NAC 3x day

500IUmg Vitamin E

Niacin trap:

100mg nicotinic acid 3x a day (moving to 200mg week 2 and 300mg week 3)

5mg melatonin for every 500mg nicotinic acid

Methyl trap:

Methyl Folate 500mcg 3x a day

B12 (hydroxycobalamin) 1000mcg sublingual 1x a day

Betaine/TMG 1000mg 3x a day

Microbiome:

1000mg psyllium fiber 3x a day

2000mg sodium butyrate 3x a day

Probiotic tablet

2 tbsp zeolite before bed

Cofactors:

Selenium 200mcg 1x a day

copper bisglycinate 2mg with a meal

Magnesium biglycidate 50mg 3x a day (doubling every day to 300mg 3x a day, to avoid bowel issues)

500mg grass fed beef liver capsules 2x a day

Iron chelation:

500mg curcumin 3x a day

250mg green tea extract 3x a day

500mg inositol hexaphosphate 3x a day

*must be done with iron panel testing/medical monitoring

Lipid replacement therapy:

1000mg sunflower lechitin 3x a day

*Talk to your doctor before any changes in supplementation or lifestyle, this is research analysis and not medical advice.

“To change something, build a new model that makes the existing model obsolete.” — Buckminster Fuller.

Nikita K. Alexandrov, Chem, MBA

7/23/21

Contact for collaboration/consultation:

NKA369@protonmail.ch

Butterfly Method Discord Group",62425
"Our World has witnessed drastic changes since Covid pandemic. Imagine the days when roaming outside without any fear or hesitation, going to work, eating and travelling anywhere, meeting friends and dating people was just a decision away. Life was absolutely amazing, challenging and busy where we didn’t even realise how quickly the day was spent.

Do you know — The current world population is 7.9 billion as of May 2021 according to the most recent United Nation estimates elaborated by Worldometer.

As per WHO, Globally, as of 11:36am CEST, 6 May 2021, there have been 155,506,494 confirmed cases of COVID-19, including 3,247,228 deaths, reported to WHO. As of 4 May 2021, a total of 1,170,942,729 vaccine doses have been administered.

Source: https://covid19.who.int/

Imagine the days when we used to wake up early for our daily routine work. Getting ready mentally, physically for our day was an important task at hand. The morning soothing meditation, healthy exercise, relaxing shower, hot tea with amazing breakfast really sets the mood for the amazing day ahead. Later we had the choice of wearing our favorite dress and shoes for office or colleges. After we dressed up, travelling to office, schools or colleges was absolutely fun. I must admit that there used to be huge traffic on roads and there was not only the noise of vehicles but also a lot of free advice heard while waiting on the red traffic signal. The same applies while travelling in a train or a bus. At that moment, it just reminded me of one popular saying which goes like “ Kitne Tejaswi log hai hamare paas?” which translates in English as “How many Glorious people we have?”

Since covid pandemic all the daily hustle vanished. Now every morning start with watching news about how many new covid cases found? Wearing a mask, maintaining social distance, washing hands have become the new normal. Lot of entrepreneurs, people, companies have lost their jobs. Technology has started replacing labours.

Let’s dive into some positive side of covid pandemic:

Increase in A.I Technology : Working in office and factories does require human intervention and interactions in order for business to carry on. Due to covid pandemic, as there is a huge chance of virus transmission from humans to humans, many countries have adopted Robotics or A.I. for cleaning infected areas, delivering foods and medicines. Drones are utilized for patrolling areas to ensure that social distance is followed, delivering medicine.

: Working in office and factories does require human intervention and interactions in order for business to carry on. Due to covid pandemic, as there is a huge chance of virus transmission from humans to humans, many countries have adopted Robotics or A.I. for cleaning infected areas, delivering foods and medicines. Drones are utilized for patrolling areas to ensure that social distance is followed, delivering medicine. Work from home: For companies, meeting the client requirements has always been a priority along with keeping work environment comfortable for their employees. The Work from home is now introduced in many countries. Be honest, at some point of time while working in office, there would have been a conversation along with our friends that “Wouldn’t it be better if we get work from home, and we could not only work but also spend time with our dear family” or how about “what if we enjoy tea, dinner along with our family at home during our work break?” Work from home has been a boom to not only ensure that the daily targets are met with ease but also our family relationships strengthens. Reward and recognition via Zoom, Microsoft Team, gave us the opportunity to connect with many other achievers at our company

Online Entertainment: When was the last time you went to a theatre? Imagine the movie tickets booking, the feeling of walking through the doors and sitting on the chairs in front of a bigger movie curtains. Watching your favorite actor or actress on screen along with your friends, family or partner was everyone’s desire. The audience shouting, whistling on their favorite scenes. I am sure a lot of us miss those memorable moments. Hence, several film production companies have started releasing movies or web series via Over The Top (OTT) platforms such as Netflix, Amazon Prime Video, Hotstar, Zee5, Voot Select etc which started providing the best contents at affordable price

Online Payments: How many of us remember the days when we waited outside various banks in huge queues for exchanging notes during a demonetization? It would’ve been tiring and time-consuming task for us. It gave rise to era of digital payments. Lots of people have now switched to secure digital payments which gave rise to Google Pay, PayTM, Amazon Pay, PhonePe, JIO Money, CRED and other apps instead of cash and checks payments. Life became easy by ordering food online, booking travel tickets, paying electricity and other utility bills, recharging your number via digital payments, paying credit card bills etc.

Online Education: I am sure that we all do dream or wish to go back in our school and college days to experience the fun and best moments of our life again. Unfortunately, it is not safe to go to schools or colleges. Hence, the concept of online education have become the new trend. Many institutions have started learning apps or conducting lectures to ensure that students’ education is not impacted.

Let’s dive into some negative side of covid pandemic:

3,247,228 deaths across the globe.

Increase in poverty across the globe.

Scarcity of food supply due to lockdown, border closures, trade restrictions which resulted in malnutrition and poor health.

Huge unemployment due to industry shut down, bankruptcy etc.

Shops, Gyms, religious places, malls, hotels closed.

Global economy has gone into recession.

Stress, anxiety increased.

Covid pandemic have made us realise some important points that we should never forget.

Life is short, live it to the fullest.

We always have the choice to be healthy.

Only few people who love and trust you, will be with you during difficult times.

Taking care of your family, friends is important.

Spending time with your family, friends is also important.

Social media is connecting people across the world.

To stay prepared well in advance against any crisis.

Unity is strength. Helping each other by maintaining social distance is vital. If we humans unite for such causes, we together can overcome lots of other world issues.

Despite the impacts of covid pandemic, we should be thankful and grateful for spectacular work done by our front line Doctors, Nurses, essential workers for saving countless lives by working tirelessly and risking their own lives 24*7.

May this pandemic gets over and we all quickly resume back to our routine work. Till then, be safe, wear a mask, maintain safe distance and wash hands.",6907
"Fire cooking with my family at the Nicola Valley Farm (2020)

My mother was a “secretary” in the 1970s until she retired as an office manager in 2005. I grew up listening to her work stories at the dinner table and came to understand it was often a thankless job, especially when it came to bringing in lunch for the team. When I went to university — and then to work — I was exposed to institutional food; years later as a restaurateur, I knew that the local food scene could offer more than the cafeteria.

My first job as a kid was scooping ice cream at our local baseball stadium. I put myself through college as a server and caterer before graduating and moving to New York for an investment banking job. When I worked in finance, I thought I’d never go back to serving food. But the thing about the restaurant industry is that it gets under your skin; so I quit my job and invested in a food truck. We expanded locations and I’ve seen first-hand just how slim the profit margins are for restaurants… even successful ones… in the best of times.

So, leading Foodee has been a lifetime in the making. It connects the two worlds I’ve spent my whole career in. In 2013, I joined as CEO with the goal of helping local restaurateurs feed innovative companies seeking to attract and retain top talent. For the past seven years, we’ve facilitated over 10 million meals from more than 800 local restaurants to modern offices in 14 cities across the U.S. and Canada — and will soon offer them in more major cities.

But in 2020, restaurants took a major hit in COVID-19. We all did. Foodee included.

At the onset of the pandemic, between March and the end of May 2020, America’s restaurant industry lost $120 billion in sales. We lost so many local institutions — the worst hit were mom-and-pop restaurants that closed their doors for good. Offices stopped catering and many sent their people to work from home. Once thriving city centers went quiet.

The pandemic forced us to rethink how we operated Foodee and the future of our business — which was bleak in April 2020 — at the beginning of COVID-19. It crystallized our focus on strategic partnerships and reinforced our grassroots mission. Now, more than ever, local restaurants everywhere need support.

In order to generate the corporate catering revenue that restaurants needed, we knew that organizations would have to work hard to keep on-site employees happy and engaged. Organizations that historically provided catering would need flexibility like never before in order to navigate changing workforces, workplace schedules, and their return to office plans.

That’s why, in late 2020, we began having conversations with strategic partners who were looking for a bold new approach to workplace solutions and on July 7th, 2021, Foodee was acquired. Sodexo (a Fortune 500 company) is headquartered in Paris and is a leading provider of food and corporate services in 64 countries worldwide. For over 60 years, the organization has provided quality food to many of the world’s leading companies, hospitals, schools, universities, government offices, military organizations, airlines, and airports. But despite their size, they are largely a family-owned and run company, with similar values: improving quality of life. Simply put: there is no better partner.

Foodee’s mission to support local restaurants and their communities just got stronger. Sodexo fully supports our vision and growth targets to extend our impact into new markets and sectors. We now have the means to support even more local restaurants in more cities and countries, getting great local food into more hands — and mouths.

Local restaurants set the tone for the vibrancy, creativity, and diversity of every city’s culture. They define a place and its people — and vice versa. Local restaurants bring communities together and have the power to become pillars in local neighborhoods, from sponsoring community activities like youth sports to helping solve food security issues. And we know that dollars spent locally stay in that community to recirculate twice through jobs, groceries, and so on. So as cities reopen, Foodee has the renewed opportunity — and responsibility — to help workplaces support local restaurants during boardroom meetings — and dinner-time stories — across North America.",4324
"The Energy Product

The Energy marketplace product at Oye! leverages the best of Battery, Charging, IoT, and Digital technologies to deliver seamless experience to end consumers and all operating stakeholders.

By design, 100% hardware at Oye! is Connected — starting with BT (to connect via proximate user’s phone) to GPS/GSM IoT for stand-alone assets. Our proprietary integration stack combines all of the hardware and software modules and provides a harmonized experience. The focus has been to modularize as much as possible — right from big stacks to internal construction to aid in hardware interchangeability and serviceability.

Batteries are custom built to suit our use case with no compromise. From the choice of cell chemistry that allows us to extract high cycle life in high temperature regions (affordable pricing per KWH), to internal construction choices that help withstand the Indian arterial roads on leaf-spring suspension vehicles, to carrying convenience, every detail is custom engineered. The BMS CAN through the IoT beams live battery parameters to the cloud for real-time applications and advanced analytics.

Swapping Stations combine the best of smart electronics & power electronics with cloud and software to provide 100% automated self-serve battery collection and dispensing points, allowing for 24/7 and manpower-light operations, and remote management.

Oye! Smart Swap Station

Digital technologies form the key pillars for enablement, efficiency gains and opportunity unlocking for the entire solution stack. The Driver mobile App enables the end consumer to plan hassle-free and efficient trips by providing RT SoC information, Station discovery & automatic/cashless battery swapping.

The IoT sensor layer, cloud platform, and deep analytics layers driven by AI algorithms, is encapsulated in the “Eagle” Command Center. This enables a host of use cases — such as Product optimization, Asset tracking and security, Demand-supply matching, Adaptive pricing, Product health monitoring and management.

Battery Data and Intelligence

At Oye! we are ushering in the future of transportation. So far, our young & growing Energy network has powered over 500,000 Kms, enabled drivers in the NCR to run 250 KM+ at a stretch, and has increased the top box driver partner income by 79%.

And we are looking for harbingers to build this further.

If you want to build the Energy Operating System (a term I picked up from a revered mentor) for micro-mobility and logistics, please gimme a buzz shuvajyoti.ghosh<at>oyerickshaw.com",2557
"Currently, Xiaomi remote charging technology is capable of 5-watt remote charging for a single device within a radius of several meters. Apart from that, multiple devices can also be charged at the same time (each device supports 5 watts), and even physical obstacles do not reduce the charging efficiency.

In the near future, Xiaomi’s self-developed space isolation charging technology will also be able to work with smart watches, bracelets and other wearable devices. Soon our living room devices, including speakers, desk lamps and other small smart home products, will all be built upon a wireless power supply design, completely free of wires, making our living rooms truly wireless.

This is a revolutionary innovation of wireless charging.

This is also a bold attempt to turn the whole house wireless.

It’s not science fiction, it’s technology.

This is Xiaomi’s self-developed remote charging technology.

Xiaomi is currently the world’s third-largest smartphone brand and has established the world’s leading consumer AIoT (AI+IoT) platform with 289.5 million smart devices connected to its platform, excluding smartphones and laptops. Xiaomi products are present in more than 90 markets around the world. In August 2020, the company made the Fortune Global 500 list for the second time, ranking 422nd, up 46 places compared to the previous year. Xiaomi also ranked 7th among internet companies on the list.

Read more",1430
"Chainlink is marching on as a dominant force among decentralized oracles.

Image by Pete Linforth from Pixabay

Chainlink is one of those altcoins that constantly pop into your feed and won’t seem to go away. The infamous “Anon frog army” plays a big part in putting Chainlink's name out there to some success. Chainlink is an underrated player in the crypto space so let’s dive deeper into it. It is not simply a cryptocurrency but instead an open-source platform designed to create decentralized oracle networks. What this means is that it is a unique and distinct computational environment that has a single purpose. They have one task at hand such as providing price data while another decentralized oracle network provides weather data related to a region etc.

Now I understand that this is not the most intuitive thing to grasp, so I will do my best to break it down for someone that is not the most technologically savvy person.

Why do Hybrid Smart Contracts and Blockchain Oracles matter?

Before we jump into it, smart contracts and oracles need to be explained. For anybody that isn’t into the rabbit hole already, smart contracts are programs that have been stored on a blockchain and when certain conditions are met, they execute. The situation is a little bit different with blockchain oracles, their job is to authenticate information and data from the outside world and provide smart contracts with information. This brings us to hybrid smart contracts which are deemed as more advanced considering they have the ability to interface with the outside world.

Photo by Ramón Salinero on Unsplash

Now you might ask yourself why it needs to interface with the outside world. So let me provide you with the infamous decentralized crop insurance example that Mr. Nazarov himself likes to use. Decentralized crop insurance gives you the ability to monitor the world's weather events through data sources. These data sources provide you with an indication of what the weather is and gives you the possibility to pay out an insurance policy to anybody on the planet that has an internet connection regardless of the insurance company. It changes things drastically by enabling you to simply using code instead, in relation to the risk for the farmer with an on-chain insurance contract. To do this you need to have a system that can reliably prove that the weather events that are covered by the insurance contract actually happened such as a drought or a storm etc. This is where Oracle networks are so important because they can reliably validate that there was an actual incident which allows the contract to be triggered automatically and then is paid out or vice versa.

Was that a lot to take in? Alright, let's slow down here. A rational question that would pop up now would be why the oracle network has to be decentralized, which I will make sense for you. The main idea is to not have a single point of failure because if you need data that is live and reliable and can’t be controlled by a bad actor for manipulation then decentralization is a better option. You need your oracle network to be resilient and incorruptible and that is what a decentralized oracle provides.

Chainlink is developing the decentralized Meta-Layer

As you have noticed by now, there are a lot of terms to get familiar with so let’s start with the Meta-Layer. The Meta-Layer is simply the layer between the smart contract world on the blockchain and the interactions that they need to operate with the real world. See it as a bridge between these two worlds. This decentralized Meta-Layer will bridge the gap between the data out there in the world and computation with the smart contracts that set the contractual definitions.

Image by Gerd Altmann from Pixabay

In other terms, it sits between the on-chain activity and the off-chain activity. An example of this would be connecting to a data source or sending a payment. However, let’s break it down again with a clearer example for the non-tech-savvy individual. When the famous car-sharing app that we use on the daily “Uber” was developed, they would have the base code for Uber and an idea in terms of how they would want the app to be developed. However, in order for them to proceed, the Uber team would have to reach out to Twilo first in order to have the possibility of sending text messages to the users. After that is settled, they would then have to need to have a clear data source to receive the customer's location which is Google Maps. Lastly, they would need to have a secure way of paying the drivers and that would be Stripe. Without all these parties involved, it is clear that Uber would reach a stumbling block.

Basically, if you would like to build a decentralized app that would require data from different sources, that is where Chainlink comes in and enables you to retrieve the real-time data using oracles and enables you to reach your goal. If you have any idea of what you want to do with a project on the blockchain that the blockchain does not utilize itself, that is where Chainlink and the Meta-Layer are valuable.

Chainlink is dominating the decentralized oracle space

Chainlink’s oracles and price feed have become the industry standard in DeFi and can be seen throughout the DeFi projects. As I am writing this, Chainlink’s oracles and price feeds have been integrated into over 650 projects and the growth is steadily increasing with the team announcing new integrations every week. The Hybrid smart contracts that Chainlink offers are even used by the stablecoin Paxos which utilize it for their price feeds and proof of reserves. Band Protocol which is arguably Chainlink’s largest competitor does not come close to the dominance that Chainlink has established due to its first-mover advantage. Most of the large DeFi project such as Aave has integrated Chainlink and it can be seen throughout the DeFi industry.

It is definitely a token worth paying attention to if you are interested in the underlying blockchain technology. Although there is a lot of technical concepts to grasp, there is a clear need for a decentralized oracle solution for crypto to keep evolving and reach its full potential and Chainlink seems to be positioning itself well to be just that solution. Projects like this are fundamental for the development of the internet of trust and value. The idea to exchange value in a secure way without a trusted intermediate makes decentralized oracles fundamental going forward. Do you believe Chainlink will be the solution and end up being a solid blue chip crypto? Time will tell.",6594
"The health and fitness industry is growing enormously, there are so many products and services available in the market to encourage, and motivate to work on the well being of health and fitness. And why not? — after all — “Health is Wealth”.

At present time, the health and related insurance costs are rising with time, leading for people to take things seriously. Also, many are giving attention to their health diverging towards natural, organic food products and a healthy lifestyle. And it is important, especially since the breakthrough of COVID-19 it had made us realize how important our health is and how much we take it for granted.

In this blog, we are going to discuss about how blockchain technology can help power fitness industry.

Fitness Industry and Blockchain Technology

Many health and fitness enthusiasts are eventually using fitness applications along with the activities they perform. There are many fitness brands and online services available in the market.

The use of gamification in the healthcare industry has led to encouragement for participation improvising outcome and along with blockchain technology, it can help boost the process through transparency that the technology provides.

With blockchain technology, the user’s data can be ensured with security and privacy while it removes the need for intermediaries as every transaction is made on the encrypted blockchain network. Since the data will be stored on the blockchain the users will gain complete access and control to its health information.

Blockchain technology also facilitates smart contracts that would enable users to access and receive tips and lessons. The users can also leverage upon connecting with their trainer through videos on the public blockchain and the industry can also provide loyalty programs to their users. The users can simply download lessons and content uploaded by their trainers through payment.

Also Read, Relationship Between Crypto Investment And Mental Health

Along with this, the gym owners can benefit too as there would be no involvement of the 3rd parties and they can receive payments through this platform.",2145
"Mapping trees may be an unfamiliar concept to many. However, tracking and mapping trees has become an essential tool in the fight against climate change. In recent years, tree planting and growing have become a global topic, especially with the increasing impacts of climate change through CO2 emissions. Some geographers have mapped potential areas where planting trees can help with climate change mitigation. Implementing strategies like this in an urban setting is a rigorous process that needs a comprehensive understanding of urban forestry at a local level.

In Dar es Salaam, Tanzania — one of the most rapidly urbanizing cities in Africa, with an estimated population of 6 million people — managing trees/urban forest is a challenge, especially with no proper tree inventories. Full city assessments of tree canopy and tree inventories used by many urban forest managers have become a standard dataset used to set policies, create management plans, sustain, extend and optimize urban forests.

OpenMap Development Tanzania (OMDTZ) and Greenstand, supported by the World Bank, have partnered in a project that aims to enhance urban forestry management — a somewhat new concept in Tanzania — by conducting tree inventories using inexpensive and easily available equipment. Proper urban tree management has proven to increase the quality of life through cleaner air, lower heat stress levels, a healthier environment and improved aesthetics.

In high-income countries, there often exist detailed inventories down to the individual stem! However, the cost of obtaining such inventories in places like Dar es Salaam using the same methods as in Manhattan is simply not feasible. Instead, by using community mapping techniques, it is possible to create standard inventories that are fit for the purpose of management.

In March, 2020, OMDTZ conducted a pilot survey to test the methodology, which resulted in more than 500 trees mapped. The pre-second phase focused on developing tree mapping guidelines on how to measure tree heights and diameters using cheap open-source tools (i.e. GIMP, QGIS, ODK Collect, Android phone, tape measure, bright-painted stick, and/or a clinometer). We also made a short-instructive tree height measurement methodology video demonstrating the mapping process.

Working with University Students

Photo: Chris Morgan, World Bank

Working with students has always been part of our methodology for effectively imparting knowledge and skills to younger generations. In collaboration with the Tanzania Resilience Academy — a program that aims to equip young people with the tools, knowledge, and skills to address the world’s most pressing urban challenges and ensure resilient urban development — we were able to work with 70 university students from Sokoine University of Agriculture (SUA), The University of Dar es Salaam (UDSM) and Ardhi University (ARU).

In Dar es Salaam, we trained 20 students from ARU and UDSM who collected data in Kinondoni and Ubungo Municipalities along the river Ng’ombe (100–200m on the riparian areas) which will support the Dar es Salaam Metropolitan Development Project (DMDP). The DMDP aims to improve urban services and institutional capacity in the Dar es Salaam metropolitan area and to facilitate potential emergency response. This undertaking includes upgrading the river to get a baseline tree database, focusing on trees that may be cut down during this process, to later add more trees in order to reduce erosion and maintain urban greening.

In Morogoro, we trained 50 Environmental Science and Management students, equipping them with skills on how tree inventories are conducted. As part of their industrial training, these students conducted ground surveys and mapped tree canopy around the SUA campus. Data collected at SUA is expected to be used for research purposes at the university.

During the industrial placement, the students also learned about a tree inventory approach used in Morelia, Mexico, showing how most of the collected parameters are the same, but cultural differences often determine the type of data that is collected.

Project Result and Way Forward

The project has helped to create a scalable model that allows us to understand the canopy of the city, enhance urban forestry management and provide the government with quantifiable data to issue cutting permits. We have developed a method using only mobile phones and tape measures that generates data with 0% to 20% error difference from that obtained with clinometers (albeit without calibration using laser rangefinders).

The outcomes of this project were:

A more targeted management of tree cutting permits

Identification of tree desert areas

Mitigation of heat islands

More effective planting and tending of urban trees

A greener, more aesthetically pleasing city.

The ultimate goal of the collaboration between Greenstand and OMDTZ with support from the World Bank is to enhance government and community capacities to establish and manage public green spaces and urban trees with their associated ecosystem.",5074
"Today we’re excited to share the newest user of The People’s Network. Please give it up for DIMO!

DIMO started as an internal software tool and data service developed by Digital Infrastructure Services. As a consulting firm and product studio that also builds The People’s Network, DIMO enables innovative technology to actualize more quickly.

While providing service to the mobility, robotics, automotive, public sector, and logistics industries, DIMO has seen interesting and worthwhile projects get scrapped or unnecessarily delayed each step of the way from:

📝 The drawing board

📊 The financing process

😣 Inapt partner selection

⁉ Unclear regulations

🚧 Hidden development complexity

📉 Operations that don’t meet initial performance

Not to mention, projects often need ubiquitous and affordable network infrastructure and connectivity — which is where Helium comes in. With DIMO and the power of The People’s Network, risks are mitigated and potential is unlocked.

Fundamentally, DIMO makes it easier than ever for tech-enabled projects to be collaboratively designed, financed, built, and managed. Think Github and LinkedIn, but for building in the real world.

Projects launched on the Helium Network

How it Works: Three Easy Steps

DIMO allows users to identify where projects have been launched on the Network, conduct function-based searches, check device compatibility, and more. You can even compare other “Full Stack” networking Projects in the DIMO public dataset.

Designers, developer, or financiers interested in completing infrastructure projects using DIMO follow three simple steps:

Access information about service providers, their costs, and their history of delivering against project requirements via the DIMO database. Select from many DIMO pre-built project templates. Use the DIMO Knowledge Graph to assess where to take action:

Where have Projects been launched on the Helium Network?

Search Functions and Projects — what can you do with the Network?

Ultimately, DIMO users are able to surface the best technologies for their projects and connect with service providers to help them build faster.

All projects are easily sorted by a variety of metrics

A DIMOrphic Pairing

The addition of DIMO to The People’s Network signifies additional community resources and new projects in the works!

DIMO is currently working with network operators to provide real-time Hotspot performance data. Soon, the data from these Hotspots will allow The People’s Network and DIMO users to access information on service providers, costs, and their history of delivering against project requirements. Gone are the days of endless internet searching — DIMO and The People’s Network provide the template for building and operating networks as never before.

“We’re excited to use DIMO to move the idea of universally accessible, and locally supported, open networks forward. Helium is a fantastic step in that direction,” says DIMO Founder, Andy Chatham.

Get in Touch

If what DIMO is doing sounds interesting to you, learn more at dimo.zone and via Twitter. DIMO is also frequently adding new additions to its database; reach out if you’d like your service, organization, or project listed.

Register today for the June 23rd Uplink webinar dedicated to discussing the newest member of The People’s Network.",3330
"When it comes to smart technology, the first examples that come to mind are megalopolises like Shanghai, Singapore, Seoul, Dubai. But there is a fast-growing smart hub here in Europe. Scandinavian countries are quickly implementing a variety of smart solutions in every possible sphere — from public transit to education. What are the good examples we can look up to? And how can we learn from their experience with intelligent technologies? Let’s find out!

What drives the smart revolution in Sweden, Denmark, and Norway?

It’s not a surprise that the Scandinavian region is leading the smart transformation in Europe. Their population is highly technologically literate and expects the governments to act in this direction. What’s more, these countries have pledged to contribute in a major way towards the European carbon neutrality scheme (Copenhagen even aims to become the first carbon-neutral capital by 2025). Sustainability has become an intrinsic part of the thinking in the region which, combined with the socially proactive citizens, is a major driving force for innovation at all levels. Finally, transparent procurement programs have helped governments attract top minds in the sustainability field from a global pool of innovators.

Which are the best smart examples from Scandinavian countries?

A single article, no matter how long, won’t be enough to list all of the smart achievements that make the Scandinavian region such a great place to live. To make our point, we’ve picked a handful of standout examples that will help you understand the innovation landscape — further reading is advised!

Norway

This beautiful country is on its way to significantly improve living standards and reduce harmful emissions in many ways.

Smart buildings

40% of energy consumption globally comes from buildings. Sensors that control lighting, heating, and cooling can drastically improve the energy efficiency of a building (as well as proper insulation), and Norway is investing heavily in this type of innovation. Stringent energy use requirements for new builds, as well as a program for government-funded construction projects, have paved the way for the mass introduction of smart sensors even in private homes. This technology is becoming more and more accessible.

Open data

Open data is one of the prerequisites for building a truly smart ecosystem. Norway is at the forefront of open data sharing with its national registry for the public sector. It includes data about traffic, agriculture, demographics, and many more. This registry lets entrepreneurs make informed decisions in their day-to-day business, helping them build more sustainable products that reflect the needs of the citizens. MaaS integrations are only one such example. This public-private relationship provides the basis for an improved living standard for the whole population of the country.

Oslo: MaaS innovations

Oslo transit authority, Ruter, has pledged to become emission-free by 2028. At the heart of this pledge lies the shared mobility dream of each MaaS innovator — mobility hubs that offer reserved parking (priority is given to electric cars), bikes, and more. This will facilitate moving around the city without the use of personal vehicles, thus reducing emissions and creating a healthier environment in terms of cleaner air, less stress, and fitter citizens.

What’s more, a massive sustainable project is underway near Oslo. Oslo Airport City will be a 1 million sq. m. commercial hub powered by sustainable energy.

Denmark

Denmark is also on its way to becoming green in many aspects — as a consequence of using smart tech. Copenhagen and Aarhus are the two cities that stand out, but many smaller communities are already benefiting from a variety of intelligent solutions.

Copenhagen: world’s first carbon-neutral capital

Copenhagen has set the bar high — but having in mind the steps the city is taking, the ‘world’s first carbon-neutral capital by 2025’ doesn’t seem like an impossible feat. More than 250 businesses are taking an active part in this endeavour, and open data is again at the heart of innovation. Smaller businesses & start-ups can take advantage of great incentives and public-private relationships are thriving to the benefit of citizens. The end goal is for the city to become completely fossil-fuel independent by 2050!

Aarhus: public-owned citywide LPWAN

Aarhus is one of the innovators in this field — and one of the first cities that implemented such an extensive LPWAN. This allows devices and sensors to connect from long distances and at a lower cost. In a test setting in their City Lab, these sensors provide valuable information about different metrics like temperature and humidity. They can also track human behaviour (completely anonymized) to optimize the city based on the needs of its citizens.

Sweden

Sweden is home to the first European green capital. An amazing mix of R&D clusters, never-ending tech opportunities, and renewable energy, it is moving steadily towards being one of the most sustainability-led countries in the world. Sweden is also very proactive in working with global innovators to bring the best of smart technologies as quickly as possible. The business atmosphere attracts many big names in the industry, but also many startups.

Smart City Sweden

An integral part of the government’s approach to sustainability is Smart City Sweden — a state-funded national export and investment platform for smart and sustainable city solutions. This organization is located in Hammarby Sjöstad, a living lab, and platform for urban projects in key sustainability areas: renewable energy projects, smart waste management, electric vehicles, water management, and even a citizens’ communication platform.

Gothenburg: low carbon mobility

Gothenburg is a relatively small city but it compensates for its size with its enormous drive to become a green innovator. This drive helped position the city as no. 1 for sustainability and innovation in the Global Sustainability Index 2017 of world cities. Its approach includes a variety of strategies to help reduce carbon emission from mobility: low-emission zones for heavy vehicles, emission-free electric busses, and cars running on renewable biogas being only a few of them.

In conclusion

What is the common thread that unites Scandinavian countries in their approach to smart innovation? For us, it’s clearly rooted in data sharing with a human-first approach. All three countries (and even their neighbours) are always putting the long-term wellbeing of their citizens first and foremost — industrial and political gains are a collateral effect. What we can learn from them is a voracious appetite for innovation and how to achieve the transparency needed for citizens to trust that smart investments benefit the whole community. We can also steal their approach to open data — it’s the best there is!",6918
"AnyLog: Taming the Complexities of Unified IoT Data Processing AnyLog Follow Aug 16 · 6 min read

Written by: Faisal Nawab — Assistant Professor at UC Irvine

The proliferation of Internet of Things (IoT) devices and the huge amounts of data that they are projected to produce pose unprecedented challenges for data management systems. A key question that needs to be answered is how can we process and utilize this huge amount of data. Big Data management in the last decade enabled us to answer this question for cloud applications. However, these solutions do not provide the complete answer to IoT data. IoT data processing involves many complexities and requirements that often lead to contradictory design decisions.

The first complexity is that IoT data, often times, requires being processed at the edge of the network. This can be due to many reasons ranging from real-time processing requirements to the need to optimize edge-to-Internet bandwidth costs and performance. This requirement alone makes cloud solutions — and the ability to utilize powerful compute nodes — infeasible.

Design requirement 1: the IoT data infrastructure needs to be deployed at the edge

The second complexity is that IoT data is produced in many different locations around vast geographical areas. To enable utilizing data from geographically-dispersed devices, the data processing infrastructure needs to be geographically distributed to retain the real-time, bandwidth-efficiency aspects from design requirement 1.

Design requirement 2: the IoT data infrastructure needs to be geographically distributed

The third complexity is that IoT data is diverse. An application may utilize data from IoT devices with different features, data structures, and schemas. The ability to utilize the data from diverse — but related — IoT devices will provide an opportunity to generate better insights and analytics.

Design requirement 3: the IoT data infrastructure needs to enable integrating data from diverse IoT devices

Deploying big data workloads at the cloud or data center requires IT expertise. These expertise are essential as big data brings multiplicity of challenges involving High Availability, scaling, security, recovery, removal of unneeded data and many more. Companies deploying Edge solutions may find themselves in a state that rather than supporting a centralized data center, they need to support thousands of distributed small data centers near the edge. In this type of setup, the needed IT expertise may exceed the IT resources available.

Design requirement 4: the infrastructure deployed at the edge needs to be self managed

Until recently, designing a system with these four design requirements is infeasible with existing Big Data technologies and current edge computing models. The first two design requirements lead to the need to control an infrastructure of powerful compute nodes that are distributed close to IoT devices across large geographic regions. This type of infrastructure is not available today and current cloud and edge computing technologies do not provide the foundations needed for such an infrastructure. Cloud and edge computing still require ownership of the infrastructure within a single or few control domains. However, the scale of such massive, geographically-distributed infrastructure is not feasible with this ownership model.

As for the third design requirement, current solutions include data integration solutions that aim to find similarities and connections between data from different sources. These solutions, however, remain to be limited and often require extensive manual intervention or complex data processing.

The fourth design requirement is not supported as current solutions are not self managed. One reason that makes it hard to develop a self-managed system at the edge is the fact that there is no unification of the way data is managed. The data management at the edge is based, in most cases, on proprietary projects — different types of data are treated differently creating not only silos of data but also setups that require significant proprietary and non-uniform knowledge. Self management, on the other hand, requires a unified process across all the functionalities deployed.

AnyLog — A Decentralized IoT Network

AnyLog is a startup that aims to provide a solution to the complexities above and provide a unified data infrastructure for IoT data processing. In AnyLog, it was observed that the restrictions standing between us and the four design requirements above have been limits of the underlying technologies that are used for data processing, whether on the cloud or the edge. However, this is now changing with the advent of blockchain technologies. AnyLog shows that by utilizing Blockchain technologies, we can finally provide an answer and a solution that combines the four design requirements above.

The AnyLog Network is a collection of nodes (a node can be a small device or gateway and up to the largest server) with the AnyLog software. The software supports p2p communication between the nodes of the network as well as read and write access to a shared metadata layer. The metadata layer is represented on a blockchain.

What is special about blockchain systems is that they enable combining decentralized coordination — through smart contracts — and compensation — through a cryptocurrency, in a unified framework. This provides the opportunity to build a data processing infrastructure that brings together decentralized compute nodes that are able to process data as a single machine.

For AnyLog, decentralization provides the ability to process data where the data resides using independent nodes that can work autonomously. At the same time, the nodes share the metadata (which is published as a set of policies on the blockchain) and can exchange messages allowing the nodes to operate in sync and share and query in real-time any needed data or state.

The fact that a large centralized database is replaced with many small distributed databases at the edge provides significant advantages:

First, queries are processed concurrently on multiple nodes (per query, AnyLog identifies the nodes with relevant data and deploy a MapReduce type of process). With this approach, performance tuning is automated as with more data, the distribution of the data is increased leading to additional nodes participating in the query process and less data on each node.

Secondly, it is much simpler to automate HA with a small database vs. a database supporting complex big data operations. AnyLog leverages the fact that a small database is simple to replicate and when an edge database is updated, one or more mirrored databases are updated concurrently. With this approach, if a node fails, one of the mirrors kicks in without downtime and a background process will either recover the failed node or create a new copy of the data.

Unified schema and data view

AnyLog unifies the treatment of the data and provides a unified data view. For the user or application, data is treated as if it is organized in a single unified relational database. However, physically, the data remains in-place, distributed at the edge of the network. This is done using virtualization, allowing users and applications to be serviced by a unified data plane.

When AnyLog Nodes receive data, they will identify the structure of the data. This structure determines a schema and with the identified schema the node proceeds with one of two options: If the schema is an existing schema (it is published on the blockchain), the node will host the data using the schema. If the schema is new (not available on the blockchain), the node will publish the schema on the blockchain such that a different node that will need to store the same type of data will use the same schema. This approach creates, automatically, a unified metadata layer across all the nodes of the network.

From here, a user and application can be offered with the list of tables (using a lookup to the blockchain data) and for each table they can view the columns and data types that make each table. This process allows to formulate queries in the exact same way that queries to a centralized database are formulated. With this approach, a query to any table can be handed to any node in the network.

Self managed processes

One value of the AnyLog approach is the fact that the data is treated in a unified way. Regardless of the type of data, the same processes apply — leading to a published schema and a MapReduced query process. This uniformity makes the self-managed feature doable — as explained HA is automated and processes like data partitioning, distribution of data, removal of data, are all automated.

The ability to decentralize high-performance and self-managed computation with the ability to utilize diverse IoT devices and data has the potential to open new opportunities and applications. These applications would create richer experiences due to the performance characteristics and unification of diverse IoT devices. This approach gets us one step closer to the future of IoT where the silos between IoT applications are broken.

For more information, read the research paper: http://cidrdb.org/cidr2020/papers/p9-abadi-cidr20.pdf",9296
"While the 2020s is likely to be remembered for its introduction of COVID-19, the lessons it has brought have undoubtedly spearheaded new ideas regarding cities of the future. According to the United Nations, the number of megacities whose populations are greater than 10 million are expected to increase from 33 to 43 by 2030. This means that by 2030, “urban areas are projected to house 60 percent of people globally and one in every three people will live in cities with at least half a million inhabitants.”¹

Populations of this magnitude can pose numerous challenges for city resources, transportation, utilities, and infrastructure. To meet these challenges, governments have turned to technology to convert cities into “smart cities”. Put simply, a smart city is an urban area that uses different types of electronic methods and sensors to collect data. Insights gained from that data are used to manage assets, resources, and services efficiently; in return, that data is used to improve the operations across the city.²

How Cities Can Become ‘Smart’

For cities to become “smart”, they must utilize a wide array of advanced technologies to host, create, store, and transfer large amounts of data. This is done by utilizing key technologies such as 5G, IoT, AI and blockchain. Ideally, 5G would provide the bandwidth necessary to connect various sensors and devices via complex IoT frameworks, blockchains can be used to store, process, and secure data quickly and efficiently, and AI could be used for recognizing patterns for optimizing utility and resource output among other applications.

Blockchain Technology- An Integral Piece

Blockchains specifically make the vision of smart cities truly complete as they can be utilized in virtually every sector from digital currency, agriculture, finance, supply chains, and much more. Additionally, blockchains enable network participants to exchange data with a high degree of reliability and transparency without the need for a centralized administrator. Cities have a variety of stakeholders and the exchange of data among stakeholders is essential for highly convenient urban services.³ Blockchain-based smart contract technology can also offer seemingly endless possibilities by adding a new level of autonomy across various different industries.

The Future of Smart Cities Are Not Isolated, They’re Interoperopable

Realizing the full potential of smart cities means that cities will no longer be isolated, instead, they too will have a high level of interconnectedness. It is essential to ensure interoperability and coordination among multiple smart cities. Some efforts are already underway to achieve this aim. In Japan, the cabinet office released a white paper on reference architecture for smart cities in March 2020, which cited interoperability as one of four fundamental concepts that are important in promoting smart cities.

A recent blockchain-focused white paper published by the World Economic Forum for the G20 Global Smart Cities Alliance cites “openness and interoperability” across cities as one of the main objectives for true smart city functionality. Blockchain unsurprisingly is at the helm of this vision⁴

Wanchain — The Blockchain Interoperability Solution Leading As The Foundation For Smart Cities

To make smart cities “smart”, they must be interconnected, this principle rings true for blockchains as well. To date, only one blockchain protocol harnesses the ability to interconnect multiple blockchains on a mass scale.

Wanchain is a well-established, decentralized blockchain interoperability solution existing to harness the exclusive advantages by interconnecting blockchain ecosystems for commercial, public, personal, and state use. Wanchain’s mission is to drive blockchain adoption through interoperability, security, and transparency by building fully decentralized bridges that interconnect many of the different existing and future blockchain networks.

Wanchain is unique in that it already supports numerous blockchain bridges across many different chains. Wanchain’s interoperability prowess isn’t just limited to the crypto sphere. On March 30th, 2020, the largest utility company in the world; the State Grid Corporation of China, announced its adoption of Wanchain’s state-of-the-art blockchain interoperability platform. Part of Wanchain’s prowess is its ability to interconnect both public and private blockchains in mass both efficiently and securely. This technology can interconnect numerous different individuals, cities, databases, and securely house and transfer data in mass.

Did You Know?

In 2017, The Economist published an article titled “The world’s most valuable resource is no longer oil, but data.” This headline title continues to ring true for both the near and distant future.

In 2018, CNBC published a segment about data. In the report, China was forecasted to be the largest producer of data at 48.6ZB (zettabytes) by 2025 while the U.S followed with 30.6ZB. In total, global data is forecasted is expected to reach approximately 175ZB by 2025.

Data is crucial for smart cities. China, being the world's largest data producer, is racing to harness all the technologies necessary to make smart cities a reality. Wanchain’s protocol is already being deployed by the largest utility company ever puts it in an extremely valuable position to capture the colossal opportunities this market has to offer.

Conclusion

While the decade of 2020 is likely to be remembered for its pandemic catastrophe, its challenges have set the stage for a new era of innovation that will transform society for decades to come. Wanchain seeks to foster Smart cities of the future to combat future pandemics, climate issues, and the challenges of tomorrow.",5769
"Welcome to Faces of Formally! We are so excited to kick off our staff interviews with the incredible Briana Das. Briana works her incredible magic at Formally from behind the scenes as a product designer! Most of the visual content you will see on our website is the brainchild of Briana, who created her own degree program at Brown University in digital experience design. But, her passion for design goes all the way back to childhood. Growing up in the Bay Area, Briana recalls “doing design for as long as I can remember, starting with random project-based stuff before, just kind of noodling around with artwork and building things,” and this interest eventually “turned itself into a love of digital design when I grew up.” And she didn’t stop there — while becoming an expert in all things design, Briana concurrently studied psychology to be able to “understand human behavior” and is now focused on “creating with and for those people.”

Like so many immigrant rights advocates, Briana’s ties to immigration stem from personal history. Her parents “came to this country when they were young, so truly the reason I have the life I have is because of immigration.” Their experiences motivated Briana to join Formally, where her goal is to “make complex processes accessible and build trust with people” — and she is doing just that. You would never guess that our design expert has only just graduated from college! Briana’s wisdom and years of experience come from hard work and dedication. While an undergraduate student, she took computer science, art, and sociology classes to fulfill her self-created degree while working at Formally. Giving us a peek inside of the atypical 9–5 day of a product designer, Briana describes her intense “design power hours,” the time when she “would turn all my devices off in the evenings and focus on my creative flow.”

Briana’s face lights up when she mentions the most enjoyable aspects of working at Formally. “Accessibility and inclusive design are the core of what I do every day,” she exclaimed when asked how important Formally’s mission is to her daily work. These tenets of Formally’s mission are always at the forefront of her mind. According to Briana, the design process is often nonlinear, “Before I even start designing everything on a computer, I ask myself questions like, ‘Okay is this the language we want to use? How can we make our platform more accessible?’” Briana strives to incorporate trust into her workflow and is always conscious of how she can further empower people by designing a platform that enables clients to own their data and complete their own forms.

So, you may be wondering why Briana decided to join our small company instead of a larger social impact startup. Well, the answer is quite simple: “One of the things I love most about Formally is the people involved. The first time I sat down and had a conversation with Amélie I was in awe of her, thinking ‘you are my role model, I want to grow up to be you.’” The community mixed with each employee and intern’s high level of emotional investment drew Briana in. Everyone at Formally can agree with Briana in saying “working with people who have this determination and drive to do so much good in the world while being incredibly intelligent, articulate, and kind at the same time is very precious.”

For Briana, the rewarding aspects of working at Formally are constant motivation to do better and refine her designs. Seeing “all of [her] designs come to life in an impactful product that people are using,” is always the most gratifying part of her workday. At Formally, we are constantly trying to improve and move the bar higher and higher, and just like Briana, it is what drives us forward. Briana’s hope for the next year is to see more people benefitting from Formally’s platform! She is amazed by what the team has created together and truly believes in the power of accessible immigration, “we have built such a powerful tool, and getting it into the hands of more people will be huge.” Although Briana will only be working part-time with us in the Fall, she has taught the entire team how to work, think, and collaborate in a more effective and empathetic way. Before you go, check out Briana’s favorite feature on Formally.us (and the hardest to create!) — our amazing attorney review!",4337
"Tech News || Taobao and Weibo | Bose QuietComfort 35 | Huawei P50 | M1 Abrams Tanks Tehnologijaviews Jul 22·5 min read

HomeVideo Game Tech News || Taobao and Weibo | Bose QuietComfort 35 | Huawei P50 | M1 Abrams Tanks

Taobao and Weibo fined by China for illegal children’s content

China’s internet watchdog has ordered some of the country’s largest online platforms to remove inappropriate content related to children. Kuaishou, Tencent’s QQ messaging tool, Alibaba’s Taobao and Weibo have been summoned by the China Cyberspace Administration (CAC).

CAC says that platforms must “rectify” and “clean” all illegal content and has fined them. The announcement comes as Beijing cracks down on tech companies . In a statement, the CAC said that “the operation is focused on solving seven types of prominent online problems that endanger the physical and mental health of minors.”

They were: Children on live broadcasts and becoming influential on social media, promoting the “cult of money” and extravagance, pornographic and violent content, inappropriate cartoons that had erotic and violent content . Forums that encourage behaviors such as suicide or participation in child pornography and fan clubs in which children participated in fundraising activities have also been fought.

The Bose QuietComfort 45 would have been leaked through an image

Time has passed since Bose launched the QuietComfort 35. Those headphones were the benchmark in the field of wireless headphones with active noise cancellation. Both that model and the Sony WH-1000XM3 dominated the market in that sector and we often found comparisons between these two audio titans. Although we can currently find models that exceed the aforementioned in range, Bose had not yet launched a direct successor to the QC35, until now. And it is that according to a leak, we have been able to know the external design that the next Bose headphones would have.

Read also: Netflix Confirms It Will Add Video Games To The Service; these will be your first steps

As you can see in the image on these lines, everything indicates that the firm will remain conservative in terms of design. And if you look at it, its design is practically identical to the QC35 , showing that today this design is still very comfortable for our ears when we go out with them. Although the QC35 had a micro USB port for charging them, everything indicates that Bose will update its headphones by incorporating a USB-C. We know little about its technical specifications, but everything indicates that it will be an update that will be well received.

The release date of these new headphones is still unknown, although according to WinFuture , everything indicates that the QuietComfort 45 would hit stores this year. We will have to wait to learn more about it.

The Huawei P50 are a reality and we will see them from next week

We are a few days away to know first-hand what will be the appearance, design and specifications of the new Huawei P50, the next premium range devices of the firm. This has been confirmed by the CEO of Huawei’s consumer division, Richard Yu on the Weibo forum . Remember that these devices come after the HarmonyOS event that the company held last June. Although we still do not know too much about these new devices, if we have a first image as a teaser that you can see on these lines.

Read more: Google Wear OS is updated to bring interesting improvements

According to Yu, one of its main advances is in its cameras, since everything indicates that it would “establish a new standard”. If the rumors are confirmed, the P50 series would include the modified Sony IMX800 sensor in its main camera, which is one inch in size and has only been used at the moment in the Sharp Aquos R6. In terms of technical specifications, the device would include Qualcomm’s Snapdragon 888 , since the latest reports (via Android Central ) state that there would beescasez de chips Kirin 9000.

The device will be presented on July 29 through a special Huawei event. For the moment the P50s would remain in China, although we may have news soon about their arrival in the West. We will have to wait to learn more about it.

Poland just bought American M1 Abrams tanks

Poland will purchase 250 M1A2 Abrams SEPv3 main battle tanks from the United States, replacing its older tanks dating from the Cold War. The tanks will arm the units of the Polish army that protect against the incursions of the Russian army.

The purchase is quite a remarkable turnaround, considering that the United States originally designed the M1 tank to fight Soviet and Polish forces on the battlefields of Western Europe. The basic design of the Abrams has remained robust enough to accept new technologies as engineers develop them.

Read more: PlayStation 4 is the new target of cryptocurrency “farmers”

The impeccably designed M1 Abrams main battle tank equipped with a 105-millimeter main gun , Chobham composite armor and a gas turbine engine, debuted in 1981 as the most advanced tank of its kind. The tank has survived various efforts to replace it, gaining improvements such as a larger and more powerful 120-millimeter cannon; a depleted uranium armor; a digital command and control system; and a separate “hunter” thermal imager for the tank commander.

Do You Know What I Have Posted on",5331
"Many of you know me — either as an architect or as the CEO of Anguleris — and my focus on aligning building product manufacturers with the way architects work. You know that I love problem-solving, value creation, and the challenge of building something new from the ground up. (You also know that I bristle at the status quo and refuse to play by rules that don’t work — ask my school teachers.)

You’ve been along for the ride as we got our start making industry-leading BIM content. You witnessed as we built BIMsmith into the leading BIM content platform and then stood by our side as we launched Swatchbox in 2019. Again, most recently, we integrated BIM with specification writing through Masterspec.

This week, our team announced yet another groundbreaking tool for the AEC community. The launch of Swatchbox Pro brings the best building material brands together onto a single platform to give architects an easy way to source multiple product samples at the same time.

Building materials speak a language of their own that can’t be heard in a digital environment. Samples provide a physical connection to products and materials that we otherwise may only experience digitally or on the building site after it’s too late. It’s vital that in the ever-faster spinning flywheel of digital transformation, we as architects find ways to get away from the screen, touch the physical, and then translate it back to the project. That’s what we aimed to do with Swatchbox in 2019 and now what we’re taking to the next level with Swatchbox Pro.

I think back to my days as an intern and an associate where I’d spend hours if not days trying to wrangle all kinds of manufacturer reps and web forms just to get the samples I needed for the projects I was tasked with. I’d spend all that time only to be disappointed when a product never arrived. I now see the time of our young professionals, who have so much more to offer, getting wasted week after week, month after month. Swatchbox Pro is for you. We hope we can play a small part in delivering you inspiration.

I’ve never been more proud of my team and the surrounding tribe that helped make this all a reality. I am humbled and ecstatic all at the same time. To all of you, thank you from the bottom of my heart. I love this industry and the community that surrounds it. Swatchbox Pro is for you.

To those of you who don’t yet know my team and me, we hope you’ll reach out and join us as we continue on the journey of changing the construction industry, not for the sake of disruption, but for the sake of a better built environment.

Cheers,

Ben",2602
"Fully automated, self-healing and self-protecting — this is how software will be operated in the future. With this vision, Dynatrace and Johannes Kepler University (JKU) are founding a joint co-innovation research lab at the Linz Institute of Technology (LIT). The cooperation between business and science will advance application-oriented basic research in the field of software intelligence. Austria will thus become a hotspot for digitization research. Looking back at Dynatrace’s rapid development — founded in Linz in 2005, IPO in New York in 2019, around 2,800 employees, $545 million in annual revenue — it is all too easy to overlook the basis for this success: Meticulous development work to expand the company’s position as a global market leader. Dynatrace holds over 207 patents. To maintain this momentum in the coming years, the company founded Dynatrace Research a year ago, a separate research unit from its operating business. Now its head, Alois Reitbauer, is forcing the exchange with academic research. The Co-Innovation Lab at LIT sees itself as a bridge between research and business and acts as a radar system for flashes of inspiration. The cooperation is deepened by university teaching activities of Dynatrace experts and two post-doc positions.

“We have a team of around 1,000 developers who ensure that our product is the best on the market and continuously extend this lead,” explains Alois Reitbauer. Because innovation cycles in IT have shortened rapidly, new paths have to be taken in order to secure a technological lead, not just in the short term. That’s why Dynatrace now seeks close personal contact with the academic elite, who pursue research projects entirely without economic constraints and deadline pressure. “We can’t afford to overlook flashes of inspiration that might trigger a surge in innovation or even disruption,” Reitbauer says. He, therefore, looked for a digitization expert with campus experience for the job of lab director at JKU — and found him in Andreas Hametner. The Upper Austrian native completed his degree in computer science at JKU and is now returning to his former place of training.

Meeting quality is a central task of the Co-Innovation Lab for JKU Vice-Rector Christopher Lindinger. “The Co-Innovation Lab is an excellent role model for research cooperation between universities and companies like Dynatrace. As an interface between science and business, the LIT Open Innovation Center at JKU embodies an ideal place for this encounter, which provides essential impulses for the further development of the business location. JKU’s research excellence in the field of IT security, Industry 4.0, Artificial Intelligence or Big Data represents a great asset for application-oriented product development,” says JKU Vice-Rector Christopher Lindinger. The Co-Innovation Lab at LIT is the perfect place for this, emphasizes Veronika Leibetseder, Director R&D Labs Operations at Dynatrace. “Our lab on campus is designed as an open space that signals openness and transparency.” This, she says, allows students to find a place to work there at any time and experts with whom they can exchange ideas. “We want to show who we are and what culture shapes us. That’s why the lab acts as a sort of Dynatrace embassy on campus.”

Return to the roots

For years, Dynatrace has been active in the international Internet standardization body W3C and is the only Austrian company involved in the Cloud Native Computing Foundation (CNCF). One of the CNCF’s credos is to make the results of its work openly and freely available to the market. The cooperation with JKU is now a return to the roots for Dynatrace. After all, Dynatrace was founded in 2005 by three JKU graduates. The technological heart of the world market leader in software intelligence beats in the newly built engineering headquarters in Linz, which will be occupied in 2019. “For us, basic research at JKU is a kind of radar system for upcoming developments,” Reitbauer emphasizes. The 7-member Dynatrace Research department, which he heads, will move to the JKU campus and is expected to more than double in size within a year. In the research focus areas of Distributed Data Systems, Realtime Analytics, Data Science, and Cloud-Native Security, there is now a need for increased cooperation with academic researchers. “This is necessary because we know that in just a few years it will no longer be possible to manage the exponentially growing volumes of data with current instruments and methods,” says Reitbauer, outlining the scenario that research and industry foresee.

Making unimaginable data volumes manageable

It is a data explosion that Reitbauer is counting on. While an IT architecture with 100 servers used to be considered large, an environment with hundreds of thousands of servers is now quite common. Reitbauer also anticipates that data volumes could increase by a factor of 100,000 or even a million within a few years. “We should be prepared for the fact that we will soon no longer be specifying storage capacities in terabytes, but in petabytes or even exabytes.” This rapid development can be observed every day on one’s own smartphone, he said. Social media services and online retailers display their web pages in millions of individualized versions. “When I log in to my account, I’m shown my own personal browsing history, my favorites, articles viewed and purchased, links and products last clicked on, or even customized recommendations and preference from friends and acquaintances,” says Reitbauer, citing one reason for the unchecked growth in data.

Escalating complexity as a key research topic

“The digital convenience we value so much when shopping, paying, traveling, parking or banking is a one-way street whose frequency has been further increased in the Covid crisis,” Reitbauer argues. In the Co-Innovation Lab at the Linz Institute of Technology (LIT), the spiral of complexity and growing data volumes is becoming a central research topic. Since one is not involved in short- and medium-term product development, one can collaborate intensively with the academic research community without economic pressure, argues Dynatrace Lab Director Andreas Hametner. “We can pursue and support exciting research approaches here at the Co-Innovation Lab, whose potential cannot yet be estimated.”

Univ. Prof. Rick Rabiser, head of the LIT Cyber-Physical Systems Lab and Hametner’s university counterpart at the Co-Innovation Lab, appreciates “the opportunity created by the Co-Innovation Lab to establish a new kind of transdisciplinary, scientific research between industry and university. Both partners, JKU and Dynatrace, bring different strengths to the cooperation. University research, motivated by practical challenges, has been a successful model at JKU for many years, especially in the field of software engineering. The Co-Innovation Lab enables JKU to further expand these strengths and to evaluate research methods based on real data. Dynatrace benefits from the closer connection to the academic research landscape, which can both create scientific foundations and provide the impetus for innovative solutions.”

Reinvention as a standing order

To drive profound innovation, Dynatrace retired a team of top developers from day-to-day operations back in 2014. Constant reinvention, as well as fully automated, self-healing, and self-protecting software, was the answer to growing complexity, he said. The artificial intelligence (AI) and machine learning required for this are the focus of research at the Co-Innovation Lab. To achieve this, new models must be developed for real-time data analysis (real-time analytics), for example. Since websites sometimes consist of several hundred microservices that constantly transmit data, they are confronted with rapidly increasing data volumes. “For this, we need to develop more efficient learning methods for AI to keep the multitude of models in real-time reporting up to date and thus also provide predictions of likely scenarios,” Hametner explains.

Alert inflation and learning models

Efficient training is also needed for the models used in data science. They must be able to recognize patterns in deeply individualized applications such as smartphone apps in order to make programs more stable and enable self-protection and self-healing. When it comes to security, there is another critical component to the complexity of the systems and the flood of data, Hametner explains. “Unfortunately, you have to assume that systems that were secure at release will not remain secure from attack. Security is the discipline with the most momentum.” That’s why it’s far from enough to identify problems early and report them reliably, he says. What matters is prioritizing between warnings of acute threats and, for example, minor problems in program libraries.",8881
"Railz x Prodigy’s IDVerifact bring secure digital transformation to financial institutions Railz Jul 22·3 min read

Railz is excited to announce its partnership with Prodigy’s IDVerifact!

Railz and Prodigy’s IDVerifact to prevent risk in accounting

Thanks to our exciting partnership with Prodigy’s IDVerifact, our users can prevent risk with real-time accounting data, offering an exciting yet secure digital transformation for financial institutions.

What is Railz?

Railz is the only financial data-as-a-service solution that connects, normalizes, and analyzes financial data from top accounting service providers. We do this to make financing decisions faster, better, and easier for financial institutions and their commercial and small business customers. Out of Toronto, Canada our fintech company recently raised our Series A to scale our Accounting Data-as-a-Service platform.

What is Prodigy?

Prodigy delivers Fintech innovation by providing leading edge platforms, including IDVerifact™ for digital identity , and new Fintech platforms for open banking and payments. Our services business, Prodigy Labs™, integrates and customizes our platforms for unique enterprise customer requirements, and provides technology services for digital identity, payments, open banking and digital transformation. Digital transformation services include strategy, architecture, design, project management, agile development, quality engineering and staff augmentation. Prodigy has been recognized as one of Canada’s fastest growing companies with multiple awards: Deloitte’s Fast 50 Canada and Fast 500 North America (2016, 2017, 2018), Branham 300 (2017, 2018), Growth List (2018, 2019 and 2020), and Canada’s Top Growing Companies (2019 and 2020).

What is IDVerifact?

A Prodigy venture, IDVerifact provides the ability to combine and access a complete suite of leading digital identity providers to meet any business use case. With IDVerifact, organizations stay ahead of fraud, quickly identify risk, and ensure compliance while optimizing their ability to grow revenues and improve client experiences with digital transactions. Providing solutions for identity proofing, KYC, AML, zero proofing and risk proof management.

Railz and Prodigy’s IDVerifact partner up

For financial institutions (FIs) the Railz and IDVerifact partnership means that FIs can integrate with our data-as-a-service platform and have the combined trust of leading digital identity providers to mitigate risk, fraud, and more in their data analysis.

IDVerifact will integrate Railz real-time accounting data into the IDVerifact platform through our single API key. IDVerifact’s goal is to create the most comprehensive digital identity platform and trusted digital asset solutions to enterprise clients. Beyond that, the Railz platform helps these enterprise clients have deeper and more impactful relationships with their small-medium business (SMB) customers

George Colwell, the Senior Vice President (SVP) of Digital Practices at IDVerifact

George Colwell, the Senior Vice President (SVP) of Digital Practices at IDVerifact states, “Railz is leading in the delivery of accounting data as a service and provides IDVerifact with a unique source of digital assets providing a clear view of a company’s cash flow and, more importantly, easily including those insights directly into a digital process. IDVerifact includes accounting data as a service as part of our risk pillar for small and medium enterprises and we are excited to count Railz as a partner, helping us build out our ever-growing list of unique digital assets in this space.”

We look forward to providing our financial institutions the ability to integrate with our data-as-a-service platform and have the combined trust of leading digital identity providers to mitigate risk, fraud, and more in their data analysis. Get in touch to request a demo or sign up to try Railz today!",3926
"Zurich May 2019

Agile Business Process (ABP) of the UK, a multinational provider of cross-sector, process automation solutions, has announced that it will use the Aurachain platform, to start delivering blockchain applications for its clients.

Leveraging the Aurachain platform for rapid development of digital process applications and smart contracts, on the blockchain, Agile will use the platform to implement customer solutions that promote process continuity and trust-based interactions between their clients and their clients’ external ecosystem of customers, vendors and trading partners.

The platform will support accelerated, but controlled deployments of client solutions that leverage the inherent value of blockchain networks.

“The Aurachain platform presents a tremendous opportunity for both Agile and our clients.”, says Jorge Soares, CEO of Agile. “For Agile, this represents an extension of our product portfolio and core organizational competency, into the promising arena of blockchain. More so, we can now offer our clients a means to achieve a broader type of process automation value, through trust based collaboration with their external universe of ecosystem partners.” “Aurachain is excited by Agile’s decision to leverage our platform for their expansion into the blockchain application space.” said Jon Wiener, CRO of Aurachain. “They are an outstanding organization with a long and deep track record of success, delivering process automation solutions, across clients in financial services, legal and government sectors, to name some. With Agile, we know we’re gaining a partner that first and foremost understands that technology must be applied for operational purpose. More so, an organization that knows how to make that happen in a meaningful way, on behalf of their clients. We look forward to aligning with Agile as partners in our respective growth.”

About Aurachain

Aurachain provides global business with a visual development environment for the design and rollout of enterprise grand applications. It allows fast implementation cycles, rapid cloud deployment and most importantly, business-driven application configuration.

Aurachain platform is the only solution that combines the power of low-code development with advanced and automatic code generation, for the creation of blockchain applications, enabling users to intuitively distribute process stages, on and off the blockchain. By doing this, it creates a bridge between the current enterprise platforms and the upcoming multi-sided, business ecosystems.

About Agile Business Process

Leveraging thousands of hours of experience delivering successful digital transformation projects, Agile Business Process ( ABP specializes in developing tailored and “fit for purpose” process automation solutions for their clients ).​

Believing that processes are the heartbeat of any organization, Agile delivers best-in-class process automation solutions that result in tangible performance metrics, continuous improvement and improved customer experiences.​

The Agile team possesses comprehensive knowledge across all areas of business process improvement, enabling them to provide an extensive set of services, covering all key methodologies and technologies in this field. Agile is recognized by clients, partners and analysts as one of the top players in the Business Process Management (BPM) and Robotics (RPA) markets.​

Visit Agile at: https://www.agile-uki.com",3464
"Technology is the sum of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation.The best definition of technology is the study and transformation of techniques, tools, and machines created by humans. Technology allows humans to study and evolve the physical elements that are present in their lives.

The term “technology” rose to prominence in the 20th century in connection with the Technological revolution.In 1937, the American sociologist Read Bain wrote that Technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them.

There are many components of information Technology but the first three components of technology systems — hardware, software, and data.Hardware(This is the physical technology that works with information).Software(The hardware needs to know what to do, and that is the role of software).Data(process of facts and figures, called data).

Knowledge, Process, Science, Engineering, Skills, Tools….. etc are also the components of Technology.In my point of view,Knowledge is the main component of Technology.Without knowledge,we cannot do something.

Technology plays an important role in society today. It has positive and negative effects onthe world and it impacts daily lives.Technology affects the way individuals communicate, learn, and think. It helps society and determines how people interact with each other on a daily basis.

Now days, many ICT gadgets are used in our life and they facilitate with mobility thus used anywhere and anytime. These gadgets operate for Information, Speed, and Communication and reduce the physical and mental human work load. By that principles, modern day gadgets truly helped mankind in daily life.We find that while some of these impacts are beneficial, like improvements in education, health, innovations, government service delivery, and participatory democracy; others are pervasively detrimental to the society as a whole,dissemination of offensive images by foreign and local media.

Islam is supportive of scientific research that brings benefit to humankind. Many Muslims appreciate technology and respect the role that science plays in its creation. As a result, he says there is a great deal of Islamic pseudoscience attempting to reconcile this respect with other respected religious beliefs.

Technology has changed the way people live their everyday lives. It’s present in almost everything you do, from how you communicate to how you perform your day-to-day tasks. Thanks to technology, it’s now easier to go to work or perform household chores.

I also make a website in which i publish about technology and science bloges.Link is https://teebom.blogspot.com

If you are intrested in technology and science news, then visits to our website.Also follow me for more latest information about technology, science and other articals.",3034
"On a Monday morning, Jamis MacNiven—equal parts restaurateur, entertainer and teller of (sometimes) tall tales—stood beneath an enormous, leathery anaconda hide that runs the length of the dining room inside his Silicon Valley restaurant, Buck’s of Woodside. There, in a cowboy hat and a bright red, short-sleeved button down shirt, MacNiven addressed a packed dining room filled with entrepreneurship students who’d just rolled in on a bus from a San Mateo university founded by third-generation venture capitalist Tim Draper.

MacNiven had joked that he was going to “scream the history of Buck’s” to his breakfast audience, an often-invoked spiel that he says is “all just fun. I tell truths and lies in equal amounts.”

His latest announcement — delivered beneath that sprawling serpent skin with a sign that boasts it was stripped from a snake in the Amazon that had attacked a child — also existed somewhere in between fact and fiction: MacNiven was putting Buck’s up for auction.

Jamis MacNiven shares the history of Buck’s of Woodside with an audience of entrepreneurship students from Draper University, on Monday, August 2, 2021. The restaurant has played an essential role in Silicon Valley history as a meeting place where deals were made and ideas hatched for startups that went on to become household names. (Photo by Sara Hayden)

No, not the actual, physical restaurant at 3062 Woodside Road that’s seen the birth of many a tech company over the years, but its digital doppelgänger, via non-fungible token. “We’ve recently launched an NFT,” MacNiven declared. Some of the students responded with a laugh, but it was true—Buck’s was indeed listed on the OpenSea marketplace as “the world’s first 3D immersive NFT.”

By the time the auction closes August 19, whoever places the winning bid for Buck’s NFT will own access to the one and only complete digital re-creation of the restaurant. These days, with something akin to a digital deed on a blockchain database, you can own digital assets like a tweet or an animation. So why not a replica of a near-legendary Silicon Valley institution?

As a place to meet and greet, wheel and deal, Buck’s is a big deal. Indeed, the NFT brochure states that since Buck’s opened its doors at the start of the 90s, “An invitation to ‘Breakfast at Buck’s’ would go on to become an official step in the high-tech startup funding process.” The practice has even been the subject of a Harvard University study.

The breakfast gathering that morning marked entrepreneur Giulianna Crivello’s first visit to Buck’s, a pilgrimage that was coinciding with a new fund she was launching, supported by Tim Draper. “It feels historical,” she says, “the inner child in me is screaming.”

But when you remove these interactions, and bring Buck’s back solely in pixels, does it hold the same kind of draw?

MacNiven seems to be betting that when treated like a digital museum filled with tech artifacts and lore, preserved at a specific moment in time, Buck’s still has plenty of appeal.",3023
"The U.S. healthcare sector has proven one of the most resistant to technological change for a host of reasons. Each state, hospital, and insurance company relies on proprietary technologies with no standardized protocols.

Patient care is continual and urgent (one can’t exactly clear out a hospital for a few days to upgrade all the systems). And with people’s lives at stake, managers are understandably cautious about experimenting with unproven solutions.

But gradually, change has happened — and it appears to be accelerating. A new wave of healthcare tech is emerging that portends a transformation on how people give and receive care.

We interviewed dozens of healthcare technology startup founders and identified six exciting solutions we believe are worth keeping an eye on. They include everything from how your health data is recorded and transmitted, to changing the way your brain functions.

eVisit

The eVisit telehealth platform may seem extraordinarily prescient, considering that it was conceived and developed before the COVID-19 pandemic hit. Now, with telehealth transforming from novel to essential, literally overnight, the future of medicine has arrived with a blast. But this system is more than just Zoom for your doctor. What makes it unique is its ability to integrate with other systems to gather medical records, analyze insurance info, and streamline the check-in process, making receiving care from home the seamless experience we’d hope for in the modern era.

“Patients will go through the intake process, which looks and feels very much like the experience you’d have in a clinic. We collect your demographic information, your insurance information, we ping the clearing houses to help understand what your co-pays are, your reimbursement rates, the deductible you’ve met, you select your pharmacy, and you submit a credit card to pay for the visit. And then you’re provided with a list of providers you can select from. Usually, it’s your own doctor. …the provider receives a text message notification that they have a patient in the waiting room, they log in and they can review your chart. And they’ll go through the same experience that you’d have in a clinic face to face.”

-Bret Larsen, founder and CEO

Watch our full interview with Bret Larsen

DHIT

It has long been known that the U.S. healthcare system is in great need of a digital transformation. Health records, insurance information, and doctor communication all feels stuck in a perpetual state of techno catch up. A part of the problem is that the U.S. healthcare system is in actuality thousands of microsystems, each with their own protocols, processes, and systems. But the Digital Health Institute for Transformation is endeavoring to bring those systems together with what they call the Health Utility Grid, a technological framework that will do to the healthcare industry what high-speed internet did for personal computers. Currently, efforts to establish the Health Utility Grid are underway in North Carolina, and the hopes are to expand from there.

“Five years from now, my expectation is that the Health Utility Grid is a brand new public utility that is rolled out throughout the state of North Carolina end to end and has made its way through the entire Southeast, if not through the country as a brand new 21st century infrastructure to power our healthcare system and our life.”

-Michael Levy, CEO

Watch our full interview with Michael Levy

Deep Lens

A crucial need for timely and efficient healthcare coordination and collaboration happens in medical trials. Pharmaceutical companies require patients suffering from specific conditions, while meeting a spectrum of criteria — and of course, the patients need treatment and must be made aware of the option to participate in a trial, most often within a narrow timeframe. Deep Lens uses artificial intelligence to help coordinate this complex process, allowing researchers to find the right patients, and maximizing the opportunities for patients to volunteer for new treatments. Deep Lens also provides a free digital pathology cloud platform called VIPER that helps cancer researchers coordinate data to improve outcomes.

“I see Deep Lens and the VIPER platform becoming the de facto system, and really enabling care teams and the communication between the providers and the pharma sponsors really being that enabler. And when folks turn to a system or a technology or a process that wants to facilitate optimizing how their patients are getting on clinical trials — and then even on the sponsor side, being able to increase their drug to market — Deep Lens is who they turn to.”

-Dave Billiter, Deep Lens founder and CEO

Watch our full interview with Dave Billiter

Embodied Labs

Caregiving has not only experienced a technological transformation, but a philosophical one as well. Once thought of as clinical functionaries, doctors, nurses, and homecare providers have begun to recognize the importance of understanding the perspectives of those they treat — their stories, their burdens, their daily experiences. And this where the explosion of VR has the potential to create an entirely new way to use technology in the field of medicine. Embodied Labs is doing just that with VR software that helps train caregivers to understand the people they’re caring for by creating immersive experiences emulating the lives of those in need of care. And the potential extends well beyond healthcare to a wide variety of sectors looking to expand their understanding of people being served.

“We have a lot of customers in the healthcare services space. We have academic institutions that integrate Embodied Labs into their training programs; we have corporate clients who serve older adults and use this to train their employees. We have lots of senior services groups, like a home care agency, a hospice, or a senior living community that uses this for training their staff and community education. But one of my favorite examples, and where we’ve really grown, is that we work with several municipal governments, their divisions of adult protective services and this group, called area agencies on aging, which is in nearly every county of the United States — they take Embodied Labs and champion the technology, the hardware, and they enable our software platform to train their social workers, their nurses, their community educators.”

-Carrie Shaw, Embodied Labs founder and CEO

Watch our full interview with Carrie Shaw

Healium

Another exciting use of VR in the healthcare field is Healium, which uses immersive technology to train your brain to manage stress. Purchased as a recurring subscription, Healium offers guided meditations in nature environments, immersive stories, and a video curriculum on neuromeditation. They also sell the BrainLink EEG Headband, a device that works in tandem with the VR headset to provide visual neurofeedback to improve mental resilience. The software also works with the Apple Watch. And if the VR headset is out of reach, augmented reality options are available for your phone. They also offer a business plan for companies looking to reduce stress among their teams.

“Healium is used in enterprises and large government contractors around the world in areas of acute situational or confined stress. The American Red Cross has used Healium before with blood donations to allow people to be somewhere else. It was being used on the front lines, most recently for healthcare workers to try to combat compassion fatigue. It’s used in schools to help students develop a mind-body connection and by teachers who just need some virtual peace. Healium’s horizontally marketed because stress is such a huge problem — it’s a $300 billion profit and people killer. This is not a niche market. And specifically in the wake of what we’re seeing with the pandemic, this is the stress Olympics and not everyone is trained for it. So the ability to have something that quickly downshifts the nervous system in a drugless way that you don’t have to go through a long, training about meditation or mindfulness has value.”

-Sarah Hill, Healium founder and CEO

Watch our full interview with Sarah Hill

Saykara

Digital technology has certainly done a great deal for medical record keeping. Though there is still much work to be done coordinating and integrating systems, can one even imagine the days when all your health data was stored in a basement file cabinet? But regardless of where and how your data gets recorded, doctors still spend a lot of time recording it. This is where Saykara is pioneering dramatic new ways for providers to document their patient interactions without spending half their days punching at a keyboard. It uses AI language recognition to process key information exchanged between doctors and patients to later be presented and reviewed at a fraction of the time it would take to assemble manually. This is how technology should function — getting out of the way to allow professionals to focus on their tasks.

“In this country physicians spend an inordinate amount of time on administrative tasks, such as documenting patient care, interacting with EHRs to put orders, and things like that. It’s a huge cause of physician burnout, not to mention productivity, and loss of revenue for health systems. Interestingly, it’s universal across all specialties. Pretty much every physician you talk to will say, I just spend way too much time on my keyboard. I spend more time on my keyboard than I do seeing patients. What we’re doing by automating the documentation process by listening in on doctor-patient conversations and creating those clinical notes automatically, we’re relieving physicians of that burden, and allowing them to do what they really signed up to do, which is see patients and provide care.”

-Harjinder Sandhu, Saykara founder and CEO

Watch our full interview with Harjinder Sandhu

Sometimes it seems like the healthcare industry is stuck in 20th century tech. But that’s changing — and fast. We spoke with dozens of tech innovators disrupting the healthcare sector, and selected six to keep an eye on. Here they are.",10177
"A brewing conflict between tech giants

China is in the hot seat as US and allies, including the European Union, the United Kingdom, and NATO, officially blame it for this year’s widespread Microsoft Exchange hacking campaign.

To keep you in the loop, these early 2021 cyberattacks targeted over a quarter of a million Microsoft Exchange servers, belonging to tens of thousands of organizations worldwide.

On a related note, the US Department of Justice (DOJ) indicted four members of the Chinese state-sponsored hacking group known as APT40 last Monday. This is with regard to APT40’s hacking of various companies, universities, and government entities in the US and worldwide between 2011 and 2018.

Could this be tied up to China’s move to develop cyberattacks capable of disrupting US pipeline operations? Hmm..

Just 2 days ago, Wednesday, Cybersecurity and Infrastructure Security Agency (CISA) and the Federal Bureau of Investigation (FBI) issued a joint advisory that Chinese state-sponsored attackers have breached 13 US oil and natural gas (ONG) pipeline companies way back 2011–2013.

This is developing news so be sure to check out our next week’s release!

Finally, we give you a quick overview of the new malwares in the cyber landscape to keep tab on.

MosaicLoader and XLoader, has joined the game

Bitdefender researchers have confirmed a novel malware posing as a cracked software via search engine advertising. MosaicLoader is a malware downloader designed by its creators to deploy more second-stage payloads on infected systems.

A more niche-specific malware which is known to steal information from Windows systems, was also reported this week to have been modified. The “new and improved” malware can now target macOS. This is definitely the upgrade we never want!

The revamped malware is dubbed as XLoader. Sounds like a console right?

That’s a wrap for this week’s happenings on cybersecurity. Never miss important updates on data breaches, new data protection policies, and other techno trends by following Privacy Ninja!",2052
"What is pegasus and how to find pegasus in your mobile full detail RG TECH NEWS Jul 23·2 min read

What is pegasus and how to find pegasus in your mobile full detail

Android hacking

You must have heard about Pegasus. This is a software that can sneak into your phone and leak all the details.

This software has been made by the Israeli company NSO Group. All politicians and journalists are also being spied with the help of this software.

Also read — poco f3 launch date in india

If you also want to know whether it is spying on you then read this post completely.Researchers from Amnesty International have released a tool. This will help you to find out whether your phone is affected by Pegasus or not.

Also read — top 10 gaming headfones under budget

Researchers from Amnesty International have published a tool. Anyone can find out whether their phone is infected with dangerous Pegasus spyware or not.The name of this tool is Mobile Verification Toolkit (MVT). This tool was designed by Amnesty’s researcher. It is being said about the Mobile Verification Toolkit that it can be found that your phone is infected with Pegasus spyware or not.

Also read — maruti Suzuki new electric car launch date in india

This toolkit works on the Command Line Interface (CLI). This simply means that a common user will feel a lot of difficulty in using this tool. It is being said about the Mobile Verification Toolkit that it can be found out whether your phone has been infected with Pegasus spyware or not.

Mobile Verification Toolkit for iPhone can read any indicator of backup and compromise. The tool for Android devices checks the backup of SMS or text sent from the domain of NSO Group. Apart from this, it also checks all the apps on the device. So that it can be known that the compromise has not happened.

For more information click here",1851
"Artificial Intelligence: Week #29 | 2021

This week in AI & Machine Learning: AlphaFold protein structures, model-free reinforcement learning, Vision AI in Demanding Environments, AI helping firefighters, organ transportation, and more! Sage Elliott Jul 23·3 min read

Author’s Note:

Next week I’ll be teaching Intro to Computer Vision: Building Object Detection Models and Datasets on July 29th at 10:00am PDT. It’ll be a fun fast-paced workshop covering computer vision applications, data annotation, and training an object detection model with python!

Top AI Highlights this Week :

Google’s DeepMind just released a massive AlphaFold database containing a prediction of nearly all possible protein structures for human cells and 20 other “key organisms” to help accelerate scientific research!

DrQ-v2 is a model-free reinforcement learning algorithm for visual continuous control that uses data augmentation to learn directly from pixels.

the first model-free agent that solves humanoid from pixels

Check out the github (also contains more fun visuals)or read the research paper: Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning

We’re excited to announce our partnership with CP Technologies to bring autonomous and edge-deployed vision AI solutions for rugged hardware and Unmanned Aircraft Systems (UAS).

More AI & Machine Learning Stories:

🤖 Artificial Intelligence News:

🛠️ Developer Tools & Education:

📅 Upcoming Online AI & Data Science Events:

🎤 Interesting Podcasts & Interviews:

📄 Notable Research Papers:

🐘 About the Author & Plainsight:

Sage Elliott is a Developer Evangelist at Plainsight & passionate about making AI more approachable. Connect with Sage on Twitter or LinkedIn.

Plainsight provides custom enterprise AI solutions, end-to-end machine learning lifecycle management, and fast data annotation for computer vision. Join the conversation on Slack.

See you next week!",1944
"I wasn’t ever too keen on the idea of employees working remotely. In fact, I was stubbornly convinced that a successful company required a total in-office presence.

I believed I was so right for so long that remote work doesn’t work that I used to turn down reputable job candidates who wanted to work at Pushnami but who were not willing to relocate to Austin to work inside the Pushnami office.

I was adamant about it to the point that we once did not hire a really senior developer that we very much needed. He wanted to work for us but another company offered to let him work remotely so we didn’t get him. That was a big miss for our company.

Even before the pandemic hit, some employees did express interest in working from home from time to time. But that just wasn’t the kind of culture I wanted for Pushnami. I used to tell people you’re either in the office or you’re not part of our team. I did not believe you could collaborate or innovate unless you’re doing it in person.

So how did the fastest-growing company in Austin evolve from staunchly opposed to firmly supportive of remote work?

A record first quarter in 2021, as well as being named the fastest growing company in Austin, reinforced that I was wrong.

But it wasn’t so easy getting comfortable with the concept.

Our first company meeting on Zoom was super weird. You can’t look at 50 people at once (although I definitely try to click through to see everyone) and it was a big adjustment. Before the city of Austin shutdown due to COVID-19, we prepared for the potential of forced remote work conditions by implementing Wednesday work-from-home “drills.” In full transparency, I really didn’t believe that this was going to be needed. But some employees were very convinced that this was coming, though, so we decided to practice to work out the kinks.

We were only able to get one trial Wednesday in before the city shut down and forced us to carry on business from the safety of our homes.

So what happened once Pushnami went fully remote?

We continues to smash monetary milestones, add new benefits to adapt to the remote culture and to hire. Most of all, employees innovated, collaborated and got their work done — even though they were doing it from their home offices.

In times like these, you can either shake your fists in the air and scream “this is not right, I’m not going to do this, it’s my way or the highway,” or you can adapt and understand that this is the way the world is going to be now. Yes, it’s hard, but everything is hard. Even in the office, things are hard sometimes. You just have to adapt.

That’s not to say there weren’t some drawbacks once the pandemic struck, though. Some employees were feeling lonely or unhappy, not because we weren’t going out of our way to cater to employees but because the impact of the pandemic was taking a toll on the team’s emotional and mental health.

Some people decided they wanted a change in their life over the course of this, so we lost good people here. After everything that was going on in the world, some of our team discovered they just wanted something different in their lives. I don’t blame them at all, I think we all hit a wall at some point where we just had enough of all of this.

Today, we hire employees outside of Austin and even outside of the state. Some recent hires live in Oklahoma, Florida and Ohio.

I find it freeing that we can hire the best of the best from all over now.

As for going back to a mandatory in-office environment? That’s just not in the cards for us. If I said it was mandatory to be in the office right now, even for the people who live right here in Austin, one-third of my team would quit. There is no going back. They love it.

Things aren’t perfect though. The team is still learning how to “turn off” when they’re at home, especially since our hires are known for having superb work ethic. We offer compensation and the challenges and the rewards that come along with that. Because of that, it is hard for certain people to turn off as we are a 24/7 business here. The notifications don’t stop. Our clients don’t stop: we’re global. This is a very hard job, but a fun one.

At the end of the day, technology companies have shown the most promise in the face of remote scenarios as our teams excel at using technology. Businesses who adjust more quickly end up with the competitive advantage when it comes to the candidate talent pool.

During everything that took place in the world, we were able to continue to offer our people opportunities to advance and we kept doing the good work inside our community. Isn’t that what really matters?

Want to join our outstanding team? We’re hiring!",4689
"Artificial Intelligence

Towards the end of deep learning and the beginning of AGI

Javier Ideami explains in this article how the volume of neurons, cortical columns in our neocortex and movement help us better understand how our brains work and therefore help us build more flexible and resilient artificial neural networks.

Voice clone of Anthony Bourdain prompts synthetic media ethics questions

The creators of “Roadrunner,” a documentary about the deceased celebrity chef Anthony Bourdain, trained an AI to generate a copy of Bourdain’s voice and use it to read one of his emails. This has sparked some ethical questions about synthetic media which this interview sheds light on.

Dynascore’s AI Music Engine Writes Tracks to Match Your Videos

In need of some cool music for your latest YouTube video? There is an AI now that will do just that.

AI-Generated Language Is Beginning to Pollute Scientific Literature

Researchers from France and Russia have published a study indicating that the use of AI-driven probabilistic text generators such as GPT-3 are introducing ‘tortured language’ into scientific journals. Some of the flagged papers seem to use GPT-3-like algorithms to bolster the English skills of the papers’ authors but some seem to be completely generated by AI.

Robotics

OpenAI disbands its robotics research team

Robotics is hard and OpenAI is another company to learn that lesson. OpenAI dabbled into robotics with a robotic arm solving Rubik’s Cube but did not decide to pursue the research further. Instead, the company decided to focus on other, more profitable, projects, like GPT-3 and its applications.

Mini Pupper: Boston Dynamics Spot in the Palm

Mini Pupper is a project for all who want to play with Boston Dynamics Spot but don’t have spare $75k to get one. Designed at Stamford, Mini Putter aims to be a “robot dog for education at an acceptable price”. And it is open source so if you want you can go on and build one yourself.

Getting dressed with help from robots

Researchers from MIT trained a robotic arm to safely put a jacket on a human. The robot had to plan its motion with a human to make sure no one gets hurt. Researchers hope their work could potentially prove to be a powerful tool in expanding assistance for those with disabilities or limited mobility.

One might think that teaching robots how to dance is a silly idea not worth pursuing. This article, however, argues that teaching robots how to dance and the emerging field of choreorobotics can help robots interact safely with and around humans.

Biotechnology

DeepMind puts the entire human proteome online, as folded by AlphaFold

The AlphaFold Protein Structure Database is a collaboration between DeepMind, the European Bioinformatics Institute and others, and consists of hundreds of thousands of protein sequences with their structures predicted by AlphaFold — and the plan is to add millions more to create a “protein almanac of the world.”

How Designer DNA Is Changing Medicine

Gene therapies based on the latest gene-editing techniques are revolutionising medicine. This article mainly focuses on applying those techniques to sickle cell disease but the same methods can be used to cure other genetic diseases.",3239
"Technology is constantly changing and Technology news websites are a great way to keep up with the latest trends and developments in the world of technology. Technology news websites cover everything from computer hardware, software, programming languages, cybersecurity issues, and more. We compiled a list of 10 of our favorite tech news sites to help you stay up-to-date with all things tech!

1. Wired

Wired, with its sleek interface and high-quality design, is a favorite of many tech enthusiasts. Wired covers everything from AI to self-driving cars to the latest in virtual reality. Technology news on wired.com can be sorted by category for easy reading!

The Wired is a technology news website with an incredible amount of detail and depth. They cover everything from the latest gadgets to emerging science. The best thing about it though is that they review all their content before publishing so you don’t have to take anything on faith.

Their articles are organized by category which makes them easy for new readers who might not know what this site covers thoroughly yet: Technology News, Business, Science & Culture, Design in Society (which has a lot of really cool stuff), Books Reviews, etc. This helps people find something interesting quickly without having to dig through each article looking for something specific — if I want more information about Artificial Intelligence or Robotics then I just click on those categories under Technology News where there are articles on those topics.

The site is also really easy to navigate with a clean layout and lots of white space so it’s not too overwhelming.

The Wired is my favorite tech news website because they’ve been in the game for a while and have seen all of the trends come and go. I trust that they’re accurate and unbiased because they’ve been around for so long, which is exactly what you want from a news source.

Click on wired.com to visit the website

This is a screenshot of their Technology News homepage, it provides easy access to all Technology news.

2. Cnet

Technology know-it-all reviews, specs of the latest smartphones and laptops, lists on how to choose a tablet or laptop — CNET has it all. One of the top tech news websites that offer comprehensive coverage for your daily dose of technology information is Cnet. This site provides an easy way to stay up to date with current events in Technology as well as those coming soon. With its clean layout and many different articles on Technology topics, you will never run out of things interesting to read! To search for the specific type of content just use their simple search bar at the top!

CNET has been around since 1994 and focuses on technology news. They have content for everyone from newbies to experts, including reviews of tech products as well as information about the industry’s latest trends. CNET is also home to plenty of videos that cover a variety of topics related to gaming consoles, mobile devices, and much more. You can watch their popular video series: “The Top 100 Gadgets” which features all sorts of gadgets ranked by popularity with commentary from gadget expert Elliot Taylor. in total you will find the best tech news over there.

Click on cnet.com to visit the website

This is a screenshot of their Technology News homepage, it provides easy access to all Technology news.

3. TechCrunch

TechCrunch is a leading online publication for technology news, analysis, and commentary. Technology coverage has been included in its headlines since TechCrunch was founded by AOL co-founder Michael Arrington on September 27, 2005. As of March 18, 2016, CrunchBase reports that the site receives more than 26 million visitors per month (38% from the U.S.) with 30 million page views coming from outside of North America. The content published ranges across breaking stories to longer features including interviews and opinion pieces or blog posts written by contributors at their discretion which are not necessarily endorsed or approved by editors before posting; topics include Internet culture as well “offbeat” items such as popular gadgets like iPhones or other social media trends such as Twitter hashtags

They also feature follow-up articles about stories that are trending in each area so you can stay up to date without getting overwhelmed from all angles at once.

Click on Techcrunch.com to visit their website

This is a screenshot of their homepage, it provides easy access to all Technology news.

4. PC World

PC World is one of the most popular websites for technology news. Technology enthusiasts come to PC World because they know that it will always have up-to-date information on what’s happening in the tech world, and also provide a forum for discussion about how new technologies can be applied to everyday life. What sets PC World apart from other publications in their coverage of industry trends; journalists at this site are experts not just in writing content but also in creating online tools such as interactive guides and infographics related to technology developments.

This blog has been around since 1984, demonstrating its longevity as an authoritative voice on all things technological. The website publishes posts every day with topics ranging from reviews of new smartphones or laptops coming out soon, trending viral videos, and all the latest news in technology.

Technology enthusiasts come to PC World because they know that it will always have up-to-date information on what’s happening in the tech world, and also provide a forum for discussion about how new technologies can be applied to everyday life.

Click Pcworld.com to visit the website

This is a screenshot of their homepage.

5. Mashable

Mashable is an online news website that covers the latest in tech, the Internet, and digital culture. Its coverage includes social media, web trends, technology reviews, and more. The site was founded by Pete Cashmore in 2005 while he was still a student at the University of Aberdeen in Scotland and has grown to have six different editions: Mashable Technology; Mashable Business; Mashable Entertainment; Mashable News; Social Media Today (a daily newsletter); and Moms With Apps for moms with young children. As of July 2015, it had over 13 million readers per month according to ComScore.

Click mashable.com to visit the website

This is a screenshot of their homepage.

6. The Verge

The Verge is a technology news website that covers the most recent and breaking developments in the world of news media. The site’s content includes reviews, opinions, blogs, videos, podcasts, and more. Technology companies like Google and Samsung are among their frequent sources for information on trends within both consumer electronics as well as tech-centric policy issues such as net neutrality or SOPA/PIPA legislation.

One benefit to Tech News Websites is that they provide live coverage from events happening across the globe ranging from CES to E-Wars. One downside however can be how quickly these websites break stories before other sites which may have an adverse effect on traffic depending upon what was reported first by one of them versus another competing publication group. In addition, sites such as The Verge may have an advantage in social media over other groups because of their large following on Facebook and Twitter.

Click verge.com to visit the website

This is a screenshot of their homepage.

7. LifeHacker

The next one is LifeHacker. This technology site is focused on being productive in your day-to-day while also offering some “counterintuitive” tricks that might make life a little bit easier. The content offers ways for you to use old tech or new tech in the most efficient way possible which makes it great if you’re looking for shortcuts of all types.

Lifehacker has been around since 2005 so they have had time to build up quite the library full of articles that are organized neatly into categories like General Technology, Money & Career Hacks and Productivity Tools. You can browse through their archives by category.

Lifehacker’s goal is to help people find the most efficient ways to do everything tech-related from setting up a home theater system, managing your gadgets for work or play, and of course how to keep them safe on the internet. They cover topics that range from gaming reviews, security features in Windows Vista, app management with LaunchBar or Quicksilver, configuring an iPod Touch as a remote control device for Windows Media Center PC — this site has it all!

The audience ranges anywhere from students looking for advice about school projects, mothers trying to figure out what games their kids are playing (as well as which ones might be appropriate), dads who are overwhelmed with how to best manage their gadgets.

Click Lifehacker.com to visit the website

This is a screenshot of their Technology News homepage, it provides easy access to all Technology news.

8.Digital Trends

Digital Trends is a leading technology media property that provides expert analysis of the latest in digital culture. It has been delivering cutting-edge news, insights, and reviews with authority since 2000.

Digital Trends is a purveyor of all things tech, from the latest smartphones to reviews on new apps. Technology enthusiasts can find content for every niche and topic imaginable at Digital Trends. This website has an extensive archive that includes guides on how-to videos for everything from photography to gaming, as well as news articles that are updated daily with fresh stories about technology trends around the world. Technology journalists love this site because it’s one of the most reliable sources out there when researching breaking stories or finding expert opinions on topics within their field. It also provides up-to-date reports in real-time about current events occurring both nationally and internationally; covering anything from business decisions made by major companies like Apple Inc., to political policies being enacted in countries.

Click Digitaltrends.com to visit the website

This is a screenshot of their Technology News homepage.

9.Gizmodo

Gizmodo is an American technology blog that covers the latest in gadgets and tech. Articles are shared on a daily basis about innovations released into the market such as new smartphones or tablets. The content includes reviews of these products along with investigative reporting to keep readers informed about upcoming trends within this industry. Gizmodo was founded by former Engadget editor Brian Lam who wanted to create his own blogging platform for topics not covered elsewhere online which led him to start Gizmodo Media Group.

Gizmodo is one of the most popular tech news websites. They cover a wide range of topics such as how to buy and use technology, what’s coming next in consumer electronics, the latest gadgets, and gear available for purchase or pre-order, reviews on new products like smartphones and smartwatches, science stories about robotics, and space exploration, etc. Gizmodo readers are interested in all things Technology.

Click gizmodo.com to visit the website

This is a screenshot of their Technology News homepage, it provides easy access to all Technology news.

10.TechRadar

TechRadar is a British online publication that covers technology. It provides reviews of smartphones, tablets, and other consumer electronics. Techradar also gives information about the latest video games with news articles and in-depth reviews. Unlike most tech websites it’s not an opinion site but rather more of a straight product review style website for all things tech-related.

The Techradar website is a popular destination for tech news, reviews, and buying advice. They cover the latest breaking technology trends in some of their sections such as ‘smartphones’, ‘computers’ or even ‘TV’s’. It has an editorial team from across the globe who are experts in various fields. They have more than 500,000 followers on social media channels like Facebook and Twitter which makes them one of the most followed websites online today.”

Click techradar.com to visit the website

This is a screenshot of their Technology News homepage, it provides easy access to all Technology news.

Conclusion

Technology is evolving faster than ever and it’s important to stay up-to-date with the latest developments. The 10 news websites we’ve included in this blog post are some of our favorites, but if you have a favorite that didn’t make the list, let us know! We’re always looking for new ideas on how to keep readers informed about all things tech-related. Technology affects each one of us differently so don’t be afraid to share your own thoughts in the comments below or tell us what article topics you want to be covered next time around!",12798
"BVB and the Central Securities Depository are accelerating the Digital Transformation with the help of the Aurachain Platform AURACHAIN CH Follow Dec 23, 2020 · 3 min read

Bucharest, Romania — December, 23

Aurachain is the new technology partner for the Bucharest Stock Exchange (BVB) and the Central Securities Depository. Two innovative solutions developed using the low-code Aurachain platform will be implemented to accelerate and optimize the shareholder voting process at exchange-listed companies using blockchain technology, and to facilitate access to the capital market by digitizing investor enrollment.

The second solution is the digitization of the online process of opening trading accounts by individual investors, through a standardized service that the Central Securities Depository will provide as a single point of entry for all new individual investors.

The technology solution developed on the Aurachain platform has the role of facilitating the investors access to the Romanian capital market in a completely automated and fully secure manner. Actions such as registration, identity verification, facial recognition and Know Your Client (KYC) processes will be performed exclusively in a digital environment and will be automated.

“The low-code Aurachain platform transfers much of the development of digital applications into the hands of business users who need innovative solutions, through intuitive visual configuration capabilities that replace the traditional code-writing approach. Key personnel across the organization, from subject matter experts and business analysts to professional developers and IT specialists, can contribute their expertise directly to the app creation process for significant gains in operational efficiency with no governance faults,” said Adela Wiener, CEO of Aurachain.

The two solutions offered to the Bucharest Stock Exchange were developed at light speed using our platform and represent yet another example of how we can help organizations accelerate their digital transformation and process automation efforts from day one of any engagement.

“ This joint project between the Central Securities Depository and Aurachain is very important from the perspective of the digital transformation of all processes in capital markets. We want to use Aurachain’s solutions in the area of ​​identifying and profiling individual clients, a solution we want to offer to the brokerage community in the capital market, and to other industries or platforms that need such a solution. Another product we want to implement is a platform for organizing meetings and voting for General Shareholders Meetings, Boards of Directors, and Committees. The Intervote platform will address primarily the needs that companies listed on both the Regulated Market and the AeRO market, within the Multilateral Trading System, have in organizing these meetings. We believe in the potential of Aurachain solutions and the blockchain technology that underpins these solutions. Through the initiative announced today, we will propose to expand the cooperation with Aurachain to other processes, with the ultimate goal of digitizing and simplifying the activity of investors, brokers and issuers in the regulated area of ​​the capital market”, stated Adrian Tanase, CEO of the Bucharest Stock Exchange, the majority shareholder of the Central Securities Depository.

Through the new partnership with Aurachain, BVB takes another important step towards the modernization and development of the Romanian capital market, by facilitating the expansion of the investor base, streamlining the decision-making process and reducing bureaucracy for listed companies.

About Bucharest Stock Exchange

Bucharest Stock Exchange runs markets for shares, bonds and other instruments, through regulated platforms and alternative systems, and provides a wide range of services to participants of financial markets. Bucharest Stock Exchange is a public company, listed on its own market since 2010.

The cumulative market capitalization of all companies listed on the Bucharest Stock Exchange (local and international) exceeds RON 163bn (EUR 33.6bn), and the cumulative value of bond issues listed on the BVB amounts to RON 17.8bn (EUR 3.6bn).

The global index provider FTSE Russell announced, in September 2019, the upgrade of the Romanian capital market to the Secondary Emerging Market status. As of September 21, 2020, Romania is effectively included in the FTSE Russell indices for Emerging Markets. For more information on the Bucharest Stock Exchange, please refer to www.bvb.ro.",4603
"Wyze Cam Login Help | +1 805–791–2114 | Wyze Cam Phone Number Lilyvictoria Follow Aug 23 · 3 min read

Are you looking for any help regarding the Wyze Cam Login? Because of their affordability and excellent capabilities, Wyze Security Cameras are one of the best cameras and are loved by its users a lot. Wyze has security cameras available for both indoor and outdoor use.

If you are a new Wyze user, you have to complete the Wyze Camera Setup to benefit from all of its features.

In this guide, we are going to discuss everything about the Wyze login and its setup. If you need any more help, dial the Wyze Phone Number now.

Points To Remember For Wyze Cam Login

It is very much important to complete the Wyze Setup to do Wyze Cam Login. Let’s know some necessary steps that are necessary to know to complete the setup of your Wyze security camera.

Good speed internet connectivity. Installing Wyze at a place from it will capture the clear and maximum vision. Don’t aim the camera to look inside or outside the transparent objects. Don’t position the camera far from the router. Download the Wyze app to complete install and set up the camera successfully.

If you have any questions revolving in your head, dial the Phone Number for Wyze Cam now.

Steps To Do Wyze Camera Setup?

In order to complete the process for Wyze Cam Login, install the camera by following the steps mentioned below.

With proper safety and care, unpack the Wyze camera and remove all the accessories. It would be better to install the camera near the power source. Take the power adapter, connect one to the power outlet, and another one to the security camera. As we have mentioned above, download the Wyze camera app on your tablet, smartphone, or even computer. If you have an android device, download the app from Google’s play store, if you are an Apple user, download it from the play store.

When you open the app, it will ask you to do Wyze Cam Login. Enter the credentials if you already have an account. Or else, create the one now using your email id.

Now, open the Wyze app and select the option of ‘New Device’. On the home screen, you will find this option. From the options mentioned on the screen, choose your Wyze camera. At the bottom of the Wyze camera, click on the ‘Setup’ button. When you press the button, you will hear ‘Ready to connect’. Once you hear this sound, it means you have done the setup in the right manner.

Is the Wyze Camera App Also Available For Pc?

Most of the Wyze users thought that the Wyze app is only available for Android and iOS devices, but you can also download and install the app on your PC.

Although a number of sources are available from where you can download the app. But, we would like to suggest you download it from a trusted source only.

Let us tell you one of the simplest ways to do it, first download the app from the Google Play Store. Now, with the help of BlueStacks, you can easily access the app on your pc.

Note: To access Android apps on a PC, BlueStacks is one of the best software.

Conclusion

In order to complete the Wyze Cam Login process, it is necessary to set up the camera first. In this guide, we have told you all the steps to complete the Wyze Camera Setup in some easy and simple steps. If you still need any more help, dial the Wyze Customer Service Phone Number now. Our experts are waiting for your call. They will help you to complete the login in some easy steps.",3442
"A Glorious guide on creating your smart contracts and interact with smart contracts using web3

1. A system to store data on the blockchain and retrieve the stored data from the blockchain

Pre-requisite:

To follow this tutorial, you need to have a basic knowledge of the following programming languages:

HTML: HTML is used to design the structure of the website JavaScript: Javascript is used for interacting with the deployed smart contracts to store, retrieve, and manipulate information Solidity: solidity is used for writing down smart contracts

Tools Requirement:

ChainIDE [an online cloud-based, multi-chain IDE] available at: https://chainide.com/

MetaMask [a wallet for web3], available at: https://metamask.io/

!Note: For this tutorial, you don’t need to download any tools or libraries except MetaMask and web3.js.

Libraries:

web3.js [web3.js is a collection of libraries that allow you to interact with a local or remote ethereum node using HTTP, IPC, or WebSocket], available at: https://cdn.jsdelivr.net/gh/ethereum/web3.js/dist/web3.min.js

Experiment Setup:

Once you have access to ChainIDE and MetaMask, connect MetaMask to ChainIDE and buy some ethers from a faucet for any of the following test networks:

Ropsten Test Network (suggested) Kovan Test Network Rinkbey Test Network Goreli Test Network

To get test ethers from a faucet, simply click choose any of the above-mentioned networks and click on Buy →Test Faucet → Get Ether, and you will be redirected to the selected faucet network. This method varies a little from network to network but all are quite easy.

2. MetaMask

After adding the extension of MetaMask to your browser, make sure you have selected the faucet network.

Method

First of all, we will write down a smart contract that can store the employee’s information such as id, first name, last name, address, mobile number, etc.,

!Note: We’ll use ChainIDE for the whole tutorial, ChainIDE supports all the programming languages that are needed to complete this tutorial.

Following code is a smart contract written in solidity that is used to store the employee’s information on the blockchain. To make use of this smart contract, we need to deploy it on the blockchain, and before, we can deploy it, we need to compile it.

The pragma keyword defines the compiler version that we will use to compile the solidity code. Once you have completed writing down your smart contract, simply compile it from the compile panel, and you will get the ABI code for the following solidity code.

3. Contract Compilation

The ABI which is known as the application binary interface is needed when you want to interact with the smart contract using web3.

Now, the next step is to deploy the complied smart contract.

4. Contract Deployment

To deploy a smart contract, you must need to have some faucet ethers in your wallets to pay a gas fee to deploy for the smart contract. In fig.4, we can see we paid 25 gwei to deploy our “Employee_Records.sol” smart contract.

Once a smart contract is deployed, it will be assigned a contract address, as we can see in fig.4, highlighted pint 3, and from the interaction panel, we can copy our smart contract address and can check our deployed smart contract on the faucet network also.",3264
"Smart Contract and Web3! All that you need to know. Ali Follow Jul 23 · 5 min read

A Glorious guide on creating your smart contracts and interact with smart contracts using web3

1. A system to store data on the blockchain and retrieve the stored data from the blockchain

Pre-requisite:

To follow this tutorial, you need to have a basic knowledge of the following programming languages:

HTML: HTML is used to design the structure of the website JavaScript: Javascript is used for interacting with the deployed smart contracts to store, retrieve, and manipulate information Solidity: solidity is used for writing down smart contracts

Tools Requirement:

ChainIDE [an online cloud-based, multi-chain IDE] available at: https://chainide.com/

MetaMask [a wallet for web3], available at: https://metamask.io/

!Note: For this tutorial, you don’t need to download any tools or libraries except MetaMask and web3.js.

Libraries:

web3.js [web3.js is a collection of libraries that allow you to interact with a local or remote ethereum node using HTTP, IPC, or WebSocket], available at: https://cdn.jsdelivr.net/gh/ethereum/web3.js/dist/web3.min.js

Experiment Setup:

Once you have access to ChainIDE and MetaMask, connect MetaMask to ChainIDE and buy some ethers from a faucet for any of the following test networks:

Ropsten Test Network (suggested) Kovan Test Network Rinkbey Test Network Goreli Test Network

To get test ethers from a faucet, simply click choose any of the above-mentioned networks and click on Buy →Test Faucet → Get Ether, and you will be redirected to the selected faucet network. This method varies a little from network to network but all are quite easy.

2. MetaMask

After adding the extension of MetaMask to your browser, make sure you have selected the faucet network.

Method

First of all, we will write down a smart contract that can store the employee's information such as id, first name, last name, address, mobile number, etc.,

!Note: We’ll use ChainIDE for the whole tutorial, ChainIDE supports all the programming languages that are needed to complete this tutorial.

Following code is a smart contract written in solidity that is used to store the employee's information on the blockchain. To make use of this smart contract, we need to deploy it on the blockchain, and before, we can deploy it, we need to compile it.

The pragma keyword defines the compiler version that we will use to compile the solidity code. Once you have completed writing down your smart contract, simply compile it from the compile panel, and you will get the ABI code for the following solidity code.

3. Contract Compilation

The ABI which is known as the application binary interface is needed when you want to interact with the smart contract using web3.

Now, the next step is to deploy the complied smart contract.

4. Contract Deployment

To deploy a smart contract, you must need to have some faucet ethers in your wallets to pay a gas fee to deploy for the smart contract. In fig.4, we can see we paid 25 gwei to deploy our “Employee_Records.sol” smart contract.

Once a smart contract is deployed, it will be assigned a contract address, as we can see in fig.4, highlighted pint 3, and from the interaction panel, we can copy our smart contract address and can check our deployed smart contract on the faucet network also.",3348
"Photo by Zoltan Tasi on Unsplash

I have been fascinated by blockchain technology for so many reasons, which I will not discuss in this article. However, it’s not a new topic that cryptocurrency is the future of money, and NFTs are doing incredibly impressive for digital artists.

So what is my aim with this article? I got interested in the blockchain space, and after doing a lot of research, I couldn’t find enough learning guides like you would, for, say, Data Science which is pretty standard. It would help if you understood that the reason for this is that there is a shortage of Solidity programmers(about 200,000* developers in the world) compared to other popular programming languages. Therefore, there aren’t enough tutorials out there. Hence, I will be writing a series of articles on learning the language, and you can learn from this to become a blockchain developer.

This is the first article of the series, and it will be on writing your first smart contract with Solidity. There are many concepts that you might not be familiar with yet, and that’s perfectly okay. I will do my best to break down each concept into bits as we move along the series, and you can do more research from there.

To get started, here are a few questions that you should be asking; What is a Smart Contract? What is Solidity? How does the blockchain work?

Follow along as I provide answers to these questions below.

What is a Smart Contract?

A “Smart Contract” is simply a program that runs on the Ethereum blockchain. It is a collection of code (its functions) and data (its state) that resides at a specific address on the Ethereum blockchain. Think of a smart contract as an account on the Ethereum network, which can not be controlled by the user and runs as a program. However, a user can interact with a smart contract by submitting transactions that execute a function defined on the smart contract.

The way smart contracts ensure that transactions occur in real life is much more complex, but here is a simple explanation to understand how things work.

Photo by Cedrik Wesche on Unsplash

Imagine that this article is for sale, and I am the seller, you the reader is the buyer, and you are buying this article by using an affiliate link you saw on the internet. We can deploy a smart contract for this transaction, and here’s how the exchange might occur.

You, the reader, creates a transaction that sends the price of this article to the seller in exchange for a copy of this article. Like you would usually in a normal bank transaction, you will provide my address as an input to the smart contract’s address. The smart contract will then carry out this transaction by verifying that you have enough money in your wallet and that I also have a copy of this article ready for sale. Then, it deducts the money from your wallet, sends it to my wallet, tells me to send you a copy of the article, and finally sends the affiliate commission to the owner of the affiliate link after deducting that from my wallet.

The above is how smart contracts can facilitate transactions between users who do not trust each other, and it is used in the real world to carry out much more complex transactions.

This leads us to the following question: What is Solidity?

What is Solidity?

We already established the fact that Smart Contracts are software programs. For Ethereum, you can write smart contracts using a statically-typed language called Solidity, which is the most widely used smart contract programming language at the moment. Solidity is a full-featured language for writing general-purpose smart contracts, and it looks like JavaScript.

How does the blockchain work?

To answer the last question, I think it’s an important concept to know. I will share a good video by Anders Brownworth, it explains the concept: How does the blockchain work?

Watch here; an elementary visual introduction to the concepts behind how the blockchain works.

Now that you have a good understanding of the concepts above let’s see how you can write your first smart contract.",4067
"Data becomes easier to perceive and draw insights from when it is properly represented in charts and maps. For everyone interested to check out how information visualization works, we continue our regular feature DataViz Weekly curating the best new examples of effective graphics.

Here are our latest picks:

COVID-19 case acceleration rates — STAT

Climate change risks across Germany — Vislab

What makes people happy the most — Nathan Yau

Driving time to Paris from across France — Nicolas Lambert

COVID-19 Case Acceleration Rates

Regular charts representing the number of new confirmed COVID-19 cases per day work well to demonstrate the current pace of the COVID-19 pandemic. They also make it easy to see the magnitude of the waves at a glance. But they are not as good to let the data tell us how quickly the pace is speeding up, which could let us know where we are heading.

Emory Parker, the data project manager at STAT, decided to look at the rate of weekly case acceleration in the United States. To calculate it, he used COVID-19 case data compiled by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU) and Our World in Data, combined with the official statistics from the Centers for Disease Control and Prevention (CDC).

On the STAT website, you will find several charts showing how quickly the weekly average of new COVID-19 cases is changing — for the United States in general, for Louisiana and Florida where the acceleration rate is currently the highest, and for Massachusetts where it is much more moderate.

Climate Change Risks Across Germany

Experts anticipate the effects of climate change on the Earth to become more and more tangible over the next few decades. However, the magnitude of shifts will vary depending on the exact location. Take a closer look at Germany, for example.

Vislab, within the framework of the Location and Context-Based Surveys and Storytelling (LoCobSS) research project funded by the German Federal Ministry of Education and Research (BMBF), published an impressive data visualization work that allows everyone to explore the risks from climate change across Germany at the local level.

Start by choosing a region. Then you will be able to learn about the climate challenges in a scrollytelling format. At the end, there is an animated map visualizing how the average temperatures have changed across Germany since 1888.

What Makes People Happy

Several years ago, researchers from the University of Tokyo, Massachusetts Institute of Technology (MIT), and Recruit Institute of Technology (now Megagon Labs) asked people to specify the things that had recently made them happy and used their answers to generate HappyDB, a corpus of 100,000 happy moments. Yesterday, Nathan Yau shared a set of fascinating visualizations providing some cool insight into that data.

First, Nathan parsed the subject (who or what), action (verb), and object (what or whom) for each of the happy moments in the database. Then, he counted the occurrence and connected the points using a packed bubble charting technique.

In total, there are three visualizations. The first one allows you to see what makes people happy overall. The second chart shows the breakdown for happy moments with other people. The third one displays the distributions for happiness with pets. All of them are available in print.

By the way, if you missed it, you may also like to check out Nathan’s previous attempt to make sense of HappyDB. Also very interesting.

Driving Time to Paris from Across France

Last but not least, check out a stunning isochrone map recently created by Nicolas Lambert. Titled Time Distance to Paris by Road, it visualizes how long it takes to get to Paris from across France by car.

The isochrone lines here connect the areas of equal travel time to Paris and are colored accordingly based on a green-to-red scale explained in the legend on the left. Hover over a line to find out how far the capital of France is from the corresponding locations in minutes of driving. Or, vice versa, hover over a certain time to discover the areas from which you can reach Paris in a car that fast.",4173
"Microsoft recently announced that they have reached a whopping 250 Million active users with Teams. Not Word or Excel but Teams is the rockstar of the Microsoft Office suite. But, It has always been plagued with performance issues as it consumes heaps of system resources. Running Teams is a nightmare on computers with less memory to work with.

Behind its ultra-modern, fluid user interface of ocean blue and pearl white widgets, It runs a demon in the form of a Chromium instance that throttles host computers and inadvertently sets their fans in motion.

Image by Uzair Ahmed from Pixabay

The Announcement from Microsoft

The senior vice president of Microsoft Teams announced that Teams would be moving to their own Edge Webview2 Rendering Engine ditching Electron for seeking performance gains. It is marketed that Teams would consume 2x less memory as a result of the transition. It would be called Teams 2.0 and might ship with Windows 11 in late 2022.

The Electron Problem

There are tons of well-known apps powered by Electron. The Electron framework helps web developers ship their web apps to the desktop platform evading platform-specific intricacies. Since a unique instance of Chrome OS runs in the backend of every Electron app, running more than two such apps takes the juice out of the host machine.

Teams, which does a lot of processing on top of Electron has become synonymous with hogging memory and slowing down computers. Microsoft even has a documentation page explaining why Microsoft Teams might have high memory use.

The Genesis of Teams on Webview2

Webview2 cannot be thought of as a replacement to Electron; It is not a wrapper like Electron to rapidly ship web apps on the desktop platform. The original Webview (Webview1 for namesake) used Microsoft’s Edge rendering engine while the Webview2 uses the Chrome rendering engine. Webview2 is already used by Outlook as a part of Microsoft’s “One Outlook” project.

Image Source: Microsoft

Unlike Electron, WV2 monitors Chromium's behaviour to detect how much system memory is available and utilizes that memory effectively to optimize the rendering experience. If other apps or services required system memory, Chromium then gives up that memory to those processes. This improves the performance significantly on low-end computers with less memory to work with.

WV2 can be thought of as a control like a window of an app; A control that renders web pages. In fact, the Webview2 control allows embedding web technologies (HTML, CSS, and JavaScript) in native apps. For an app of Teams size to transition to WV2, A lot of abstraction provided by Electron will have to be rewritten. As a result, Teams would essentially become close to a native Windows app.

The way ahead

Teams does a lot of things with audio and video and Microsoft thinks that it is best to offload some of that work on the native level which Webview2 facilitates that are not done efficiently with the abstraction of Electron.

Teams basically got too big for Electron. Electron was a great and only choice to ship on desktop as well as web back in 2017 but optimization is the only way forward in Tech. This may be a pivotal milestone in the ever-changing landscape of cross-platform frameworks. It could also be just a Teams specific change that only Microsoft can pull off.",3330
"Within the information field, security professionals must keep in mind that they perform a supporting service to the organization. Every organization has its own unique business objective and mission.

The tale of two hats

Security professionals often wear two different hats. Certainly, they are the subjective matter experts on information security and are tasked with securing the IT infrastructure, respond to security incidents, and the protection of value assets, they also have to remember the other hat they often have to wear: the business hat. Security professionals must understand the primary mission of the organization and both the strategic and tactical objectives to support the organizational mission. This includes short-term and long-term goals, how security controls impact business efficiency, and design a control environment that balance both security and business needs. This is a task security professionals often struggle with. A business case is a tool security professionals can incorporate to justify the investment of time and money in new security controls by providing a solid case on the impact to organization assets in a language management can understand.

Information security as a business function

As a business function, information security must align with other business functions and the governance processes within the organization. The governance process can take place at the different levels. This can be in form of committees such as risk management committee consist of senior level executives and with publicly traded companies including independent board of directors and other governing bodies. Despite the difference in governance structure, security professional must determine the best way to incorporate information security into the process to ensure responsible bodies understand the security risks facing the organization and the controls that are in place to manage those risks. Governing bodies should also be included in the incident response process and be informed when security incidents take place at applicable severity levels and be provided with regular audit reports to review the current security posture of the organization. There is certainly no one size fits all in security governance as every organization is unique. Security professional need to considering additional scenarios such as acquisition and divestitures as well as they happen often within the business world. It’s important to access the compatibility between systems, eliminate any redundancies, and ensure additional risk factors are not carried over during the process such as inadequate controls, insecure systems, and etc.

Role and responsibilities of security professional in Security Governance

While roles and responsibilities of security professionals can differ based on organization size and structure, the Chief Information Security Officer (CISO) is usually the individual bearing the responsibility of information security and is considered the senior information security leader within the organization. Under the CISO is the information security team that is composed of security professional with broad backgrounds in information security as well as subject matter experts in areas such as incident response, network security, identity and access management, and security awareness. Regardless of responsibilities, all information security professionals must also exercise both due care and due diligence.

Due care — The principle of due care states that information security professionals must fulfill the legal responsibilities of the organization as well as professional standards of information security.

Due diligence — The principle of due diligence states that security professionals should take reasonable measures to investigate the risks associated with a situation.

As you can see, security professionals have a wide variety of responsibilities within the organization ranging from the design, implementation, and management of various controls to protect confidentiality, integrity, and availability. It’s important to ensure adequate controls are in place to protect and cover the organization against a wide range of risks that exist today. This can be seen as a formidable task. Fortunately, there are several existing security control frameworks out there today that can assist with establishing an effective security program. These include COBIT, ISO 27001/27002, NIST 800–53 just to name a few.

COBIT

The Control Objectives for Information Technology, also known as COBIT, is a security control framework developed by the Information Systems Audit and Control Association. COBIT is a detailed framework covering six different principles: meeting stakeholder needs, enabling a holistic approach, creating a dynamic governance system, separating governance from management, tailoring the governance system to enterprise needs and covering the enterprise end to end. COBIT also offer implementation guidance to assist organization in implementing the framework within the organization.

ISO 27001/27002

The International Organization for Standardization, also known as ISO, also publishes a series of control frameworks centering around security and privacy. ISO 27001 covers information security management systems and provides generalized control objectives. ISO 27002 provides additional details on specific controls and implementation to meet organizational goals.

NIST 800–53

While specially designed for federal information systems, the NIST 800–53 published by the National Institute of Standard and Technology is widely adapted by non-federal agencies and organizations as well. It covers the fundamentals of information security with multi-tiered risk management, security control structures, baselines and designations, the use of external service providers, and how to assess assurance and trustworthiness for information systems. The 492-page publication goes into detail on the process of implementing security and privacy controls, selecting appropriate security control baseline, tailoring the baseline to the specific needs of the organization, and creating a process to document the control selection process for both new and legacy systems.",6242
"Before you read this, there is some key information about Zensei in my previous post.

To make Zensei’s platform a reality, I need to build a website from the ground up. It’s going to need some complex features and, I must admit, I have very little experience with this. Although I realise I’m venturing into the unknown, I am very excited to finally create something real.

Here are my options:

Hire

I could use a service such as Fiverr or Upwork to hire a web-developer online. This would garner a semi-professional service at a variable cost but would allow me to access skills globally.

I could contact people already in my networks who have the relevant expertise. This would allow me to be able to communicate with them more effectively and potentially give me access to a discounted rate.

I could contact relevant departments at the university and potentially involve student volunteers. This would be the cheapest option but I may pay the price in lack of quality. Still, considering the experience it will give the students, this seems like a win-win situation at the least cost.

Design it Myself

To be honest, I will most probably start developing the website myself but I believe that there will definitely come a time where collaboration is required further down the line.

Buying a domain name will be cheap and easy, starting at around 10 pounds a year. (Google Domains). Trouble is, the more desirable names for Zensei (such as zensei.com) are taken. A simple ‘.com name’ like that would be perfect. But… It looks like I will have to go back to the drawing board in terms of a brand name. However, this early on, that shouldn’t be too massive a problem.

I am aware of many different web-hosting platforms that will facilitate the building of a website such as WordPress, Squarespace or Wix.com. However, from what I understand, the capabilities of platforms like this are limited to simple web pages such as contact, about, blog, portfolio and products pages.

Further detail in coding will be required to bring the functionality of Zensei’s platform to fruition. I will seek further advice from someone more knowledgeable on the topic.

While I’m here though… A personal website could be very useful to put myself out there. So I bought the domain ‘dilanalexander.com’ for 10 pounds a year, and with that, I shall get some experience setting up my own personal website.

From here, I’m going to get in touch with a few computer science contacts and figure things out with Zensei’s platform. That being said, I understand that this may take a while until I am happy with the finished product, but to be honest, I am just as excited about the journey.",2672
"Wondering if you should use PCF, AWS or Azure for Static Website? Read to find out more about the pros and cons of each.

Although web pages in today’s world are becoming powerful and extremely interactive, there is no replacement for a brilliant static website. With technological improvements such as introduction of MFEs and SPAs using ReactJS/Next.js, static websites these days do a lot more than just displaying relevant content to the user.

In this article, I would like to take you through my journey of deploying a static website for an enterprise, built using React on PCF, AWS and Azure and comparing some of their different aspects, and outlining the Blue-Green strategy for each method, which enables zero-downtime deployment. For the sake of brevity, I won’t be delving into overly technical details. Also, for the sake of simplicity, let us consider a bundle named app.zip which is the final build to be deployed and has the directory structure below.

Pivotal Cloud Foundry (PCF):

Steps:

Create the project repository as required and then add ‘manifest.yaml’ in the root folder. Sample content for manifest.yaml is present below.

2. Configure your pipeline to create build using ‘npm run build’ command and add a file named ‘Staticfile’ in the build directory of the app with the content.

3. Once this is done, the pipeline should navigate to the appropriate app folder and perform ‘cf push’ command.

4. You should now be able to access the website with the route generated

Blue-Green Deployment:

The Blue-Green Deployment when using PCF to host static website can be done with the following steps:

1. Create a new app similar to existing app

2. Deploy the files

3. Test the application using new route URLs

4. Point your DNS to new route URLs

Pros:

1. Ability to auto-scale

2. Monitoring and deployment is easy

3. Applications run stably and smoothly

4. Excellent use case for deploying applications dealing with data such as PI or other critical information and cannot be deployed onto public cloud

5. Dynamic Routing and Infrastructure security

6. Integration with external logging components such as ElasticSearch and Logstash

Cons:

1. Integration with external tools is a little tedious

2. User Interface navigation needs improvement

3. You need to take care of the security and access control

4. The maintenance of SLAs is to be done by you

5. PCF will create a route at the time of deployment, and it can be customised but there is no readily available DNS Service or CDN Service along with PCF

6. Automatic encryption and rotation of keys is now dependent on the underlying hardware

7. Automated Cost alerts for your application is not available out of the box

8. Setting up of infrastructure to meet compliance requirements is now to be done by you

9. As applications grow, the cost of infrastructure becomes an overhead

Amazon Web Services (AWS):

Steps:

1. Create an S3 Bucket with desired configurations — this activity can be done manually or by automation with a CloudFormation Script via AWS Console/AWS CLI/AWS SDK

2. Create an SSL Certificate using AWS Certificate Manager for the Route53 domain name and add relevant CNAMEs as required

3. Create pipeline to deploy to S3 bucket — do note that this requires updating the bucket policy to allow access from your deployment tool

4. Create a CloudFront distribution — Shown below are the sample settings that can be done on the CloudFront distribution. You can also restrict access to your bucket only from a certain origin or country (Geo-Restriction) by updating CloudFront OAI settings

5. Deploy the application and enjoy the access

Blue-Green Deployment:

The Blue-Green Deployment when using AWS S3 to host static website can be done with the following steps.

1. Create a new bucket with the same configuration as existing bucket

2. Update the pipeline to deploy files to the new bucket

3. Create a new CloudFront distribution for the new static website set up

4. Test the application using CloudFront URLs

5. Point your DNS to new CloudFront URLs

Pros:

1. AWS WAF can be attached to the CloudFront distribution to enhance security of the website

2. Granular control of objects in bucket can be achieved by updating bucket and object access policies

3. 99.999999999% durability across regions and 99.99% availability over a given year without the headache of maintaining infrastructure

4. Storage auto-scales and is available in abundance

5. Compared to traditional hosting, bandwidth is cheap

6. Leverages the Edge Location Caches by using CloudFront distributions, reducing the transfer costs associated with the access of files

7. Take advantage of object lifecycle policies to save cost

8. Meets regulatory and other compliance requirements — such as deploying files only in a certain region or performing cross region replication

9. Multiple encryption options are available to secure the bucket content such as SSE-S3, SSE-KMS, SSE-C

10. Logs can be enabled on the bucket for monitoring and audit purposes

11. CloudFront and S3 allow you to configure the CORS settings, caching behavior, Caching rules, Redirection rules for each type of file

Cons:

1. Blue-Green deployment requires creation of new S3 bucket and CloudFront distribution

2. If Route 53 or DNS is not used, then website name would be non-user friendly

3. For bigger file sizes, the cost of maintaining them on the bucket increases

4. There is no hosting package or service for static website. To know the approximate cost for S3 based static website, one has to make use of the AWS Total Cost Calculator Application

5. You might be required to add another CDN on top of CloudFront to reduce the costs further

6. Initial configuration of services is tricky. For example, to use existing domain name instead of static website domain name, you will have to set up redirection rules on the server or CDN to point to the end URLs

7. The bucket name has to be globally unique even though it is created under your account in a region

High level Architecture of Enterprise Static Website hosting on AWS

Microsoft Azure:

Steps:

1. Create a Blob Storage container — first create a storage account and then create a container. This activity can be done manually or using Azure Resource Manager via Azure Portal/Powershell/Azure CLI/Azure SDK

2. Create an SSL Certificate using Azure App Service for the Route53 domain name and add relevant CNAMEs as required

3. Create pipeline to deploy to blob container — do note that this requires updating stored access policy to allow access to container from your deployment tool

4. Create an Azure CDN — You can also restrict to allow access to your container only from a certain origin or country (Geo-Filtering) by updating Azure CDN settings

Blue-Green Deployment:

The Blue-Green Deployment when using Azure Blob to host static website can be done as below.

1. Create a new container with same configuration as existing container

2. Update the pipeline to deploy files to the new container

3. Create a new Azure CDN for the new static website set up

4. Test the application using CDN URLs

5. Point your DNS to new CDN URLs

Pros:

1. Caching behavior and caching rules can be controlled in Azure CDN

2. Azure WAF can be attached to Azure CDN to enhance security of the website

3. 99.99999999999999% durability and 99.99% availability with Geo Redundant Storage (GRS) enabled over a year without the headache of maintaining infrastructure

4. Storage auto-scales and is available in abundance

5. Compared to traditional hosting, bandwidth is cheap

6. Leverages the Azure CDN cache, reducing the transfer costs associated with the access of files

7. Take advantage of Object lifecycle management to save cost

8. Meets Regulatory & other Compliance requirements — such as deploying files only in a certain region or performing replication to other regions

9. Blob content can be encrypted by using Customer-Managed Keys or Microsoft-managed Keys

10. Logs can be enabled on the container for monitoring and audit purposes

11. Your storage account serves as unique namespace for the container

Cons:

1. Blue-Green deployment requires creation of new container and CDN distribution

2. If DNS is not used, then website name would be non-user friendly

3. For bigger file sizes, the cost of maintaining them on the container increases

4. Azure Static Web Apps Service allows you to build and deploy full stack web apps but there is no hosting package or service for just static files. To know the approximate cost for blob storage based static website, one has to make use of the Azure Cost Calculator Application

5. You might be required to add another CDN on top of CloudFront to reduce the costs further

6. Initial configuration of services is tricky. For example, to use existing domain name instead of static website domain name, you will have to set up redirection rules on the server or CDN to point to the end URLs

High level Architecture of Enterprise Static Website hosting on Microsoft Azure

Conclusion:

Below is a comparison table I have created of various aspects involved in deployment of static website using PCF, AWS and Azure. In terms of ease of deployment and ease of set-up, PCF fares better than AWS and Azure. However, AWS provides better access control and security, global reach, monitoring and compliance. Azure lies between PCF and AWS with respect to all the factors. In my personal experience, if you are willing to spend a little more, AWS is the best pick to host static websites. Which is your go-to place to host static websites?

N.B: The information present in the above article is based on personal experiences & knowledge and in no way should be considered as an expert advice/opinion

Appendix:

Below are the abbreviations used in the article.",9854
"United Nations? Not really.

One world government?

Could that really be a thing?

If so how could it be established?

A world run by the United Nations?

A world presided over by Marxists?

World domination has been the prime motivation and ultimate goal of every communist from Marx and Engels to Obama and Pelosi.

From my perspective here in the USA the push toward an authoritarian global government seems to be a symptom of a certain faction of radical leftists within western democracies. A futile myopic euro-centric fantasy that most of the rest of the world would most likely reject with extreme prejudice.

Are the nations of Islam going to relinquish their sovereignty in favor of being ruled over by a godless bureaucracy? No. Would China submit to being subsumed under a UN rubric? No. Xi has his own vision for world domination. And China is in the process of installing a bevy of new nuclear missile silos as a deterrent to anyone who might want to challenge its sovereignty or its imperialistic ambitions. And then there’s Putin with his nuclear arsenal. He Is not about to step aside and give up his ultimate goal of reinstating the USSR.

Also, as we see, individual countries are incapable of uniting their own populations. But they would be somehow capable of uniting the whole world? Of course not.

But, really, there is no interest in uniting anything by anyone.

In the USA the radical left is not interested in uniting the country. It is weaponizing political power to engineer a complete takeover by force.

The nations of the world are not united either and they are not about to be. So, in order to create a one world government run by the so-called United Nations it would have to be by force.

The push toward a global government is a push toward the next world war tempting a nuclear holocaust.

But such a holocaust could also come about when Iran succeeds in producing its own nuclear weapons, which they are on their way to doing. Iranian Mullahs believe that their Messiah, Mahdi, has been hiding out at the bottom of a well for some 1300 years and will come out of hiding to save the world for the Shiites only after the world is destroyed. So, Iran will need no provocation at all to instigate a global conflagration.

We see a world now where all parties are only interested in their own blinkered dogmatic mindset. From nation states to political parties to individuals myopia is endemic. It’s a world where no one can see beyond their ideological noses. Quasi global visions have to do with insular state actors only interested in pursuing their peculiar designs for global, or at the very least, regional dominance.

How cooperation between nations can serve the interests of every nation is not considered.

Imagine what could have been accomplished if at the start of space explorations the nations of the world had decided to cooperate in that endeavor? If there had been an International Space Agency formed in the seventies with nations pooling resources there could have been manned missions to mars by now.

There is much to be gained through international cooperation and only catastrophic loss through international conflict.

And that also goes for nation states as well. But the factional conflicts running rampant within them prevent any notion of cooperation from serious consideration.

It is as if our minds have been shut down. Most likely because of information overload via computer technologies.

Overwhelmed by information on a minute by minute basis we become prisoners to the present and lose a big-picture-context that would enable a broader perspective. No time for reflection we rely on prefabricated mindsets by which we can immediately react to all the info bits we are constantly bombarded with.

Computer technologies favor, if not require, even demand decentralization. And the world needs to become a decentralized synthesis. The world cannot be managed on a global basis. Communities can be managed within a global context, thinking globally acting locally.",4031
"How do BM & AziT tokens work together?

AziT is a blockchain-based, point integration platform for real estate and residential services. Azit provides a service that allows users to conveniently use points as they see fit for residential services related to real estate or community facilities by integrating points that can be used anywhere.

Welcome to AziT!

In the previous article, we explained what Blockchain Mileage(BM) and the AziT’s tokens and their structure. Today, we want to share how to trade BM and AziT tokens. It is important to understand how BM and AziT tokens work on the AziT platform, and what the benefit is when you exchange AziT tokens for BM. We hope that this article is helpful to our readers.

BM’s Pricing

The AziT Alliance aims to create a structure in which BM (Blockchain Mileage) and AziT tokens can easily be traded according to market principles. The value of BM will be set according to market supply and demand. Each of the partner companies will form a small market where transactions between BM and AziT tokens take place. The value of BM is determined by the supply and demand in this small market and the algorithm set on the platform.

The more active the trade in this small market, the more stable BM will be. BM, like cash, can be used as a payment source for each service. BM is limited in use, so its transaction price is lower than that of the more general-purpose AziT token.

For example, if you buy 10 AziT tokens from a cryptocurrency exchange, you will receive 10 AziT tokens because they are used much more often than BM. However, if you buy the same amount of BM, you will receive 11 BMs instead of 10 BMs because its transaction price is lower as mentioned above.

Trading through the platform

BM and AziT tokens from each partner company’s small market are not directly linked to the seller and buyer, but the AziT platform becomes the seller or buyer instead, automatically closing the deal at the price set by supply and demand and the algorithm.

For every BM and AziT token transaction that occurs on the AziT platform, a certain level of commission is generated for the purpose of the stable operation of the platform. Some of the generated commission is reinvested into the platform for marketing to help the partner company grow.",2297
"CapCar is an online platform on which consumers can buy and sell their used cars. It was launched in 2015 by Louis-Gabriel de Causans, Guillaume-Henri Blanchet and Alexandre Hudavert, three entrepreneurs driven by a passion for cars and the desire to revamp an industry in need of a bit of disruption.

A dream well within their reach as the used vehicle market is growing. Every year in France around 5.7 million used cars are sold for just 2.11 million new ones.

In spite of sector growth and the arrival of new actors, the reputation of the used vehicle industry, sadly, still leaves a lot to be desired. Synonymous with shady second-hand car dealers, high prices and questionable practises, it continues to attract criticism with public authorities even deeming it, in recent times “ unsatisfactory and risky for consumers “.

CapCar is aiming to change this. Positioning itself as a trusty intermediary “ who takes care of everything”, the startup offers customers a fast, simplified and efficient buying and selling experience. Their “ safety-first approach “ means that before being featured on the platform, all cars are inspected and certified by their in-house mechanics who confirm that the cars are in mint condition, safe to drive and sold at a fair price.

Thanks to it’s accessible model, CapCar is proving to be a great success. The vehicles on offer, although second-hand, tend to be recent and so pollute less than older ones. The growth of the used-car market also means that less energy needs to be spent and materials used building new ones, thus appealing to today’s more environmentally aware consumer.

Why CapCar ?

The size of the market and, if you’ll forgive us the pun, the vision and “ drive “ of it’s founders meant that CapCar was an easy pick for Breega. And their track record so far has made for steady progress. After raising just over a million in 2017, and then 5 million at the end of 2019, the company has grown from 10 to 50 employees and multiplied its business figures sevenfold to become France’s number one used-car marketplace. In spite of a bumpy ride last year due to the ongoing global pandemic, and large losses for the automobile industry, the company still managed to achieve +40% growth. So with this in mind, we asked CapCar why they continue to choose Breega as their main investor?

Why Breega?

While doing the funding rounds, Louis-Gabriel, the determined CEO of CapCar admits to being a little sceptical where potential investors were concerned:

“There are a lot of preconceived notions in the tech world where the automobile industry is concerned. Many of the young VC analysts we met, most of whom didn’t have a driving licence or had never owned a car, were quick to dismiss it as a dying industry. However, people living outside of the major cities will tell you that driving is a necessary part of their everyday lives. 90% of French people are reliant on their cars”.

According to Louis-Gabriel, the first meetings with potential investors Breega was, and continues to be, a meeting of minds :

“Breega had a more thoughtful approach. Maximilien asked all the right questions and understood quickly what we were trying to do and why we were trying to do it. He shared our passion for cars and had a clear understanding of their importance for the population”.

Breega’s operations team was also a winning argument for the three entrepreneurs:

“There aren’t many funds that offered the same level of support as Breega. Thanks to their HR, Biz Dev and Coms teams Breega has helped us grow and structure our teams, create partnerships with major insurance companies and helped us with our PR and rebranding”.

And how has Team Breega helped CapCar on its journey?

“With Max we have a great and healthy relationship, very frank, no filter. We say when we agree and when we don’t. We listen to each other. We have a call every 3 weeks or so. He is really present and active”.

On the road

So where is CapCar headed next? Louis-Gabriel tells us that 2021 will be all about consolidation, readjusting their offer, investing in and improving their platform and customer experience and strengthening their team before looking, at year’s end, into expanding into Europe:

“The CapCar adventure continues and it’s an exciting one. We want to succeed in building a strong and qualitative brand and a great company that’s good to work with and in”.

Well, you’ve sold it to us, team CapCar. We’re delighted to be along for the ride. Here’s to smooth sailing in 2021!

You can check out their website here

Check in next week for our next installement of our founder x Breega story",4638
"Enhancing the Profits of Partner Companies

The companies that have signed an agreement with the AziT platform are part of the AziT Alliance. Partner companies participating in the AziT Alliance increase consumer traffic for all partners through the integrated point service, which in turn leads to new customer acquisition and increased service utilization.

The AziT Alliance platform introduces blockchain distributed ledger technology to solve existing point integration limitations such as a lack of interoperability, balance, and trust between partner companies. They will also issue a new point system that interlocks with the other partner companies’ point systems. Partner companies participating in the alliance are provided with a solution to make it easy to join the AziT Alliance.

In addition, part of the transaction profits generated when exchanging existing points for AziT tokens are given to participating companies to be used as marketing resources. AziT tokens can be secured through external cryptocurrency exchanges as well.",1047
"Summary: At the onslaught of the COVID-19 pandemic, Singapore-based start-up Girlsourced Tech launched its mobile app that offers on-demand professional styling services for ladies and a fashion community platform for crowdsourcing style votes. It is first in the region to do so, after spotting a gap in the market. Personalisation of style advice can help fashion consumers reform their shopping patterns to buy only what suits them best and what they really need. By leveraging on technology and digitising bespoke personal styling on a remote platform, the future of personal styling will not only be free to operate anytime and anywhere in the world, but also doing its part in the value chain of fashion sustainability.

Sustainability of Sustainable Fashion

In the last decade of fashion business, the demise of brick and mortar retail stores phased out brands which did not evolve with changing times. Retail demise has taken a new meaning during the COVID-19 pandemic. Zara-owner Inditex SA just announced the permanent closure of as many as 1,200 stores — and will pivot more aggressively toward selling online (WSJ, Jun 2020). This took place shortly after popular lingerie brand Victoria Secrets announced its permanent closure of 250 stores across the US and Canada due to dire sales losses of 46% (Forbes, May 2020). Having online presence was insufficient in keeping these brands from taking the hit. Whilst loyal customers grief with struggling brands during these hard times, fashion sustainability advocates might celebrate this as a rite of passage to a true circular economy.

Yet the sustainability of sustainable fashion is a conundrum to solve. Brands that responded to the increasing market demand for a more circular fashion economy are now curtailing sustainability projects to keep above the red amidst the health pandemic (Business of Fashion, May 2020). What can be done, then, to ensure the sustainability of sustainable fashion?

The answer lies not just in textile and apparels production but also in what is done with the existing clothes produced and sold in excess that eventually end up in the landfills. Industry value chain sustainability per se does not make fashion consumers sustainable. What brands might be blind-sighted to see is that sustainability is a lifestyle, not a product. Lifestyle habits and behavioural patterns need more inculcation for lasting change than a quick fix through purchasing sustainably manufactured products.

Reports show that 72% of all returns in fashion were due to poor buying decisions; 73% of consumers threw unfitting clothes away while 25% threw away more than 10 items. These demonstrate unsustainable shopping patterns. Moreover, rising return rates hurt retailers’ profitability. Shopify reported that in 2018, the amount of unwanted purchases returned was worth US$369 billion. Information is required to educate consumers on how to change their online fashion consumption patterns to effect lasting impact on the future of fashion sustainability. A large part of sustainability in fashion can be initiated by consumers themselves not just through purist ideals of buying only from sustainable clothing manufacturers but also by not buying items that do not suit them in the first place.

The Lack of Online Personalised Style Advice is a Problem

In our modern age of information overload, existing solutions for fashion styling are fragmented, sales-focused and not personalised to meet the individual consumer’s needs. Passive methods of getting online fashion style inspirations through social media, magazines, lookbooks etc. are mostly free but are not customised and are time consuming to browse. “Free” styling services by retailers usually require purchase or subscription. The trend of following fashion influencers is growing exponentially but is not anywhere near promoting the concept of personalisation; quite the contrary.

Conventional personal styling services come with high service fees (ranging on average from S$200- S$2000), marginalising consumers who cannot afford them. Furthermore, most of these services require physical presence, hardly possible during the current and post- health pandemic. Yet the access to quality information that can determine well-informed buying decisions is the key to personalising items to the individual customer. Personalisation of style advice can help fashion consumers reform their shopping patterns to buy only what suits them best and what they really need.

On-demand Personal Styling for Personalisation

At the onslaught of the COVID-19 pandemic, we launched the Girlsourced mobile app to offer on-demand styling services for ladies and a fashion community platform for crowdsourcing style votes. The app is the first of its kind in the region, aimed at filling a gap in the market. The Girlsourced app allows the consumer to style from both her wardrobe and retailers, encouraging the mindset of sustainability — buying only what you need.

Existing fashion styling apps are predominantly based in the United States, such as Stitch-Fix, a US-based fashion technology company that IPOed in 2017. The success of Stitch Fix has demonstrated the viability and demand for online personal styling services, a call to the digital transformation of fashion.

Enter the new breed of personal stylists ready to go beyond conventional ways of servicing customers, and who embrace connecting with consumers across geographical boundaries digitally. Girlsourced stylists are from Asia Pacific and Europe and have each gone through a strict selection process before being onboarded as the pioneer batch of on-demand personal stylists. These fashion experts are assisted further by Girlsourced technology which matches their specialisations to each unique customer profile. This model aligns with Capgemini’s view on digital clienteling, “Being able to identify what customers are looking for and the digitisation of customer experience driven by advanced analytics and AI will be the key area of opportunity for retailers.”

The solution is in the service. Clothing wastage can be significantly reduced by styling items from existing wardrobes and making conscious choices for new purchases after decluttering.

COVID-19 Resilient Businesses

Businesses that are able to digitise and automate their operations see a significant resilience to the pandemic. WWD observed the following:

The digital channel was largely left unscathed and operational during the pandemic — a lifeline for fashion companies e.g. virtual showrooms and widened online sales bandwidth.

Hybrid fashion and tech startups with low expenditure on physical assets have the advantage of leveraging on AI and AR to scale digital operations.

Brands that invested in areas to increase client knowledge and consumer understanding reaped the benefits during retail lockdown.

Today’s technology offers us infinite opportunities to find solutions and shape the future especially during such economic situations. By leveraging on technology and digitising bespoke personal styling on a remote platform, the future of personal styling will not only be free to operate anytime and anywhere in the world, but also doing its part in the value chain of fashion sustainability.

Author: Jessie Imakoji

References

1. Chaudhuri, S. 2020, ‘Zara Owner to Close 1,200 Stores as It Outlines Post-Coronavirus Future’, The Wall Street Journal, 10 June, accessed 12 June 2020,<https://www.wsj.com/articles/zara-to-close-1-200-stores-as-it-outlines-post-coronavirus-future-11591794618>

2. Sandler, R. 2020, ‘Victoria’s Secret To Close 250 U.S. And Canadian Stores As Sales Plummet Due To The Coronavirus’, Forbes, 20 May, accessed 12 June 2020,<https://www.forbes.com/sites/rachelsandler/2020/05/20/victorias-secret-to-close-250-us-and-canadian-stores-as-sales-plummet-due-to-the-coronavirus/#4a4127532d08>

3. Kent, S. 2020, ‘Fashion’s Sustainability Goals Threatened By the Crisis’, Business of Fashion, 7 May, accessed 10 June 2020,< https://www.businessoffashion.com/articles/professional/can-fashions-sustainability-drive-survive-the-covid-19-crisis>

4. Lieber, C. 2019, ‘Customers Love Free Returns. Here’s How Brands Are Navigating the Costs’, Business of Fashion, 31 May, accessed 10 June 2020,<https://www.businessoffashion.com/articles/professional/customers-love-free-returns-heres-how-brands-are-navigating-the-costs>

5. Bee Khim, K. 2019, ‘Uniqlo is cutting water usage by 99% so your jeans don’t kill the planet’, CNA Lifestyle, 06 Sep, accessed 10 June 2020 <https://cnalifestyle.channelnewsasia.com/style/how-are-vintage-jeans-made-uniqlo-11836724>

6. Orendorff, A. 2019, ‘The Plague of Ecommerce Return Rates and How to Maintain Profitability’, Shopify, 27 Feb, accessed 10 June 2020 <https://www.shopify.com/enterprise/ecommerce-returns>

7. Socha,M. 2020, ‘Tipping Point: Will Fashion Finally Complete Its Digital Transformation?’, WWD, 28 May, accessed 10 June 2020, <https://wwd.com/business-news/technology/fashion-must-complete-its-digital-transformation-1203633908/>",9081
"How Does Your Digital Product Perform When Users Get Angry? truematter Aug 5·3 min read

When people get frustrated by digital products, they become angry which impedes task completion. Understanding this common exasperation will help you make better sites, apps, and software.

Poorly designed, frustrating digital experiences make otherwise harmoniously calm people angry — really, really angry. This probably doesn’t come as much of a shock. Interface rage is something we’ve all felt.

Some hurl offending computers from windows. Others write exceedingly long research articles on the subject. We all cope in our own way.

I’m infamous around the office for, shall we say, colorful pronouncements when using maddening apps, sites, and software. My infantile rantings sail right past the PG-13 standard into Rated R land. I’m not proud of it. But at least I’m not alone. Apparently a good many of us confess to verbal or physical abuse of our computers.

The Downward Spiral of Digital Fury

The worst thing about getting worked up over bad digital products is once we surrender to anger, we create a self-reinforcing cycle that makes the problem worse.

Maybe we’re filling out an online form and miss a required field, instruction, or error. Perhaps the form is just plain broken. What should have been insanely easy becomes a time-sucking ordeal. Anger is a natural response, but it makes us harried and mistake-prone. When flustered, we miss obvious things we’d otherwise see. Problems multiply, making us all the more furious.

Even worse, sometimes we come to a digital product already angry, already irrational. Those of us who tackle the Free Application for Federal Student Aid (the dreaded FAFSA) know what it means to start at our wit’s end. In this case, the downward spiral begins immediately.

Death. Taxes. Anger.

User frustration is depressingly common. Online it is the rule, not the exception. The interface rage it generates is a fact of modern life. I get mad. You get mad. I bet even the Dalai Lama has lost his digital cool when trying to order new Warby Parkers. The question is, what exactly can you, a digital product maker, do about it?

Begin by focusing less on the anger itself and more on the commonality of it all. Instead of wondering what to do if someone gets upset when using your product, ask instead what you should do when they inevitably become blind with anger.

A Different Model for UX Success

People are naturally impatient online. Anger flows easily from this. Angry folks exhibit poor judgement, make rash decisions, and are generally irrational. Your software must be truly amazing if it performs well in the face of this emotional tsunami. Your fancy app might be wonderfully usable under calmer, even-keeled circumstances. Perhaps sturdiness under duress is a more legitimate measure of success.

Testing this hypothesis with users would be difficult to say the least. Fury is so dang subjective. And making people break blood vessels on purpose feels a tad unethical. So we’ll have to settle for the next best thing.

Agree with Reality

Assume everyone using your digital product is having an exceedingly bad day. They are upset, irate, and stressed. It’s doesn’t matter why. Maybe your app is the cause. Maybe not. Perhaps they have a hundred things to do and your thing is just one of them. Perhaps they just finished the FAFSA before turning to your app. Life’s not fair.

You can rely on people to be frustrated by technology. This is never going to change. Adopt a mindset that assumes perpetual user exasperation. It will revolutionize how you think about, build, and deliver your product. Everyone from the newest employee to the CEO will make better choices as a result.

About truematter

Our team has been doing the real work of user experience since the earliest days of the commercial web. We’re out to make your digital products a whole lot better.

That means ensuring they can withstand the endless onslaught of irrational human behavior.

Author: @ExperienceDean

Graphic: @djosephmachado

Image Source: Engraving by Martin Engelbrecht

Acknowledgement: Cian O’Connor for conceptual inspiration",4165
"How do you tell the story of something so intrinsic to all of us — our human need to communicate and connect?

The story of communications is intimately entwined with the story of people, of place, of communities and of belonging. When we think about the technologies that have enabled our ability to connect with one another, we can consistently see the role these technologies play in enabling, building and transforming societies.

As a technology centred museum, our key risk is that we focus on the tech rather than what the tech enables, which would fail to recognise that communication technologies were always developed in service of this fundamental human need.

We’ve inherited a lot of stuff from which to build this museum. Some of the technology objects are impressive and shine a light on a unique history and development of communication technologies in Australia. More interesting though is the macro view of the collection and the realisation that these objects, considered together, are rather mundane and ubiquitous tools of communication. It is, however, our universal embrace of these technologies that ensured their ubiquity and therefore provide us with the unique opportunity to speak to our collective experiences and shared narratives.

If ever there was an opportunity to create a more democratic, interactive and engaging museum, I feel this might be it. There is a shift in the mandate of museums occurring worldwide, the need for this shift was highlighted by Alpha Oumar Konaré, the prior president of ICOM:

The traditional museum is no longer in tune with our concerns; it has ossified our culture, deadened many of our cultural objects, and allowed the essence, imbued with the spirit of the people, to be lost.

This view, reminiscent of philosopher Adorno’s scathing regard for the ‘museum as mausoleum’, could not be further from the clanking sounds of our still-working, step-by-step exchange that forms the heartbeat of this retired telephone exchange, the site that will eventually become our new museum.

Positioning a new museum within a working building ties the museum to a tangible past. On our first visit it felt as if this site had been abandoned, the remnants of the human traces that once made this exchange a hive of activity were still very much present. The main distribution frame still in situ commanded a presence, workers tools remained scattered around, and etchings into the walls provided clues of the characters that may have once inhabited this place. This presented an intriguing dialogue with the still-working network site directly behind.

In considering our positioning, we have unpacked loaded words that speak to outdated and Eurocentric models from which to reframe and build this museum. Terms such as ‘innovation’, ‘technology’ and ‘progress’ that speak to old fashioned notions of nation building, and fail to recognise that communication technologies have been in existence in Australia for thousands of years. In shaping our own definitions, we create space for multiple diverse voices and pave the way for shared narratives to emerge; as the story of communication is a story that is universal and belongs to everyone.

When finding the storytellers of communication, there are the inventors, the operators, the users and the receivers. All of whom play a unique role in the transmission and reception of messages. We can consider all communication as code, and unless we are equipped with the tools to unlock that code, the message is rendered meaningless. Therefore, our interpretive principles frame this building as a machine for conversation, and our visitors we hope will participate in this active exchange.

As we embark on building The National Communications Museum in Hawthorn, we hope you can join us on this journey as we continue to post our works in progress over the course of the year.",3878
"In the world of medical sales, no two days ever look the same. One day you could be talking with a surgeon who just left the operating room and the next day you find yourself sitting down with HR staff. The variety of clients and relationships you develop as a medical sales representative are extremely diverse. Finding ways to create a message that resonates with each of these people and adapting your presentation for the point of contact you are talking with is key to being successful in the world of medical sales.

Throughout a single workday, a medical sales rep could be talking with up to ten different people who have purchasing power. Every individual customer will be responsive to different types of messaging and different aspects of the presentation will resonate more so than others. With so many different variables, how do you make the most of your time in front of each type of medical professional?

It’s the Scout Motto: Be Prepared

Before even stepping foot in front of the customer, you MUST be an expert on every aspect of the product AND an expert on the different job roles you are presenting to. Knowing what features of the product that each position finds important will help you better navigate the conversation. Once it is clear who you are presenting to and where they find value in the product, it becomes easier to shape the presentation strategy.

However, what happens when the surgeon you were supposed to meet with gets pulled into an emergency surgery? You’re already there and there’s no time to reschedule. Having the ability to turn on a dime and still be successful in front of whoever you are presenting to is what can set you apart from all other competition.

Understanding and preparing for the potential challenges a medical sales rep may face helps to avoid running into unfavorable outcomes. Here’s a list of potential challenges and ways to work through them before you even step foot in front of the customer.

Timeliness: Time is of the essence when it comes to interacting with a medical professional. There is a lot of information that needs to resonate in a very small window of time. You can’t get caught digging through files and folders of information to locate the answer to very specific questions. Doctors and nurses need the information on the spot. Having an organized and intuitive presentation strategy will best prepare reps to face the challenges of time constraints.

Time is of the essence when it comes to interacting with a medical professional. There is a lot of information that needs to resonate in a very small window of time. You can’t get caught digging through files and folders of information to locate the answer to very specific questions. Doctors and nurses need the information on the spot. Having an organized and intuitive presentation strategy will best prepare reps to face the challenges of time constraints. Evolution in Technology: The pharmaceutical industry is always evolving, therefore the reps need to be able to keep up with constantly changing product information. The ability to update and change presentation materials whenever necessary is an imperative aspect of staying relevant in this field. Change is inevitable, so developing a plan early on with how to deal with the ebb and flow of information keeps reps from struggling when it comes time to make adjustments.

The pharmaceutical industry is always evolving, therefore the reps need to be able to keep up with constantly changing product information. The ability to update and change presentation materials whenever necessary is an imperative aspect of staying relevant in this field. Change is inevitable, so developing a plan early on with how to deal with the ebb and flow of information keeps reps from struggling when it comes time to make adjustments. Competition: Knowing your value proposition and where you stand out amongst your competitors can help you capitalize on those aspects when in front of the customer. Being able to make comparisons between your product and the competitor’s and also give examples of why you are superior instills confidence in the product. Presenting data to back up these claims is a very impactful way to capture the customer’s attention.

Knowing your value proposition and where you stand out amongst your competitors can help you capitalize on those aspects when in front of the customer. Being able to make comparisons between your product and the competitor’s and also give examples of why you are superior instills confidence in the product. Presenting data to back up these claims is a very impactful way to capture the customer’s attention. Depth of Information: Having the ability to hit all touchpoints throughout your presentation will best prepare you to address any questions, concerns, or aspirations from the customer. Medical professionals want to know every detail about the product before they use it on patients, so being able to answer any question builds confidence in the product and reassures the customer of its value.

Having the ability to hit all touchpoints throughout your presentation will best prepare you to address any questions, concerns, or aspirations from the customer. Medical professionals want to know every detail about the product before they use it on patients, so being able to answer any question builds confidence in the product and reassures the customer of its value. Confidence: No medical professional will believe in your product if you don’t believe in your product first. Walking into the presentation with the utmost confidence in what your product can deliver is extremely reassuring to the customer. Especially in medical sales, when the product can be a matter of life and death, having persistence and confidence in what your product can deliver is an important aspect to remember.

Taking time NOW to work through ways to avoid these dilemmas will set you up for long-term success. So, you may be wondering…once you have a plan in place, what can you do to maximize your time in front of your customers? Use technology to help you achieve your goals, namely a mobile app platform that works on the device you currently use. With tested strategies and an easy-to-use, interactive presentation platform, everything you need to be successful in front of a customer can be at your fingertips.

Choosing to onboard a platform to organize your presentation materials can increase your effectiveness and help you to close more deals. Your number one priority should be demonstrating to the customer how your product can fulfill their needs. There is no reason to waste any more time trying to work through presentation glitches or find the answers to tough questions. Instead, work harder on generating more leads or meeting with more clients.

As a medical sales rep, you hold a lot of responsibility and there are a lot of people who trust your judgment. Properly communicating your level of expertise on your products can ease the minds of medical professionals. The way you present your materials is just as important as the material itself. The number one thing to remember about making the most of your time in front of medical professionals is to be able to tailor your presentation directly to their specific needs and do it as quickly and efficiently as possible. Utilizing a mobile app platform gives you the flexibility and the assets to do just that.

Command.App is a customer engagement platform designed for situational selling. No matter who you are meeting with, the presentation can be catered to what each individual customer is looking for. Have confidence in uncertain situations knowing everything you need to answer any questions that can be found on one localized platform.

Click HERE to learn more about situational navigation and how Command.App prepares you for any interaction.",7862
"This market research study involved the extensive use of secondary sources, directories, and databases to identify and collect information useful for this technical, market-oriented, and financial study of the global biologics safety testing market.

[185 Pages Report] The global biologics safety testing market is projected to reach USD 6.2 billion by 2026 from USD 3.4 billion in 2021, at a CAGR of 12.4% during the forecast period.

The growth of the global biologics safety testing market is driven by factors such as the growth in the biologics and biosimilars markets, growing concerns over cell culture contamination, and rising biopharmaceuticals R&D activities and investments. In addition, emerging markets and increasing biopharmaceutical outsourcing are expected to provide significant growth opportunities for players in the Biologics safety testing market.

Market Dynamics

Growth in the biosimilars and biologics markets

Growing concerns over cell culture contamination

Rising biopharmaceuticals R&D activities and investments

Emerging markets offer lucrative opportunities

Increasing biopharmaceutical outsourcing

Many companies are investing heavily in the development of biologics and biosimilars. Currently, more than half of the drug candidates in the discovery stage are biologics, such as proteins, peptides, and monoclonal antibodies. Biologics are expected to contribute around half of the revenue generated by the top 100 pharmaceutical products in the coming years.

Covid-19 Impact On The Global Biologics Saftey Testing Market

COVID-19 is an infectious disease caused by the most recently discovered novel coronavirus. Largely unknown before the outbreak began in Wuhan (China) in December 2019, COVID-19 has moved from a regional crisis to a global pandemic. The World Health Organization (WHO) officially declared the outbreak of COVID-19 a pandemic. A mix of established pharmaceutical and biopharmaceutical companies, along with the players of the Biologics safety testing market, have stepped forward to contribute to worldwide research efforts by providing biologics safety testing for developing safety test kits and testing for treatments and vaccines manufacturing that target the infection caused by the novel Coronavirus.

Based on application, the biologics safety testing market is segmented into monoclonal antibodies manufacturing, vaccines manufacturing, blood and blood products manufacturing, cellular and gene therapy products manufacturing, and other applications. In 2020, the monoclonal antibodies manufacturing segment accounted for the largest market share. The large share of this segment can be attributed to the rising prevalence of diseases and increasing government initiatives for the development of monoclonal antibody drugs.

The geographical regions mapped in the report are:

1. North America

2. Europe

3. Asia-Pacific

4. Rest of the World (RoW)

5. Latin America

6. Middle East & Africa

North America accounted for the largest share of the biologics safety testing market in 2020. Rapid growth in the biopharmaceutical industry is the major factor driving the growth of the North American biologics safety testing market. Growing academic and government investments, rising demand for high-quality research tools for data reproducibility, increasing awareness among consumers about product safety, and the presence of major market players in this region are also driving the market for biologics safety testing in North America.

Some key players mentioned in the research report are:

Prominent players in the biologics safety testing market are Charles River Laboratories, Inc. (US), Lonza (Switzerland), Thermo Fisher Scientific, Inc. (US), Merck KGaA (Germany), SGS SA (Switzerland), WuXi AppTec (China), and Eurofins Scientific (Luxembourg). The key players in this market are focusing on strategic expansions, partnerships, and product launches and approvals to expand their presence in the market.",3980
"New for Agents: ListPacks®, Premium Shareables, ListReports LIVE, and more!

From ListPacks® to ListReports LIVE and more, you won’t want to miss this update on our newest features for agents.

ListPacks®

Instantly share curated, customizable collections of listings on social media or directly with your buyers.

Capturing new leads has never been easier, or more delightful. With just a few clicks, agents can now share beautiful “packs” of listings with their buyer community.

Choose from pre-made categories such as “Homes with pools” and“Single-level homes”, OR set simple preferences to create your own custom ListPacks.

Everybody enjoys looking at homes — serious buyers and lookie-loos alike. ListPacks® provide consumers a truly differentiated experience with unique features including our interactive neighborhood infographics. Even better, each listing prominently displays your contact info, keeping viewers tightly connected to you when they’re ready to express interest in a particular property.

Ready to get started? Click here to start sharing ListPacks® now!",1079
"Space exploration remains a herculean effort. Due to the immense challenges imposed by time and distance, missions to near-Earth objects have been successfully accomplished using traditional means of propulsion. However, the outermost planets in our solar system are 2 to 3.7 billion miles from the Sun. Reaching them within a reasonable time frame requires propulsion systems that exceed the capabilities of conventional propulsion methods.

Using the Sun for Propulsion

Solar sails harness the radiation pressure exerted by light on a reflective material to provide thrust to spacecraft. With few moving parts and the propellant offboard, solar sails provide cost-effective operations and long operating lifetimes. The technology is proven, having been successfully demonstrated by Japan’s IKAROS mission’s fly-by of Venus in 2010.

The Aerospace Corporation is researching new approaches to solar sail technology that could exponentially increase its already considerable potential, paving the way for a new era of breakthrough science missions.

In collaboration with UCLA’s Mechanical and Aerospace Engineering Department and NASA’s Marshall Space Flight Center, Aerospace is now exploring the use of solar sails as a pathway to faster exploration of the outer solar system and interstellar space. The Extreme Solar Sailing concept offers to employ the Sun as a figurative launchpad, using a “slingshot” maneuver to propel lightweight cubesat-class spacecraft to extremely high velocities.

This technology has the potential to enable missions to distant worlds in far less time, opening up new possibilities for space exploration. “With chemical propulsion, you can attain velocities between two to five astronomical units (AU) per year, with one AU being the distance between the Earth and the Sun,” said Dr. Henry Helvajian, Senior Scientist at Aerospace’s Physical Sciences Laboratories. “The beauty of extreme solar sailing is that you can gain velocities that surpass pretty much any kind of propulsion that we know of today, by factors of two to 10 to 30 depending on how close you want to get to the sun.”

Using this maneuver, solar sails can reach 0.1% of the speed of light (i.e., approx. 300 km/s), reaching the furthest planets of our solar system could take only months, interstellar space could be reached in a few years, and 1000 AU could be attained in less than 20 years.

These capabilities come with some concessions, however. The immense velocity of solar sail-driven spacecraft requires greatly reduced mass, necessitating new spacecraft architectures and materials that can withstand the initial close proximity to the Sun. In addition, solar sails must be resilient and able to provide consistent thrust while also withstanding impacts from dust and debris in their path.

Advancing Solar Sails for Deeper Space Exploration

Two new solar sail mission concepts will assess spacecraft communications and power requirements and explore the design of higher fidelity sail control systems to ensure precise navigation around the Sun and interstellar locations.

The Extreme Solar Sailing proposal received NASA’s Innovative Advanced Concepts (NIAC) Phase 1 grant in 2020. While the Phase 1 study demonstrated the conceptual feasibility of extreme solar sailing and the materials it would require, the next phase will require the development of key elements and systems for mission success. The study has received a NIAC Phase 2 grant for the development, fabrication and testing of new, ultra-lightweight metamaterials for solar sails, as well as the design of spacecraft architecture that provides ultra-low mass with the greatest payload functionality, and the conduction of mission studies.

Proceeds from the Phase 2 grant will also explore the application of extreme solar sailing to two breakthrough mission concepts: the Fast Transit Interstellar Probe, which aims to send a probe to 500 AU in 10 years, and Corona-Net, a precursor mission, which aims to send a fleet of solar sails to examine the inner heliosphere at high inclination. Both mission concepts will also assess spacecraft communications and power requirements, and will explore the design of higher fidelity sail control systems to ensure precise navigation around the Sun and interstellar locations.

In addition to advancements in solar sail velocity, Aerospace is also working to expand other aspects of solar sail functionality such as the Atomic Planar Power for Lightweight Exploration (APPLE) concept, which also recently received a 2021 NIAC Phase 1 grant.

Two new solar sail mission concepts will assess spacecraft communications and power requirements and explore the design of higher fidelity sail control systems to ensure precise navigation around the Sun and interstellar locations. Credit: Aerospace

APPLE: Atomic Power and Energy Storage

The APPLE concept is a new type of spacecraft power system that will open previously inaccessible parts of the solar system to human exploration and make a range of rapid transit missions possible. (Aerospace illustration)

While solar sails have the unique advantage of accessing the virtually infinite propulsive power of the Sun’s light, research is currently underway to explore the feasibility of adding energy generation and storage to solar sail technology in a bid for added resilience and functionality.

While nuclear power has typically been used in traditional spacecraft, limitations on mass have precluded its use in solar sails. Aerospace’s APPLE concept allows for a radioisotope energy source to be combined with energy storage in the form of a flat “power tile” battery that can be attached to the solar sails themselves.

The APPLE power tile is made of a sandwich of batteries covering a hot isotope layer. The heat is converted to energy and stored in the batteries. Credit: Aerospace

APPLE’s unique design combines a radioisotope source and energy storage capability in a single, scalable, flat “power tile” that can be attached to solar sails. This power tile is made of a sandwich of batteries covering a hot isotope layer. The heat is converted to energy and stored in the batteries. (Aerospace illustration)

“With APPLE, we’re integrating a few elements that are usually separated in spacecraft architecture. The radioisotope decay that powers spacecrafts in deep space creates heat, and that heat is converted into electricity in another material, which is then used by the spacecraft,” said Dr. Helvajian. “For APPLE, we’re integrating a type of lithium battery into the radioisotope itself. The reason we haven’t integrated batteries before is that when you place a traditional battery near a radioisotope decay source, the radioisotope emission destroys the battery. So, we’re exploring the development of a unique, isotopically-selected lithium to make a battery that’s radiation-resistant.“

If successful, this new generation of rechargeable battery could allow for storage of electricity generated by radioisotope decay for later use in higher-powered applications. In addition, waste heat generated by the radioisotope decay, which is normally dissipated, can be directed to keep spacecraft electronics warm in the frigid recesses of deep space. The APPLE’s concept may pave the way for more resilient battery architectures that can survive the punishing conditions of space, while further extending mission lifespans.

The Aerospace team of Drs. Joseph Nemanick, Kristine Ferrone and Helvajian are currently providing system and mission modeling to determine the power requirements for missions such as a Kuiper belt object fly-by and a mission to the solar gravity focal point, with radiation testing to be completed by the Oak Ridge National Laboratory (ORNL).

Expanding Capabilities, Expanding Access

Until other forms of propulsion become practical and attainable, solar sail technology may provide us a means of bypassing the limitations of conventional spacecraft propulsion, and may ultimately broaden access to space, making space exploration far more accessible to private enterprise and countries with nascent space programs. Aerospace continues to contribute its technical expertise toward the evolution of solar sail technology, and the new era of space exploration it may yield.",8286
"Abhijeet Patra

Abhijeet has worked on technologies at the interface of life sciences and engineering for the last 8 years. Some of the technologies he built during his graduate studies at the National University of Singapore are now being commercialized by startups. He is drawn to anything which looks like science fiction but is just, in fact, science. He loves to study businesses built around deep tech offerings. He is very interested in figuring out routes to productization and using technology refinement as a growing moat for startups.

Cait Collins

Cait was most recently COO at Mango Health (sold to TrialCard in Sept. 2019). She is a startup executive with depth in operations and consumer experience and a background in product development, mobile marketing, and scaling & automating systems. Cait is driven to have an impact on people’s lives by building teams who are happy, healthy, and productive and by creating products and services that improve people’s health and well-being. Outside of work, her goals are to get a good night’s sleep and to hike every mile of Tilden Park with her wife, 2 sons, and 2 dogs (2 is her lucky number).

Derick En’Wezoh, MD, MBA

Derick is an entrepreneurial physician specializing in the area where clinical medicine, business, and politics overlap. Currently, Derick is the VP of Growth at Viz.ai where he is leveraging artificial intelligence to identify and triage acute medical conditions. Before Viz.ai, he was a surgical resident at Stanford Hospital. He completed his MD at Harvard and MBA at Stanford.

Elliott Mack

Elliott’s career experience spans across banking, marketing, and technology. As a banker in JPMorgan’s Global Funds Group, he managed a portfolio of alternative investment firms, and also led institutional partnerships at Swivl, a Silicon Valley based education technology startup. An early employee at OneTeam Partners, Elliott currently leads strategic partnerships and sits on the OneTeam Ventures diligence committee. Elliott is a graduate of William & Mary, where he was captain of the Tribe football team, and Georgetown, where he received the Tropaia Award for Outstanding Student Achievement. Outside of work, Elliott enjoys spending time with his close-knit family and friends.

Emma Qian

Emma is an undergraduate at Caltech studying computer science. She has experience in research engineering from her time at DeepMind and Facebook AI Research, and she has worked in quantitative research at a top proprietary trading firm. Emma is a member of the first undergraduate team to win the Data Open, the largest university-level data science competition in the world. At Caltech, Emma helped start the Data Science Organization and serves as the Vice President at Society of Women Engineers. In her spare time, Emma enjoys winter sports and poker, previously winning the annual poker tournament at Caltech.

Iyore N Olaye

Iyore is a product leader at the intersection of the digital and physical worlds. She thrives at the forefront of disruption, directing B2C & B2B product launches and strategies in a range of industries: consumer technology, IoT, CPG, micro-mobility, e-commerce, and marketplaces. Currently, at Bird, Iyore builds products to solve core growth, reliability, and operations problems crucial to the future of cities and transportation. Prior to Bird, Iyore headed up R&D at Walker & Company Brands (acquired) and was named to Forbes 30 Under 30 list in 2018. Iyore earned a BS from Cornell.

John Morrison

John currently works as a Strategic Partnerships Lead at Affirm, where he builds relationships with leading e-commerce and payments platforms. Prior to Affirm, he worked in partnerships at Lively, an HSA provider (YC, W17), and in strategy at LendingClub. Previously, John also worked in investigative journalism, nonprofit media, hospitality, and physical commodities trading. He’s a native of San Francisco and a graduate of Boston College and the London School of Economics. Outside of work, he’s active in rock climbing, martial arts, kettlebells, and cycling. John also advocates for underrepresented and marginalized communities through Out in Tech (LGBTQ+) and Techqueria (Latinx).

Kalvin Karsito Wang

Kalvin was born in Jakarta and lived in Indonesia, Singapore and Taiwan before moving to the U.S. After graduating from Stanford with a degree in Computer Science, he started his career on the engineering side, working as a Software Engineer at various start-ups of different stages before transitioning to Product Management — where he’s currently a Growth PM at Noom. Outside of work, Kalvin enjoys marrying his love for Soccer and Maths to delve into sports analytics. He also enjoys exploring new foods and restaurants in whatever city he finds himself in.

Kam Phillips-Sadler

Kam founded Dream Delivered, a subscription box service for career exploration and Dream Outside the Box, an app connecting college students with PK-5th graders for imaginative career experiences. Over the last decade, DOTB has scaled to 16 chapters nationwide & been recognized by President Obama. Kam had the honor of serving as the University of Missouri Commencement Speaker in 2019 and her TED talk, “How Mentorship Can Change the World” has garnered global recognition. She is a Venture Associate at The Fund & serves as co-Director of The Black New Venture Competition at Harvard Business School where she is an MBA candidate.

Lajhem Cambridge, MS, MBA

Lajhem is a discoverer at heart, with a love for the scientific method. After pivoting away from bench research, she pursued a career in operational and strategic clinical research management roles at Memorial Sloan Kettering and UCSF. Currently, she is living and breathing healthcare data at Verana Health. Lajhem holds a BA in Molecular Biology with a minor in Neuroscience from Princeton University, as well as an MS in Human Nutrition and Metabolic Biology from Columbia University and most recently received her MBA from Wharton. She is passionate about health equity and doing work that contributes to the social good.

Manji Pal

Manji was raised in India and moved to the US to study computer science and engineering at UW-Madison. During an internship with Intuit, Manji learned how much he loved spending time with customers and went on to join Microsoft as a Product Manager, where he worked on communications (Outlook) and collaboration apps (Wunderlist/To-Do & OneNote), used by millions worldwide. Currently, Manji is Product Manager at Houzz (Sequoia-backed). His mission in life is to create an environment that helps others be successful, which is what he wants to apply by investing in founders and helping their companies succeed.

Megan Ruan

Megan is an entrepreneur, investor, and advocate based in New York City. Born and raised in the Midwest, she is passionate about building communities in which women and minorities have equal access. Currently, Megan is an investment analyst at the Johnson Company, the family office for Robert Wood Johnson IV, US Ambassador to the UK. In addition, Megan is also a Director at Gold House, the nation’s largest nonprofit organization dedicated to advancing Asian American success and representation, where she leads founder and investor initiatives, including the Gold Rush accelerator. Megan earned dual degrees from Yale in Economics and Psychology.

Riley Finch

Riley is an Accounting Associate at Clearbanc, a financial technology firm based in Toronto Canada, where he focuses on helping founders properly account for new funding, and optimizing their business. Prior to Clearbanc, he spent 4 years campaigning for the Rio 2016 games on the Canadian National Sailing Team. Outside of work — he is an avid cyclist, sailor, and skier.",7770
"It goes without saying that I believe technological innovation has a great role to play in curbing humanity’s emissions and reversing some of the damage we’ve already done.

That’s why I do what I do, and why we started Carbon Re.

But in this rush towards technology-based solutions, we should not forget about the role of nature-based solutions, as well as the impact of technological solutions on the environment. I want to give two examples here to highlight these issues:

If whale numbers (1.3 million today) returned to pre-whaling numbers (4–5 million), this would enable the removal of 1.7 Gigaton of CO2 every year (global emissions are 51 GT/year). This is a result of the C02 absorbed by whales and, more impactfully, the growth in pythoplankton they enable. Thank you Professor John Shawe-Taylor for talking about this. You can read more about it here.

Climate Tech solutions are not risk-free for nature; they can have a substantial impact on the environment, and we need to take these into consideration. We all know that wind farms can be harmful to birds, for example, but these harms can be mitigated by good planning and designs (see this excellent article by the RSPB) and as the RSPB points out, climate change is “the single greatest long-term threat to birds and other wildlife”.

We can’t hope to save the planet from runaway climate change without new technologies, but increasingly, many have been pinning their hopes on direct air capture (DAC) solutions.

DAC plays a small but important role in the IPCC’s modelling of mitigation pathways. Whilst important, an over-reliance on DAC comes with its own challenges:

Relying on direct air capture to meet climate mitigation goals, could lead to 300 exajoules of energy being required by 2100, more than half of global demand today and a quarter of projected demand by the end of the century.

It would also require building 30,000 large-scale DAC installations. To put this in perspective, there are fewer than 10,000 coal-fired power plants stations in the world.

For more, read this excellent analysis by Carbon Brief and a comprehensive paper in Nature Communications.

So what should we do?

Focus on two things: cutting emissions at the source and restoring the natural environment.

The first is so obvious that it needs re-stating: we need to cut emissions at the source and we need to do this in the so-called ‘hard to abate’ sectors. For example, according to the International Energy Agency, construction and buildings are responsible for 39% of global CO2 emissions. But they are too often overlooked. We hear so much about wind, solar and cars as the vehicles (pardon the pun) for decarbonization but how often do we see coverage of decarbonization in cement, steel, or heating and cooling in buildings? My take: nowhere near as much as we should.

This needs to change. The only reason we would need DAC at an unimaginable industrial scale is because we do not decarbonize our biggest sources of emissions. So instead of figuring out how to remove carbon from the atmosphere in 2100, we should aim to remove emissions from sectors such as buildings, shipping and aviation quickly.

The second area of focus should be restoring the natural environment. I didn’t think nearly enough about the environmental and ecological aspect of climate change until I interviewed Will Marshall, CEO of Planet, at last year’s CogX. Will talked about an ecosystem emergency ecosystem. He also wrote about this in his Medium blog, highlighting some of the horrifying statistics:

82% of the wild animals by mass already gone… 50% of live corals already gone… 75% of freshwater ecosystems gone…

When we look at the impact of restoring whale population levels on climate mitigation, the huge potential in afforestation or reforestation, or the threat to global food security posed by declining bee populations, it becomes evident that nature is as at least as important as technology to our climate problem, if not more so.

Until recently, it was generally assumed that people and businesses would be resistant to the short term costs of decarbonization as the impact would only be felt generations later. The idea that there won’t be much suffering from climate change for decades now looks increasingly foolish. The heat dome that has caused so much misery and deaths in Western Canada and the Western USA is one of numerous yearly reminders that the planet’s climate is already dangerously disrupted. Scientists used to be reluctant to assign specific events to climate change; this is rapidly changing.

We’re already late, let’s at least show up.",4624
"XGBoost: A Complete Guide to Fine-Tune and Optimize your Model

Photo by @spacex on Unsplash

Why is XGBoost so popular?

Initially started as a research project in 2014, XGBoost has quickly become one of the most popular Machine Learning algorithms of the past few years.

Many consider it as one of the best algorithms and, due to its great performance for regression and classification problems, would recommend it as a first choice in many situations. XGBoost has become famous for winning tons of Kaggle competitions, is now used in many industry-application, and is even implemented within machine-learning platforms, such as BigQuery ML.

If you’re reading this article on XGBoost hyperparameters optimization, you’re probably familiar with the algorithm. But to better understand what we want to tune, let's have a recap!

PART 1: Understanding XBGoost

XGBoost (eXtreme Gradient Boosting) is not only an algorithm. It’s an entire open-source library, designed as an optimized implementation of the Gradient Boosting framework. It focuses on speed, flexibility, and model performances. Its strength doesn’t only come from the algorithm, but also from all the underlying system optimization (parallelization, caching, hardware optimization, etc…).

In most cases, data scientist uses XGBoost with a“Tree Base learner”, which means that your XGBoost model is based on Decision Trees. But even though they are way less popular, you can also use XGboost with other base learners, such as linear model or Dart. As this is by far the most common situation, we’ll focus on Trees for the rest of this article.

At that point, you probably have even more questions. What is a Decision Tree? What is Boosting? What the difference with Gradient Boosting?

Don’t worry, we’ll recap it all!

What are Decision Trees and CARTs?

CART: Does this person play video games? — Image from XGBoost Documentation

Decision tree is one of the simplest ML algorithms.

It is a way to implement an algorithm that only contains conditional statements.

XGBoost uses a type of decision tree called CART: Classification and Decision Tree.

Classification Trees : the target variable is categorical and the tree is used to identify the “class” within which a target variable would likely fall.

: the target variable is categorical and the tree is used to identify the “class” within which a target variable would likely fall. Regression Trees: the target variable is continuous and the tree is used to predict its value.

CART leaves don’t simply contain final decision values, but also real-valued scores for each leaf, no matter if they are used for classification or regression.

What is Boosting?

Boosting is just a method that uses the principle of ensemble learning, but in sequential order.

If you’re not familiar with ensemble learning, it’s a process that combines decisions from multiple underlying models, and uses a voting technique to determine the final prediction.

Random forests and Bagging are two famous ensemble learning methods.

Ensemble Learning example with the Bagging method and a majority-vote strategy — Image by author

Boosting is a type of ensemble learning that uses the previous model's result as an input to the next one. Instead of training models separately, boosting trains models sequentially, each new model being trained to correct the errors of the previous ones. At each iteration (round), the outcomes predicted correctly are given a lower weight, and the ones wrongly predicted a higher weight. It then uses a weighted average to produce a final outcome.

Ensemble Learning example with the Boosting method, using weighted-Average strategy — Image by author

What is Gradient Boosting?

Finally, Gradient Boosting is a boosting method where errors are minimized using a gradient descent algorithm. Simply put, Gradient descent is an iterative optimization algorithm used to minimize a loss function.

The loss function quantifies how far off our prediction is from the actual result for a given data point. The better the predictions, the lower will be the output of your loss function.

Example of loss function: Mean Square Error

When we construct our model, the goal is to minimize the loss function across all of the data points. For example, Mean squared error (MSE) is the most commonly used loss function for regression.

Contrary to classic Boosting, Gradient boosting not only weight higher wrongly predicted outcomes, but also adjust those weights based on a gradient — given by the direction in the loss function where the loss “decreases the fastest”. If you want to learn more about Gradient Boosting, you can check out this video.

And as we said in the intro, XGBoost is an optimized implementation of this Gradient Boosting method!

So, how to use XGBoost?

There are 2 common ways of using XGBoost:

Learning API: It is the basic, low-level way of using XGBoost. Simple and powerful, it includes a built-in cross-validation method.

import xgboost as xgb



X, y = #Import your data

dmatrix = xgb.DMatrix(data=x, label=y) #Learning API uses a dmatrix params = {'objective':'reg:squarederror'}

cv_results = xgb.cv(dtrain=dmatrix,

params=params,

nfold=10,

metrics={'rmse'}) print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

Scikit-Learn API: It is a Scikit-Learn wrapper interface for XGBoost. It allows using XGBoost in a scikit-learn compatible way, the same way you would use any native scikit-learn model.

import xgboost as xgb X, y = # Import your data xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2) xgbr = xgb.XGBRegressor(objective='reg:squarederror') xgbr.fit(xtrain, ytrain)



ypred = xgbr.predict(xtest)

mse = mean_squared_error(ytest, ypred)

print(""RMSE: %.2f"" % (mse**(1/2.0)))

Note that when using the Learning API you can input and access an evaluation metric, whereas when using the Scikit-learn API you have to calculate it.

Objective function

XGBoost is a great choice in multiple situations, including regression and classification problems. Based on the problem and how you want your model to learn, you’ll choose a different objective function.

The most commonly used are:

reg:squarederror : for linear regression

: for linear regression reg:logistic : for logistic regression

: for logistic regression binary:logistic: for logistic regression — with output of the probabilities

PART 2: Hyperparameter tuning

Why should you tune your model?

How would an untuned model perform compared to a tuned model? Is it worth the effort? Before going deeper into XGBoost model tuning, let’s highlight the reasons why you have to tune your model.

As a demo, we will use the well-known Boston house prices dataset from sklearn, and try to predict the prices of houses.

Here how would perform our model without hyperparameter tuning:

import xgboost as xgb

from sklearn.datasets import load_boston

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error



boston = load_boston()

X, y = boston.data, boston.target

dmatrix = xgb.DMatrix(data=x, label=y) params={'objective':'reg:squarederror'} cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20) print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

## Result : RMSE: 3.38

Without any tuning, we’ve got a RMSE of 3.38. Which isn’t bad, but let’s see how it would perform with just a few tuned hyperparameters:

import xgboost as xgb

from sklearn.datasets import load_boston

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error



boston = load_boston()

X, y = boston.data, boston.target

dmatrix = xgb.DMatrix(data=x, label=y) params={ 'objective':'reg:squarederror',

'max_depth': 6,

'colsample_bylevel':0.5,

'learning_rate':0.01,

'random_state':20} cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20, num_boost_round=1000) print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

## Result : RMSE: 2.69

With just a little bit of tuning, we’ve now got a RMSE of 2.69. It’s a 20% improvement! And we could probably improve even more. Let’s see how!

Deep dive into XGBoost Hyperparameters

A hyperparameter is a type of parameter, external to the model, set before the learning process begins. It’s tunable and can directly affect how well a model performs.

To find out the best hyperparameters for your model, you may use rules of thumb, or specific methods that we’ll review in this article.

Before that, note that there are several parameters you can tune when working with XGBoost. You can find the complete list here, or the aliases used in the Scikit-Learn API.

For Tree base learners, the most common parameters are:

max_depth : The maximum depth per tree. A deeper tree might increase the performance, but also the complexity and chances to overfit.

The value must be an integer greater than 0. Default is 6.

: The maximum depth per tree. A deeper tree might increase the performance, but also the complexity and chances to overfit. The value must be an integer greater than 0. Default is 6. learning_rate : The learning rate determines the step size at each iteration while your model optimizes toward its objective. A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.

The value must be between 0 and 1. Default is 0.3.

: The learning rate determines the step size at each iteration while your model optimizes toward its objective. A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum. The value must be between 0 and 1. Default is 0.3. n_estimators : The number of trees in our ensemble. Equivalent to the number of boosting rounds.

The value must be an integer greater than 0. Default is 100.

NB: In the standard library, this is referred as num_boost_round .

: The number of trees in our ensemble. Equivalent to the number of boosting rounds. The value must be an integer greater than 0. Default is 100. NB: In the standard library, this is referred as . colsample_bytree : Represents the fraction of columns to be randomly sampled for each tree. It might improve overfitting.

The value must be between 0 and 1. Default is 1.

: Represents the fraction of columns to be randomly sampled for each tree. It might improve overfitting. The value must be between 0 and 1. Default is 1. subsample: Represents the fraction of observations to be sampled for each tree. A lower values prevent overfitting but might lead to under-fitting.

The value must be between 0 and 1. Default is 1.

Regularization parameters:

alpha (reg_alpha): L1 regularization on the weights (Lasso Regression). When working with a large number of features, it might improve speed performances. It can be any integer. Default is 0.

(reg_alpha): L1 regularization on the weights (Lasso Regression). When working with a large number of features, it might improve speed performances. It can be any integer. Default is 0. lambda (reg_lambda): L2 regularization on the weights (Ridge Regression). It might help to reduce overfitting. It can be any integer. Default is 1.

(reg_lambda): L2 regularization on the weights (Ridge Regression). It might help to reduce overfitting. It can be any integer. Default is 1. gamma: Gamma is a pseudo-regularisation parameter (Lagrangian multiplier), and depends on the other parameters. The higher Gamma is, the higher the regularization. It can be any integer. Default is 0.

Approach 1: Intuition and reasonable values

A first approach would be to start with reasonable parameters and to play along. If you understood the meanings of each hyperparameter above, you should be able to intuitively set some values.

Let’s start with reasonable values. It would usually be:

max_depth: 3–10

n_estimators: 100 (lots of observations) to 1000 (few observations)

learning_rate: 0.01–0.3

colsample_bytree: 0.5–1

subsample: 0.6–1

Then, you can focus on optimizing max_depth and n_estimators.

You can then play along with the learning_rate, and increase it to speed up the model without decreasing the performances. If it becomes faster without losing in performances, you can increase the number of estimators to try to increase the performances.

Finally, you can work with your regularization parameters, usually starting with alpha and lambda. For gamma, 0 would mean no regularization, 1–5 are commonly used values, whereas 10+ would be considered as very high.

Approach 2: Optimization Algorithms

A second approach to find the best hyperparameters is through Optimization Algorithm. Since XGBoost is available in a Scikit-learn compatible way, you can work with Scikit-learn’s hyperparameter optimizer functions!

The two most common are Grid Search and Random Search.

Grid Search

A Grid Search is an exhaustive search over every combination of specified parameter values. If you specify 2 possible values for max_depth and 3 for n_estimators, Grid Search will iterate over 6 possible combinations:

max_depth: [3,6],

n_estimators:[100, 200, 300] Would result in the following possibilities:

max_depth: 3, n_estimators: 100

max_depth: 3, n_estimators: 200

max_depth: 3, n_estimators: 300

max_depth: 6, n_estimators: 100

max_depth: 6, n_estimators: 200

max_depth: 6, n_estimators: 300

Let’s use GridSearchCV() from Scikit-learn to tune our XGBoost model!

In the following examples, we’ll use a processed version of the Life Expectancy dataset available on Kaggle.

import pandas as pd

import xgboost as xgb

from sklearn.model_selection import GridSearchCV data = pd.read_csv(""life_expectancy_clean.csv"") X, y = data[data.columns.tolist()[:-1]],

data[data.columns.tolist()[-1]] params = { 'max_depth': [3,6,10],

'learning_rate': [0.01, 0.05, 0.1],

'n_estimators': [100, 500, 1000],

'colsample_bytree': [0.3, 0.7]} xgbr = xgb.XGBRegressor(seed = 20) clf = GridSearchCV(estimator=xgbr,

param_grid=params,

scoring='neg_mean_squared_error',

verbose=1) clf.fit(X, y) print(""Best parameters:"", clf.best_params_)

print(""Lowest RMSE: "", (-clf.best_score_)**(1/2.0))

estimator: GridSearchCV is part of sklearn.model_selection, and works with any scikit-learn compatible estimator. We use xgb.XGBRegressor(), from XGBoost’s Scikit-learn API.

GridSearchCV is part of sklearn.model_selection, and works with any scikit-learn compatible estimator. We use xgb.XGBRegressor(), from XGBoost’s Scikit-learn API. param_grid: GridSearchCV takes a list of parameters to test in input. As we said, a Grid Search will test out every combination.

GridSearchCV takes a list of parameters to test in input. As we said, a Grid Search will test out every combination. scoring : It’s the metric(s) that will be used to evaluate the performance of the cross-validated model. In this case, neg_mean_squared_error is used in replacement for mean_squared_error. GridSearchCV is simply using a negative version of MSE for technical reasons — so it makes the function generalizable to other metrics where we aim for the higher score instead of the lower.

: It’s the metric(s) that will be used to evaluate the performance of the cross-validated model. In this case, neg_mean_squared_error is used in replacement for mean_squared_error. GridSearchCV is simply using a negative version of MSE for technical reasons — so it makes the function generalizable to other metrics where we aim for the higher score instead of the lower. verbose: Controls the verbosity. The higher, the more messages.

More parameters as available, as you can find out in the documentation.

Finally, the lowest RMSE based on the negative value of clf.best_score_

And the best parameters with clf.best_params_

Best parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 500}

Random Search

A Random Search uses a large (possibly infinite) range of hyperparameters values, and randomly iterates a specified number of times over combinations of those values. Contrary to a Grid Search which iterates over every possible combination, with a Random Search you specify the number of iterations.

If you input 10 possible values for max_depth, 200 possible values for n_estimators, and choose to do 10 iterations:

max_depth: np.arrange(1,10,1),

n_estimators: np.arrange(100,400,2) Example of random possibilities with 10 iterations:

1: max_depth: 1, n_estimators: 110

2: max_depth: 3, n_estimators: 222

3: max_depth: 3, n_estimators: 306

4: max_depth: 4, n_estimators: 102

5: max_depth: 1, n_estimators: 398

6: max_depth: 6, n_estimators: 290

7: max_depth: 9, n_estimators: 102

8: max_depth: 6, n_estimators: 310

9: max_depth: 3, n_estimators: 344

10: max_depth: 6, n_estimators: 202

Now, let’s use RandomSearchCV() from Scikit-learn to tune our model!

import pandas as pd

import numpy as np

import xgboost as xgb

from sklearn.model_selection import RandomizedSearchCV data = pd.read_csv(""life_expectancy_clean.csv"") X, y = data[data.columns.tolist()[:-1]],

data[data.columns.tolist()[-1]] params = { 'max_depth': [3, 5, 6, 10, 15, 20],

'learning_rate': [0.01, 0.1, 0.2, 0.3],

'subsample': np.arange(0.5, 1.0, 0.1),

'colsample_bytree': np.arange(0.4, 1.0, 0.1),

'colsample_bylevel': np.arange(0.4, 1.0, 0.1),

'n_estimators': [100, 500, 1000]} xgbr = xgb.XGBRegressor(seed = 20) clf = RandomizedSearchCV(estimator=xgbr,

param_distributions=params,

scoring='neg_mean_squared_error',

n_iter=25,

verbose=1) clf.fit(X, y) print(""Best parameters:"", clf.best_params_)

print(""Lowest RMSE: "", (-clf.best_score_)**(1/2.0))

Most RandomizedSearchCV’s parameters are similar to GridSearchCV’s.

n_iter: It’s the number of parameter combinations that are sampled.

The higher, the more combinations you’ll be testing. It trades off runtime and quality of the solution.

As for GridSearchCV, we print the best parameters with clf.best_params_

And the lowest RMSE based on the negative value of clf.best_score_

Conclusion

In this article, we explained how XGBoost operates to better understand how to tune its hyperparameters. As we’ve seen, tuning usually results in a big improvement in model performances.

Using our intuition to tune our model might sometimes be enough. It is also worth trying Optimization Algorithms like GridSearch and RandomSearch.

But most of the time, you’ll get an even better result with a mix of Algorithms and adjustments through testing and intuition!",18711
"The Ever Evolving/Expanding Toolkit

If there is one thing that data scientists are good at, it’s catching a buzz (and a few new tools along the way). The concept of data science itself is a buzz term that many professionals with any statistical understanding in business attached to themselves in order to improve their marketability, and to good effect. Why should we expect the building wave of artificial intelligence to be any different? As the concept of data scientist has evolved, so too have the tools associated with it, and thus the professionals in this field have been caught in a constant race to remain relevant by exposing themselves to the newest tools being made available. Although the rate of change has been near to overwhelming, those who have survived and been able to demonstrate competence around the core functionality of these data science technologies are well poised to take advantage of the tools of artificial intelligence. Thus, the data scientists who learn to evolve will learn how to rebrand themselves as practitioners of artificial intelligence. But to be able to convince others of this rebrand, such professionals will need to continue to expand on their toolkits. Whereas the early 2000’s brought us Hadoop, NoSQL, IoT, Python’s scikit-learn, Tensorflow, and Spark, the next generation will be leveraging AI-as-a-Service, cloud computing, intelligent automation, and containerization for analytics. This means that data scientists must continue to learn how to leverage API calls, architect cloud environments that support data science, and deploy analytics to expose API endpoints.

Image by Ed Zilch from Pixabay

The Importance of the User Experience

As you can see from above, statistical tools are not the only tools that will help data scientists to survive in this quickly changing landscape. Artificial intelligence is not merely statistical technologies but rather it is the embedding of those statistical technologies into user experiences. Thus, the savvy data science survivalist will identify opportunities to solve problems using embedded statistical analytics. Such efforts will require a greater understanding of software programming concepts, which the data scientist is already well-poised for through the acquisition of open source scripting tools, and the ability to work more closely with application development teams. There are many ways to tackle the user experience problem from both a technical as well as a theoretical (see our previous blog post as one example) perspective and what works will always depend on satisfying the user but the key is to identify strategies whereby statistical models improve the user experience. In this way data scientists will need to continue to evolve their approach to problem solving. Where once we focused on using cutting edge modeling techniques to extract insights from data, we now need to focus on their utility within an application.

Image by Gerd Altmann from Pixabay

Evangelizing a Trade

And finally, because the true test of our data science products depends on the user’s ability to get value from them, we must be prepared to take our specialized understanding of these AI-enabling technologies and empower the citizen data scientist rather than pontificate over the sacredness of our special anointed knowledge. Despite the apparent ease-of-use promised by the onslaught of automated data science products, citizen data scientists will still lack understanding of their application. As one Reddit user so elegantly put it, “most people can barely use Excel, and even most data/business analysts have a hard time understanding anything beyond basic aggregation and statistics.” Thus, businesses will look to data scientists to train the citizen data scientist of the future to use those tools as use cases permit. The reason that data scientists will be required is because data science is not a tool but rather it is a way of thinking and tackling problems. Tools certainly enable new ways of thinking, but people need to be trained on how to think about the tool in order for the tool to change their approach to solving problems. In short, we must evangelize the tools that enable the artificial data scientist. In this vain, data scientists become the hub of both artificial and human data science products within an organization and the citizen data scientists the spokes.

Image by Comfreak from Pixabay

From Data Scientist to AI Practitioner

In conclusion, the data scientist is not dead, or dying for that matter, but is, instead, in need of a coming evolution. Those who are most successful in continuing to expand their tool kits to leverage AI services, expose results to and interact with applications, and impart their way of thinking to enable others will be the most confidently poised to meet the coming needs of the AI practitioner for the future of digital enterprise.

Like engaging to learn more about data science? Join me.",4965
"How Germany is Building a Quantum Computing Ecosystem Qiskit Follow Jun 15 · 9 min read

By Ryan F. Mandelbaum, Sieglinde Pfaendler, and Fabio Scafirimuto

This year, Germany kicked off its Roadmap Quantencomputing to lay the groundwork for a German-based quantum computing ecosystem. The country has earmarked two billion euros in order to see the vision through. But building a quantum computing landscape is about more than just hardware; it requires collaboration between research institutions, industry, government, and most importantly, the community.

Germany’s quantum community is an extremely vibrant space. Quantum research is underway at many of the country’s universities, companies, and research institutes under the umbrella of Fraunhofer-Gesellschaf, DLR, the Max Planck Society, the Helmholtz Association institutes, and others. While no single story could fully capture the complexity of the German quantum field, these efforts are all guiding the country toward a singular goal: to become a European quantum leader.

“Germany has a history of being the machine shop of the world,” said Stefan Filipp, professor in Physics at the Technical University of Munich and Director of the Walther-Meißner-Institut of the Bavarian Academy of Sciences. “There’s a feeling that we need to continue this for other technologies to come — such as quantum computing.”

Among the most notable efforts include the Roadmap Quantencomputing, providing a core goal for the country’s quantum research: Build a German quantum computer in the next five years. Meanwhile, Fraunhofer has recently purchased exclusive access to Europe’s first IBM Quantum System One, providing quantum hardware access to thousands across the country. Germany’s quantum vision extends to Europe as a whole, as researchers work with key European partners on strategic projects such as OpenSuperQ to build an open source quantum computer for anyone around the continent. Meanwhile, the Qiskit team is working to foster and support the community growing around these developments.

A Central Vision: The Roadmap Quantencomputing

Back in 2020, as part of an overall economic stimulus, the German government first announced that they had earmarked money devoted to developing a quantum computer while creating a roadmap to build a broader quantum ecosystem. German legislators noticed that large international companies and other countries were building technology and communities, and wanted to ensure that Germany, and Europe more generally, could have a stronger stake in the technology’s development.

The roadmap serves a few important purposes, explained Filipp, one of the roadmap committee’s co-chairs. Not only does it identify problems and map out technology development, but it also charts a way to attract more quantum expertise to Germany and to Europe as a whole. And, of course, it provides a strategy for workforce training of students, technicians, and other experts.

It was clear to the roadmap committee that no single large player could lead the way alone, so the roadmap prioritizes link building between various segments of the quantum space. Filipp explained that the roadmap calls for the designation of centralized regional hubs — Centers of Competence — to form networks between universities, research institutions, and industry partners. The roadmap recommends that research should focus around technology that has reached a certain level of maturity, such as superconducting, trapped ion, and cold atom quantum computers. However, it also recommends that basic research into other hardware architectures continues, too.

The roadmap committee specifically called out the importance of fostering commercial development and technology transfer, including ensuring that companies can buy components from local businesses, rather than importing them. “We need to identify players that can actually commercialize the product,” said Filipp. Hubs will focus on creating guidelines to determine what can be built locally and what should be imported. Filipp also foresees direct information exchange between industry and research in order to further strengthen these links.

Opening Quantum to Thousands of Beginners

As Germany works to build a quantum computer of its own in the next few years, Chancellor Angela Merkel, herself holder of a quantum chemistry doctorate degree, was impressed to see IBM’s progress on their quantum computers, and decided that the country should stake a larger claim in the field as quickly as possible. That was the beginning of the relationship between Fraunhofer-Gesellschaft and IBM, with the aim to bring the first IBM Quantum System One to Europe. Fraunhofer, Europe’s leading application-oriented research organization with more than 80 research facilities, including 75 institutions across Germany, has had exclusive access to an IBM Quantum System One operated by IBM at the Ehningen location in Baden-Württemberg since January of 2021. The common goal between Fraunhofer and IBM is to stimulate and support German research in the applied quantum computing field overall.

The initiative benefits Fraunhofer researchers hoping to develop algorithms to solve practical problems, of course, but Fraunhofer also offers system access to external partners such as universities and companies who may be interested in this technology for civil research and education, yet don’t have a large budget to experiment with it at the moment, explained Prof. Dr. Oliver Ambacher from the Fraunhofer Institute for Applied Solid State Physics (IAF). Fraunhofer aims device access at beginners who want to find out more about quantum computing, as well as experts who are looking for partners on research projects or who want to develop their own quantum algorithms. Access requires a usage contract, but use is on the basis of a monthly ticket. This gives partners the greatest possible flexibility without a long-term commitment. As Fraunhofer’s purpose is to bridge research with industry, the availability of the device to German companies and researchers will hopefully help further the development of the German quantum ecosystem.

Fraunhofer’s IBM Quantum System One

Fraunhofer’s IBM Quantum System One also serves as an example of the Quantum Roadmap’s Center of Competence model in action. The federal state of Baden-Württemberg helped fund the device as well as some of the projects. At the moment, seven Fraunhofer Institutes and a host of universities and companies are getting involved in these quantum computing projects, with even more Institutes and organizations involved in the Fraunhofer Competence Network Quantum Computing.

Scientists have already had the chance to work with the IBM Quantum System One. The current projects focus on quantum optimization with resilient algorithms, software engineering of industrial quantum applications, and algorithms. Applications include new simulation approaches for materials in future energy storage systems, the optimization of financial portfolios, and the improvement of stability parameters in critical energy supply infrastructures, said Ambacher. He also thinks that the system can help build trust and important relationships for the betterment of quantum computing overall.

An Open Quantum Computer For All of Europe

German scientists are thinking beyond Germany, and are working to create an open quantum system for the whole of Europe. Frank Wilhelm-Mauch, Professor of Physics at Saarland University, hopes to offer European researchers access to a quantum computer that itself is a research and development tool to help users truly understand the depths of quantum hardware via the OpenSuperQ project, based at the Forschungszentrum Jülich in North Rhine-Westphalia.

“The public sector has a unique place in quantum computing to create tools for hardware developers to plug components into a complete machine and test them in this context,” said Wilhelm-Mauch, coordinator of the OpenSuperQ project. “With OpenSuperQ, we can train quantum computer engineers and developers on real hardware.”

The OpenSuperQ project started during some earlier German quantum computing ecosystem discussions around 2018, said Wilhelm-Mauch, as a non-commercial option for researchers in Germany and across Europe to experiment on quantum hardware. The group is now constructing a traditional superconducting quantum computer — but it’s more than just an experimental tool. Training engineers who will be able to construct and maintain quantum hardware and components is crucial to the expansion of quantum computing. Wilhelm-Mauch hopes that the OpenSuperQ device can serve as a training ground for those budding engineers.

Though its reach is continental, the OpenSuperQ project serves an important role in the German quantum ecosystem as well. The project’s location at Forschungszentrum Jülich is helping to attract and retain local quantum hardware expertise. Plus, through Forschungszentrum Jülich’s JUNIQ (Jülich UNified Infrastructure for Quantum computing) platform, businesses and researchers around Germany have access to both the OpenSuperQ device as well as other quantum hardware.

Supporting Quantum in Germany with the Qiskit Community

Since the turn of the 20th century, Germany’s scientists have played a significant role in contributing to the fundamental concepts and understanding underlying quantum technology, such as the Heisenberg Uncertainty Principle or Planck’s constant. These scientists were strongly networked in education and research on an international scale, and especially with with their German-speaking neighbors in Austria and Switzerland. These countries are famed for their unique education system that ensures a significant supply of practically trained engineers who can convert concepts into robust technological solutions, thus leading to successful innovation through collaboration with industry.

The Qiskit community team is working at several levels to engage the quantum computing community in Germany. A large German cohort attends global Qiskit events, such as the Global Summer School, for example. The Qiskit team in Europe has created events targeted to the DACH (Germany, Austria, Switzerland) audience, including our IBM Quantum Fridays seminar series, the Qiskit Hackathon Europe, and several workshops. Qiskit community interns, advocates, and enthusiasts from all over the world allow us to develop a local network of quantum computing expertise. “There are so many opportunities for young people to get into quantum computing and quantum information science that we’re practically flooded with events, conferences and opportunities to connect and network,” said Caroline de Groot, former Qiskit intern and Ph.D student at the Max-Planck-Institute for Quantum Optics.

Qiskit also collaborated with universities and Fachhochschulen in Berlin to organize Girls Day for introducing high school girls to STEM topics, in particular to quantum computing. “Girls day was an awesome way to inspire and motivate the quantum computing experts of tomorrow, and to help create an equitable German quantum workforce,” said Isabel Nha Min Le, Qiskit Intern and masters student at RWTH Aachen University.

Not only does Qiskit organize events, but it also fosters relationships with local professors and institutions, as this is perhaps the most effective way to develop a local community — building a local network requires teaming up with and between local institutions. Plus, quantum students and developers across Germany have begun to adopt Qiskit as a go-to language for programming quantum computers.

History has demonstrated that the shift to an open source mentality leads to an acceleration of innovation and advancement of technology in high-power computing. Learning from this, many in quantum computing have embraced an open source mindset from the start — this is a foundational concept of the Qiskit Community.

Get started with Qiskit here!

Access Programs for Educators and Researchers

The IBM team has developed programs to promote access to real quantum computers for different audiences. For example, the IBM Quantum Educators program provides access to real quantum hardware for educators and lecturers at universities, Fachhochschulen, and high schools that use the free, online Qiskit Textbook in their courses so that their students have access to quantum computers during their teaching sessions without having to deal with long wait times. Those who are interested can submit applications for review.

The IBM Quantum Researchers program instead focuses on academic researchers, including postdoctoral researchers and Ph.D students, working on quantum computing research projects that could potentially lead to impactful publications in areas such as the development of new algorithms or improved control of qubit systems. Currently, there are various 5- and 7-qubit systems available for academics enrolled in this program. Additionally, for researchers requiring more advanced systems for their work, IBM offers project-based Access Awards for temporary access to a 16-qubit system. Scientists can apply for the program by submitting a project proposal for a technical and merit review, with the expectation that the award will lead to results that will be shared with the scientific community via a publication or conference presentation.

—

Thanks to a multitude of efforts, quantum is flourishing in Germany, with an ecosystem built on the country’s leadership and strengths in areas such as basic research and hardware development. The way forward is one rooted in open science and collaboration at academic institutions, supported by the network building and technology transfer efforts of Centers of Competence to ensure that research supports the growth of a quantum business sector, too. This model will hopefully not only make Germany a global quantum leader, but also push the field of quantum computing forward in Europe and in the rest of the world.",14016
"My Kaggle Journey from Novice to Notebooks Masters

Ever got overwhelmed by Kaggle, or want to know about ways to succeed in Kaggle? Don’t worry, this blog covers it all. Chanakya Vivek Kapoor Follow Jul 2 · 7 min read

Kaggle constitutes an important part of every data scientist’s career, being active on Kaggle not only helps you to build a great career, but most importantly it’s a place where one gets to learn from the best in the field and build connections for life. For those of you who are not aware of Kaggle, don’t worry I will be explaining the functioning of the platform in a bit.

I have been quite active on Kaggle for the past 6 months, learning a lot from other’s work and showcasing my work to the outside world. There are five tiers in Kaggle, namely Novice, Contributor, Expert, Master, and GrandMaster and there are various goals that you need to fulfill before you move from one level to another. I became Kaggle Notebooks and Discussion Expert on 27th January 2021, and to be honest, I thought that was the maximum tier that I could ever achieve, but after working hard for some more time, I finally became Kaggle Master on 25th June 2021. The feelings that I had on that day can’t be expressed in words. My current rank is 235 out of 176,033 in the world.

Screenshot of my Kaggle Profile

The main aim of this article is to share my Kaggle journey with you, how I overcame my fears or imposter syndrome, how I learned new things from the platform, and to encourage you to start your own Kaggle journey. Before we discuss these things, let’s discuss in brief about Kaggle.

What is Kaggle?

Kaggle is a community that allows all the data science enthusiasts around the globe to gather and take part in various competitions, upload datasets for others to experiment on, share notebooks or kernels and participate in discussions about Machine Learning, Deep learning, or anything in the field of Artificial Intelligence.

There are four different categories in Kaggle:

Competitions

You can team up or participate individually in different competitions posted by various companies or Kaggle itself. Competitions allow you to enhance your overall data science skills because you will have to do feature engineering, feature selection, and hyperparameters tuning, etc to select the best model and make final submissions to the competition. Competitions allow you to discover your true potential and make you learn many new approaches to deal with a problem.

Datasets

If anyone finds any unique or interesting dataset, he/she can upload it on Kaggle and can also mention some tasks like performing Exploratory Data Analysis and finding out answers to some questions related to the data uploaded, etc. Many times, if you have to perform some Machine Learning tasks for which no dataset is available, you can always refer to the datasets available on Kaggle.

Notebooks

Once you find out any interesting dataset, you can make notebooks or kernels for the dataset. Notebooks are highly interactive multi-purpose tools that allow you to write and execute code. You can explore the notebooks of other people as well, and for me, this is the best part of Kaggle, because when I am out of ideas or have some doubts about the approaches, I can learn from other’s notebooks and come up with new ideas of my own.

Discussions

In the field of Data Science, often you can come up with many doubts related to certain concepts or any execution-related queries, for such doubts discussion forum is the place to be. You can post your doubts, learn or answer other people’s doubts that you might not have. People will notice you as you ask more doubts or resolve doubts with time.

Whenever you do some activity in any of the four categories, other people can upvote(or like) your work. You get medals whenever your work crosses a certain number of Upvotes. There are three different kinds of medals: Bronze, Silver, and Gold.

For example, if you have made a new notebook, other people can check out your notebook and upvote it if they found it useful. For notebooks, the condition to get a bronze medal is to have 5 upvotes, 20 upvotes for silver, and 50 upvotes for gold, and this number varies for each category. As you get more medals, you advance from one tier to another. You can refer to the progression system here.

Whenever you sign up for Kaggle, you are considered a Novice for all four categories. There are some tasks and information that you need to complete to become a contributor. After that, the actual fun begins. You get a rank in respective categories only when you become an expert. Let’s say you want to achieve the master tier for Notebooks, first, you would need at least 5 medals to become an expert(at this stage you will get a rank for notebooks category), after that if you gain 10 silver medals in total, you will become Notebooks Master! More the upvotes you get better will be your rank. Likewise, there are different criteria for each category.

It’s okay to Get Overwhelmed

All of these things might look too overwhelming, and honestly, I could not start my Kaggle journey after reading about Kaggle. I wondered if I am good enough for the platform, whether people will like my work, and what not!

But believe me, it’s only about making that first notebook, asking or maybe solving that one doubt in the discussions forum. After that you will soon realize how welcoming the people are on Kaggle, you will discover new approaches to dealing with a dataset or maybe learn a new ML algorithm! The possibilities are endless.

Points to Succeed

Following are some of the elements that you can keep in your mind during your Kaggle journey:

Don’t just make anything for the sake of getting medals

A lot of people just make notebooks for the sake of getting medals, without taking care of the quality of their work. Please remember that whatever notebooks you make on Kaggle can be used to showcase your skills during a technical interview, if your quality is excellent and showcases your true potential, the employer might get impressed as well. Plus better the quality of the work, chances of getting upvotes increase.

Be Patient

A lot of times when you make your notebooks public, you might not get upvotes immediately, this does not mean your work isn’t good so don’t lose confidence. You will ultimately get upvotes at the end if your work is good. You can also promote your notebooks on Social Media Platforms like Linkedin, Twitter so that more people can check out your work.

Bookmark the interesting and amazing works

Whenever you come across an amazing notebook or an interesting discussion, do bookmark them to use that concept in your future work(don’t forget to give credits to the original creator). For example, I came across a code snippet that embedded photos of football players in the plotly bar chart for visualization purposes, I had never imagined that was also possible, so I immediately bookmarked it and will use it whenever an opportunity presents itself.

Interact as much as you can

Whenever you are reading some notebook or participating in any competition, do indulge yourself in interactions with other Kagglers, there are several benefits to this: first, people might check out your profile if you show appreciation towards their work, second, you will be well known in the community(this does not happen in days). Whenever you check out a person’s Kaggle profile, make sure to connect with them on Linkedin or Github, or Twitter, as they might help you in your time of need.

Explain your work

It is always good to write out comments for each step that you are performing or the reason for performing that step. Let’s say that you have plotted a scatterplot for visualization purposes, you can write out the inference from that plot so that other people can also understand your thinking process. Writing good notebooks is no less than portraying a story.

Succeeding in Kaggle is not one day’s job and there is no end to it, it is dependent on how you want to make the maximum benefit out of it. One piece of advice that I would give is to set up short-term targets, for example when I started, my first target was to become a discussions expert, once that target was achieved, I wanted to become a notebooks expert. After becoming an expert, I set the target of becoming Kaggle Notebooks Master. My next target is to participate in competitions. Therefore, I have always benefited by setting short-term goals and never got overwhelmed or got scared due to Kaggle.

Feel free to connect with me on Linkedin, Github, or Twitter.

Hope you can get inspired by this article and get started with your Kaggle journey. Here are some of the datasets or competitions that you can try to get your hands dirty with:",8787
"The Origins of Unmanned Aerial Vehicles

The military history of these unmanned aerial vehicles (UAVs) goes back further than you might think. In 1849, Austrian forces had the water city of Venice surrounded. But their progress was slow and Captain Ludwig Kudriaffsky had an idea to fasten their surrender.

Burning balloons.

Note, these are probably not suitable for your child’s next birthday party. As for a gender reveal party — if you want to follow recent idiotic trends — then maybe.

Fast forward to WWI when British Archibald Low created the world’s first remote-operated airplanes. While the guise was to use them for anti-aircraft target practice, the real purpose was to attach big fat bombs on their front before kamikazing into enemies.

While these planes were never employed to do so, their development went far. He even went on to create the world’s first guided rocket in 1917. While the UK government wasn’t too excited about his spiffy new tech, the Germans seemed to be aware of the potential mass damage they could cause.

So much so that they tried to assassinate him twice, and failed.

In WWII, the V-1 rockets were largely based on Archibald’s designs, at least initially. Fortunately for the Allies, these were only deployed near the end of the war in June of 1944.

Even still, the Germans managed to launch 9,521 of the mechanical beasts into the UK from France’s invaded shores. Of these, 2,340 managed to hit London, killing at least 5,475 people and causing over 16,000 deaths.

Thankfully they weren’t deployed at the beginning, or that number would be drastically higher.",1606
"Alternatively, I have a different list of the best 18+ webtoons here. What are the dirtiest webtoons? check 18+ webtoons here.

Photo by Toa Heftiba on Unsplash

The dirtiest Webtoon is increasing in popularity in the entire world. 18+ is the most reading novel by young peoples. Also, on-demand on adult Webtoon, many of the artists move into adult webtoons space. Here is a list of some dirtiest webtoons, which is 18+ Webtoon you need to check.

Drug candy

It is one of the most describing husband-wife relationships Webtoon. Seung-gu is a handsome man with a beautiful wife in this Webtoon, but his demotion at work gets depressed him. Soon he met with another lady who knows him, and she addicted him. At last, he comes to know his mistake is not easy to correct.

Also read: Tapastics Vs Line webtoon: Which platform is best for posting webtoons?

I Love Yoo!

I love Yoo is the best romantic Webtoon you need to check. It is about Shin-Ae, who decides not to interfere with her life. But her life is changed when he unsuspecting the clothes of a stranger.

Love shuttle

In this comic, a Doyoun is a handsome, beautiful gay. His body is blooming and bloom all time because of this Taehan get benefit from him. He takes advantage of the situation and is the more interesting character in the series.

Also read: Enlisting 101 Good Yaoi comics on Line Webtoon

Lady garden

In this comic, Kang Doo is a guard of the lady garden and allows only hot and beautiful ladies in the garden. He takes care of all the lady in her life problem. In return, all ladies reward him daily and fulfill their desire.

Household Affairs

Si-Yeon-Hong is a wife of Ha-jin who never takes an interest in his wife. At last, his wife takes intercourse with the delivery boy to satisfy herself. Unfortunately, her husband does not know, ends up desiring, and realizes it affects her job.

Also read: What are the dirtiest webtoons? Check the hottest webtoons

Lets Play

It is the hottest and romantic Webtoon in which a girl dreams of creating a video game. But he does not do it, and then a streamer gives him a task to create the game. This Webtoon is full of sexy, funny, and real-story about gaming, memes, and social anxiety.

The all above dirtiest 18+ Webtoon is full of fantasy, romance, sex, and adventure. This comic urges you to read it more again and again. This story is actual and also explains the real-life problems of people that they face. We recommend you to read all these comics one by one.

Also, read Best Webtoons similar to ‘Solo Leveling’ and ‘beginning after the end’

I hope this helps.",2598
"Recently, Tracxn published a report on the state of ConstructionTech in 2021. One of the key highlights is that not only is construction tech one of the most active sectors for investors — with overall funding of $11.8B in over a thousand companies — but more than half of the funding has been raised in the past 3 years (2018–2020).

At the beginning of this “wave” of funding, in February 2018, I was fortunate enough to be introduced, via Ilai Rotbain (the exceptional GM of Autodesk) to Meirav Oren, the Founder and CEO of Versatile Natures. Meirav is spearheading the Israeli chapter of The Society of Construction Solutions (“SCS”)* and was kind enough to invite me to one of the gatherings.

*The SCS is ultimately a club for folks passionate about construction (and a curious venture capitalist) to learn about the market, its challenges, solutions, opportunities and innovations.

Three years , dozens of meetups, hundreds of pitches and two investments later — including one in Meirav’s startup, now (just) called Versatile — this is what we learned:

The market is truly gigantic — certainly one which can justify the returns sought by VC’s. There are (almost) no companies that are constrained by a small TAM, something I’ve found to be exceptionally unique to this space. This is ultimately attributed to the “top line” being huge i.e. $16 trillion by 2025. Another unique attribute to ConstructionTech is how fragmented the customer base is. There are thousands, and sometimes hundreds of thousands, of potential customers (General contractors, Architects, Developers, Insurers…) spread across the globe which makes distribution a serious challenge for startups building in the space. Lastly, the famous chart from the global digitization index done by Mckinsey shows just how behind the Construction market is in terms of technology adoption ( at least per it’s latest report in 2015):

Fun fact from the Procore S-1: 80% of contractors surveyed in a USG Corporation and U.S. Chamber of Commerce survey believes that they will use some newer technology by 2022 so I am betting the above chart will look very different in a couple of years.

This second chart below shows how the vast majority of Construction tech startups only solve for 1–3 use cases, to me this implies that we are so early in the digitization of this market and so there is much more greenfield left for startups to explore (case in point, Procore is a $12 bn startup solving the communications problem)

The industry itself is also riddled with many interesting problems waiting to be solved:

🧨 There continues to be a massive amount of “Rework” done on every project due to errors made during the initial building phase, resulting in many work being taken apart and reconstructed( $500bn a year!)

🤳🏽 The industry is plagued by a lack of streamlined communication between the different stakeholders working on the design and preconstruction phase of each project (design, developer, contractors). This perpetuates itself on site too, where teams and subcontractors lack the infrastructure to properly communicate with one another

ℹ️ There remains a large information gap/asymmetry between what is actually happening on the site itself and what the general contractor thinks is happening, not to mention the developer who in many cases have a far less accurate perspective on the status of the site

💸 Manual labor causes an erosion of margins in multiple steps of the value chain. Starting from the design phase all the way to building operation

👷‍♂️ Shortage of labor which is a global pain and in the US in particular, that results with theses manual jobs to be handled by untrained trades or even worse without anyone handling them

🏗️ Major delays continue to be caused by inefficient and sub-par design that creates enormous operational hurdles (RFI due to conflict, or the realization that something cannot be built as planned)

💣 There is also an acute shortage of information on the exact status and profile of the area that comprises the construction site (Do I have a WWII 3 ton bomb trapped beneath my site?)

Crucially, a number of technological tailwinds are supporting a shift in the market:

Artificial Intelligence enabling the optimization of all the activities across the value chain

IOT, novel sensors (unearthing data and insights)

Robotics (automating tedious labor)

Rapid adoption of BIM (Building Information Modeling) enabling tech adoption

Lastly, there are incredibly talented teams who are brave enough to tackle these problems and get into this market to build truly successful companies and win big. We have met with dozens of these teams around the world and have decided to back two companies which we felt are most connected to our thesis and of course — the founding teams are absolutely amazing (shoutout to Eitan from Swapp & Meirav of Versatile!)

The result of this construction tech wave is moving the industry away from being an uncontrolled, fragmented and unstructured process, towards a fully visible, streamlined and digital one.

We initially fell in love with construction due to the research but are now in a committed relationship due to the amazing talent that is trying to solve these problems which impact the lives of everyone on the planet.

TL;DR for the impatient reader :)",5321
"NoSQL for Semi-structured Data

NoSQL datastores cater to semi-structured data types: key-value, wide column, document (tree), and graph.

Key-Value Datastore

A key-value store is a dictionary or hash table database. It is designed for CRUD operations with a unique key for each record:

Create(key, value): Add a key-value pair to the datastore

Read(key): Lookup the value associated with the key

Update(key, value): Change the existing value for the key

Delete(key): Delete the (key, value) record from the datastore

The values do not have a fixed schema and can be anything from primitive values to compound structures. Key-value stores are highly partitionable (thus scale horizontally). Redis is a popular key-value store.

Wide-column Datastore

A wide-column store has tables, rows, and columns. But the names of the columns and their types may be different for each row in the same table. Logically, It is a versioned sparse matrix with multi-dimensional mapping (row-value, column-value, timestamp). It is like a two-dimensional key-value store, with each cell value versioned with a timestamp.

Wide-column datastores are highly partitionable. It has a notion of column families that are stored together. The logical coordinates of a cell are: (Row Key, Column Name, Version). The physical lookup is as following: Region Dictionary ⇒ Column Family Directory ⇒ Row Key ⇒ Column Family Name ⇒ Column Qualifier ⇒ Version. So, wide-column stores are actually row-oriented databases.

Apache HBase was the first open-source wide-column datastore. Check out HBase in Practice, for core concepts of wide-column datastores.

Document Datastore

Document stores are for storing and retrieving a document consisting of nested objects. a tree structure such as XML, JSON, and YAML.

In a key-value store, the value is opaque. But the document stores exploit the tree structure of the value to offer richer operations. MongoDB is a popular example of a document store.

Graph Datastore

Graph databases are like document stores but are designed for graphs instead of document trees. For example, a graph database will suit to store and query a social connection network.

Neo4J is a prominent graph database. It is also common to use JanusGraph kind of index over a wide-column store.

SQL vs. NoSQL Database Comparision

Non-relational NoSQL datastores gained popularity for two reasons:

RDBMS did not scale horizontally for Big Data

Not all data fits into strict RDBMS schema

NoSQL datastores offer horizontal scale at various CAP Theorem tradeoffs. As per CAP Theorem, a distributed datastore can give at most 2 of the following 3 guarantees:

Consistency: Every read receives the most recent write or an error.

Every read receives the most recent write or an error. Availability: Every request gets a (non-error) response, regardless of the individual states of the nodes.

Every request gets a (non-error) response, regardless of the individual states of the nodes. Partition tolerance: The cluster does not fail despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.

Note that the consistency definitions in CAP Theorem and ACID Transactions are different. ACID consistency is about data integrity (data is consistent w.r.t. relations and constraints after every transaction). CAP is about the state of all nodes being consistent with each other at any given time.

Only a few NoSQL datastores are ACID-complaint. Most NoSQL datastore support BASE model:

Basically Available: Data is replicated on many storage systems and is available most of the time.

Data is replicated on many storage systems and is available most of the time. Soft-state: Replicas are not consistent all the time; so the state may only be partially correct as it may not yet have converged.

Replicas are not consistent all the time; so the state may only be partially correct as it may not yet have converged. Eventually consistent: Data will become consistent at some point in the future, but no guarantee when.

Difference between SQL and NoSQL

Differences between RDBMS and NoSQL databases stem from their choices for:

Data Model: RDBMS databases are used for normalized structured (tabular) data strictly adhering to a relational schema. NoSQL datastores are used for non-relational data, e.g. key-value, document tree, graph.

RDBMS databases are used for normalized structured (tabular) data strictly adhering to a relational schema. NoSQL datastores are used for non-relational data, e.g. key-value, document tree, graph. Transaction Guarantees: All RDBMS databases support ACID transactions, but most NoSQL datastores offer BASE transactions.

All RDBMS databases support ACID transactions, but most NoSQL datastores offer BASE transactions. CAP Tradeoffs: RDBMS databases prioritize strong consistency over everything else. But NoSQL datastores typically prioritize availability and partition tolerance (horizontal scale) and offer only eventual consistency.

SQL vs. NoSQL Performance

RDBMS are designed for fast transactions updating multiple rows across tables with complex integrity constraints. SQL queries are expressive and declarative. You can focus on what a transaction should accomplish. RDBMS will figure out how to do it. It will optimize your query using relational algebra and find the best execution plan.

NoSQL datastores are designed for efficiently handling a lot more data than RDBMS. There are no relational constraints on the data, and it does not need to be even tabular. NoSQL offers performance at a higher scale by typically giving up strong consistency. Data access is mostly through REST APIs. NoSQL query languages (such as GraphQL) are not yet as mature as SQL in design and optimizations. So you need to take care of both what and how to do it efficiently.

RDBMS scale vertically. You need to upgrade hardware (more powerful CPU, higher storage capacity) to handle the increasing load.

NoSQL datastores scale horizontally. NoSQL is better in handling partitioned data, so you can scale by adding more machines.",6087
"tl;dr — While no longer as nimble as a startup, Google’s scale, strong culture and awesome people make it the ideal place to learn the nuts and bolts of product management and offers incredible opportunities to create products for millions/billions of users across the globe.

If you find my career development blogs interesting, you might also checkout:

I was a Product Manager(PM) at Google Health for the last ~2 years, launching health features on Search and Maps (like these features that help users find telehealth options on Search). I’m leaving Google to pursue an opportunity at a startup (more to come here). It’s a bittersweet moment since I loved my time at Google, so I decided to reflect on what I’ve learned as a Googler.

1) The “Googley” ethos is real and awesome

Everyone that interviews at Google is assessed for “Googleyness”. This includes being ambitious, humble, and doing the right thing. In addition to being whip smart, the large majority of Googlers I’ve met are so incredibly nice and helpful (every employee having the ability to give cash “peer bonuses” multiple times per quarter also helps :P). It makes working here a joy. Google corporate does everything it can to make our work environment as safe and comfortable as possible (e.g., the food, money to buy wfh accessories, lots of working hour flexibility, “face time” isn’t really a thing). An especially great part of the culture, is the “zero-blame” aspect. This transforms the company as it allows people to feel safe taking risks and when something does go wrong, teams can have transparent retrospectives and implement useful processes to prevent the mistake from happening in the future. Side note: imposter syndrome is real here — I constantly felt very lucky but also hopelessly unqualified to be surrounded by smarter/better people that I could learn tons from. Example, my manager was CEO of a Series B startup before coming to Google — so many ex-CXO examples like this!

2) Google is a large bureaucracy, launching something takes a village

Google is a $180B+ revenue company. The downside of doing something that harms its golden goose (ads, search, maps, etc.) is extremely high. Thus, there are very extensive processes in place to rigorously check/limit any potential user harm, production defects, PR risks. PMs need to be patient as this process can take months. As a result, things take a long time at Google. This is not unlike other large companies and I’d imagine Google is likely more agile than other companies of its size. These processes are important for the user experience, whether it’s making sure the search experience stays whip-fast or that user privacy is meticulously preserved according to various state and country-level regulations.

3) Core PM skills",2778
"Neptune is glowing like a iceberg , NASA is decided to start a new adventure again.

Neptune

Neptune is the farthest planet from the solar system. And this time, NASA, the US space research center, wants to conduct all its experiments with the

Photo by NASA on Unsplash

satellite of this planet. The space agency is looking into whether Neptune’s satellite Triton has oceans and why ice is coming out of the ground. If all goes well, NASA will send a spacecraft to Titan in 2025.Neptune is the farthest from the sun in the solar system. Triton is a satellite of this planet. NASA sent a spacecraft 30 years ago to find out about the satellite.

Voyager II

The spacecraft Voyages 2 was able to capture 40 percent of the satellite’s image. Scientists have suggested that NASA conduct the ‘Trident’ mission again to get some more information. The spacecraft Voyages 2 was able to capture 40 percent of the satellite’s image. Scientists have proposed to NASA to conduct the ‘Trident’ mission again to get some more information.

Research of NASA

They have proposed research into what is on the surface of the entire satellite, whether there is an ocean, and why ice glowing is coming out of the ground. Any 2 out of 3 subjects will be approved for research next year. Then in October 2025, NASA will send a spacecraft to Triton. However, it will not reach Triton before 2038. Neptune is 270 million miles from Earth. So it will take at least 13 years to get information from the mission. The question of why it is glowing like an iceberg and coming out of the satellite has aroused the curiosity of the researchers. Because even though it is far away from the sun, the ionosphere of the satellite is 10 times more active than any other satellite in the solar system.

Opinion of Carl Mitchell

“Triton is very strange,” said Carl Mitchell, a scientist at the Trident Project. As we know, there are some elements on its surface that have never been found before. We want to know how the satellite is still active. If this mission is not carried out by 2040, the sun will move further north. In that case you have to wait 100 years.

Written by — Md. Wasif Bin Hafiz .",2167
"Overview

In this article, you will learn how to make a basketball using Materials and Textures in Unity. This microconcept can be extended to create any kind of ball i.e football, tennis ball, snooker balls. That said, it is not just limited to creating balls. You should be able to use this concept to customize the look of any type of geometry(or mesh).

Here is what you will create👇🏻

Basketball🏀

Think of the basketball as a sphere wrapped around with a pretty paper.

Let’s get started.

Pre-requisites

Your sample scene should have —

A plane A sphere with a custom material

This tutorial is based on the techniques mentioned in an earlier one👇🏻.

Step 1: Create a folder that contains all the textures

To keep things organized, let’s create a folder that will contain all the textures.

Right-click in Assets Panel > Create > Folder > Name it “Textures”

Step 2: Download a texture

Since we need a texture for a basketball, simply download one online. A texture is just an image in .png or .jpg format. For now, you can download the basketball texture from here.

P.S — Make sure you have the appropriate license to use a texture that you download. The above ones are free to use.

Step 3: Drop the texture into your Unity project

Drag and drop the downloaded texture into the “Textures” folder that you created in Step 1 above.

Step 4: Apply the downloaded texture to the sphere

It is important that your sphere has a custom material in this step. Otherwise, you will not be able to view or modify the various properties of the Material.

Drag and drop the downloaded texture to the box at the left of the Albedo property.

Step 5: Make your basketball bounce

You can do this by following this tutorial that I wrote earlier👇🏻

That’s it!🎉

You successfully used Materials and Textures to create a basketball. Now, you can do the same for all your games or AR/VR experiences that you develop.

Different types of balls behave differently. Try experimenting around with the bounciness and tweaking the different parameters of the Rigid Body Component attached to the sphere to create a golf ball, a football, or a tennis ball.

Enjoy!👏🏻

This article was also published in FreeCodeCamp.",2203
"Apple, Google, Samsung Will Not Include Charging Adapters. What are the Best USB-C Chargers for Replacement? Anthony Oliva Just now·9 min read

Photo by SCREEN POST on Unsplash

According to the verge, following Apple and Samsung, Google has decided not to include a charging adapter with the Google Pixel 6. The cost-saving might be the most significant consideration in this decision, the report says. Google claims that most people have USB-C charger, which is no longer essential. If necessary, customers can purchase it on Google store for $35.

Since the iPhone 12, Apple has removed the charging adapter from the box because of environmental stewardship. In early 2021, Samsung also announced that the Galaxy S21 would not come with a charger. It seems like all these tycoons have a consensus on removing the in-box chargers and promoting environmental protection. Removing in-box chargers can reduce the cost and change customer’s habits. The box design and size can be streamlined, which increases the shipping amount with a lower cost. In addition, they can sell the charging adapter individually, which increases the profit of phone accessories. If you remember the day Apple changed the 3.5mm headphone jack to lighting, most people had opposing views. However, Apple provided an in-box adapter to maintain the use of traditional headphones. Many standard headphones have changed their habit during the accommodation period and started using Bluetooth headphones or Apple lightning headphones. Apple finally decided to stop providing in-box adapters after iPhone XS. But the public has already adopted the use of lightning ports unconsciously. This is only one of the classic examples of how Apple changing customer’s habits. Therefore, what does “removing in-box charger” really mean to manufactures and us? How will it change our using habits? I think they have the answer already.

On the other hand, environmental consideration is also another significate factor of not including the in-box charger. Samsung’s TM Roh believed that it could address the issue of sustainable consumption.

“We believe that the gradual removal of charger plugs and earphones from our in-box device packaging can help address sustainable consumption issues and remove any pressure that consumers may feel towards continually receiving unnecessary charger accessories with new phones.”

It is an effective way to address the excess problem and provide options to customers to purchase chargers based on their needs. When Lisa, Apple’s vice president of Environment, Policy and Social Initiatives, promised to be carbon neutral by 2030 in late 2020. It added the persuasiveness that the reason for not including the charging adapter is based on environmental protection. Sustainable development involves the economy, society, and environment. Their decision demonstrates their social responsibility of promoting sustainable development. Although there are some oppositions from the public and green groups because they believe this move only increases the sales of charging accessories and Apple should give up the use of lightning cable if they genuinely support environmental protection, it is clearly all these tycoons are working on it, which is excellent news to the world. Ecological protection is never a one-day effort. It needs to be well-planed and put into practice consistently. Let’s wait and see what’s their next move!

Photo by Daniel Romero on Unsplash

Although the charging adaptor is no longer included in the box, it doesn’t mean that the demand for the charging adaptors doesn’t exist. Customers still need a powerful charger to support their new electronic devices and reduce the charging time. Meanwhile, the competition of charging adaptors has already started in the phone accessories market. Different phone accessory manufactories are now targeting the charging adaptor market and fighting over Apple and Android users. In today’s world, everything relates to efficiency. When it applies to the charging adaptor, speed, effectiveness, and size are the primary measurements. Customers are looking for a compact, quality, and safe, fast charger for their electronic devices. Traditionally, Apple only provided a 5w charging adaptor and no fast charging. The customer might not notice how slow it is before they try a fast-charging adaptor. There is no return after owning a fast charger because the 5w charger no longer fulfills your satisfaction. People thought 3G networks were speedy before. But I believe no one would like to go back to the 3G network now because we love a convenient and efficient life and always look for improvement.

Photo by Onur Binay on Unsplash

What’s fast charging?

A charging adaptor usually indicates a “V/A.” “V” refers to the voltage, and “A” refers to the current. To multiply the voltage and current, you will get the wattage: the higher wattage, the faster the charging speed. In addition, fast charging also requires permission for fast charging protocols. The most common fast-charging protocols in the market include PD, PPS, QC, AFC, FCP, Apple, etc. Each protocol corresponds with different branding or model of electronic devices. If the 20w charging adaptor doesn’t have protocols, it can’t fast charge your devices. Meanwhile, people have to ensure the cable and the machines can both support the same wattage. Otherwise, fast charging wouldn’t work as well. Therefore, fast charging is a tripartite operation.

“Fast Charging can complete 0–50% charging in 30 minutes, which shortens the charging time rapidly.”

Fast charging is a new trend, and it will be accelerated while all these phone brands stop providing in-box chargers. We can foresee much higher wattage fast chargers with multiple functions in the future. The charger is no longer only supports phone devices but also laptops, tablets, and even large electric appliances. The technology of a charging adaptor will develop rapidly and bring us a new life experience.

How should we pick the right USB-C charger?

There are many USB-C chargers in the market, and here are some points we can keep a keen eye on, other than the wattage.

1. Branding

We should purchase a charger from trustworthy brands because of the quality guarantee, such as Anker, Ugreen, INIU, and Amoner. At the same time, they usually have better customer services to support the product’s issue and customer’s inquiry.

2. Compatibility

Before purchasing the charger, we must ensure that the fast charging function can support your phone model. Different phone models might have a different fast-charging protocol; even their brand is the same. Therefore, read the compatibility before your purchase.

3. Safety

The fast charger has high wattage to provide enough for your phone. But it might bring some safety issues, such as heating problems, short circuits, case melting. Therefore, we should look at the protection design of the charger before we place the order.

4. Design, weight & Size

A charger is no longer just a charger. It can be designed in different appearances and colors. The weight and size are also part of the consideration. It will be an extra point if a charger is well-designed with lightweight and mini size.

5. Price

The price is always a consideration while shopping. You can compare the price and function of different chargers and see which one has the best cost-effectiveness.

6. Warranty

A responsible branding should provide a warranty. The longer warranty they provide, the higher confidence they have in their products.

Top 5 USB-C Charger at Amazon

Source: INIU

INIU Safest 20W PD Fast Charging Wall Charger $13.99

Pro

l Fasting charging for both Apple and Android

l UL-V0 rated fire-retardant casing

l 3-years warranty

l Little green indicator

l Low Price

Con

l A bit bulky

l Unfoldable

INIU is an uprising brand, which provides a 3-years warranty on all their products. This charger is designed for the new iPhone and matching up the 20w fast charging capability. It can charge iPhone 12 up to 60% in 30 minutes. Although it is advertised as a fast-charging iPhone series, it also supports fast charging Android phones. The UL-V0 rated fire-retardant casing also improves the safety of using a high wattage fast charger, protecting both chargers and your electronic devices. The little green light also signals if the charger is working or not, which is user-friendly. This charger is only selling for 13.99 at Amazon, which is relatively cheap but of high quality.

Source: Anker

Anker 30W PIQ 3.0 USB-C Fast Charger Adapter $23.99

Pro

l Foldable plug

l Little blue indicator

l 18-month warranty

Con

l Not cheap

l A bit bulky

l Not enough power for MacBook Pro

Anker is a well-known brand in the phone accessory industry, and they produce quality products which the market and customers have approved. This charger is capable of iPhone, Android phones, and Apple Macbook, which is impressive. The 30w output can provide more than enough fast charging for phone accessories, but it’s a bit slow for MacBook charging. The foldable plug also saves space and minimizes the size of the charger. The little blue indicator is user-friendly as well. It’s $23.99 at Amazon now.

Source: INIU

INIU 25W PD 3.0 Fast Charging Mini Wall Charger [2 Pack] $15.99

Pro

l 2 Pack

l 25w Output

l Mini size

l 3-years warranty

l UL-V0 rated fire-retardant casing

l Low Price

Con

l Unfoldable

l Only Black color

This is another INIU 25w charging adaptor, which provides super-fast charging for Samsung phones and fast charging for all iPhone series plus iPad. It can fully recharge the iPad Pro in 2.2 hours. The size of the charger is also impressive, which is 48% smaller than the Samsung 25w charger. The 3-years warranty and UL-V0 rated fire-retardant casing provide comprehensive support and protection to you and the charger. You can get it for 15.99 at Amazon, and it’s 2 packs.

Source: UGreen

UGreen 20W USB C Wall Charger — PD Fast Charger [2-Pack] $19.99

Pro

l Mini size

l Foldable plug

l Intelligent chip

l 2 Pack

Con

l 20w only

l A bit heavy

l Not cheap

UGreen is also another well-known brand in the industry, which has excellent quality products. This charger is 50% smaller than the Apple 20w charger and provides max power output to the iPhone series. Recharging iPhone 12 from 0 to 58% within 30 minutes. The foldable plug is always a great design for the customer, which reduces the difficulty of carrying around. The intelligent chip also avoids the heating issue. You can get it for 19.99 (with a coupon) at Amazon.

Source: Anker

Anker 20W Fast Charger with Foldable Plug [2-Pack] $23.99

Pro

l Mini size

l Foldable plug

l Lightweight

l 18-month warranty

l 2 Pack

Con

l 20w only

l Not cheap

Another Anker 20w charger is capable with iPhone, iPad, Samsung phones, and other brands. It can charge iPhone 12 series to 50% in 25 minutes. The mini size and light weight make it easy to carry around, plus the foldable design reduces space usage. With the Anker quality and 18-month warranty, you don’t worry about any product issues. It’s $23.99 at Amazon.",11151
"Get to know VPN up close.

VPN is not foreign to our ears. We often use VPN for convenience and data security. But what is a VPN? VPN (Virtual Private Network) is a virtual private network or virtual private network to extend a private network on a public network and allow users to send and receive data on shared or public networks as if their computing devices were directly connected to the private network.

The VPNs available in the global market consist of free VPNs and paid VPNs. Paid VPNs further strengthen the security of our personal data. But did you know that high-class hackers with the best skills are able to hack data even though we have used a VPN. Therefore, these shortcomings and problems are used as business opportunities by VPN entrepreneurs. VPN continues to be improved in quality so that it further reduces the risk of data misuse when we make transactions with the highest risk such as financial transactions, personal data and matters related to government and state data. Paid VPNs guarantee data security and that can be strengthened by the existence of protective laws.

But do you know the history of VPNs?

The history of VPNs dates back to 1996, when a Microsoft employee named Gurdeep Singh-Pall started developing the Peer to Peer/PPTP Tunneling Protocol. In 1999, the specifications were published and over time, VPNs have grown more and more. VPN technology was originally developed only by and for large companies and organizations with business purposes not for general use purposes. This was done because companies needed a secure and private method to enable communication and file sharing between different offices, and make it easier for employees to access important files remotely without any risk of data theft.

How do VPNs work?

When we have successfully connected to the web with a VPN, our performance traffic on a network is encrypted by the VPN application on a computer or smartphone. The VPN goes to the ISP server, and then the VPN server. When it reaches the VPN server, the traffic is decrypted and forwarded to the wider web with an IP address that has been assigned to the VPN service provider rather than by region. It's different when browsing the internet without using a VPN.

The following are some types of VPN protocols: 1. Internet Protocol Security (IPsec)

IPsec has been created for use with IPv6. This protocol encrypts network traffic by encapsulating IP packets inside IPsec packets.

2. SSL/TLS: Transport Layer Security (SSL/TLS) can route all traffic on the network through a VPN connection.

3. SSH: Secure Shell (SSH) VPN uses tunneling to add security to intra-network links.

4. SSTP: Microsoft Secure Socket Tunneling Protocol (SSTP) uses Point-to-Point Protocol (PPP) tunnels to send traffic over SSL 3.0 channels.

What are the drawbacks of a VPN?

Each thing has advantages and disadvantages. The following are some of the disadvantages of a VPN:

1. VPN cannot block cookies

Cookies in large organizations such as Facebook, Twitter to Google can monitor the activities that everyone does on the internet.

2. Even though we have used a VPN we are not completely in incognito or anonymous mode.

The reason :

- VPNs can go offline suddenly or have a DNS leak opportunity, both of which can expose your data to snoopers, ISPs, and a country's government without warning.

- Even though by using a VPN, our ISP (Internet Service Provider) no longer has a copy of the browsing data, the VPN provider still has a copy. So that our privacy does not belong to us completely nor trust. There is a risk of data misuse.

3. Still vulnerable to Malware

Even if you use a VPN, it doesn't guarantee that your device is safe and free from malware. Even some malware has the ability to turn off the VPN without notification and destroy the VPN functionality.",3836
"Why I Fired Him

The problem started from day one, but our CTO cut him some slack. As he was new, we thought he would need some time to catch up with our project.

After three days, our CTO told me that something was not right about him. He could not do simple tasks. He did not understand the basics of our project. Even our junior programmer could do it easily.

So, we decided to give him more time.

After ten days, he was assigned a simple task with some junior developer. He couldn’t guide them and was getting uncomfortable in the office.

Junior programmers also complained about him to our CTO.

Then we started to realize he lied on his resume. We didn’t tell him this, but I think he understood.

Then after 15 days, I called him into my office and told him we had to let him go for some reason. I sensed that he actually felt relieved!

I felt bad for him and tried to help him

Though he lied and we could prove it easily but we didn’t because it was part of our fault in the hiring process and we didn’t want to disrespect him!

It was still hard for me to fire him. I tried to help him as much as possible I could. I offered him a junior developer position but it was not easy for him for self-respect. I understood that.

My CTO discussed in detail with him about his lackings and how he could improve his skills. He also gave him some good resources for improving his skills.

Now we have added another layer in the hiring process to avoid this type of event in the future.",1490
"3 Programmers Got Fired (Including Me) Due to a Single App Crash

Photo by Lucian Alexe on Unsplash

I am now doing my third job. In my previous two jobs, one time I got fired and another time I resigned.

But getting fired was a horrible experience for me. I cried the whole day. I never told anyone about that — my friends, my new colleagues. I felt so ashamed and humiliated that I made up some lie for all about why I left the last job.

I couldn’t even tell my parents because they would get very upset. I only shared it with my boyfriend. He was so supportive and helped me to get a new and better job.

Let’s get into the story.

Problems of a low-funded startup

I worked in a startup aged only one year. There were a total of four partners. They only had one angel investor and were looking for more investors. They mainly made enterprise solutions.

You know startups have many problems. One of the main problems is the funding problem. In the beginning, a startup has to do a lot of work but doesn’t have enough resources. It pays less but expects a lot of output from the developers.

I joined there in February 2019. After three months, I got promoted from intern to junior developer. In the internship period, I got only $100/month. I had no regrets about that because I needed job experience.

There was a total of five programmers. All of us had to do a lot of work. We had to do overtime at least four out of six days a week. But the company didn’t pay us for the overtime. They never even said thanks. They acted like we were supposed to work overtime.

This is a red flag for developers. I would suggest to all the developers that if you find that your company always pushes you to work overtime without extra benefits, make a plan to change your job.

Because the scenario probably won’t change.

Arrogant CTO gave us more tasks than we could do

The CTO gave us all the tasks for a whole week. He didn’t care how fast or how slowly we did them. But he always gave us tons of work that would be very hard for even senior programmers to finish in a week.

We had no senior programmers. All of us were junior programmers. There was no tester, no designer. We had to work a minimum of nine or ten hours a day, 54–60 hours/week. If you didn’t work, you would lose your job.

If anyone couldn’t do all the tasks within time, he would humiliate them in front of all the other developers. He was one of the partners, so we couldn’t complain about him to a higher authority.

If anyone came to the office five minutes late, he cut their pay to a half day’s salary. But no one got extra money when they had to work for one or two hours extra.

I missed two interviews because I couldn’t manage time to attend them and I was not in a position to take the risk of losing this job.

App crashed

Then one day the CEO took on a new project that needed to be done within one and a half months: a mobile app and a web platform for building a customized delivery platform.

The initial target was to build a prototype to show a potential investor in order to raise money. The CTO told us that it’s very hard to get an appointment with that investor, so we had to build it within one and a half months.

One and a half months would be a very tight schedule for any team. We were very depressed when we heard we must have it made within that short time. We knew we all would have to do a lot of overtime.

They chose three developers for this project, including me. One was a backend developer, one was a Flutter mobile developer, and the other one was a frontend web developer (me).

But we still did it within time. Of course, there were bugs. We told this to both the CTO and the CEO. They seemed bothered but didn’t say anything at first.

The app crashed on mobile when it was being demonstrated in front of investors. It crashed because of a text field. The requirement of that text field was for numbers, but the CEO gave numbers and characters.

For fast development, we used Firebase’s Cloud Firestore to keep the data. When the user pushed string data instead of number data in Firestore from the mobile frontend side, the app crashed.

The investment was rejected, and we were the scapegoats

Investors rejected the investment. In my judgment, of course, the first fault was the CTO’s. He should never have taken this project on in this very short timeline.

The second fault was the CEO’s. He didn’t even try the app once before presenting it to the investors. He should have taken proper preparation because every app has bugs. And if you develop a project within one and a half months without testing, it will have many.

Today’s young entrepreneurs might have many advantages but one major problem. That’s a lack of experience. If they had more, the working environment in a startup would be more productive.

However, after getting rejected by the investors, the CEO wanted a clear explanation from the CTO. As usual, he didn’t say that the timeline was the problem.

The CTO said we were the problem. So we were the scapegoats. ☹

Two months’ salary and we are gone

I was involved in front-end web development. I had no engagement in the mobile development side. Still, they fired me. They said the design was not good. I am not denying that. But I am not a designer, and they should have considered that.

According to the job agreement, the company had to notify us two months before firing us. As the CEO got too upset, he fired us immediately with two months salary.

I am thankful to that company because I had a two-month gap. In the meantime, I had applied to seven companies and found a job.

But I will never forget the shame. I know maybe I shouldn’t feel that way. But still, that memory haunts me down.

Last few words

No one is perfect. Employers, please try to understand that. I am not saying we were the best programmers, but you shouldn’t demand that much output with poor management, an inexperienced CTO, and low-paid junior developers.

Not all management is the same. I have faced and heard some great CEOs and CTOs. Somehow this was my worst job experience, and one of my worst life experiences.

And I will say to all the developers to please don’t do this kind of job where you have no respect, no value, and a lot of pressure. If I had stayed there six more months, I would have gotten two years behind in my career.",6362
"When we reflect on the many revolutions of the last 30 years, the access to resources that were formerly reserved for institutions, the elite and centralised powers has gradually been distributed to the people through various innovative technologies. Like the ocean, resources accumulate in mountainous centralised waves. With the help of technologies, the waves overflow and dissipate into wider expanses.

Here we reflect on past technological eras and the teams that ushered in revolutions.

6.

In 1984, the founding Apple team and a small advertising agency came together to create an Orwel-inspired Super Bowl advert to introduce a revolutionary personal computer for the everyday person.

A ‘way of life’ technology that even changed icons into art, an act that revolted against elitist constraints of IBM and the likes. A technology that made the computer, personal, the Macintosh.

Did you know that MP3 format was invented in the 1970s?

But only brought to life in 1998 by South Korean company Saehan with the world’s first MP3 Player, MPMan.

This inspired the heart of 19-year-old student, Shawn Fanning, to program his own community sharing software, he called it Napster.

Napster was famously pursued for allowing its users to trade music and videos for free without paying royalties to production companies and artists. However, the software’s concept revolutionised and led a new media sharing era which manifested in platforms such as Spotify, ITunes, SoundCloud, BandCamp and more.

Shawn later joined a startup company in the midst of a new social movement, a social movement that would revolutionise how we interact with each other today.

That movement was Facebook.

https://arubaito.io/wp-content/uploads/2021/09/mobile-mp3.mov

4.

In the mid 90s, to make a simple email account was a daunting task that could only be done and used from a computer. But in San Francisco.

A bored Apple employee, Sabeer Bhatia, left the tech giant and pursued an idea to create a browser-based email that was accessible from anywhere on any device around the world with access to the internet, he called it Hotmail.

Hotmail, by today’s standards, was one of the internet’s first viral sensations spurred on by its ingenious

‘Get your free email at Hotmail” button at the bottom of every email sent, which is now today’s ‘Share’ button. Hotmail received over 100 thousand subscribers in the first few weeks and was later bought by Microsoft for $400 million.

The Hotmail model is still copied today.

3.

In the year 1440, religion went viral thanks to a technology that we seldom think as a technology, a book.

A man from Germany, Johannes Gutenberg revolutionised the printing of words to paper with a new type of printing press. It differed from the Chinese and Korean ‘wood block printer’ because it used mechanics to press ink to moving paper, making it easy to print 20million volumes by 1500.

This led to a widespread golden-age of knowledge for the first time in history. Resulting in access to written resources and religious text such as the bible that was followed by the mass adoption of the Martin Luther “95 Theses” and a protestant reformation.

2.

Imagine a time when you had to send a cheque to pay for something online.

Horrifying, yes.

This was an era not too long ago, that was revolutionised by a virtual payments service that affected the way in which we buy things online to this day.

That service is called PayPal.

PayPal, originally named Confinity, was started in 1998 by Max Levchin, Peter Thiel, Luke Nosek and Ken Howrey.Elon Musk, a competitor at a virtual payments company X.com, joined and eventually became CEO.

PayPal gave un-banked peoples in developed to developing nations the ability to access online global payments, bringing prosperity to many.

95 per cent of people in the UK are now shopping online and most of them have probably paid for their goods via PayPal.

1.

If you have ever heard of the origin story of Facebook, then you surely would have heard of Cameron and Tyler Winklevoss. In popular media, the Winklevoss twins were portrayed as the privileged, blonde, athletic embodiment of the establishment in Mark Zuckerburg’s genius underdog narrative.

But how much do you really know about the revolutionary Winklevoss twins?

After the twins sought compensation in court for stolen intellectual property, the idea for Facebook, they eventually settled.

Facebook went on to build a siloed empire that covertly extracts data from its users to turn a profit.

Meanwhile, the Winklevoss twins pursued decentralisation of control and cut out gatekeepers through a platform they called Gemini.

Gemini was named after the twin star sign and NASA’s moon-bound mission Project Gemini, evoking the cutting edge technology and the “number go up” culture of the platform’s main service, cryptocurrency.

The Winklevoss twins say, “We’re all astronauts building on the frontier of money and the frontier of art and the frontier of finance. We feel like we’re on a spaceship, exploring a new frontier.”

Gemini grants the everyday person the level of returns that they should have been receiving by saving money in the bank, increasing returns from 0.01% to 8.05%, had centralised banks not taken the majority of the profit earned from reinvesting your savings.

Gemini’s exchange lets you buy and sell bitcoin and 40+ crypto assets. They even protect you from global inflation, political instability, theft, and unknown risks with their wallet app.

Recognising that the future is becoming more digital, the Winklevoss twins are helping people tie their real-life assets to the digital world through ‘Non-Fungible Tokens’ or NFTs.

The unprecedented empowerment of the individual and community stands before us.

‘Decentralised Technology’ through Blockchain & Cryptocurrency is a dot com shadowing era, that will radically transform and bring hope to the furthest nooks of our world.

Will you join the next revolutionary team?

Find a Career in Crypto, Join the Revolution Today. Register Free.",6057
"Whilst many prophets of the future, or “futurists”, as they are often called, preach a distinctly Martian future, our most ever-present celestial is often forgotten. The moon hangs there watching.

Despite being a stepping stone and metaphor for our future in space, many forget how crucial the role the moon will play in our future is. With 60s tech, we could reach it in under 3 days, whilst is still takes our most experimental (yet still feasible) advanced space propulsion approximately 3 months (Utilizing Nuclear Thermal Propulsion) to reach the crimson orb of Mars.

Ease of accessibility aside (We are built to fight adversity), the moon is incredibly rich in all kinds of minerals and precious materials due to the lack of a magnetosphere and atmosphere shielding it from radiation and asteroids.

In particular, there are two materials that we specifically want. The first is Helium 3, a particular isotope of Helium that can be fused with deuterium in a Nuclear Fusion plant. More info can be found here about extraction and usage.

Chang’e-1 map of the Moon’s Helium-3 inventory

As the old proverb goes,

“Nuclear fusion is 30 years off, and always will be.”

Recent advancements aside, it will be at least half a century before we see full scale commercial plants. Right now, and most likely for the next century, the main commercial enterprise of the moon will be water.

When first hearing this, many are confused and in dismay, but what you must understand is two things. The first is usage. H2O is obviously made of two compoments, Hydrogen and Oxygen. In a ratio of approximately 6:1 (ratios are given in oxidiser:fuel), they can be used to make perhaps the most basic rocket fuel, Hydrolox.

Hydrolox is not very dense, and is therefore rarely used in launch stages, whilst it has had some minor prevalence in second/third stages due to the actual size of the craft not being a major problem in space. Luckily, that’s exactly where we intend to use it. The fuel can be flown from the low gravitational pull of the moon to orbit using part of it for propulsion, allowing repeated reuse. The fuel can then either stay in lunar orbit or move to earth orbit.

The second thing that must be understood is how cost effective this is. Due to the reusability and self containment (because it uses its own produced fuel) of the system, the first purchase and flight to the moon is the only cost. This is vastly cheaper and superior to flying fuel in the cargo of an earth launched rocket repeatedly.

Credit: NASA

But How Will This Affect Earth?

With most likely more than one provider of orbital fuel, there will be a surplus that will be guaranteed to be used once companies build their systems to utilise this service. This will mean that much less will need to be spent on large spaceships, as only being able to get to orbit, and then continuing with a full refuel will be all that is necessary in almost all crafts. This will reduce the overall cost of spaceflight to the moon, mars, and outer planets with substantial payloads by a large amount.

Recently, in the past few years, especially utilising companies such as SpaceX, many private satellite companies have paid for their tech to be placed in earth orbit. With the price being similar to send to other planets, and with companies having basically free reign over the land, the moon could become an industrial zone of gigantic magnitude in less than a century.

What did you think? This is the first entry of a multi part series, so stay tuned, and support me by following @exzie9 on Twitter, thanks!",3578
"Inspiration4 mission, which is the world’s first all-civilian mission to Earth’s orbit, will carry four private citizens to space and aims to raise funds for St. Jude Children’s Research Hospital.

Crew of the Inpiration4 (Image credit: Inspiration4)

On September 15, four civilians will blast off into space as part of the Inspiration4 mission of SpaceX and NASA.

The mission, called Inspiration4, includes four private citizens who will fly on a Crew Dragon spacecraft for an Earth-orbiting mission.

Inspiration4 is the third spaceflight by a billionaire in 2021. The other two — both suborbital missions — were the flight of Virgin Galactic founder Richard Branson and company employees aboard the Unity 22 mission on July 11, and the flight of Blue Origin founder Jeff Bezos and three other passengers (including noted aviator Wally Funk) flew aboard a New Shepard spacecraft on July 20.

Like these other two flights, Inspiration4 is largely made up of civilians with no professional space experience, although the crew has undergone basic training to get a sense of what to expect. But this time, the crew will spend three days orbiting the Earth, as opposed to the brief suborbital flights of Bezos and Branson. Learn more about the flight below.

Billionaire Jared Issacman, the founder of Shift4 Payments, purchased the flight as part of an effort to raise millions for St. Jude Children’s Research Hospital. He is joined by Haley Arceneaux, Sian Proctor, and Chris Sembroski.

Meet the crew

Jared Isaacman (“Leadership”), 37, Shift4 Payments founder and CEO. Isaacman also has roughly 6,000 hours accumulated as a private pilot. Isaacman had a lifelong dream of going to space and in media interviews, said he wanted to do so while donating other seats to deserving people. He will serve as the flight’s commander.

Jared Isaacman(Imgae credit: Fastcompany)

Hayley Arceneaux (“Hope”), 29, is a St. Jude physician’s assistant and childhood bone cancer survivor from Louisiana (who was also treated at St. Jude as a child). She was selected to represent the charity for which Isaacman plans to raise money. She will be the first person to fly in space with a prosthetic limb. Isaacman personally invited Arcenaux to join the mission as its chief medical officer.

Hayley Arceneaux(Image credit: New York Times)

Chris Sembroski (“Generosity”), 42, is a data engineer for Lockheed Martin from North Carolina. Sembroski is a long-time space enthusiast with amateur experience as an astronomer and a rocketeer. He is a former camp counselor at Space Camp and like many astronauts, is a veteran of the United States Air Force. Sembroski is the winner of a sweepstakes held by Isaacman to raise money for St. Jude. He will serve as a mission specialist.

Chris Sembroski(Image credit Wikibio)

Sian Proctor (“Prosperity”), 51, is a geoscientist and science communication specialist who has participated in four space analog missions. Proctor was chosen as the winner of the Shift4Shop competition from Isaacman, which asked entrants to set up an e-commerce site and record a video about their business. Proctor’s “Space2Inspire” shop offered postcards and prints of her AfronautSpace art, to spark conversations about women of color in the space industry. Proctor will be the first person from Guam to fly in space, and she will serve as the mission pilot.

Sian Proctor(Image credit: Wikipedia)

Fundraising efforts

The Inspiration4 mission is part of an effort to raise $200 million for St. Jude Children’s Research Hospital. Isaacman didn’t want just another billionaire spaceflight on the books, he wanted the mission to really mean something, so he led a campaign to raise money and awareness for childhood cancer research.

Isaacman donated $100 million directly to St. Jude, as well as two of the other seats on the Dragon spacecraft. One went to Arceneaux, whom the hospital selected as a frontline worker. The other seat was designated to a winner randomly chosen from sweepstakes entries that raised $13 million for the cancer institute. Sembroski was selected after a friend of his (who was technically chosen) decided not to go to space.

Proctor won her seat through a “Shark Tank”-like contest that set up online stores using Isaacman’s Shift4 Payments platform as a way to raise money for St. Jude. Each contestant had to set up a shop and campaign on Twitter. The more interaction their videos received on Twitter, the more likely they were to make it to the finals.

Which Rocket And Spacecraft Will Be Used For Inspiration4 Mission?

The Crew Dragon spacecraft will be launched atop SpaceX’s Falcon 9 rocket, which is a reusable, two-stage rocket, designed for the reliable and safe transport of people and payloads into Earth and beyond. This is the world’s first orbital-class reusable rocket, which means it is a reusable rocket powerful enough to launch objects into orbit around Earth, or even beyond, depending upon the size of the payload.

SpaceX crew dragon on top of Falon 9 rocket

The Dragon spacecraft, which will carry the crew, can ferry up to seven passengers to and from Earth orbit, and beyond. In May 2020, it became the first private spacecraft to take humans to the International Space Station.

Crew Dragon will travel at more than 17,000 miles per hour, and the astronauts will conduct experiments to expand our knowledge of the universe. The cargo consists of crew essentials as well as scientific equipment designed for microgravity research and experimentation, which together weigh 365 pounds.

The design of the dome window of the Crew Dragon Spacecraft is inspired by the Cupola, which is an observatory module built by the European Space Agency, on the International Space Station.

SpaceX dragon with the glass cupola in orbit

What will the crew be doing?

Because the Inspiration4 mission consists entirely of regular people, not professional astronauts, they have not undergone the same rigorous medical disqualification process that NASA and other agencies follow during astronaut selection. So throughout the mission, the Inspiration4 crew members will perform a variety of medical experiments and record health data to support future human spaceflights.

Inpiration4 crew

What Is The Purpose Of The Mission?

Inspiration4 is meant to conduct valuable research, and inspire projects that otherwise can’t overcome the high barriers of traditional space-based research, by providing access to space.

By raising funds and awareness for St. Jude Children’s Research Hospital, the mission believes, the crew will represent a new era for human spaceflight and exploration.

What will they take with them?

Since Inspiration4 is part of a massive fundraising effort, the crew will be taking items up with them to be auctioned off, as well as personal items.

Some of those items include mission jackets featuring artwork made by St. Jude patients, a Fisher Apollo 11 50th Anniversary Space Pen and Coin Set containing a piece of material from Apollo 11, a ukulele from Martin Guitar that Chris Sembroski will play in space, and much more, according to a mission statement.

In addition, Arceneaux will play a non-fungible token, or NFT, of a never-before-released performance by the band Kings of Leon during the flight. Another NFT onboard will replicate a piece of art that previously went to the Mariana Trench, making it the first artwork to reach both the deepest part of the ocean and orbit.

What will the crew be doing?

Because the Inspiration4 mission consists entirely of regular people, not professional astronauts, they have not undergone the same rigorous medical disqualification process that NASA and other agencies follow during astronaut selection. So throughout the mission, the Inspiration4 crew members will perform a variety of medical experiments and record health data to support future human spaceflights.

Mission timeline

A detailed timeline for mission activities has yet to be released on the Inspiration4 website, but we do have some clarity about when the official launch time will be figured out, and backup opportunities.

The launch time of Sept. 15 opens at 8 p.m. EDT (or 1200 GMT Sept. 16) and will last for 24 hours, according to Inspiration4 mission updates.

“Approximately three days before liftoff, SpaceX will narrow down the launch window to five hours based upon weather conditions at the launch site, along the ascent corridor, and at landing locations off the coasts of Florida for a safe return of the crew and splashdown a few days later,” a Sept. 3 mission update stated.

The splashdown is scheduled three days after the launch, and as with previous Crew Dragon flights, the spacecraft will splash down off the coast of Florida so that the crew and science samples can be swiftly and easily returned to the NASA Kennedy Space Center.

The live event will be streamed on SpaceX YouTube channel",8902
"Yes, you heard that right! SpaceX has a new mini-bakery! But it’s not feeding all the hungry SpaceX engineers working on the Starship at their Boca-Chica site. Instead, they make their heat shield tiles here.

SpaceX Van outside of the mini-bakery in Florida

As Starship prepares for its first orbital flight, the thermal protection system will play a crucial part in making the mission a success.

The SpaceX factory which makes the tiles have been widely rumored around on the internet for many years. But in 2019 we started seeing some SpaceX vans outside, something that looked more like a warehouse in Cape Canaveral, Florida. But oh boy, was it a normal warehouse, in May 2020 there was a site inspection that gave us a much closer look at the new SpaceX facility.

It’s said to have 20 employees at the time of inspection, they run 24 hours a day and they work 7-days a week, in 3 shifts. The facility is said to be about 40,000 Sq.ft in size. The number of employees at the time of writing this blog must have surly gone up as the SpaceX Starship prepares for its first orbital test flight.

Space Shuttle heat shield tiles

So now that we know where they are made, let’s talk about how they are made. The new SpaceX heat shield tiles are very similar to that of NASA’s Space Shuttle’s thermal protection system. They need to be able to withstand very high temperatures during the re-entry. For this, they need to have low thermal conductivity and high specific heat capacity and melting point. Elon Musk has mentioned that the tiles are made out of Silicon and Aluminum Oxide. The tiles are 90% air and 10% Silica and they are a bit like hard form. That’s because Air has a low thermal conductivity and high specific heat capacity. This is very similar to those of the Space Shuttle.

1/2 of the new SN20 is covered with these tiles.

SpaceX SN20 Covered in the new heat shield tiles

If we take a closer look at these tiles, we can see that they are labeled with red and green stickers.

Red and green labels on SN20

Closer look at labeled tiles

The tiles with red ones are found to have been broken or damaged somehow during the inspection. The ones labeled in green are found to be misaligned during the fitting.

Now, this might be one of the very first and very important problems for SpaceX to solve. It’s not as simple as baking a few foams like tiles and sticking them on the Starship. For the Starship to be fully reusable, it needs to avoid such inspections. To understand this better, SpaceX plans on reusing the Starship at least three times a day. NASA’s Space Shuttle had a similar technology and it took literal months for inspection and maintenance between launches. The main reason for this is that the Space Shuttle had a much more complex shape than the SpaceX Starship.

NASA’s Space Shuttle

Thermal protection system(heat shield tiles) on the Space Shuttle

The Space Shuttle had many different shapes of tiles, and during the launch ice would fall from the main tank hit these tiles thus, damaging them. SpaceX has about 15,000 tiles compared to 20,000 of the Space Shuttle. The Space Shuttles tiles were glued in place, but SpaceX uses a red robot to weld the mounting pins onto the body of the Starship and a person just comes along and gives it a nice push into place. The reason they choose the hexagon shape is that if they were to go with for example a square shape. then the heat would go between the tiles and the body of the Starship would be exposed to the Starship. The thermal protection on Starship is much more simpler and efficient than compared to that of the Space Shuttle.

On June 7th 2021 a Boca Chica watcher and Twitter user @StarshipGazer took some really good photos of a few shipments from the so-called “mini-bakery”.

One of them was a wooden crate and was labeled incoming mini-bakery. This could mean that they are moving the mini-bakery, near the production site. Again this cannot be confirmed yet, since it could be some more tiles from Florida as well.

What do you think about new SpaceX ‘mini-bakery’? Let me know in the comment section below!",4113
"Since the beginning of time, we humans have always been explorers. From simple discoveries like fire to the transition from the stone age to the iron age to the age of technology today, everything was created by our need to explore. We need to challenge the norm.

We are built to grow, to dream big, to challenge everything around us. Only by exploring can we understand everything we are missing and work towards achieving it. We need to explore further. Farther. Smaller?

Now we are in a stage where exploring farther means outside our planet. We have already put a man on the moon, but why should we stop there? Why can’t we put more astronauts on the moon?

Dare I even say, on other planets like Mars? Is any of this even possible?

The answer 10 years ago would have been no. The answer today would be no as well. The predicted answer in 5 years is probably still a no.

Elon Musk’s aspirational timeline for SpaceX to put a human being on Mars is in 2025. He plans on using a truly massive (387 ft) rocket + booster combo and launching it to Mars. It's called Starship (previously BFR), click here to learn more about Elon’s plan for SpaceX and commercial spaceflight to Mars.

SpaceX Starship. Source

But all of that can change with the concept I am going to be explaining today. Turns out, nanotechnology is the key to making this a reality.

Before I get ahead of myself let's walk through why traditional rockets just aren’t cutting it.

Issues with Traditional Rockets

Chemical (traditional) rockets have been used to do some crazy things, including putting the first man on the moon, but we need to dream bigger.

The overall method that chemical rockets use is often referred to as the ‘brute force’ method because the oversimplified principle is to create a continuous explosion that generates enough thrust to break Earth’s gravitational pull and enter orbit.

Check out the chemical rockets section on the article below to dive deeper into exactly what are the issues with chemical rockets and why do they exist.

TLDR:

Method: Chemical rockets have to carry up all their fuel and oxygen for the combustion to occur, which is just added weight and cost

Efficiency: These rockets only have a fuel efficiency of 30% because of their combustion system. The specific impulse of a new chemical propulsion system can range from 300–500 SI (if you don’t know what specific impulse, read that section on the article above).

Cost: It costs around $20,000 to send 1 kg of cargo to space and each space shuttle launch can range from around $50– $400 million.

Size and Mass: For every bit of added mass, we need to carry more fuel and oxygen, which then in turn also increases the mass, which then requires more fuel and so on. This is called the Tyranny of the Rocket Equation.

What about other forms of propulsion?

For the last several decades, to solve the tyranny of the rocket equation, scientists have turned to other forms of propulsion.

Ion Engine. Source

Innovative methods that have been developed are nuclear propulsion, ion thrusters, solar sails, even something as crazy as antimatter propulsion (by the way, I cover each of these in detail in my other article here).

Nuclear Propulsion Rocket. Source

But these new forms of propulsion, don’t solve the initial problem of the rocket equation, they just tackle it from a slightly different angle. To completely change the approach, let's first understand how rockets get to different places in space.

How rockets reach orbit and beyond

The first step in getting rockets to orbit is breaking Earth’s gravitational pull. For this to happen, the spacecraft needs to be launched and maintain a speed of over 40,000 km/h!

Once it reaches its desired height, the rocket then rotates horizontally and enters the orbit around the Earth, all while maintaining its speed of over 40,000 km/h.

The way rockets and satellites stay in Earth’s orbit is by balancing their forward velocity with the gravitational force of the Earth and the curvature of the Earth.

Think about it like this. If you were to shoot a cannonball with its normal force, it would travel for a very little bit and then fall back down to Earth (Scenario A). If you increase the force to a very specific amount, the forward velocity of the cannonball will match the curvature of the Earth and instead of falling back down, it will enter Earth’s orbit (Scenario B). Lastly, if you increase the force too much, the forward velocity of the cannonball would just fly straight ahead (Scenario C.)

To calculate the specific velocity required, we use the following formula, where v is the orbital velocity, g is the gravitational constant, m is the mass of the planet and r is the radius of the orbit (distance from the rocket/satellite to the centre of the planet).

The purpose behind that explanation was to explain how traditional rockets use the orbits of Earth or other planets to propel them towards their destination to increase speed and save rocket fuel. This is called gravity assist.

Demonstration of Voyager 1 using Gravity Assist. Source

Now, what if we ignored the complicated trajectory calculations and just built a machine that takes us straight up, straight towards our destination. Sort of like an… elevator?

An elevator to… Space?

Precisely.

The space elevator is a concept that has been around since 1969 when Jerome Pearson conceived the concept at the NASA Ames Research Center.

The fundamental principle of a space elevator is an elevator with one end attached to Earth and the other end in space.

Diagram of a Space Elevator Concept. Source

To keep this article short and sweet and focus on the technical nanotechnology aspect, I highly recommend watching the Kurzgesagt video below which explains it beautifully.

TL;DR

The space elevator has 4 main parts:

Anchor: a material attaching the tether to the Earth Tether: a cable that extends thousands of kilometres into space, only bound to Earth by one end Counterweight: a weight on the space end of the tether with enough force to keep the tether held tight Climber: similar to a traditional elevator car, a machine that would travel up the tether and release the load

Almost there…well, not really

While space elevators have a variety of logistical issues mostly around the production and deployment of the mechanism, by far the most complicated challenge to overcome is the manufacturing of the tether. The tether must extend over 42,000 km into space while also being light, flexible, affordable and durable enough to withstand the harsh conditions of outer space. This includes dangers like radiation, space debris and asteroids.

However, scientists are optimistic that these smaller issues can be dealt with shortly, as long as the production of the tether is solved.

The material for the tether needs to have a tensile strength (force needed to pull apart) of at least 63 gigapascals (GPa) or 63,000 MPa, whereas steel has an ultimate tensile strength of 400–500 MPa. Not even remotely close to what is required.

Graph showing the stress-strain relationship of steel. Source

Nanotechnology to the rescue!

Right off the bat, scientists and researchers turned to “the wonder material”: graphene. More specifically, carbon nanotubes, which are just rolled up sheets of graphene.

Instead of finding a material 200 times stronger than steel to increase the tensile strength, we can turn to much less dense materials, which decreases the amount of tensile strength it will have to endure.

Single-crystal graphene has a tensile strength of 130 GPa (130,000 MPa). One of the many reasons it is the strongest material ever tested. Single-walled carbon nanotubes have a similar tensile strength of 100–200 GPa (100,000–200,000 MPa).

Single-crystal graphene (AKA crystalline) and other materials are when the formation of the crystals is even and structured throughout, without any grain boundaries (2D defects in polycrystalline materials in between sections of the material — signified by the gray lines).

Before we dive any deeper, I just want to clarify that graphene itself will not be used for the tether, neither will raw carbon nanotubes. Instead, scientists will develop a fibre made out of several layers of carbon nanotubes.

Carbon Nanotubes

Carbon nanotubes can be fabricated in 1 of 3 different configurations: Armchair (default), Zigzag and Chiral. The difference between each of them is in the orientation of the graphene when it is “rolled”.

Scientists tested over 16 different variations of the 3 configurations to come to the conclusion that the armchair configuration had the highest tensile strength of over 60 GPa. But that is not nearly what the production result is.

Depiction of the tensile strengths of the armchair configuration. Source

Due to the nature of carbon nanotubes, one single misaligned atom turns two of the congruent hexagons into a pentagon and a heptagon, decreases the ideal tensile strength from 100 GPa down to 40 GPa, which is unable to fulfill the role of the tether for the space elevator.

Structure of a carbon nanotube. Source

In other words, out of a potential 42,000 km long fibre of carbon nanotubes, not even a single misaligned atom is allowed. Every misaligned atom creates a week point in the tub, which snaps the normally extremely strong bonds between the atoms. This causes a chain reaction of the subsequent bonds to break as well.

Carbon nanotube manufacturing methods are either very expensive, time-consuming and produce very little quantity for very high-quality nanotubes. Due to mass fabrication methods being heavily flawed, using carbon nanotubes is currently not a viable option.

One method of carbon nanotube production. Source

You either choose quality or quantity for nanotubes and in our case, we need both.

While carbon nanotubes are definitely not the only nanomaterial which exhibits extraordinary properties, it is the only one that has been successfully produced with a tensile strength of over 100 GPa. The next best materials had tensile strengths of around 20 GPa, only 40% of the efficacy required for the tether.

The Kurzasagt video above mentioned another possible material, diamond nanothreads. Diamonds are actually made from carbon atoms as well, just arranged in a specific 3D lattice.

Diamond nanothreads are essentially 3D graphene. In standard graphene, each carbon atom is bonded extremely strongly to 3 other carbon atoms, constructing the 2D lattice. In the nanothreads, each carbon atom is bonded extremely strongly to 4 other carbon atoms, creating a 3D shape. Watch the video below to learn more.

These nanothreads have been proposed back in 2015 as a stronger, stiffer and more resilient upgrade of carbon nanotubes. However, the same obstacles occur. To start off, to the best of my knowledge, scientists haven’t even produced these diamond nanothreads yet but rather constructed it in molecular simulations. There is a long path ahead for the mass production of these diamond nanothreads, especially for space elevators. On top of that, the same issues with misaligned atoms and the efficacy of current fabrication techniques result in the possibility of using these turning into a pipe dream.

Is it possible? By when?

The production of the tether is only one of the many obstacles that space elevators including:

The deployment of the tether

Managing the forces of Earth and its rotation

The power source of the climber

Anchoring the tether to the Earth

Safety of the entire elevator and the passengers

Hundreds of thousands of smaller logistical problems

and of course, the price! Each scientist has developed their own variation concept of a space elevator, the optimal materials, the deployment technique and more, so each price estimate will be different. As of right now, the estimated cost to fully build and deploy a space elevator is between $8 billion and $90 billion! While that might seem like a lot, keep in mind that the cost of the International Space Station was $150 billion.

The Obayashi Corp in Tokyo has announced that it plans to build a fully functioning space elevator by 2050. Following that, China announced its goal to build one by 2045.",12218
"Bitcoin wallet — how to choose?

With plenty of Bitcoin wallets in the market, it feels quite hard to choose one. Each one says it’s the best; often they have similar features. Looking up user reviews is a good idea, but it only leaves a general impression and doesn’t tell if the wallet will be comfortable specifically for you.

To choose the best Bitcoin wallet, answer the following questions: do you need a cross-platform, a desktop-only, or a mobile-only wallet? Do you need ultimate security, or is usability more important? Are you ready to pay money to buy the wallet?

Let’s overview the most common types of Bitcoin wallets and see how they suit the needs of different users.

Bitcoin hardware wallets — ultimate security

These wallets look like flashcards, and they store your private keys inside. Hardware wallets are also called cold wallets because they store your keys out of the internet’s reach — this reduces the risk that your keys can be stolen in an attack.

The most popular Bitcoin cold wallets are Ledger and Trezor. Hardware wallets are considered the most secure ones on the market, so use them if you have considerate stashes of Bitcoin and it’s critically important to preserve them. Hardware wallets cost about $80–100.

To sign any transaction, you’d need to insert your hardware wallet into the computer. This makes cold wallets inconvenient for those who make many transactions daily and especially from their cell phones. Also, it’s not recommended to have a cold wallet for those who are prone to losing small things.

Bitcoin desktop wallets — enhanced usability

Desktop wallets are hot wallets — they are connected to the internet. This doesn’t mean wallets have access to your funds: they only generate private keys that are further stored on your computer. Desktop wallets allow accessing your crypto whenever you’re at your computer.

One of the prominent Bitcoin wallets is Electrum. It was created in 2011 by Bitcoin enthusiasts and hasn’t changed much ever since; as an old open-source wallet, it’s considered one of the most secure desktop wallets. As for the cons, the user interface is not perfect here, and it’s a Bitcoin-only wallet. However, it has a number of advanced security features such as 2FA, cold storage options, and on.

Another trusted Bitcoin wallet is Atomic. It allows for storing 500+ crypto assets. It also has a mobile version, so we’ll take a look at it in the next paragraph.

Bitcoin Mobile Wallets — accessibility above all

For a Bitcoin mobile wallet, the ease of use is a key factor. Here, Atomic Wallet stands out — a multi-asset wallet with a built-in exchange that allows you to swap your Bitcoin to other cryptos instantly without leaving the app. In Atomic, you can buy Bitcoin with a credit card, monitor the Bitcoin price chart, and set the network fee manually in case you want your transaction to be delivered faster or cheaper. The same features are available for the Atomic desktop version, so you’ll have access to your coins basically everywhere. Atomic is available for Android and iOS.

Cutting it short

If security is your top priority, choose a hardware Bitcoin wallet such as Ledger or Trezor. If you want to have access to your crypto from your desktop, use Electrum. If you are keen on usability and want to store all your crypto portfolio in one place, choose Atomic Wallet.",3372
"Plug, a user-friendly browser extension that allows users to conveniently access and manage ICP utility tokens and cycles, announced on Twitter that it has eclipsed a 100,000-user milestone. Similarly to how MetaMask is an extension for interacting with Ethereum, Plug is both a wallet as well as an authentication provider service for logging into Internet Computer dapps with one click.

Plug was built by the team behind Fleek, a Web 3.0 venture production studio building open internet services on the Internet Computer. The tool facilitated the recent launch of ICPunks NFTs. In a matter of minutes, 10,000 of the clown NFTs were scooped up by an influx of users who collectively stress-tested the Internet Computer blockchain. Plug’s integration with the Entrepot NFT marketplace by Toniq Labs also contributed to the spike in users.

“Plug grows in conjunction with the dapps that integrate plug,” said Fleek co-founder and CEO Harrison Hines. “So Plug growth is a reflection of the ecosystem as a whole growing.”

The product’s user growth milestone comes three months following its Alpha launch, when an Alpha V0.1.0 that included ICP token features became available for download and installation via GitHub. Visit the Plug wallet website to learn more.

“Reaching that milestone so quickly just means that we will be putting more emphasis and resources on Plug going forward,” Hines added. “It means a mobile app version of Plug will come out sooner than previously anticipated (now currently planned for late October/November launch), and a bunch of new features will be added to Plug to meet user feedback/demand.”

The extension’s entire codebase, including the Plug Controller and the Plug Inpage Provider that power the product’s features as well as its interactions with the Internet Computer, is open source and available on GitHub. Plug doesn’t require a user’s personal information to function, nor does it collect any from users.

In addition to Plug, Fleek has released other products for the Internet Computer ecosystem:

Dank — ​​A Canister-based service running on the Internet Computer that enables users to hold a balance of cycles associated with their principal identifier.

Cycles Faucet — Launched in partnership with the DFINITY Foundation, the Cycles Faucet offers between $20-$100 worth of free cycles to qualified developers to help them get started building and deploying Canisters on the Internet Computer.

In a recent DFINITY Foundation Community Conversations talk, Fleek founder Harrison Hines announced that Fleek has acquired a gaming studio. He also revealed that the studio has raised a fund to invest in Internet Computer projects — and noted that the team has already started doing so in stealth.

You can read a Q&A with the Fleek team here, as well as a Reddit AMA about Fleek here.

_____

Start building at sdk.dfinity.org and join the developer community at forum.dfinity.org.",2927
"Context

This post is a continuation of previous post . In that post I shared how in an active whatsapp group, suddenly conversation died and reached to zero engagement. Its almost 2 weeks to that event and traffic on that group is still not any where near to earlier level.

What caused this ?

Its a complex problem. There are many reasons which leads to such situations, where a group of adults stop talking with each other. Much of it is do with social conditioning. but there is a technological aspect to this problem ,which I want to write about in this post,

(I do believe s/w can help in social aspect as well, but that discussion is not for this post).

Kitchen Knife vs Social Media

Kitchen knife and social media share some common traits.

Let’s start with kitchen knife .

We all use kitchen knife. Its a tool which we use day in and day out.Something which we can not deal without. Its a necessity. Kitchen knife I use looks something like this. (image taken from google).

I also have a 9 yr old daughter. Few years back when she was a toddler, she expressed interest in the cutting vegetables. Now at her age at that time(~3/4 yrs), the knife shown above was not a good tool for her.

So my wife bought an age appropriate knife for her. Something like following (image taken from amazon). She was happy and safe.

After 1 (or 2 years ) she got an upgrade. But still not something similar to the knife we were using, but quite close. (image from amazon)

So she is slowly preparing her self for graduating to use a bigger and real kitchen knife.

While this transition was going on, she also observed something else. She witnessed grown ups cutting their finger while chopping onion or lettuce.

(image taken form a google search)

She witnessed injury and pain caused by knife and understood what it means when someone says “handle with care”.

Whats the point ?

Point I am trying to make is that by gradually adopting the tool and witnessing side effects of improper use of knife, she is better prepared for using a real kitchen knife. She will for sure have her share of accidents, but they will be less traumatic and wont influence her mood much.

What Social media has to do with it?

Most of social media users (specially the one in whatsapp group I mentioned above) are like small kids with a big and sharp kitchen knife. This is specially true in countries like India, where many people start their internet use with social media.

Most of them do not know all the features of apps they are using. They learn them while using it. Back to kitchen knife analogy, if I give a 3 yr old a big knife , for sure they will injure them self. Not only they will injure themself, they might injure sibling.

They also dont know about impact of their actions. They havent seem anyone getting impacted in -ve way due to social media. As a matter of fact , they do not realize they are getting impacted. When they understand , sometimes its too late.

They end up having traumatic experience on social media. Many of them start blaming social media for it. Which is true to some extent, but problem is more deep inside the user learning habits (thats my opinion).

What we can do ?

I will write more about this in my next post. But I hope this post helps you understand that, social media is like a knife. You need to gradually master it, other wise it will impact you and your friends. Often in a way, you may not appreciate.

Thanks for reading!",3459
"I believe that humans are more biased than algorithms. For algorithms, their bias depends on the rules it was programmed with to make decisions and the data set they were trained with. On the other hand, we humans have a more complex decision-making approach because of our System 1, automatic but susceptible to bias, and System 2, takes conscious effort but is analytical and less prone to bias (Douglass). Because of this, algorithm behavior can be more flexible when we adjust it towards a more controllable and favorable level of performance to solve problems, compared to human behavior.

Human bias is part of the technology humans create. In 2015, Amazon realized that their AI recruitment tool was biased against women when showing the top tech job candidates (Dastin). This was mainly because the training data set provided to the AI recruitment tool was based on a male-dominant 10-year resume pool. In the past, human resources have perceived men to be rightfully dominant to land tech jobs such as IT and software development (Pot’Vin-Gorman 23). Although the AI’s biased results were unintentionally programmed, the prevalence of men candidates in the results reflected the human bias that tech jobs are often applied for by men.

After Amazon’s attempts in revising the algorithm to be more gender-neutral, Amazon halted the AI recruitment tool by the start of 2017 “because executives lost hope for the project” (Dastin). Perhaps the loss of hope was not due to not being able to solve the bias of the AI tool, but solving human bias seemed impossible. Diving deeper into the issue of gender discrimination in the workplace, researchers have looked into two types of gender discrimination: statistical discrimination, the perceived average gender group differences in abilities or skills, and taste-based discrimination, driven by stereotypes, favouritism, and bias (Gerdeman). Statistical discrimination shows that when we know little about an individual, we tend to categorize them to a certain group. If individuals belong to a group that is perceived to perform worse than the other group, it creates a bias against them. Between the two, statistical discrimination is much harder to root out (Gerdeman).

Despite the unsuccessful attempts of creating a bias-free AI recruitment tool, I believe Amazon should continue to develop it. According to Moore’s Law, computers can remarkably grow faster over time and become exponentially more powerful (McAfee and Brynjolfsson 35); thus, if the AI recruitment tool becomes more fine-tuned to fairly and reliably recommend candidates, it can significantly minimize hiring costs and hiring issues prone to humans. Using resumè metrics, such as keywords like “women’s chess club captain”, can categorize candidates into groups. An algorithm would then use keywords to decide if the candidate belongs to the perceived inferior group or the superior group. If this would be the main basis for determining how qualified a candidate is for a job position, it can lead to statistical discrimination. To minimize bias, Amazon can implement AI in its recruitment process by adapting Google’s hiring approach (McAfee and Brynjolfsson 57). Instead of resumès, Amazon can use a standardized questionnaire and a consistent rubric for hiring. In replacement of a human interviewer, the AI recruitment tool can indicate how the candidate did on the hiring process based on measurable and comparable results. By understanding better the contributing factors to human bias, we can use methods to eliminate or at least control them better through algorithms that will reduce the bias that exists in these human-created technologies to create faster and more effective solutions.

References:",3735
"What’s up guys! Today’s blog is on the Top Five Best Budget Sound Bars In 2021. Through extensive research I’ve put together a list of options that all meet the needs of different types of buyers. So whether it’s price performance or its particular use. Let’s get started if you’re looking not just for any budget item but the cheapest of the cheap. The Absolute Lowest price that will still deliver the goods and cover all the key points.

The best ultra cheap budget sound bar available on the market in 2021 switching from TV to cinema is now possible on the budget with the TCL TS 6110 sound bar. A slim speaker designed to enhance your home theater given on this price range. This product boasts a fairly powerful sonic output, the device is equipped with a wireless Subwoofer that is as practical as it is efficient. The woofer is a real plus because it emphasizes the base to deliver the intensity of the action as faithfully as possible.

A major plus for music lovers that’s for sure this product packs another quite important feature Dolby audio surround technology a system whose reputation is well established and known by music and tech fans. This technology spreads the sound 360 Degrees so you are completely immersed and experience your favorite movies as if you were a part of them. As you know all the latest high-tech in the world will do nothing if your sound system is not equipped with significant power to satisfy you.

The TCL t66110 Sound bar supports 240 watts of audio power. You will have enough to adjust the sound to your taste and adjust all content types with the preset equalizer for Movies, Music and Television also. The quality of the connection is crucial and the TCL alto 6 plus sound bar has an Hdmi port equipped with auto return channel technology, a system that allows you to enjoy crystal clear sound through a single cable without adding an optical cable for diffusion, perfectly adapted to the layout of your room you will appreciate the wall mounts. The TCL alto 6 plus comes inelegant black and has the size of 34.25By 13.75 by 10.25 inches while the weight is 14.7 pounds.

Also known as the best value budget Sound bar that can be found on the market in 2021. With this you’ll have the opportunity to experience top audio technologies and a smooth sound that will complement the TV experience in any room. You’ll be able to improve the sound of the TV even in smaller spaces such as a dormitory or an apartment.

It integrates incredible sound quality into a compact aesthetic to add an enveloping sound with smaller space such as a bedroom or children’s playroom. Also included in mix is a thin subwoofer with a completely new design whose goal is to discreetly fit in a variety of places in the corner of a room or even under the sofa and which also provides even more bass that makes the whole room vibrate. The product is designed with Dts virtual x technology for a larger more comprehensive audio experience.

Another item that comes with this product and no less important is the chromecast set of features as well as of course the built-in Bluetooth, this will allow you to stream your favorite apps and artists whenever you want. The vizio sb3621n-H8 comes with all the necessary audio cables which will allow users to install the device with ease. This product comes with an elegant black finish and the size of 3.2 by 36 by 2.1 inches while the weight is 17.31 pounds. For the listed price you really get the whole package you can’t find another item in this range that will beat this.

Also known as the best mid-range budget sound bar that can be found on the market in 2021. At a fairly affordable price tag the Samsung HW-T450 sound bar is versatile enough to meet most needs with decent sound quality. Let’s start with the design, Samsung sound bars are quite versatile, simple and fit easily into any living room. what makes the Hw-t450 stand out a bit is the subwoofer that is included in the price. The whole package can easily fit under the TV.

The absence of cables to interconnect the box and the rod allows them to be laid as we deem appropriate without any restrictions on cable storage in the case of placing a bar on the wall. The pairing is done quite fast via Bluetooth by pressing a button on the back of the subwoofer. To suit all content types the Samsung hwT450 sound bar offers a variety of audio modes, Preset values can be selected via the supplied remote control including standard surround game and smart in the game mode.

The base is even more pronounced to emphasize explosions, shocks and all the epic gaming stuff. Surround mode also allows you to interconnect compatible additional speakers to form a complete 4.1 kit and improve immersion. The default settings are well balanced and will suit all types of content. If you change the content type frequently you’ll choose smart mode which automatically adjusts the sound settings according to what you’re listening to. The sonic output always maintains good quality depending on the chosen mode of operation. It is easy to notice the difference in the emphasis of the bass and middle frequencies. The audio spectrum is top notch and distortion free even at maximum volume regardless of the strength.

The quality of listening is the same Dolby audio and Dts certifications help to maintain optimal listening quality even with the addition of satellite speakers. To improve sound immersion with 200 watts of total sound power the Samsung Hw-t450 sound bar delivers on all fronts to cover even large rooms. This is more than enough. The product comes in the size of 33.9 by 2.9 by 2.1 inches while the weight is 12.8 pounds.

If you’re looking for a premium product We say check out the Sony Ht-S350 the best premium budget sound bar available on the market in 2021. The Sony HT-S350 sound bar is a 2.1 model with a wireless subwoofer that delivers a total of 320 watts and is capable of reproducing surround sound. Thanks to S force pro front surround technology. Also this product has an optical digital input Hdmi arc output as well as a Bluetooth connection for easy streaming of music from a smartphone, Tablet or computer compact and sleek the Sony HT-S350 sound bar features a digital S master amplifier responsible for powering two stereo speakers located at the left and right ends of the sound bar.

To provide a wide sonic focus low frequency reproduction is delivered via a wireless subwoofer that carries a 12 Inch diaphragm mounted in a base reflex in a volume of 28 liters to provide deep and rich bass. This Package allows the Sony HT-S350 sound bar to deliver those 320 watts of total power and stereo enough to bring all of your movies and music to life for multi-channel listening. The Sony ht-s350 sound bar features S-force pro front surround simulation technology that is able to reproduce cinema sound without additional speakers. This digital processing system can turn even stereo output into covered sound at the touch of a button on the remote control that comes with the product.

Several preset sounds are available to customize the display of the Sony HT-S350 sound bar according to the program being watched. Among the many DSP modes the device offers the cinema mode to optimize immersion in movies, the game mode for a better gaming experience, a sports mode that accurately reproduces the ambient noise of the crowd, the music mode to optimize stereo listening and a news mode designed to emphasize dialogue. On the connectivity side this product includes an optical input compatible with Dolby digital, Dolby mono and dual channel LPCM streams.

There is also an Hdmi output compatible with arc technology to provide audio feedback conveniently. The Sony ht-s 350 sound bar integrates a Bluetooth receiver for easy music sharing from a smartphone, tablet or computer. This f delivers on all fronts.

Finally after all the carefully summarized experiences, opinions and reviews we came to the conclusion that the Polk audio signa S2 is overall the best budget sound part that can be found on the market in 2021. As you may or may not know Polk audio has been producing speakers for over 45 years. All sonic areas are covered both for Hi-fi and home theater with the new signa s2 sound bar. Polk offers you an amazing home cinema experience far beyond what the small speaker has built into your Tv can deliver.

An Hdmi cable is provided for quick and easy installation while the company’s patented voice adjust technology helps to improve dialogue clarity further more. Dolby digital decoding creates unsurpassed immersive surround sound. The package includes a wireless subwoofer to enjoy powerful deep bass. You can also stream your favorite music via Bluetooth.

Also there are performance focus speakers and a Dolby digital 5.1 decoder that provides exceptional surround sound. And a backdrop from the compact sound bar with a slim and sleek profile and a wireless subwoofer exclusive to Polk audio patented voice adjust technology allows you to adjust the level of voices on the sound bar to your liking to reproduce sharp clear sound and never miss a word from your favorite sporting events, movies, Tv shows and more. The device also boasts universal compatibility making it compatible with Hd and 4k Tvs and you can watch your favorite shows with exceptional contrast and clarity. Also users will be able to enjoy a quick and easy connection to the Tv. The thin design allows you to fit the product into most spaces. You can easily place it on the wall or in front of the Tv.

Additional Bluetooth technology allows you to stream music directly from your smartphone, tablet or any other compatible device. The tape has three inputs Hdmi, optical and analog to accommodate all Tvs. The Hdmi connection is of course Arc compatible it allows you to restore all streams even those encoded in Dolby digital.

In conclusion the sound section is primarily focused on efficient playback of movie sound bars. Thanks to three technologies voice adjust to improve voice clarity preset from movie, music to depth. The display to the type of program and night mode to enjoy movies even when the whole house is asleep. The Polk audio signa S2 comes in the size of 3.22 by 35.43 by 2.15 inches as well as weight of 3.9 pounds.",10294
"Today, Smart Home Automation systems have wide use in home building to make Smart applications. Thus, many automation protocols such as BACnet, C-Bus, 1-wire, and KNX are developed rapidly. The KNX protocol is the famous one among these. Also, there are some specifications such as easy and fast installation process, user-friendly software, and adaption to the current system. KNX system is widely used in a home with the rise of a smart home. It is called KNX smart home.

In this article, we’ll cover:

What is the KNX smart home?

What is the KNX smart home app?

What are the features of the KNX smart home?

Importance of KNX Smart home

10 best products of KNX smart home

How to start with KNX?

How KNX smart home works?

How KNX smart home control?

Comparison of software package with KNX

Price and color comparison of KNX smart home product?

Which one should I choose: KNX or wireless?

What is the KNX smart home?

KNX is an open standard for domestic and commercial building automation and wired automation. This standard is regulated by the KNX association. Also, KNX devices can manage blinds and shutters, lightings, security systems, HVAC, energy management, white goods, audio, video, and remote control.

Mainly, it can work by confirming all devices, components, functions, and features of any building communicate through one common language remotely. It also gives the best guarantees in terms of future compatibility with all your automated devices.

KNX smart home is any set of devices, appliances connected into automatically controlled and monitored using KNX system. You can able to control a variety of functions using one system for the whole architecture. There is no need for additional control centers. It makes things more comfortable and convenient. It also increases the energy efficiency of our urban habitat. Also, it is the most secure smart home technology according to the encryption standard AES 128.

KNX system might seem outdated with the rise of the smart home. They are hard to extend automation. Also, they don’t feature fancy smartphone apps. It can’t easily integrate with Google Assistant, Alexa, and other new smart home devices such as Sonos, Philips Hue, or Fibaro.

However, there is good news for KNX system owners. They also can combine their wireless smart home devices using the Homey app. You can also control your KNX installations with Amazon Alexa or Google Assistant easily.

This can link up to create customized If This Then That (IFTTT) technology functions. The wires running directly through every wall. Hence, it can be considered as the central nervous system for your house. Also, KNX wiring (bus system) integrated into an existing home. It is commonly installed in a newer home.

Hence, the protocol is applied in the standardized technology. There are many KNX products from different manufacturers that are friendly to each other. That is the famous reason why the KNX system is generally used at home.

You can easily adjust or control a range of KNX functions and applications directly from your tablet or smartphone using the KNX smart home app.

What is the KNX smart home app?

There are few KNX smart home apps available such as the iBEMI control app or Homey app. They can be used to adjust switching, light scenes, dimming, temperature, air conditioning, shutter control, RGB, blinds, times and dates, energy indication with scale and offset, as well as a value transmitter.

iBEMI KNX control app is compatible with Android tablets and phones with Android 5.0 and later. Also compatible with iPhone, iPad, and iPod touch with iOS 5.0 or later.

It lets you monitor your home instantly with only a few clicks. Rooms and buttons can be guarded with the secure PIN feature. User-friendly design is simple to navigate via various control features throughout the house.

What are the features of the KNX smart home?

Features of KNX Smart Home

Electrical distribution (domestic wiring cabinet with KNX service shaft, low-voltage switchgear, etc.)

HVAC (Thermostat, heating, ventilation)

Lighting control (light sensor, light variator, presence detector)

Lighting (LED, Energy serving)

Bus configuration and supervision

Video surveillance (Connected IP camera to Home supervision)

Communication (KNX and Ethernet)

Touchscreen for home automation control (Tablet)

Energy and fluid metering and consumption monitoring

Distributed and internetworking application model for the various tasks in building automation.

Scheme for the management and configuration of resources on the network.

Permits the blinding of portions of a distributed application in various nodes.

Capable of hosting a distribution application

Model of the comprehension of these elements when developing real devices to be linked and mounted in an installation

Key features of KNX smart home with Homey

There are several key features when you turn your KNX system into a smart home using the Homey app.

Smartphone control from everywhere in the world

You can incorporate your KNX system into the Homey app. So, you can be able to control your all KNX devices on your smartphone or tablet from anywhere. Your favorite devices are now at your fingerprint at any time. Also, you can control your devices from Apple Watch using the iOS app.

Integrated with your KNX system with smart home devices

There are many smart home devices compatible with Homey such as smart thermostats, Spotify connects speakers, and many more. You can easily expand your KNX system with your smart home device and control them in a single place.

Voice control for KNX

You can use your favorite voice assistant to control your KNX system using the homey app. It also can available in Google Assistant, Amazon Alexa, and Siri shortcut.

Insights and energy management

Homey is a fantastic smart home hub when making a KNX smart home. There are so many functionalities. This is ranges from the hardware antennas in Homey pro on Z-wave, Zigbee, Wi-Fi, and infrared to energy management, Homey Apps Store with Homey energy.

Importance of KNX Smart home

Blinds

Close or open your shades with only one click on your tablet or smartphone. You can regulate your settings according to the weather. Also, you can able to close your screen remotely to guard your home against heavy storms and winds.

Lighting

You can easily control your light when you leave or go to bed. Switch on the light when you enter the room using movement sensors. Also, you can color your every occasion using light scenes and adjust scenes according to your mood with only one touch.

Heating, Ventilation, and air conditioning

According to the time of the day, you can manage the temperature in your room. You can preheat your bathroom before taking a shower in the morning. You can save your energy cost by automatically deactivated heating when you are not at home. During a high temperature, control your ventilation or air conditioning remotely. This creates a comfortable and energy-efficient home.

Security

When you lock the door, this allows you to activate automatically your alarm. Inform you or the local police when recognizing suspicious movement at your door. When you are not at home, it can manage your alarm remotely and set up a routine to give the impression.

Door communication

You can see visitors at your doorstep with a door entry system before opening your door. Also, you can check your visitor on the entry system when the doorbell rings. There has a warm welcome for visitors when opening the door.

Visualization

With visualization tools, keep your smart house and energy usage up to date anywhere. You can get insights into your energy usage and customize your interface to your needs.

10 best products of KNX smart home

KNX offer variety of design option and unique range of products. This lets you create your own unique smart home that answers your requirements perfectly. KNX devices are generally connected by a twisted pair bus.

ZENNIO Z70

ZENNIO Z70 is a color capacitive touch panel with a 7” display. It is a built-in luminosity and proximity sensor. Also, it has an internal temperature probe, 4 analog-digital inputs, and a thermostat. It is easy to install using a standard double mounting box and has Ethernet and USB port for program update.

ZENNIO Z41 TOUCH PANEL

ZENNIO Z41 TOUCH PANEL is a KNX touchscreen with 6 pages of 8 functions. It is also available in silver, white and black colors. It can be installed in round European back boxes only. It can display timers, graphs and also offers a smartphone app with push notifications. Also, It integrated with a temperature controller.

ZENNIO Z35

ZENNIO Z35 is a KNX touch screen with 7 pages of six functions. Also, it available in silver, white, anthracite and gloss white color. But it can only be installed in round European back boxes. It is integrated with a temperature controller, luminosity sensor, and proximity.

ZENNIO TOUCH MY DESIGN ROOM CONTROLLER

It is a KNX capacitive touch room controller with a display and 8 buttons. It is available in black, white, or custom printed finish. Also, It can be installed in round European back boxes only and has 4 pages with 32 functions. It is also integrated with a temperature controller and LED backlight.

LITHOS KNX KEYPADS

LITHOS KNX KEYPADS is a KNX keypad with several buttons with an extensive range of finishes. It can be installed in round European back boxes only. It is also integrated with LED backlight and temperature sensors.

JUNG SMART CONTROLLER 5

JUNG SMART CONTROLLER 5 is the latest extension to the smart control series with a slim, compact design. It fits into a standard flush wall box. It also has a high-resolution touch display. Also, it is an integrated brightness sensor and proximity sensor to wake up and adjust the display. It is available in any size upon request.

JUNG F40 AND F50 ROOM CONTROLLERS

It is a KNX room temperature controller with a 2/4 button. It is also available in a wide range of finishes. Also, color or laser engravings are available through an online tool. It is integrated with temperature regulation.

ABB TACTEO KNX KEYPADS

ABB TACTEO KNX KEYPADS is a KNX capacitive touch keypad and room temperature controller. It is finished in black glass or white glass. It can be installed on EU or UK back boxes only. It comes with a different model for vertical and horizontal. Online configurator used to finish and labeling selected.

BASALTE ELLIE

It is an elegant touch-sensitive panel with a slim design and premium finish. It can be used to control climate, lights, music, and many around the house. Ellie blends in beautifully with any interior. It comes with a wide range of high-quality finishes. It also requires Basalte Core Mini or Basalte.

GIRA KNX KEYPAD WITH TEMPERATURE CONTROL

It is with a KNX keypad with 4 buttons with an extensive range of finishes. It can be installed in round European or square UK back boxes. It is integrated with the scene controller and temperature regulator. It is available in red, blue, or green LEDs with variable brightness for feedback and status.

How to start with KNX?

There are several options to start with KNX technology.

Find a certified professional

You want to find a certified professional for the setup of your KNX smart home. They will help you to plan your smart home and create routines in every room. There are many professionals worldwide. They will help you with installation and wiring with hardware and software.

Learn the basic of KNX technology yourself

It is better to learn yourself. You can connect with their online course and successfully finish your test. Then you can gain additional chances to get your first KNX software license.

You can purchase their ETS home edition

If you are already a KNX expert, the ETS home edition is the best solution for homeowners. It allows you to configure up to 64 smart home gadgets. It will also give complete ownership of your setup and data.

You can connect with the unlimited option with unique choices of products

You can simplify and automate your daily life with KNX. It can combine different kinds of home products.

How KNX smart home works?

It has a main central nervous system for all automation. It is called the KNX bus line. It also a green cable installed in addition to the conventional core supply. It can be installed during a new renovation project and a new building.

According to the KNX standard for building automation, all of the different technology components are connected via the main KNX bus line. The cable system can be managed by sensors, parameters, detectors, etc. It also can be easily controlled by end customers with a smartphone, laptop, or tablet. This bus is routed in equivalent to the electrical supply to all of the devices and systems on the linking network.

How KNX smart home control?

There are different options available to homeowners. A wall unit, a smartphone such as Gira G1, or you can use something like the Gira pushbutton sensor 4. And yeah! The smart home hub can link it all together between the bus system and the control device. Ii has an important component that links it all together such as the Gira Home Server.

Comparison of software package with KNX

This is the comparison of ETS lite, ETS inside, and ETS professional. It helps you to choose which software package is the best for you.

Price and Color comparison of KNX Smart Home Product?

KNX smart home system is the best option. Because it has the future-proof solution for cutting edge houses. It is the globally accepted standard for wired building technology. It offers the best guarantees in terms of future flexibility and compatibility. Their networks are very robust offer exceptional performance. It also can install in virtually any single-family home or apartment to a big complex with multiple talents. It can be used to take your functionality outdoors. For instance, you can use it to automate your garden.

However, building a KNX system is expensive to already built houses. Hence, new ducts and cables need to be replacing in the wall to every light point, wall switch, sensors, or outlet. It is also inflexible. You want new wiring for relocated or new devices.

Which one should I choose: KNX or wireless?

It depends on what you need, also your living situation. Both systems have their advantages. if you want to build or renovating a home for long-term use, KNX smart home is the best option. It can be customized to your wants and needs. It built using an international standard. Meanwhile, your house will probably be larger; appropriate with a Gira Home Server confirms that you have full command of your home at every time.",14735
"A faulty internet connection is one of the main reasons why is your Wyze Cam Not Connecting to the available network connection. Until and unless your security camera will not connect to the internet, you won’t be able to use the access on your mobile phone.

In this troubleshooting guide, we are going to discuss all the methods with the help of which you can easily connect your Wyze to the available internet connection.

If you don’t want to be in trouble and are looking for a permanent and immediate solution then call us on the given Wyze Cam Phone Number.

Wyze Cam Not Connecting To Internet, Why?

Wrong wifi password, Weak internet connection, The camera is placed too far from the router, Faulty Sd card, and The outdated Wyze camera app is some of the main reasons why your Wyze Cam Not Connecting To the Internet.

For the complete resolution, you will have to repeat all the below-given steps in the same order. Before the end of this troubleshooting guide, your Wyze cam will start recording again.

6 Easy Ways To Fix When Wyze Cam Not Connecting

If your Wyze Cam Connection Failed then verify the wifi password, verify your network connection, restart the modem/wifi, move Echo a little close to the router, check the SD card, reset your Wyze camera.

Wifi Password

If you have recently come to notice that your Wyze Cam Won’t Connect to the internet then check the wifi password in the first place. It happens sometimes that someone may have changed the password but the camera is trying to connect with the old one. Hence, verify the same and update it from the Wyze app.

For any help, dial the given Wyze Customer Service Phone Number now and let our experts fix the issue for you.

2. Restart The Wyze Camera

Several times, the technical glitches themselves resolve just by a quick restarting method. So, without wasting the time, turn OFF the Wyze camera and then remove the power plug from the power source.

After waiting for 3–4 minutes, turn ON the Wyze camera again and check if the issue gets resolved or not.

3. Verify Your Internet Connectivity

It is really very necessary to connect the security camera with a good speed of internet connection. Hence, you need to verify whether your internet has a good speed or there is something wrong with it.

Before calling your network service providers, try to fix the modem/router-related issue by a quick restarting process.

4. Move Wyze Camera Close To Router

It may be possible that you have positioned the security camera outside the network connectivity range. Hence, you will have to minimize the distance between the Wyze camera and router.

5. Check The SD Card

Make sure that you have not inserted a corrupted SD card inside the security camera. If it is, then you have to replace the card with the new one. After doing this check if still Wyze Cam Not Connecting To Internet or the issue gets resolved.

6. Reset Wyze Security Camera

When all the mentioned troubleshooting methods will fail to fix the issue, reset the Wyze camera to factory defaults. To do so, you will have to hold and press the setup button for at least 30 seconds. Soon your camera will reset, hence you will have to be a little patient. when the camera will restart again, it will be connected to the available internet connection.

Conclusion

In this guide, we have given all the steps that are required to troubleshoot when your Wyze Cam Not Connecting to the available internet connection. If the problem still exists then do call us on the given Wyze Cam Phone Number. Our experienced team of experts will resolve the issue in a short time.",3612
"Why Is My Blink Camera Not Working? Blink Customer Service | 1–800–983–7116 Lieke Follow May 13 · 3 min read

If you have found that your Blink Camera Not Working properly then you must check all the cable connections and internet connection properly. These two are the most common factors why sometimes the Blink security camera stopped working without any reason.

In this guide, we are going to tell you all those steps that will help your Blink camera to start working again. So, repeat all the steps in the same order until your security camera will not start working properly.

If you are looking for an expert’s help then dial the given Blink Camera Customer Service toll-free number now. Our team is all prepared to deal with the issues like this.

My Blink Camera Not Working, How To Fix It?

There are a number of reasons that are responsible for why your Blink Camera Not Detecting Motion. Your Blink Camera Connected but Not Working because of the issues like Blink Camera Offline, Blink Motion Detection Is Disabled, Unforeseeable Error Is Lurking, Incorrect settings, etc.

Now, you have two choices, either read this guide completely and apply the steps one by one until your camera will not start working properly. Or else, dial the given toll-free helpline number for our expert’s help.

Verify All The Cable Connections

If you have found that your Blink Camera Not Working properly, the first and foremost thing that you should do is to verify all the cable connections. It may be possible that any of the connected cables are loose or damaged completely.

2. Check Your Wifi Device

If you have checked the cables and found everything fine, you should check your internet connection now. There are chances that your wifi device has stopped working because of which the Blink Camera Offline issue occurs. To fix this issue, you either have to call your network service providers or restart the router.

3. Give Your Router A Quick Restart

If you wish to restart the router then turn it off and then remove the cables and power plug from the router. After waiting for at least 2–3 minutes, reconnect the cables again and turn ON the router. Check if still, your Blink Camera Not Working or the issue gets resolved completely.

4. Minimize The Distance Between The Router And Base Station

The base station and the router may be placed too far from each other. This could be one of the main reasons why your Blink Camera Connected but Not Working. Hence, minimize the distance between both of these devices if your Blink Camera Not Recording Clips.

5. Motion Detection Is Disabled

If your Blink Camera Not Detecting Motion then it could be possible that motion detection is being disabled.

First, you will need to download the Blink Home Monitor App . If you haven’t downloaded the app yet, do it now to fix the issue.

If you haven’t downloaded the app yet, do it now to fix the issue. Open the app and enter the login credentials.

Tap on “Settings”.

You will then see a motion detection slider that should be turned to ON.

Arm your system, once the motion detection will be enabled.

After doing that, you would need to restart your Blink security camera. To do so, turn it off, remove the cables and then let it be for a while. Now, reconnect the cables again and turn ON the Blink camera.

Conclusion

We would like to conclude this article on the point that if your Blink Camera Not Working properly you should check all the cable connections properly. In this guide, we have arranged all those methods that are required to resolve this issue. If still your camera is not working then dial the given Blink Camera Customer Service Number. Our technicians will resolve the issue in a short period of time.",3732
"Why Does Wyze Cam Not Connecting To Wifi? Wyze Cam Connection Failed Kate blackwell Follow May 17 · 3 min read

A wrong wifi password, faulty internet connection, distance between the camera and router, could be some of the major reasons why Wyze Cam Not Connecting To Wifi. Don’t worry because in this guide we are going to tell you all those methods that will make Wyze start working again.

We can understand that your security camera has stopped working and you are sad because of this reason. In today’s time, it is very necessary to install the security camera for security and safety purposes. From the time people start installing these security gadgets, there is a major decrease in thief stealing.

What To Do When Wyze Cam Not Connecting To Wifi?

Your security camera should be connected with a network of 2.4 GHz or 5 GHz. If you want to check if your wifi is connected to the 2.4 GHz network or not, do follow the below-given steps.

For ios Users

If you are having an ios device, go to the “Settings” and choose the wifi option. From here, you can easily check whether you are connected to the 2.4Ghz network or not.

For Android Users

If you are having an android device then open the “Settings” and choose “wifi and wireless networks”.

Moving forward, let’s have a look at the troubleshooting steps if still your Wyze Cam Not Connecting to the available network connection.

Check The Wifi Password

If the security camera is connected to the 2.4 or 5 GHz networks then do check the wifi password. Sometimes, we forget to update the password from the Wyze app after changing it. Hence, verify the same and update the password if your Wyze Cam Stopped Working.

2. Restart The Wifi Device

If you have found that the wifi password has not changed but still the Wyze Cam Won’t Connect to the internet. In this scenario, a quick restart method would be one of the best options to deal with the issue.

So, press the power button to turn OFF the router, remove the cables that are connected to it, reconnect the cables after a while and then turn ON the router again.

3. Restart The Router

If you have restarted the camera but still the problem persists then restart the router now. Once you restart the router, it will give your system a fresh start to work properly and fix tiny glitches.

4. Move Base Station Near To The Router

It may be possible that you haven’t positioned the Wyze security camera close to the router. This is the only reason why your Wyze Cam Not Connecting to the wifi device. Hence, minimize the distance between the base station and camera to resolve the issue.

5. Reset The Wyze Camera

After trying the above-given steps if you have found that still, Wyze Cam Connection Failed then reset your camera. This is the last troubleshooting method to fix the Wyze camera problem. After you reset the Wyze camera, you will have to connect it to the internet again.

Remove the SD card from the camera is inserted.

Connect the power plug of the camera to the power outlet.

Press and hold the setup button for at least 30 seconds.

Wait until your security camera will not be reset completely. The LED light on the camera will be changed to solid yellow to blink yellow.

Turn ON the camera and complete the setup again.

Conclusion

If your Wyze Cam Not Connecting to the available internet connection then verify the cable connection and your wifi device. We have arranged all the reasons and troubleshooting methods to resolve the issue. So, do follow all the steps one by one so as to make your security camera start working again.",3577
"What To Do When Blink Camera Not Recording Anything? Blink Camera Not Working Lieke Follow May 17 · 3 min read

Faulty cable connections, damaged batteries, outdated router, the distance between the camera and router, unwanted devices are some of the common reasons why your Blink Camera Not Recording anything.

If you are also one of those users who is facing the same Blink camera problem then this is a must-read troubleshooting article. In this guide, we will arrange the reasons and the troubleshooting steps why Blink Camera Not Recording Clips.

Do you want to ask anything from our expert? Dial the toll-free Blink Camera Customer Service Number now. Our technicians are 24*7 ready to serve their customers.

Blink Camera Not Recording: Reasons

If you have found that your Blink Camera Not Recording Clips then a number of reasons are there that could be responsible for the same. We have listed all the possible causes below, let’s have a look.

The Blink camera is not connected to the power outlet properly. The sync module is not communicating with the camera. The camera is not getting appropriate signals. Faulty wifi connection. The Blink camera is placed outside the connectivity range. Cables are not well connected.

Until and unless our experts are there to help you, you don’t have to be worried about anything. Dial the Blink Camera Customer Service Phone Number now and let our experts deal with the issue.

5 Easy Steps To Fix When Blink Camera Not Working Properly

If your Blink Camera Not Detecting Motion, then do follow all the methods that have been mentioned below in this guide.

Verify The Cable Connections

It may be possible that they may have not connected the cables properly. This is the reason why your Blink Camera Not Recording. Do check the power adapter and the power outlet properly. Fix the issue related to the cables and if found nothing then jump to the next troubleshooting step.

2. Check The Batteries

The batteries that you have inserted inside the camera should be placed well and in an accurate position. Most of the time, we used to insert the batteries inaccurately. Or else, it may be possible that the batteries get damaged completely. This is why your Blink Camera Connected but Not Working.

3. Check Your Wifi Router

The internet connection that you are using currently may be faulty or stopped working. Hence, you need to verify the network connection in order to fix the issues like Blink Camera Offline, Blink Camera Video Stopped Unexpectedly, etc.

4. Restart The Camera And Router

After following the steps mentioned above, if still your Blink camera Not Recording then restart the camera and router. This is the best medicine that you can provide to the technical device. Almost half of the technical problems could be fixed just by a quick restarting method.

5. Move Blink camera Close To the Router

Another possible reason why your Blink Camera Live View Not Working could be the distance. It may be possible that you have placed the camera far from the base station. If it is so, then move the camera close to the router.

6. Factory Reset Blink Security Camera

Resetting the device to the factory defaults is the last possible way to resolve the issue. Hence, without thinking much, reset the Blink camera to factory reset in order to fix the issue. Let us say that resetting will delete all the saved settings. Hence, perform this step when only after trying other troubleshooting methods.

Conclusion

In this guide, we have discussed all the methods to fix the Blink Camera Not Recording issue. Two main reasons that could be responsible for this problem are faulty cable connection and outdated wifi devices. If there is anything that you want to consult with our experts then dial the Blink Camera Customer Service Number now.",3811
"LIECTROUX Robot Vacuums Are Highly In Demand!

Why do you need to have this robot vacuum at home ?

Many people hate vacuuming, it’s a boring task that feels too time-consuming and is requiring effort. For many people, purchasing a robot vacuum is a good way to clean the floors of your home without putting time or energy, which is totally right; but why are we advising you to buy the C30B LIECTROUX RoboVac?

Fig. 1.1: Smart Dynamic Navigation 👆

When you try to find out which robot vacuum you should purchase! You may have to know first that several different models of robot vacuums take different approaches to navigate. But earlier, models of robovacs cleaned randomly across the room and it took longer and the robots would sometimes miss some spots. Scientists took it as a challenge to make it smarter. Therefore, they equipped the C30B with Gyroscope + 2Dmapping. The 2D mapping helps the robot vacuum to gather information about your home so that it can choose the most efficient cleaning path, detect the best cleaning mode for the floor type, and recognize where they are. And the Gysroscope was put to double the speed efficiency. Researches found that it really increase the speed to 100%. So a task of 100 minutes, would take 50 minutes to be completed with this technology. That is why if you are looking for something to clean when you are busy with your responsibilities, this is an extraordinary and hard cleaning machine to make the floor shining ! Simply because our vacuum robots like to vacuum and mop the floors hardly at the same time !

This, would promise you good cleaning habits.

☑ Automatic return to base

☑ Wet mopping and disinfection

☑ 2D cartographic algorithm + Gyroscope

☑ Voice control and Wi-Fi app

☑ Quiet motor and noise below 45 dB

☑ Adjustable speed up to 4000 Pa

☑ Smart memory to recognize your whole house

☑ Effective on all types of floors

☑ Anti-torsion design and Anti-fall system

☑ Schedule cleaning anytime

Stop Manual Cleaning, Let Advanced Technologies Take Care of Your Home

If you suffer from mobility issues or physical ailments, a robot vacuum is the perfect cleaning device for you. Not only it does improve your cleaning routine, but it does so without putting in any extra strain on you or your situation. Simply turn it on and let it clean.

These devices contain various sensors, which allow it to clean independently. This eliminates the need to stand, walk, and bend over for extended periods of time. Robotic vacuums are also great for elderly individuals who may be suffering from arthritis or joint pain. Many of these vacuums can be preset to clean when needed, and they typically require very little maintenance.",2695
"Future Computers: Universal System Architecture

Photo by S Sjöberg on Unsplash

Today’s computers can be considered ‘junk boards’ as compared to future computers which will be based on this ‘Universal System Architecture:’

USA

Circular-Linear Relationship

Where a junk board mimics a city(scape), many different ‘buildings’ (architectures) (operations), in a circular-linear relationship with, a unified architecture:

Many Architectures

One Architecture

That is, no matter the ‘architecture,’ they all depend on ‘one’ ‘zero’ and ‘one’ ‘one.’ (Giving us the decimal system in a circular-linear relationship with the binary system.)

One Zero One One (Binary) (Decimal)

So, we arrive at a ‘city’ of ‘diversity’ (a junk board) because it is impossible to have a circumference without a diameter (the arithmetic numbers ‘one’ and ‘two.’)

One

Two

Universal System Architecture

Meaning, the background state, is not the ‘board,’ but, rather the diagram above (a universal system architecture).

Thus, to make our existing ‘computers’ (and microprocessors) more efficient (effective) (elegant), we can re-think the logarithms that support a zero and a one.

That is zero log one is zero (a string of zeroes) and one log zero is one (a string of ones). Where one is dependent on zero and, always, vice versa. (It is impossible to have a string of zeroes without a string of ones (and, again, always, vice versa)).

And this is because it is impossible to have a circumference without a diameter (as the diagram shows) (as the diagram proves).

String of Zeroes

String of Ones

String of Zeroes and Ones

Thus, we have now designed (provided) the architecture for a universal block-chain. Where, again a block and a chain are dependent on the diagram (a continually conserved circle).

Core Architecture: Nature

Conservation of the Circle is the core, and, thus, the only, dynamic in Nature (the diagram is the most basic architecture of Nature) (responsible for junk boards) (responsible for the streamlining of junk boards) (explanation for blockchain technology).

Continue with: Is Reality a Hash Graph?. Yep. Figure it out for your ‘self.’ | by Ilexa Yardley | The Circular Theory | Jun, 2021 | Medium",2211
"Blink Camera Not Working | Blink Camera Not Recording Clips | +1–800–983–7116 Lieke Follow Jul 5 · 3 min read

If your Blink Camera Not Working properly then, check the internet connection, placement of Blink Security Camera, and the motion detection settings.

There are many reasons why you should trust the Blink security camera. It includes the sleek design, zero monthly fees, data storage, Alexa integration, etc.

But, sometimes you may face one or other kinds of Blink security camera-related problems which include, Blink Camera Connected but Not Working, Blink Camera Not Recording, How to Reset Blink Camera, Blink Camera Not Detecting Motion, Blink Camera Not Working Red Light or Blink Camera Offline.

In this guide, we are going to discuss the troubleshooting methods for one of those issues. Before the end of this guide, your issue will be resolved and the security camera will start working.

Why Is My Blink Camera Not Working?

A number of factors could be responsible if your Blink Camera Not Working properly. It includes the weak internet connection, loose cable connections, and distance between the base station and camera. In addition to it, don’t forget to check the motion detection settings of your security camera.

How to Fix When Blink Camera Not Recording?

We are going to tell you some of the easy and effective troubleshooting methods. If you want an immediate solution then dial the given toll-free Blink Camera Customer Service Phone Number.

Check the Wifi Connection

It could be possible that your wifi has stopped working. This is the main reason why your Blink Camera Not Recording Clips. Hence, contact your network service providers or else try to resolve the issue by restarting the router.

2. Restart the Router

Many times, the technical glitches could be easily resolved just by a quick restarting method. To restart the router, turn OFF the button and remove the power plug from the wall outlet. Wait for a while and then reinsert the power plug again. Turn ON the router and check if the issue gets resolved or not.

3. Verify the Cable Connections

It could be possible that you have not connected the cables properly. This is why your Blink Camera Not Working properly. So, verify all the cable connections and replace the ones that are damaged.

4. Move Base Station Close to the Router

You may have placed the base station far from the router. This is the main reason why your Blink Camera Offline and not working properly. Hence, minimize the distance between the security camera and the base station.

5. Restart the Blink Security Camera

To restart the Blink security camera, remove the power plug from the wall outlet and wait for 2–3 minutes. Now, reinsert the power plug again and turn ON the security camera. Check now if the issue gets resolved or still the Blink Camera Not Working properly.

Steps To Disable the Motion Detection of Security Camera

It could be possible that your Blink Camera Not Detecting Motion or someone has turned it off from your Blink security app. Don’t worry, we have already arranged the steps that will help you to set the motion detection.

Open the Blink camera application on your smartphone. The app can be downloaded from the play store or app store. If you are a new user then create the account from the official Blink security website. Once the account will be created successfully, open the camera settings from the app. The slider that is given below the Motion Detection should be turned ON. You need to arm your systems when the motion detection button is enabled. You can now check if the issue gets resolved or still, Blink Camera Connected but Not Working.

Conclusion

If your Blink Camera Not Working properly then, you must check the internet connection, motion detection settings, and placement of the base station. In this guide, we have mentioned all the steps to resolve the Blink camera-related issues.",3922
"⭐A Target Package is short for Target Package of Information. It is a more specialized case of Intel Package of Information or Intel Package.

✌ THE STORY ✌

Its and Jeremy Camp (K.J. Apa) is a and aspiring musician who like only to honor his God through the energy of music. Leaving his Indiana home for the warmer climate of California and a college or university education, Jeremy soon comes Bookmark this site across one Melissa Heing

(Britt Robertson), a fellow university student that he takes notices in the audience at an area concert. Bookmark this site Falling for cupid’s arrow immediately, he introduces himself to her and quickly discovers that she is drawn to him too. However, Melissa holds back from forming a budding relationship as she fears it`ll create an awkward situation between Jeremy and their mutual friend, Jean-Luc (Nathan Parson), a fellow musician and who also has feeling for Melissa. Still, Jeremy is relentless in his quest for her until they eventually end up in a loving dating relationship. However, their youthful courtship Bookmark this sitewith the other person comes to a halt when life-threating news of Melissa having cancer takes center stage. The diagnosis does nothing to deter Jeremey’s love on her behalf and the couple eventually marries shortly thereafter. Howsoever, they soon find themselves walking an excellent line between a life together and suffering by her Bookmark this siteillness; with Jeremy questioning his faith in music, himself, and with God himself.

✌ STREAMING MEDIA ✌

Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the procedure of delivering or obtaining media this way.[clarification needed] Streaming identifies the delivery approach to the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies especially to telecommunications networks, as almost all of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio tracks CDs). There are challenges with streaming content on the web. For instance, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of this content. And users lacking compatible hardware or software systems may be unable to stream certain content.

Streaming is an alternative to file downloading, an activity in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user may use their media player to get started on playing digital video or digital sound content before the complete file has been transmitted. The term “streaming media” can connect with media other than video and audio, such as for example live closed captioning, ticker tape, and real-time text, which are considered “streaming text”.

This brings me around to discussing us, a film release of the Christian religio us faith-based . As almost customary, Hollywood usually generates two (maybe three) films of this variety movies within their yearly theatrical release lineup, with the releases usually being around spring us and / or fall respectfully. I didn’t hear much when this movie was initially aounced (probably got buried underneath all of the popular movies news on the newsfeed). My first actual glimpse of the movie was when the film’s movie trailer premiered, which looked somewhat interesting if you ask me. Yes, it looked the movie was goa be the typical “faith-based” vibe, but it was going to be directed by the Erwin Brothers, who directed I COULD Only Imagine (a film that I did so like). Plus, the trailer for I Still Believe premiered for quite some us, so I continued seeing it most of us when I visited my local cinema. You can sort of say that it was a bit “engrained in my brain”. Thus, I was a lttle bit keen on seeing it. Fortunately, I was able to see it before the COVID-9 outbreak closed the movie theaters down (saw it during its opening night), but, because of work scheduling, I haven’t had the us to do my review for it…. as yet. And what did I think of it? Well, it was pretty “meh”. While its heart is certainly in the proper place and quite sincere, us is a little too preachy and unbalanced within its narrative execution and character developments. The religious message is plainly there, but takes way too many detours and not focusing on certain aspects that weigh the feature’s presentation.

✌ TELEVISION SHOW AND HISTORY ✌

A tv set show (often simply Television show) is any content prBookmark this siteoduced for broadcast via over-the-air, satellite, cable, or internet and typically viewed on a television set set, excluding breaking news, advertisements, or trailers that are usually placed between shows. Tv shows are most often scheduled well ahead of The War with Grandpa and appearance on electronic guides or other TV listings.

A television show may also be called a tv set program (British EnBookmark this siteglish: programme), especially if it lacks a narrative structure. A tv set Movies is The War with Grandpaually released in episodes that follow a narrative, and so are The War with Grandpaually split into seasons (The War with Grandpa and Canada) or Movies (UK) — yearly or semiaual sets of new episodes. A show with a restricted number of episodes could be called a miniMBookmark this siteovies, serial, or limited Movies. A one-The War with Grandpa show may be called a “special”. A television film (“made-for-TV movie” or “televisioBookmark this siten movie”) is a film that is initially broadcast on television set rather than released in theaters or direct-to-video.

Television shows may very well be Bookmark this sitehey are broadcast in real The War with Grandpa (live), be recorded on home video or an electronic video recorder for later viewing, or be looked at on demand via a set-top box or streameBookmark this sited on the internet.

The first television set shows were experimental, sporadic broadcasts viewable only within an extremely short range from the broadcast tower starting in the. Televised events such as the 2020 Summer OlyBookmark this sitempics in Germany, the 2020 coronation of King George VI in the UK, and David Sarnoff’s famoThe War with Grandpa introduction at the 9 New York World’s Fair in the The War with Grandpa spurreBookmark this sited a rise in the medium, but World War II put a halt to development until after the war. The 2020 World Movies inspired many Americans to buy their first tv set and in 2020, the favorite radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name “Mr Television” and demonstrating that the medium was a well balanced, modern form of entertainment which could attract advertisers. The firsBookmBookmark this siteark this sitet national live tv broadcast in the The War with Grandpa took place on September 5, 2020 when President Harry Truman’s speech at the Japanese Peace Treaty Conference in SAN FRARick and Morty CO BAY AREA was transmitted over AT&T’s transcontinental cable and microwave radio relay system to broadcast stations in local markets.

✌ FINAL THOUGHTS ✌

The power of faith, love, and affinity for take center stage in Jeremy Camp’s life story in the movie I Still Believe. Directors Andrew and Jon Erwin (the Erwin Brothers) examine the life span and The War with Grandpas of Jeremy Camp’s life story; pin-pointing his early life along with his relationship Melissa Heing because they battle hardships and their enduring love for one another through difficult. While the movie’s intent and thematic message of a person’s faith through troublen is indeed palpable plus the likeable mThe War with Grandpaical performances, the film certainly strules to look for a cinematic footing in its execution, including a sluish pace, fragmented pieces, predicable plot beats, too preachy / cheesy dialogue moments, over utilized religion overtones, and mismanagement of many of its secondary /supporting characters. If you ask me, this movie was somewhere between okay and “meh”. It had been definitely a Christian faith-based movie endeavor Bookmark this web site (from begin to finish) and definitely had its moments, nonetheless it failed to resonate with me; struling to locate a proper balance in its undertaking. Personally, regardless of the story, it could’ve been better. My recommendation for this movie is an “iffy choice” at best as some should (nothing wrong with that), while others will not and dismiss it altogether. Whatever your stance on religion faith-based flicks, stands as more of a cautionary tale of sorts; demonstrating how a poignant and heartfelt story of real-life drama could be problematic when translating it to a cinematic endeavor. For me personally, I believe in Jeremy Camp’s story / message, but not so much the feature.

FIND US:

✔️ https://cutt.ly/gmzeaIT

✔️ Instagram: https://instagram.com

✔️ Twitter: https://twitter.com

✔️ Facebook: https://www.facebook.com",9164
"Role of Technology during Covid-19 Pandemic

The covid-19 Pandemic has wreaked havoc on the whole world. Our lives, both personal and professional have been greatly impacted. During this time of fear and uncertainty, our willingness to adapt to technology has been our lifeline. Pranav Mohan Jul 5·2 min read

Covid-19 has directly impacted close to 1.45 billion students as schools and colleges have shut down. Most educational institutions have started to offer online courses to make sure that education is not disrupted due to the pandemic.

The online grocery market has witnessed tremendous growth as people are preferring home delivery to stay safe. Online teaching and work from home is a blessing that comes due to technology and is the greatest solution that helps in social distancing. Work from home has ensured business continuity. But just like the sides of a coin, technology has its own minus points.

We have become more dependent on technology during the pandemic. More exposure to mobile radiation is increasing the cases of eye problems like myopia. People are watching movies late at night which affects their sleep cycle which results in fatigue and sleepiness. People are falling prey to fake news which is one of the factors in the increase of Covid-19 cases. An increase in digital gadget use is causing reduced physical activities. Increased use of technology by children can cause low creativity.

I do not mean to say that we should not use technology. But we should not let it rule over us.

The pandemic has hit labour-intensive sectors like food, retail, logistics and manufacturing businesses. Robots are used to clean infected areas and for delivering food to quarantined people. Swab collection is conducted to help medical professionals. Thanks to technology, we can develop vaccines. Thermal guns are used to measure the temperature of people.

Technology has helped in three medical activities- diagnosis, surveillance and prevention.

Covid-19 has proved that technological innovations have been helping in managing the pandemic in a timely, systematic and calm manner.

In the end, I would like to add that advancement in technology is steadily progressing; it will undoubtedly continue to grow exponentially. It’s we humans who have to adapt to changes in technology faster and continue to invest in building the technology systems for better preparedness.

I hope you like this blog. Don’t forget to share your suggestion in the comments section. Do share this blog.

Visit my Blog website- https://dailyboxchat.blogspot.com/",2566
"Every successful business is built on successful partnerships. Particularly in the IT sector, choosing the right technology partner for startups is a big decision. You need to ensure they add value to your business and offer services that will help your business grow.

There are several factors to consider while choosing a technology partner for startups.

What to Look for in Your Technology Partner?

Strong Business Understanding

A suitable tech partner for your business will have plenty of experience helping clients across multiple industries to meet their business goals. They have a strong understanding of their role, which is supporting business agility and reducing costs by modernizing their IT infrastructure and applications. The technology partner you choose should have knowledge and awareness about your business domain, your target audience and know how to help you grow through their expertise.

Working Experience

Entrepreneurs choosing technology partners for their startups tend to look at the cost first. However, the truth is experience matters more than the cost because, with an experienced partner, you can recover your partnership expenses eventually. An experienced tech partner understands how technology engages with existing infrastructure and systems and can provide a firm plan for your business.

To understand the duration and quality of experience a technology firm has, you can research them a little to find out about their past customers and clients and understand the types of successes they have achieved. If you find them on the first page of Google and have a list of completed projects with businesses similar to yours, that is a good sign. Go through their feedback and reviews to get a better understanding of their service offerings and quality.

Also Read: Top 10 Mobile App Development Trends to Watch Out for in 2021

Designing Solutions on a Budget

The technology partner you choose should understand the importance of budgets and costs while building solutions for you. Small businesses and startups usually have a lean budget, so the design needs to be broken into smaller chunks. This way, you can roll out your minimum viable product at least within your budget. As business and audience size increases, the solution can be scaled up to match them.

Deadlines and TAT

A technology partner should get along with your team quickly and turn deliverables around in a shorter time. While screening partners, check how quickly the tech firm developed past projects and solved business problems and how they respond to requests for changes. Also, look into the original project deadline and whether the technology partner adhered to that timeline. What could some of the causes for delay have been? Speedy delivery alone is not the goal. The aim is to understand requests, deliver high-quality outputs and respond to change requests in the shortest time.

Trust and Transparency

It would be best if you had open conversations with your technology partners. This would include conversations about resources, time management, timelines, and business goals. Every tech partner you choose should keep you updated on any changes in timelines or deliverables through status reports. It helps if your tech partner has a simple roadmap of project milestones to aid non-technical clients in understanding how the implementation and different stages will look in the end.

Customer Support

Unexpected downtime and app failure are huge problems in today’s digital-first businesses. This can frustrate customers and negatively impact services and your brand. A technology partner for startups will ensure that they can offer product support even after the development is complete. This includes maintenance services to ensure operations run smoothly, along with analytics that can help you scale up or make strategic decisions. They understand that their solutions need to match the pace of your business.

Considering each of these factors will help you make the right choice for your business.",4040
"Pension Central is currently helping employers fulfil their monthly pension remittance obligation. ChamsAccess Jul 5·3 min read

The Pension remittance space in Nigeria is filled with so much great promise, but still faced with inefficiencies particularly in funds reconciliation and delivery to PFAs. For some organisations, pension remittance is a task that is usually dreaded, particularly the continuous monthly cycle of signing multiple payment cheques, processing multiple schedules, and visiting multiple banks to submit each schedule. Ultimately, the manual efforts being done most times lead to funds not hitting Pension Fund Administrators (PFAs) till days and weeks, even months.

Indeed, the pension industry deserves a remittance solution that eases and solves problems around pension schedule preparation, remittance and funds reconciliation.

In a bid to bringing efficiencies to this space, Pension Central was launched, with a promise of enabling organisations remit pension funds easily and securely, with few clicks, guaranteeing same-day delivery of funds to all PFAs and eliminating all reconciliation issues.

Organizations can sign up for a free employer account in seconds by inputting just their PenCom code which is then validated to be true, after which they enter their email addresses, reset their default password and they begin to remit! Existing solutions don’t provide a sign-up process as seamless as this, with companies still having to complete some paper and leg work, going to a bank, then having to wait some days before signup process can be completed.

We basically want employers and corporate organisation to focus on the core of their businesses and allow Pension Central take care of their pension remittance processes.”

Every year, organisations are expected to get their PenCom Compliance certificate, a report that attests that those organisations successfully remitted pension for the year. That process is currently filled with lots of manual processing for most organisations who have to compile pension schedules and payment receipts for every pension schedule processed all through the year, then compile for processing. However, those processes have been completely eliminated, all schedules and receipts are accessible in one place, no need for manual efforts, no need for multiple paperwork.

All with few clicks, Pension Central enables organisations to process their compliance reports for the year seamlessly.

Some of the Key Benefits are:

Pandemic Compliant (e.g Covid-19): Pension remittance through your computer from the comfort of your office/home eliminates any form of exposure to any virus.

Pension remittance through your computer from the comfort of your office/home eliminates any form of exposure to any virus. Instant Delivery of Schedule & Payment : By integration, all pension schedules and payments are delivered to PFCs, shortening the remittance cycle of pension from days to seconds.

: By integration, all pension schedules and payments are delivered to PFCs, shortening the remittance cycle of pension from days to seconds. Operational Overhead Reduction: No more preparation of multiple physical schedules, signing of cheques, and physical delivery to the banks. Schedule and payments are purely electronic.

No more preparation of multiple physical schedules, signing of cheques, and physical delivery to the banks. Schedule and payments are purely electronic. Payroll Integration: flexibility of integrating with Payroll systems so that all pension items for all employees are extracted and remitted to PFAs on the fly

Since launching in October 2020, Pension Central has helped over 500 employers to seamlessly fulfil their monthly pension remittance obligation till date with an impressive return rate of close to 90% of signed up organisations month on month.

Pensioncentral.ng is an initiative of Chamsaccess Limited, Nigeria’s leading provider of Identity and Access Management Solutions, located in Victoria Island, Lagos Nigeria.",4025
"Excited to announce our seed investment in Teiko.bio, which calculates an immune fingerprint for each person. Tau Ventures is an AI-first fund in Silicon Valley investing primarily in mature seed, typically when there is a pipeline of customers, but we occasionally take earlier or later bets when we see immense promise. Teiko falls on that first exception, having been just founded at the end of 2020, but impressing us with (1) addressing a strong need, (2) building an execution-focused team, and (3) creating a differentiated product.

1) The Need

Helping design more immune-specific clinical trials and therapies is in many ways the holy grail of personalized medicine. The company is fundamentally grounded on the view that the blood matters for immunotherapy response. For instance, a lot of the thinking in the field has historically focused on the tumor. This idea of targeting blood is now increasingly established in the field and has been validated by many other labs. What Teiko is doing a step further is showing what configurations of your immune system, as measured in blood, are likely to predict response. Below is a snapshot around cancer — the company’s vision is that by personalizing treatment you can reduce costs and improve outcomes.

2) The Team

CEO / cofounder Ramji Srinivasan was previously CEO / cofounder of Counsyl for 10 years, eventually selling to Myriad Health for $375M. He has too many Stanford affiliations having enrolled in the MBA and MS in Biomedical Informatics programs (and dropped out to build Counsyl) and before that done a MS and BS there. Cofounder Matt Spitzer is currently an Assistant Professor at UCSF, holding a PhD in Immunology. The team is rounded off by another PhD expert in flow cytometry, a Stanford MBA who placed silver in the Rio Paralympics, and a Stanford Immunology graduate from the Bollyky Lab.

3) The Product

Teiko’s core product is a TokuProfile built on the analysis of your blood cells. Using rapid on-site computing and a panel of 35+ markers they create a curated summary and a sheet of recommendation for each person. The idea is to discover what works for each person so we can guide treatment best — imagine personalized cancer therapies. For a deeper discussion of the science check out teiko.bio/publications. There are also several dozen signatures in the public domain, for instance https://www.nature.com/articles/s43018-019-0022-x, and a paper from Matt’s lab laying out a blueprint for immune monitoring of clinical trials: https://www.cell.com/cell-reports/fulltext/S2211-1247(19)30822-8:

We are honored and excited to be part of Teiko’s journey, using technology to solve a big problem. More at https://teiko.bio.

Originally published on “Data Driven Investor,” am happy to syndicate on other platforms. I am the Managing Partner and Cofounder of Tau Ventures with 20 years in Silicon Valley across corporates, own startup, and VC funds. These are purposely short articles focused on practical insights (I call it gl;dr — good length; did read). Many of my writings are at https://www.linkedin.com/in/amgarg/detail/recent-activity/posts and I would be stoked if they get people interested enough in a topic to explore in further depth. If this article had useful insights for you comment away and/or give a like on the article and on the Tau Ventures’ LinkedIn page, with due thanks for supporting our work. All opinions expressed here are my own.",3443
"Glossary

Let’s begin by collectively visiting some of the terminologies used in this writeup. If you’re already familiar with them, jump straight to Considerations section.

Modular Monolith:

While referring to a monolith in this article, I’ll be referring to a “single process deployment”, i.e.; a system in which the code is deployed as a single process. There might be multiple instances of this process for scale, resilience etc. But conceptually the entire code is packed and deployed as a single process at a time.

Generally, we think of modularity only from code perspective. However, having modular code is just half the job done. The underlying persistence must also be made modular. So, when you structure your monolith into modules, also have a decomposed DB. Try hard not to have that foreign key reference to another module’s table even if it seems the most sane thing to do.

It might seem an overkill in the beginning to have so many DBs, and it makes sense to have this apprehension. But you can even harness the power of schemas that mostly all the DBs now provide and have separate schemas for different modules.

Modular Monolith with decomposed modular Databases

Segregating the code into microservices, still, is relatively easier as compared to separating the data into individual databases (owned by respective microservices) later and dealing with tedious migrations. DB migration is a “hard problem”.

Feature based packaging:

Firstly, if you’re not familiar with software packaging principles, it’s a good place to begin with and understand the underlying concepts behind them. They are as fundamental to writing and structuring code as are SOLID principles.

“Package by layer” is typically what we’re all aware of and most widely used in projects. If say, we’re working on an API service, we’ll try to segregate the packages by layers. So all the code related to routes will go to a route directory, all the code related to services (or adapters) would fall into the infamous services package and then the pattern continues with entities and repositories and so on so forth.

An example of code structure using package-by-layer packaging style

While the most easy to get started with, Package by layer is a maintenance hell, especially as the project grows, more people start contributing and more and more features get developed. Imagine browsing through the entire entities package to find out just the relevant entities that actually require change. Now imagine doing that for all such layers and imagine all the developers and teams doing that, the pain it would cause. In short, discoverability of code is very low.

“Package by Feature”, on the other hand, uses packages to reflect the feature set. It’s an attempt to place all items related to a single feature (and only that feature) into a simple directory/package. This leads to packages with high cohesion and modularity, with minimal to no coupling between the packages. You may still have the layered architecture in the feature package to begin with, as shown below, but a good north star is to adopt a more Domain Driven Design based packaging structure. For, e.g. if “Food delivery App” had “Restaurant Search” & “Delivery” as capabilities, each of them would end up as a feature package containing relevant layers/sub-packages within them as shown below:

Code structure using package-by-feature style of packaging

Each feature team can individually grow and maintain their feature packages with minimal conflict and bending their minds to search for the right code file to change.

From package by layer towards package by feature code structure

Cross-Functional teams:",3673
"Hosted by Global Pakistan Tech Summit Powered by Ejad Labs.

The insightful session was conducted by Saba Kalsoom, a community manager and partner at Work Nation Space. With Raheel Bodla, a Business Coach based in Silicon Valley as a guest speaker.

Raheel Bodla shared vital knowledge for startups in the hour-long session. He explained the role of a mentor as a person you reach out to ask advice because you want to do something similar to what they are doing. The role of a coach was also explained. Mr. Bodla described coaching as a formal engagement that offers three more befits that mentorship does not.

Coaching provides clarity. Mentors can confuse you as different mentors will tell you different things based on their experiences. Coaching is sustainable. Mentors might not always have time for you because they do not do this full-time. Coaching has the factor of Accountability. You can hold coaches accountable, and they can do that for you as well. Coaches work with a plan and discuss your weekly progress alongside.

Raheel stressed that startups need both coaching and mentorship as it is a collaborative effort. The founder of a start-up already has subject matter expertise, so they need the coaching and business matter. A coach provides a business plan and tells you the market position. They help in marketing, sales, operations, finance, and customer success. These are the 7 dimensions of business. A lot of work is also put into the mindset and strategies.

Q: What are the criteria to choose a coach or mentor in the digital and startup ecosystem?

A: Reach out to a mentor or coach that has already done (successful) what you want to do.

Q: What do Pakistani startups look like compared to startups in the U.S?

A: A lot of people who study in the U.S and go back to Pakistan to start their work are doing an exceptional job because the purpose of universities is to surge the economy.

Q: Where does Pakistan lie on the global scale?

A: There is a lot of talent in Pakistan. Pakistan Tech Summit, Ejad Labs, and Startup Grind Pakistan bring startups to Silicon Valley. Venture Capitalists are considering funding Pakistani startups and consider them an amazing investment.

Q: Should one focus on High Tech, that is, go innovative when it comes to startups, or does it not matter?

A: Go with the one idea that you believe in. The one you would risk your life’s investment on.

Watch the full session here.",2439
"How Firebase Started To Become a Problem

Despite getting the benefits, we started to feel the problems after six months when we tried to add more features and scale more for our growing number of customers.

First problem: Query

The first problem was queries. To add more features, we had to do more complex queries.

But there are many limitations for queries or filtering the data in Cloud Firestore. You could easily perform simple queries, but it was very difficult to run a complex query.

You have to understand that it’s not SQL. So, we had to create more collections and we duplicated more data for simplifying the complex queries.

But duplicating data also created some other problems. We had to update 5-6 collections when any data changed.

Second problem: No solid backend

Firebase’s purpose is to create a serverless environment, so it’s obvious that it won’t have a backend as we would use in another system.

As there was no backend, we accessed the database directly from our app. But it was a huge mistake. Reading or writing data directly caused many problems.

There was another way to access the data. We also tried that. We used the Firebase cloud function to make APIs. But cloud function APIs don’t perform like normal APIs. There are some limitations. It’s slow and not developer-friendly.

Third problem: Crash!

When the tech team’s size increased, the real problems started. Every developer had to have a clear understanding of the database and how it worked.

Because they had to read or write data directly from the database, we had to waste a lot of time on that. As we decided to build three different platforms (Android, iOS, and web), it was very impractical to allow developers to write data directly in the database.

If any developer from any platform pushed the wrong formatted data, then the other platforms crashed. The productivity of the whole tech team became slow.

Fourth problem: The security rule!

Security is very important for every tech product. There is a special feature in Firebase for enhancing security. As Firebase allows to read and write data directly from the frontend side, they have a system to write “security rules” in the admin panel.

I find it very inconvenient. It’s much easier to write security rules in an API.",2284
"Intro

If you are a founder who recently started their first startup, then the topic of writing a pitch deck has most likely already become relevant for you (or will very soon). Writing a good pitch deck takes time but it will help you in many ways, both internally and externally. Internally because it will help to challenge your main assumptions on why you are building this specific startup and externally because a pitch deck is a great tool to present/pitch your startup to early outside stakeholders. These can include potential co-founders, investors, pilot customers or accelerator programs. Pitching your startup and idea to others in the early stages can be very difficult and thus a good pitch deck can support you in convincing others to follow you on your journey.

Many founders do not fully utilize this opportunity because they do not have a good pitch deck, be it unstructured, complicated or missing information. If externals are not able to understand or follow your pitch deck, it conveys a negative signal and therefore hinders the success chances of your startup. This pitch deck guide gives you the fundamentals that will help you create a great pitch deck independent of the specific content.

Note on differences between pitch decks you present in person and those you sent out via email. This pitch deck guide is focusing on the content and the design of the slide, not the presentation style in front of an audience. The presentation style is also important but another topic that needs to be addressed separately. Generally speaking, a pitch deck can have different purposes (winning a customer or raising money from investors) and you should always be clear with respect to what your target is.

The team at STARTPLATZ created a template for a pitch deck that includes many of the tips that are written in the following guide. Feel free to use it as an inspiration. You can find it here: Link to pitch deck template

📖 Telling an interesting story

Whether you send your pitch deck via email (typically, investors spend 2–3 minutes on a pitch deck) or present it in front of an audience (the attention span is now down to 8 seconds, link), people have a very short attention span these days and get distracted if your pitch deck does not catch their attention. Therefore it is helpful to build your pitch deck around a specific storyline that supports your argument on why you started this company and why you want to solve this specific problem. In a good story a person or company faces the specific problem you are solving, therefore your startup improves their lives by solving this problem. Humans are triggered by stories because stories have always been the main format that uses emotions. The story you are telling should therefore trigger the reader’s emotions, as this can enhance their reading experience. It is crucial to include various elements of emotion in your written pitch deck that you send out via email, even if the story is harder to portray than if you were pitching it live on stage.

✅ Use a logical structure that follows a thread

A good pitch deck should make logical sense when you read it from the beginning to the end. That means each slide builds upon the information of the previous slides and strengthens the argument on why your startup has a high potential. Remember, this should still feel like a story and not like a list of boring slides. Boring slides mean a lot of text and too much information.

A logical structure may look like the following example:

COVER — Use a strong picture and a very short sentence to give the reader an initial idea about what you are doing. What is the essence of your startup? PROBLEM — Explain and if necessary quantify the problem you are solving. Who is facing the problem? Why is it relevant? How is this problem being solved today? SOLUTION — Now that the problem is understood, is there a solution that can fix or mitigate it? What exactly does your solution do? Why is it a good solution? Why now? MARKET — OK, there is a relevant problem and you have a good solution for it. But is there a big enough market for it? Is the market growing? Specify TAM, SAM and SOM. BUSINESS MODEL — Now the reader knows that there is a big enough market to justify going after it (or it is growing so fast, that it is attractive enough). Is it possible to monetise this? How do you plan to make money? COMPETITION — Alright. There is a market and you can make money. How about the competition? What sets you apart from other companies in this space and/or from substitutive solutions? There is almost always competition, even if it is just a small startup on the other side of the world. GO TO MARKET — Cool, you seem to have an interesting opportunity here. How are you planning to acquire customers and get market adoption? Depending on whether you are B2B or B2C focused, it is important to show the relevant channels and strategies here. STATUS QUO/TRACTION — Now that you have covered many important parts: What is your status quo? How many users do you have? Are you already making money? If you are a B2C company, make sure to show your CAC and CLTV. ROAD MAP — What is the roadmap ahead? How do you want to scale? TEAM — The most important part of any early-stage startup. Who is the team behind this idea? And what makes them uniquely skilled to execute on this problem? Do they have the key positions covered? (Some pitch deck guides will place the team slide in the beginning of the pitch deck but in my opinion, it is helpful to elaborate on all aspects of the startup first, before introducing the team. This allows the reader to identify whether the team’s professional expertise is suitable for the execution of this problem.) The ASK — What do you want from the reader? An investment? Customers? Have one slide at the end with a call to action. SUMMARY — You can wrap up the pitch deck with a summary and — very important — your contact data including your email and website.

Using this kind of logical structure immensely helps the reader to follow your story and argumentation.

👀 Thinking about the reader’s perspective

Who is the audience and how much do they already know about your business, market or customers? How much do they need to know? Throughout your whole presentation it is crucial that you always put yourself in the perspective of the reader or listener. In general, the reader will know much less about the problem you are solving than you. That means, it is always useful to keep your pitch deck simple to understand and also not filled with too much information. Most of that is probably not super relevant to the reader in the beginning. Remember that a good pitch deck is often just the key to getting a second meeting or call in which you can go into more detail.

Depending on your audience, you can use different versions of the pitch so that it is tailored more specifically: If you’re pitching to investors, your main goal should be to show that your startup has the potential to scale rapidly and for a billion-dollar exit. You need to include information such as competition in this scenario. Also, investors often know many industries and you don’t need several slides explaining the problem you are solving. They get it. When pitching to potential customers, however, it’s much more important to go more in depth on your product offering. In this version, you probably do not want to show them all potential competitors.

🎨 Design versus content

The design and layout of your pitch deck are as important as your content but often do not receive the equal amount of love. A bad design distracts you from the content itself and is confusing. A great design doubles down on the content provided by making the content easier to read, easier to understand and easier to follow, supplementing the logical structure provided before.

In fact, the content itself is made up of design. The text on your slides can be influenced by many factors such as the position on the slide, the size, the intensity, the colour or the font. So what are some design features that you should think about when creating a pitch deck? Here are three design features that will go a long way:

1. White space as a guidance tool

The most important space is the one you don’t use. Think about a slide with a lot of information. It is very difficult to focus on what to look at and read first. You want to use lots of empty space instead to guide the viewer’s attention towards the important parts of the slide (similar concepts apply to websites). As a founder, you often feel like there is a lot of information you want to convey that you want to share and need to cover. That is why many startup pitch decks feel overly stuffed and sometimes chaotic. Don’t do that and use space to guide your reader.

2. Colour matching

If you look at a very well-done pitch deck, you will see a common theme when it comes to colour picking throughout the presentation. Only very few colours are used and they support each other. It is also helpful to use one colour that generally highlights key points or visually guides the reader through the slides. For that, you don’t necessarily need to have a complete CI (corporate identity) document ready but it is advisable to stick to two to three colours max throughout the pitch deck. An easy start here is to decide on your main colour and then use the colour wheel. Mark your main colour and then look at the opposite colour on the wheel for the complementary colour. Another option is to pick the two colours to the left and to the right of your main colour, called analogous. Or you pick two colours that form a triangle with your main colour, called triad. According to color theory these combinations work best. This should give you a good enough start and obviously you can then use various intensity levels of the colours that you picked to add certain emphasis.

3. Icons and pictures or screenshots

Visual elements such as icons, pictures, or product screenshots can make a pitch deck much more appealing to look at, as well as help you deliver the message more easily. Humans can understand pictures faster than text and emotions are transferred much easier. You should therefore use this to your advantage. Specific parts in the pitch decks where this makes more sense are for example: The problem slide, the solution slide or the business model slide. It works better for some startups and industries than others. So play around and see what works for you.

Note: It is generally good to have (at least) two versions of the pitch deck, one for sending out and one for presenting in front of an audience. If you are presenting in front of an audience, you can keep the slides even cleaner and focus mainly on visuals, everything else you will explain verbally.

🚫 Common mistakes

Important info is missing. Often: competition and go-to-market slides.

Too many slides. Try to keep it at around 12 slides.

Too much info/content on some slides. Don’t confuse the reader, help them focus on the most important points instead. You want to create curiosity to get a follow-up meeting/call, not dump all info at once.

Slides are in the wrong order, thus making it harder to follow the logical flow and structure of the pitch deck (see B)

The design is ugly or distracts too much from the content

Ambitions are too low. This is a specific problem for mainland European startups. You should aim high if you want to convince investors, meaning setting high goals for yourself and show that you as the team are hungry to execute

🤓 Final advice

Your pitch deck is a never-ending document and requires constant iteration and improvement. So don’t worry if it is not perfect, it will probably never be and many people will have opinions about it. If you can manage to keep it clean and simple, clearly convey your vision and make it easy to understand, you have a higher chance of succeeding with your investor or customer meetings. This guide hopefully helps you to avoid some mistakes and gives you some useful tips on how to improve your pitch deck.

As a best practice for pitch deck, you often come across the deck that AirBnb used to raise their Seed round. It is indeed very clean and simple and you can study it in detail here: link.

Many thanks to the following people for feedback on the draft: Emmet, Victoria, Marius and Johannes.

About us

STARTPLATZ accelerator is all about execution speed, rapid testing and user growth. We have a unique two-phase accelerator program with 16 weeks of curated high-quality expert sessions, run by our mentors. Furthermore, participating startups meet in weekly founder sessions where they have to show weekly progress. Finally, we put a lot of focus on fundraising, introducing the startups to relevant investors & much more.

You can find out more about the STARTPLATZ Accelerator here: Link to website

This article was originally published on: https://www.startplatz.de/pitch-deck-guide/",12999
"Alexis Block with her HuggieBot

Alexis Block was 19 years old and a freshman at the University of Pennsylvania when her father passed away. Unable to afford the plane ride back home to be close to family whenever she felt lonely that year, she wanted a hug more than anything else, from someone who could put some emphasis and meaning behind the gesture.

A few years later, for her master’s thesis, she got hold of a broken robot that had been hacked and abandoned in her lab. With the help of a lab mate, she fixed it up, gave it custom hardware and software upgrades and got it to hug people, to see how receptive they’d be. Soon after, Block started exploring the physical and behavioural characteristics of hugs.

“Softness and warmth are key components, and so is responsiveness — the tightness and duration of the hug, which vary depending on each person’s preferences,” Block says. “I didn’t feel like any robots that were commercially available were designed with close physical interaction in mind, and so I decided to build my own that was specifically designed for intimate interactions.”

For her PhD research with the Max Planck Institute for Intelligent Systems and ETH Zurich, Block created the first human-sized, interactive hugging robot with visual and haptic perception. It has an inflatable torso that can sense when people put their arms around it and when they release, and she developed a machine learning algorithm to detect the particularities of each hug, as well as a behavioural algorithm so the robot can respond naturally to the gesture.

HuggieBot can adaptively wrap each user in a secure embrace that doesn’t leave any air gaps and doesn’t apply any excessive force on the user’s body. By using an Intel Realsense depth camera, Block can estimate users’ height and approach so that people can be hugged back more naturally — no matter their size.

“The kind of touch that you receive from someone hugging you is called deep pressure touch therapy,” Block says. “It’s extremely calming and relaxing; it can help alleviate stress and anxiety.”

For Block, that’s what’s unique about her robot. Unlike others that were designed to be hugged by people without the ability to reciprocate, hers can hug back. Now, Block has started researching the impact of her robot’s hugs on people, with early findings suggesting that HuggieBot shares similarities with human hugs and can positively affect users’ health.

Block is also developing an app that would allow users to send customised hugs that could be redeemed by their loved ones who are far away. The app would let people set the duration of the hug and any defining characteristics like intra-hug gestures, and record a personalised video that would be shown through the robot’s screen face when the user scans a QR code.

“I don’t view this as a replacement of human physical connection and physical touch,” Block says, “but I hope it could help people going through difficult times, and hopefully ease the pain of separation.”

With the pandemic, Block has witnessed a growing interest in her HuggieBot. While recruiting participants for a recent study, she received countless messages from people who wanted to get involved because they hadn’t been hugged in over a year.

“It just shows an innate human need for social and physical connection,” she says.

Block aims to find ten early adopters among large institutions that could benefit from the technology, such as universities, hospitals, nursing homes, and more. The long-term goal is to eventually create a robotic household helper for emotional support.

Subscribe to our monthly newsletter to follow Block’s journey",3659
"DeCypher DataLabs is a 2021 Clutch Top IT [Artificial Intelligence] Service Company in Illinois — DeCypher DataLabs LLC Leith Akkawi Follow Jul 5 · 2 min read

The Clutch 2021 research states that DeCypher DataLabs is one of the best B2B companies in Illinois.

AI, machine learning, and advanced analytics are quickly becoming ever more integrated into automated daily decisions for organizations. With the abundances of big data, super computers on the cloud, and advancements in neural network algorithms, organizations still do require an experienced Data Scientist to guide them through all the cloud technologies, big data and algorithmic programming code. This is the reason whyDeCypher DataLabs LLC is in business!

DeCypher takes pride in delivering AI solutions based on the principles of Truth and Data Ethics. DeCypher is operational only in the US since Jan 2019, and our portfolio of clients include organizations in the US Federal Government, as well as Commercial and Not-for-Profit sectors. These guiding principles of Truth and Data Ethics have withstood the early challenges of the entrepreneurial journey followed by the the COVID-19 pandemic national crisis.

With determination, perseverance and resilience, the genius work in Artificial Intelligence from DeCypher has been recognized by Clutch. DeCypher DataLabs has received a Clutch award as the 2021 Top IT [Artificial Intelligence] Service Company in Illinois. Clutch is an independent online reviews and ratings platform that operates in Washington DC. Clutch publishes reviews form the bases of their research in determining the best companies in every industry.

Clutch caters specifically to the B2B industry and stands out from their competitors because of their unique verification system. This allows Clutch to verify the accuracy of the information sent to their platform before publishing the review. When a company submits a report to clutch about their experience with a partner such as DeCypher, the review submitted goes through the verification process. If it passess successfully, then the review is published on the Clutch platform.

DeCypher DataLabs Founder and CEO, Leith Akkawi, would like to take this opportunity to thank our clients and partners who were kind enough to leave reviews on Clutch on our behalf. Because of how the clutch research works, this 2021 award would have never be possible without the original reviews from our clients and partners. If your company is thinking about delving into AI for your operations, get the best in the business as your partner.

Contact DeCypher …. and we’ll get started today!",2622
"To Office or Not To Office

As Covid is slowly retreating in some countries many of us lucky enough to have been working remotely will be perhaps able return to the office, most likely under some flexible or hybrid arrangements. Should we though? Or should we just stay working from home?

Here’s 10 reasons to stay at home, and 10 reasons to get back to that office.

10 reasons to stay working at home

More family time. I spent 11 years working full time in a previous company that didn’t allow any working from home. I left each day about 7:40am and got back about 6:20pm, so I saw my wife and young kids for a few hours each day and at weekends. Suddenly with Covid I’ve had so much more quality time with them, and we’ve all benefited. No commute — saving time and money. I’ve been saving about 2.5 hours a day and probably a couple of grand a year in train tickets. Healthier food. Let’s be honest, working in the office = McDonalds Tuesday, Thursday, Friday … I’ve been trying at home to vary my lunches, though they do sometime end up like this:

4. Less micro-management — remote working has led to workers getting more autonomy, even just by the fact their managers aren’t sitting in a desk in the corner looking over at them all day.

Which has of course raised the question … do we really need all those managers in the first place?

5. Performance based on performance, not being present in the office. It’s hard to judge someone’s performance mainly on their being present in the office, or coming in early and staying late, when there is no office. Instead, performance has to be now judged on … actual performance.

6. More comfortable place to work. My awesome company let us buy home ‘office equipment’ so of course I bought the largest bean bag I could find …

7. Easier to change job in a remote first world. Employers are realising that all of a sudden they can hire people that don’t live close to their offices — which is win-win for companies and employees, and should theoretically lead to more choices to change jobs more often.

8. Flexibility — the traditional 9 to 5 is out the window, and the “work whichever hours during the day suits you, as long as the work gets done” is in.

9. More inclusive teams and meetings — maybe. This one really depends on what’s going on in your own company, but I’ve seen more efforts to include people in meetings now that we’re all remote, whereas in the “olden days” .. . of last year before Covid … it was actually more likely some people wouldn’t even be asked for their opinion in in-person meetings.

10. Able to watch Loki as soon as it comes out. Have you seen episode 4?! There’s no way I’m waiting until ‘after work’ when the next ones come out.

10 reasons to get back to that office

Some of which are remarkably similar to the stay at home reasons! Which shows why none of this is simple!",2865
"Unicorn FinTech Founder Steps Down And Makes a Stand for Mental Health

Created by the author — original image from Wikimedia commons

“I’m very happy to talk about what’s gone on with me, because I don’t think people do it enough” — Tom Blomfield, Founder of Monzo

He’s absolutely right. If you read enough entrepreneurship advice, you’ll realize many people seem to have missed the mental health awakening completely.

Work harder than everyone else!

Do you only work 100 hours a week?!

Never give up!

Who needs friends?!

Do you want to be average?!

It’s easy to get brainwashed into the cult of hard work above all else. There’s this weird paradox where boasting about your results makes you arrogant but bragging about how hard you work makes you a hero. It’s an outdated badge of honor.

I’ve talked to many entrepreneurs and freelancers who are scared to admit they need a rest. They’ve been told so many times if they just keep trying, eventually they’ll be successful. To keep this mentality going, they tell themselves anyone who grows too effortlessly must be a fraud.

This is what makes Tom Blomfield’s confession so important. He founded Monzo, the app-based bank, which now has over 5 million customers and a valuation of over a billion dollars. It even raised money during the pandemic through big names such as Accel Ventures, Y Combinator, and Stripe.

The hype may not have reached the US but I’ve never seen an app gain such vigorous support amongst tech-savvy millennials in London. A quick scroll through my phone shows me over 100 of my contacts have accounts. Pretty impressive considering it lacks 15-second dance videos.

At 35, Tom was living the dream of many aspiring entrepreneurs with this cutting-edge and cool product. Yet he didn’t feel like he was living his dream. We read success stories where companies skyrocket and failure stories where they collapse. We can assume the “success” makes their founders happy because we rarely hear about the times when it makes them miserable. It’s the blindspot scenario as it seems counter-intuitive.

It all started turning sour around two years ago for Tom.",2138
"Introduction

Cloud Storage is designed to help you quickly and easily store and serve user-generated content, such as photos and videos.

Firebase storage dependency. Add the dependency to your pubspec.yaml file. Check the latest version here.

Introduction

Cloud Storage is designed to help you quickly and easily store and serve user-generated content, such as photos and videos.

Firebase storage dependency. Add the dependency to your pubspec.yaml file. Check the latest version here.

Introduction

Cloud Storage is designed to help you quickly and easily store and serve user-generated content, such as photos and videos.

Firebase storage dependency. Add the dependency to your pubspec.yaml file. Check the latest version here.

Introduction

Cloud Storage is designed to help you quickly and easily store and serve user-generated content, such as photos and videos.

Firebase storage dependency. Add the dependency to your pubspec.yaml file. Check the latest version here.

dependencies:

flutter:

sdk: flutter

firebase_core: ""^1.5.0""

firebase_storage: ""^10.0.2""

Before we start using Storage we must first ensure we have initialized FlutterFire. This is usually done before any Firebase service is used. This step is asynchronous meaning flutterfire usage is prevented until initialization is complete.

await Firebase.initializeApp();

The initializeApp() method is asynchronous and returns a future. Update your main flutter function.

N.B You can change the rules on your storage to allow a user who is not signed in to upload files.

Now let’s create a new dart file to upload our files to storage

Step 1: import cloud storage package to our project.

import 'package:firebase_storage/firebase_storage.dart' as firebase_storage;

Step 2:Create a storage instance by calling the instance getter on Firebase Storage.

firebase_storage.FirebaseStorage storage =

firebase_storage.FirebaseStorage.instance;

Step 3: To select image from gallery or camera, add path and image picker dependency to your application.

path: ^1.8.0

image_picker: ^0.8.4

reference:https://fabcoding.com/2020/06/08/adding-an-image-picker-in-a-flutter-app-pick-images-using-camera-and-gallery-photos/

Happy Coding!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",2238
"One thing that puts people off cryptocurrency is confusion around obtaining it. Admittedly, it can be mind-boggling if you’re new to cryptocurrency, but breaking it down into manageable steps makes it far easier to understand — sort of like baking a cake…

Recipe: Purchasing OPCT

Let me guide you on the steps required to bake your own OPCT cryptocurrency cake — and once you take that initial bite, you’ll want more!

Note: Please ensure you are considering your own personal data, security, and finances carefully at each step — precautions are there so you, the buyer, don’t get burned! There is a valuable lesson learned from all the stories about exchange accounts being hacked — do not leave your assets on an exchange. If you do, you’re leaving yourself to potentially being hacked and your assets stolen. Keep your assets safe in a hardware storage solution, such as the Ledger Nano.

In this guide, I’ve utilized Coinbase, KuCoin exchange, and MetaMask. There are other sites you could use, but I’ve found the ones listed to be far easier and more reliable than their competitors. Although these are what I use, you are, as always, free to choose other sites for purchasing once you get used to maneuvering cryptocurrency purchases. Let’s get started!

Here is what you will need:

Ingredients:

§ KuCoin account

§ MetaMask in-browser Ethereum wallet download

§ Coinbase account

§ A mobile smartphone

§ Two forms of ID*

§ Payment method — ideally a credit card for added security

*ID is required by external exchanges to ensure money laundering regulations are being met. The ID requirement is not a stipulation from Opacity Storage directly.

Method: Let’s Bake!

1. Create your accounts

First, if you do not hold active accounts with Coinbase and KuCoin, you will need to set these up.

Coinbase

Head to Coinbase here and click the ‘Get Started’ button in the top right-hand corner. This will open the account creation form which requires a name, email and password to be entered. It’s all very standard stuff at this point.

KuCoin

Head to KuCoin here and click the ‘’Sign Up’’ button in the top right-hand corner. This opens the account creation form which requires only an email. You will next hit the ‘’send verification code’’. You will be emailed a code to your personal email which you enter into the KuCoin sign up to complete email address verification. Once verified, you can set up a password.

The next step is to head to the ‘’Assets’’ tab on the top-right section of the account and click the ‘’ deposit’’ button. This will require you to set-up two methods of security. The first is a trading password — this consists of six-digits. The second is via the Google Authenticator app via smartphone or browser. Connect your account to your smartphone by scanning the QR code on the KuCoin website.

MetaMask

Head to MetaMask here. MetaMask is a browser-based wallet and by far one of the easiest methods for storing your Cryptocurrency. Download their browser extension which is available on Chrome, Firefox, Opera and Brave. Once the extension is set-up, it will show as a fox icon, usually on the top right (especially if you’re using Chrome).

When you’ve set-up your account, you will be given a generated backup phrase consisting of twelve randomly generated words which are specific to your account. Keep these safe to recover your account in case you need to!

Finito! That’s the security methods and accounts set-up. Let’s keep going!

2. Buy Ethereum (or bits of ETH) on Coinbase

First, make sure you have a payment method on Coinbase. I would advise using a credit card over a debit card as your transactions are covered, but you can choose the method you prefer. To reach the end-goal of buying OPCT, you need to purchase Ethereum (ETH). The quickest and smartest way to do this is via Coinbase.

On Coinbase, you can purchase a minimum of £1.99/$2.53 worth of ETH and a maximum of £6000/$7,628.68 (later, you will require 2 OPCT for an Opacity account, currently around £0.06/$0.08 each, plus a small amount of ETH for the transaction fees).

Once you’re ready, hit the ‘’Buy Ethereum instantly’’ button and enter the amount (recommended is £10/$12.61 minimum to cover fees and the purchase of ETH bits) and confirm once it pops up on your screen.

Congratulations! You are now a cryptocurrency adopter.

It won’t take long for the ETH to reach your wallet. You can check that your ETH purchase has cleared by checking on the accounts tab of your Coinbase account.

3. Deposit on KuCoin

On the KuCoin assets tab, click the deposit button. The next page is bare except for a search box — type ETH into the search to populate the drop-down box. Once you’ve clicked on ETH, you will note that it lists your personal wallet address. This is an important code-address that you will use to receive your funds from other places.

KuCoin has a nice little feature: with the click-of-a-button you can copy the wallet address to your computer clipboard. This ensures the wallet address is correct in its entirety as there have been horror stories where people have sent funds to wrong wallet addresses and ultimately lost that money. There is no way to return the money if this happens so use the clipboard facility where possible.

We will now use this address to transfer the ETH you purchased from step-two on Coinbase.

4. Buying OPCT— Nearly There!

With your KuCoin ETH wallet address copied to the clipboard, open the Coinbase tab in your browser. We will now be sending the ETH that’s currently sitting in your Coinbase account over to KuCoin account so we can purchase OPCT on Kucoin.

On Coinbase, you can transfer your ETH by pressing the ‘’send’’ button. A pop-up window will overlay so you can fill out some details such as the recipient. This is where you will paste your KuCoin wallet address from the clipboard.

Once you’ve confirmed that the recipient section (your wallet address) is correct, you can click on the ETH input box in the amount section. Click on ‘’send max’’. You will note the full amount you purchased isn’t listed, this is because a small amount is deducted to go towards transfer and miner fees. The fees are so small, it’s not worth worrying about.

Next, you can choose to add a note — maybe a date, a reason for sending or something else. You can opt to leave this blank if you wish. Once completed, hit continue and confirm your transaction. The transfer may take a little bit of time depending on the gas prices and strain on the Ethereum blockchain; typically it’s quick.

You can check receipt over on your KuCoin assets page.

5. Transfer into Your Trading Account on KuCoin

Now that your ETH is stored on KuCoin you will need to transfer the ETH into your trading account in order to buy OPCT. Simply hit the ‘’transfer in’’ button. It really is that simple.

Type ETH into the coin field, click the number in the available amount section to send the full amount to your trading account, then confirm.

6. Let’s buy some OPCT

Now the fun begins! Your trading account has funds so you can now buy your OPCT.

On KuCoin, head to the exchange which is located next to the KuCoin logo and the market button. Once you have done this a few times, it will become far less daunting so bear with it.

Currently, you are the proud owner of Ethereum (ETH), if you’ve followed all the steps. You will use ETH to buy OPCT, so you want to select ETH as a pair to OPCT in the Exchange section. Click on the small ‘’select pair’’ search box and type in ‘’OPCT’’.

This will drop down a box populated with OPCT/ETH and OPCT/BTC. Click the OPCT/ETH option as this is the only applicable one to this guide (unless you own Bitcoin). The next page shown is where you will purchase OPCT for use on the Opacity platform.

You will need to enter your trading password before you proceed. This is the password you set up in step one along with the Google Verification Authenticator. Once you’ve entered your trading password, you will see the main order book with several options.

Ideally, you want to try to purchase the cheapest sell-order in the order book. There is a withdraw fee of 34.7 OPCT (subject to change) and a minimum withdrawal amount of 69.4 OPCT. I’d advise you to hit the 100% button to spend all of your ETH on OPCT, but you can choose a lesser amount. When you’re ready, press the green ‘’Buy OPCT’’ button.

Congratulations! You are now the proud owner of OPCT.

7. Metamask Transfer

This is where MetaMask comes in. You want to move your cryptocurrencies into a wallet for safekeeping rather than keep them on exchanges. To do this, you will need to move your OPCT from your trading account into your main account on KuCoin. You will need to withdraw it by pressing the ‘’Transfer Out’’ button in the trading account card.

Once the funds have been released to your main account, hit the ‘’withdrawal’’ button and then select OPCT. You’ll note that the fee has already been deducted from your OPCT amount. This is normal for most exchanges.

To move your OPCT into MetaMask, you will need to click the fox icon on your browser (this is the extension we discussed in step one) and sign in, if required.

Hover over the account section on MetaMask and select ‘’copy to clipboard’’ — once again your wallet address will be copied to your clipboard, so paste this into the wallet address form on KuCoin. In effect, this is similar to what we did when moving from Coinbase to KuCoin. You’re simply moving your currency in and out.

On KuCoin, once you’ve pasted your MetaMask wallet address on the form, hit confirm and then follow the steps as before to authenticate your trading password.

Decoration: Optional

For convenience, you can add the OPCT token information into MetaMask to save time from having to go through the Blockchain record.

1. Open MetaMask via the fox icon on the browser bar and click the three lines in the top left to open the menu.

2. Click the ‘’add token’’ button which will take you to a new page.

3. OPCT needs to be added manually to display the tokens in the wallet. Add the following information into the form and click next. Once it’s complete, you will now see your OPCT balance listed on MetaMask.

Contract Address: 0x77599d2c6db170224243e255e6669280f11f1473

Symbol: OPCT

Decimals: 18

Serve, and enjoy once cooled alongside a cold Scotch-Whisky with a dash of water.",10390
"You might have 2 common CSS properties in mind when you read content and visibility.

display: none

visibility: hidden

They are often used to control visibility of the content. The new content-visibility property is similar, but it provides us with several advantages. The main one is improvement of performance!

The content-visibility property, launching in Chromium 85, might be one of the most impactful new CSS properties for improving page load performance.

It enables the browser to skip rendering work, including layout and painting, until it is needed. That means that off screen content won’t be rendered and we can save several milliseconds on initial load and boost the performance.

How to use it? The easiest way is to use auto as a value, so the browser determines when and how to use it.

.container {

content-visibility: auto;

contain-intrinsic-size: 1000px;

}

Notice contain-intrinsic-size property.

In order to realize the potential benefits of content-visibility , the browser needs to apply size containment to ensure that the rendering results of contents do not affect the size of the element in any way.

We need to set contain-intrinsic size to the element otherwise the browser might think its height is 0, and in that case we won’t gain performance benefit.

To have a better control over showing/hiding elements you could also use content-visibility: hidden. It behaves like display: none, but it preserves the rendering state of the element!

For more details I recommend you to read this article.

Browser compatibility

It’s similar to aspect-ratio, but here Firefox doesn’t have even a partial support. It doesn’t matter much since this is a feature that won’t break your site if you use it and it’s not supported. It can only help you with performance when applied correctly.",1816
"Photo by Karolina Grabowska from Pexels

Congress passed the Communications Decency Act (CDA) in 1996 to “protect the public from the misuse of the telecommunications network and telecommunications devices and facilities” (Communications Decency Act, 1995). Specifically, Section 230 of the CDA “protects providers and users of interactive computer services from liability for defamatory content posted to their platforms by third parties” (Murcia, 2020, p. 235). While the US Supreme Court ruled large parts of the CDA unconstitutional, they ruled Section 230 itself to be constitutionally valid (Reynolds, 2019).

This article will review several aspects of Section 230. First, it will review the potential users affected by Section 230. The article will then examine the ethical issues around Section 230, specifically which ethical framework best suits reviewing the section. From there, it will pivot to an abbreviated legal analysis of Section 230 (contrasting those with the previous ethical considerations). Finally, this article will propose mitigations to address legal and ethics concerns arising from the section.

Users Affected Under Section 230

Section 230 was Congress’s attempt to bridge competing interests. On one hand was the newly emergent Internet and the potential it provided for unfettered communication and commerce. On the other hand, was the desire to protect society’s most vulnerable members, namely children (Leary, 2018). As Leary (2018) notes, some see Section 230 as a key element of what the Internet is today; others see that as the exact problem. They see it allowing for the trafficking of sex slaves, child pornography and exploitation giving equal opportunity to both the legal and illegal economies in the world (Leary, 2018 and Carney, 2019). This is because Section 230 states, “No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider” (Cornell University, n.d.).

Section 230 protects many categories of users society feels require special protection. For example, Section 230 provides legal liability protection for providers who host protected speech, such as activists who create potentially controversial content. This enables the free expression of ideas (Electronic Freedom Foundation, 2021). However, it also protects more mundane content, such as uploaded YouTube videos, Instagram pictures, and comment sections on websites according to the Electronic Freedom Foundation (2021). Section 230 does this by ensuring content hosted by a third party cannot be removed just because someone does not appreciate it; there needs to be a legal action directed against the actual creator of the content. Of course, there are exceptions, such as the Digital Millennium Copyright Act (DMCA). The DMCA allows providers to avoid legal trouble for copyright infringement if they remove the offending content within a certain time limit (Reynolds, 2019). The DMCA is focused on intellectual property rights though.

However, as with any technology-focused law, Section 230 of CDA is double-edged. For all the protection it affords protesters and regular users looking to comment on a Chicken Cordon bleu recipe, it also allows for the posting of nefarious content with no ramifications for the hosting provider. Section 230 enables sex and human trafficking via many Internet-enabled methods: “[t]he impunity for facilitating sex trafficking that the Internet offers goes beyond advertising to include so called ‘hobby boards,’ where purchasers rate prostituted people and victims of trafficking as they would rate a restaurant on Yelp — except with graphic, vulgar, and violent detail” (Leary, 2018, p. 572). This is because the court system has broadly interpreted Section 230 to create an immunity for hosting websites for all (legal and illegal) content (Leary, 2018). Clearly, the range of users affected by Section 230 is broad; it covers all users of the Internet in the United States (Cornell University, n.d.).

Ethical Analysis of Section 230

When discussing ethical issues, it is valuable to have a framework to gauge the actions undertaken. Velasquez et al. (2015) offer five such frameworks to view the goals and effects of Section 230: utilitarian, rights, fairness/justice, virtue and common good. The utilitarian argument says that to be ethical, an action or set of actions should create the greatest good for the greatest number of people or create the least amount of harm for the least amount of people (Velasquez et al., 2015). A rights-based approach would begin with the idea that “humans have a dignity based on their human nature per se or on their ability to choose freely what they do with their lives” (Velasquez et al., 2015, para. 5) and that these rights need to be protected via actions undertaken and laws enacted.

The fairness/justice view of ethical questions starts with the simple idea that people should be treated equally, or baring that, fairly while the common good view states that actions should be performed in the best interest of the community at large (not to be confused with the utilitarian definition of the greatest good for greatest number of people) (Velasquez et al., 2015). Finally, the virtue approach requires that actions or laws be consistent with idealistic concepts such as honesty, courage, and tolerance so society can maximize its potential (Velasquez et al., 2015).

When considering Section 230 , the virtue framework clearly does not apply as the ideals to which it ascribes are not universally defined and agreed upon; courage and tolerance for example mean different things to different audiences. However, the others are all viable candidates for reviewing Congress’ actions and the effects of the law. The utilitarian approach, with its focus on the greatest good for the greatest number of people (and the converse about harm) makes sense. Congress was attempting to limit the harm for hosting platforms and make sure content was widely accessible.

The rights framework for ethical consideration says individuals have dignity, and that dignity is derived from choosing how to live our lives: “the best ethical action is that which protects the ethical rights of those who are affected by the action” (Bonde & Firenze, 2013, para. 12). Here, the question of whose rights are being protected arises. Did Congress enact Section 230 to protect the rights of the commenters to express themselves freely or Congress enact Section 230 to protect the business rights of platforms hosting content? The text of the statute makes no claim to protect rights (only the potential of the technology as perceived in 1996) so perhaps the rights approach is not the best framework (Cornell University, n.d.).

Under a fairness/justice approach to evaluating Section 230, all content is treated equally. Specifically, clause A(4) states the Internet is a “benefit to all Americans” (Cornell University, n.d., para. 5); therefore, Americans benefit from unfettered access to the content covered by Section 230. Perhaps treating both legal and illegal activity equally is fair, but is it just? Intuitively, this argument feels wrong. Letting human traffickers post, as Leary (2018) alleges, with the same impunity as someone commenting on the latest Britney Spears legal drama seems to tear at the collective sense of right and wrong. However, as Bonde and Firenze (2013) note, the fairness/justice approach does not consider the consequences of actions to be of a viable concern.

The last framework under consideration is that of common good. This view focuses on the idea that “actions should contribute to ethical communal life” (Bonde & Firenze, 2013, para. 9). Here, Congress determined the community to be the whole of the nation. However, there are sub communities that need to be discussed. The community of content creators or comments engaging in legal free expression co-exists with the community of criminals using the Internet to facilitate crime. Are both considered equally? Congressional intent on this aspect is unknown based on the text of Section 230.

When viewed within the context of the four viable frameworks, it is clear the utilitarian approach is the best one to view Section 230. Congress was attempting to maximize the value the Internet could bring to the lives of everyday Americans as they represented “an extraordinary advance in the availability of educational and informational resources” (Cornell University, n.d., para. 2). When viewed through the lens of utilitarianism, the facilitation of the free expression of ideas creates the greatest good.

Legal Analysis of Section 230

Under Section 230, hosting platforms are not considered publishers of content and are immune to legal action, unlike traditional media ((Cornell University, n.d. and Burke, 2011). This is due primarily to a Fourth Circuit Court of Appeals interpretation in Zehan v. America Online (Burke, 2011). In its ruling, the Court ruled that a print newspaper could be liable for defamation if it printed a defamatory letter to the editor, but a website could not (Burke, 2011). The law defines defamation as the sullying of one’s character or standing (Garner & Black, 2021).

Prior to enacting Section 230, under a case known as Stratton-Oakmont, hosting providers became liable for the content if they exercised editorial control over the content on their platform (Burke, 2011). With the passage of Section 230, platforms were no longer liable for defamation as they were not considered publishers in the traditional sense (Cornell University, n.d). Many, including Leary (2018) and Burke (2011) feel the Courts are misinterpreting Section 230. As Butler (2000) notes, the original intent of the law was to “prevent the Internet from becoming a ‘red light district’ and to ‘extend the standards of decency which have protected users to new telecommunications districts’” (pp. 251–252).

If this was the original intent of Congress, some consider it a failure. According to Carney (2018), “buying a child for sex is as easy as ordering a pizza online” (p. 353) via sites such as Backpage.com. While Backpage.com is now defunct (the US Department of Justice eventually seized Backpage.com for facilitating prostitution under the US Travels Act (Loew, 2021)), sites like it are still up and selling sexual encounters online (Gamiz, 2019). Under Section 230, sites like Backpage.com and its brethren can claim they are not responsible for the content posted on their website because they provide no editorial control, i.e. they are not publishers as defined by the law (Carney, 2018). State cases in court have also ruled against those seeking remedy to remove offending or defaming material from platforms (Murcia, 2020).

Ethical Versus Legal Considerations

There is a fundamental difference between ‘can’ and ‘should.’ Section 230 illustrates that difference in very stark terms. Under the law, courts can interpret a web platform as possessing immunity from the effects of posting defamatory material or hosting illegal activity. However, should they? Perhaps Congress should lean more into the utilitarian nature of the argument, and not as Barque-Duran et al. (2017) suggest is a half-hearted deontological approach (“prompted by the emotional content of a given dilemma” (p. 184)).

If the argument under utilitarianism is for the greatest good or least amount of harm, Congress’ efforts could be seen as a failure. By removing the ability to hold people accountable for defamatory material and illegal activities (80% of prostitutes are women (Lubin, 2012) and women are nearly twice as likely as men to be targets of abuse (Duggan, 2020)), Carney (2019) suggests Section 230 de facto promotes abusive behavior towards those women (Carney, 2018). How does this reconcile with the least harm goal when it appears Congress privileged Internet expansion as a greater good for the country?

Mitigating Ethical Concerns

The obvious question to ask is how to address these concerns about problematic content while still fostering the innovation the Internet made possible. One solution was the FOSTA-SESTA law, signed into law by President Trump in 2018. Congress designed this law to address the challenges Leary (2018), Carney (2019), and others bring up. It amended the the CDA to “not stand in the way of civil and criminal action against those sites that violate sex trafficking laws” (p. 365). However, recent reports on FOSTA-SESTA’s effectiveness show it is struggling as federal law enforcement has only brought a single case to court using it (Pexton, 2021). Of course, this change in the law brought unintended consequences as well. FOSTA-SESTA drove underground many voluntary sex workers and made their lives more dangerous (Tung, 2020). One solution to address this unintended consequence is to legalize voluntary sex work, i.e. prostitution, thus removing it from the reach of FOSTA-SESTA.

However, FOSTA-SESTA was not designed to deal with the defamatory content issues (at least under California law) that Murcia (2020) notes. While not as vulgar as child sex trafficking, reputational damage to one’s livelihood and standing can have significant consequences (Murcia, 2020). Murcia (2020) proposes a solution that allows those claiming defamation to allow injunctions against platforms that host defaming material. Without the benefit of a formal legal education, it is beyond this paper to determine if this proposal passes constitutional muster, but it seems appropriate.

Conclusion

Section 230, while crafted with arguably good intentions, has caused several problems through its legal interpretations in the courts. First, it allows defamatory content to remain online because there is no legal way to force platforms to remove it. This has specific ramifications for those targeted by the defamation. Second, it prevents federal law enforcement from targeting online sex trafficking (and remedies to this problem have proven less than useful). Section 230’s effectiveness can be improved in these two areas by clarifying Congressional intent through additions to the law around defamation and by removing the dragnet approach to all sex work by focusing specifically on the illegal trafficking of human. Section 230 served a purpose as the birth of the Internet, but it needs to be adapted for changing times.

References

Barque-Duran, A., Pothos, E. M., Hampton, J. A., & Yearsley, J. M. (2017). Contemporary morality: Moral judgments in digital contexts. Computers in Human Behavior, 75, 184–193. https://doi.org/10.1016/j.chb.2017.05.020

Bonde, S., & Firenze, P. (2013, May). A framework for making ethical decisions. A Framework for Making Ethical Decisions | Science and Technology Studies. https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions.

Burke, M. (2011). Cracks in the armor?: The future of the Communications Decency Act and the potential challenges to the protections of section 230 to gossip web sites. Boston College of Law, 17.

Butler, C. (2000). Plotting the return of an ancient tort to cyberspace: towards a new federal standard of responsibility for defamation for internet service providers. Michigan Telecommunications and Technology Law Review, 6(1). https://repository.law.umich.edu/mttlr/vol6/iss1/6/.

Carney, E. (2018). Protecting internet freedom at the expense of facilitating online child sex traffifficking? An explanation as to why CDA’s Section 230 has no place in a new NAFTA. Catholic University Law Review, 68(2), 353–378.Communications Decency Act, Congress.gov (1995). bill. https://www.congress.gov/bill/104th-congress/senate-bill/314/text.

Cornell University. (n.d.). 47 U.S. Code § 230 — protection for private blocking and screening of offensive material. Legal Information Institute. https://www.law.cornell.edu/uscode/text/47/230.

Duggan, M. (2020, September 18). Online harassment 2017. Pew Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2017/07/11/online-harassment-2017/.

Electronic Freedom Foundation. (2021). Section 230 of the Communications Decency Act. Electronic Frontier Foundation. https://www.eff.org/issues/cda230.

Gamiz, M. (2019, April 6). Backpage is gone, but a more Graphic version is AGAIN Fueling prostitution busts in the Lehigh Valley. The Morning Call. https://www.mcall.com/news/breaking/mc-pol-sex-trafficking-backpage-shutdown-skip-the-games-20190322-story.html.

Garner, B. A., & Black, H. C. (2021). Black’s law dictionary. Thomson Reuters.

Leary, M. G. (2018). The indecency and injustice of section 230 of the Communications Decency Act. CUA Law Scholarship Repository, 41, 553–622.

Loew, M. (2021, August 30). Trial begins Wednesday in the case against Backpage.com founders. AZFamily. https://www.azfamily.com/news/investigations/cbs_5_investigates/trial-begins-wednesday-in-the-case-against-backpagecom-founders/article_37997c1e-0952-11ec-8367-33fa6e6b719a.html.

Lubin, G. (2012, January 17). There are 42 million prostitutes in the world, and here’s where they live. Business Insider. https://www.businessinsider.com/there-are-42-million-prostitutes-in-the-world-and-heres-where-they-live-2012-1.

Murcia, E. A. (2020). Section 230 of the Communications Decency Act: Why California Courts interpreted it correctly and what that says about how we should change it. Loyola of Los Angeles Law Review, 54(1), 235–274.

Pexton, P. B. (2021, June 29). Advocates for sex Workers vindicated in Section 230 debate by new report. Roll Call. https://www.rollcall.com/2021/06/29/advocates-for-sex-workers-vindicated-in-section-230-debate-by-new-gao-report/.

Reynolds, G. W. (2019). Ethics in information technology. Cengage Learning.

Tung, L. (2020, July 10). FOSTA-SESTA was supposed to thwart sex trafficking. Instead, it’s sparked a movement. WHYY. https://whyy.org/segments/fosta-sesta-was-supposed-to-thwart-sex-trafficking-instead-its-sparked-a-movement/.

Velasquez, M., Moberg, D., Meyer, M. J., Shanks, T., McLean, M. R., DeCosse, D., Andre, C., & Hanson, K. O. (2015, August 1). A Framework for Ethical Decision Making. Markkula Center for Applied Ethics. https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/a-framework-for-ethical-decision-making/.",18375
"Back when I started out in venture in 2015, companies like Workday and Zendesk were changing the game of enterprise sales with their subscription pricing models. Now, we find ourselves talking with many of our Lightspeed portfolio companies about whether or not they should incorporate usage-based pricing.

With usage-based pricing, a customer’s cost is directly proportional to their consumption of the product or service. A number of recent public companies — Snowflake, Twilio, Agora, JFrog — have successfully leveraged this pricing model.

Given the general interest in this subject, I’ll be sharing a series of interviews over the next several weeks with executives in the Lightspeed network who have scaled usage-based companies, starting with Mike Scarpelli, CFO of Snowflake.

Last year, Snowflake had the largest software IPO in history. One part of their appeal to customers is that Snowflake only charges for the volume of data stored and compute resources consumed. The company grew revenue by 124% and posted a 168% net revenue retention (!). This is hands-down best in class when compared to any public software company today.

Mike Scarpelli, CFO of Snowflake

Anoushka: Mike, you have built your career working with iconic software companies including EMC, ServiceNow, and Snowflake and have seen both SaaS and consumption-based pricing. How should a business evaluate usage-based pricing?

Mike: Over time, I believe most software will be delivered in a usage-based model. But it will take some time. As a CFO, what I constantly hear from customers is that they only want to pay for what they are using. No one wants shelfware. Yet today, I negotiate with software vendors all the time who increase deal sizes each year even when our company is not utilizing its seats or licenses.

A usage-based model can work very well for any software that has a lot of transactions and compute. If the software product is an HR tool that people are looking at once a quarter for bonus allocations, then a usage-based model doesn’t make sense. However, there could be many SaaS businesses that can be priced on a consumption basis instead. For example, reps are constantly pinging and updating their Salesforce CRM — this could be a priced on usage-based model. Service Cloud, where customers are logging tickets, could be usage-based. For our customer-facing applications at ServiceNow, customers wanted a usage-based model. However, they did not know how to forecast their growth in usage, making budgeting a challenge.

Anoushka: Is there a “why now” that makes you believe most software will be delivered in a usage-basis over time?

Mike: We now have the ability to really track compute resources per query that any customer is running. If you went back 5–10 years ago, this would be much more challenging. In the days of on-premise software, you were buying the hardware. It did not matter how much compute you were using. You had to do everything on a license or per-seat model.

The cloud vendors have already adopted the model. If you want to compete with the big guys, you will need to evolve your pricing strategy.

Anoushka: What are the challenges of moving to a consumption-based pricing strategy?

Mike: For the business, you need to build an ability to predict your customers’ future consumption to forecast your business on a quarterly and annual basis. At Snowflake, we forecast usage on a customer-by-customer basis and have predictive modeling around future use. There is no out-of-the-box module or product for doing these calculations. We built this system in Snowflake. Our ERP does the invoicing and then the calculation that goes into usage all gets billed within Snowflake. We reforecast revenue daily at Snowflake.

With usage-based pricing, a customer often can’t forecast their own usage growth. Thus, it becomes much more challenging to figure out how to budget for the software. A per-seat license model can be much easier to budget for. Your software needs some type of capability that lets customers model out their growth in consumption

Finally, governance is important. You want to make sure only the people in your customer’s organization who should be using the software are using it. Customers want alerts to see if certain groups’ consumptions are spiking so they can plan around it.

Anoushka: How has this pricing model impacted Snowflake from a team and organizational structure?

Mike: It changes your compensation plan for sales reps. You can’t pay everything on new deals. You have to pay on consumption. Every one of our reps has a big consumption quota. We fundamentally want our reps to be involved in customer success.

We don’t believe companies should have a separate customer success function. The first thing we did when Frank joined Snowflake was we blew up was the customer success function. You are either going to do support, sales or professional services. Customer success is not accountable for anything.

You may hear some pushback from your salespeople and you will also have some portion of your salesforce focused on landing new accounts. However, a bigger percent of your commission dollars are going to be tied to consumption.

In the SaaS world, your FP&A group can develop a forecast in a silo in isolation from salespeople. The days of FP&A doing forecasting in this manner are gone. Our revenue and FP&A team is aligned with sales and our sales reps. The group has a regular cadence review of our top 50 to 100 customers. Our FP&A group spends a lot of time with salespeople to understand every element of our large accounts and the trends happening with consumption.

Anoushka: What are the core KPIs you think about? Have these changed at all given Snowflake’s usage-based model?

Mike: In the SaaS world, billings is the key metric you look at. Snowflake lets customers pay quarterly, monthly or on-demand so we don’t focus on billings at all. We focus on remaining performance obligation — for all your booked business, how much is going to roll up into revenue. It is the best leading indicator for our investors. Naturally, we care about revenue and revenue growth as well. Investors also want to believe there is leverage in your model. It is important to show a path to higher gross margins and pounding that with investors as that continues to grow for us

Anoushka: How have you managed pricing and pricing discounts which becomes very important for scaled customers in a consumption model?

Mike: You target the margins you want to meet. There are different discount bands based on the size of the deal. We spent a lot of time around this. As a new company breaking into new geographies and verticals, you need to do discounting for those customers. As you become mature, customers talk and you need to be very disciplined around your discounting. You need to make sure to treat your customers like for like because they all talk.

We have become much better disciplined around discounting like for like customers. We put in place a sales finance function to ensure discounting discipline and to help customers model their spending based on the discount. We also put in place a deals desk function that can quarterback a deal process to ensure salespeople are not spending their time pinging various internal groups.

Anoushka: Any final thoughts?

Mike: Many startups today espouse the belief in growth at all costs. I totally disagree with that. Growth is important but too many young companies waste money on things that there is no ROI. You need to spend the money wisely.

*This interview is part of a series of posts exploring usage-based companies. Interviews have been edited slightly for clarity.",7693
"What Really IS a Matrix Determinant?

Image by Author (marcelmoos.com)

Thinking back to my high-school years, linear algebra was a topic I was particularly fascinated with. It gave me the skill to solve large systems of linear equations and a geometric perspective on the problem making the whole process intuitive.

However, regarding matrix determinants, I was taught that they are numbers for matrices, how to compute them, and not much more. It took until my university courses that I learned the beauty behind determinants.

As soon as I learned about determinants’ geometric meaning, I was wondering why this wasn’t already taught in high school as it is very easy to understand and mind-opening.

In mathematics, how to compute something should never be the first question. The first question is always: “What really IS it?”. Only then should we ask: “Ok now that we know what it is, how can we compute it”. Take derivates, for example, as most of us know what derivatives are:

Given a function, its derivative is its slope or rate of change.

It is such a simple description. Yet, defining derivatives in this way is so powerful and liberating. We understand what a derivative IS, regardless of the concrete function, or the dimensionality of the function, and regardless of how to compute it. The actual computation of derivatives differs a lot between different functions. However, derivatives’ fundamental meaning ties everything together and brings order into chaos.

No teacher would introduce derivatives to students like: “Given a function, a derivative is just another function and this is how you compute it…”. Still, for matrix determinants, such explanations seem to be widely spread. Defining determinants by their geometric meaning instead of just some numbers is just as powerful as thinking of derivatives as slopes instead of just as functions.

Determinants & Geometry

Before diving into determinants, let’s quickly recap what they are defined for: Matrices.

A matrix is a table of numbers, representing a linear function taking a vector as input and producing another vector as output:

Instead of a matrix transforming one single vector we can also think of a matrix transforming multiple (or even all) vectors at the same time:

Can you see it? It looks like our chosen matrix stretches space apart. Whatever area in the input space we choose, it seems that after the transformation the area gets bigger. This is precisely what the determinant is!

T he determinant of a matrix is the factor by which areas are scaled by this matrix.

Because matrices are linear transformations it is enough to know the scaling factor for one single area to know the scaling factor for all areas. Let’s go back to our example:

The rectangle inscribed by the pink and blue unit vectors and has an area of 1. After applying our matrix transformation, this rectangle has turned into a parallelogram with base 2 and height 2. So it has an area of 4. This means, that our matrix scales areas by a factor of 4. Therefore, the determinant of our matrix is 4. Neat, isn’t it?

There is one caveat to the story: Determinants can be negative! If we start with an area of 1 and scale it by a negative factor, we would end up with a negative area. And negative areas are nonsense. So how can we make sense of our nice geometric definition in the presence of negative determinants? Luckily the fix is straightforward: If a matrix has a negative determinant, let’s say -2, areas are scaled by 2. The minus just means that space reversed its orientation. “What does that now even mean?”, you might rightfully ask. Let’s take a look:

We can see that the given matrix scales areas by a factor of 2. If we look closely we further notice that the blue vector was on the right of the pink vector but ended up on the left side. This is what’s meant by “space reversed its orientation”. That’s why the determinant of the matrix is not 2 but -2. Including negative determinants we get the full picture:

T he determinant of a matrix is the signed factor by which areas are scaled by this matrix. If the sign is negative the matrix reverses orientation.

All our examples were two-dimensional. It’s hard to draw higher-dimensional graphs. The geometric definition of determinants applies for higher dimensions just as it does for two. In three-dimensional space, the determinant is the signed scaling factor for volumes and in even higher dimensions for hypervolumes.

With Knowledge Comes Power

Equipped with this new geometric definition of determinants we can solve things with ease which would be much harder to handle without it. For example, you might or might not have heard the following fact:

If a matrix has a determinant of 0 it is non-invertible.

A matrix being non-invertible means that the transformation the matrix represents cannot be undone or reverted. If we only know how determinants are computed and nothing about their geometric meaning, justifying this fact is tough. In contrast, using our freshly established intuition about determinants explaining why this is true becomes not that hard:

Let’s say we have a matrix with determinant 0. This means that the matrix scales all areas by a factor of 0, which in turn means that all areas become 0 after the transformation. This can only happen if the matrix squishes the whole space into a lower dimension. For example, the two-dimensional space would be squished into a single line or point and such a transformation cannot be undone.

Arriving at this point, we can be proud of ourselves. We introduced matrix determinants as area scaling factors and managed to justify a famous property of matrices and determinants. And we did all of this without even considering how determinants are computed. But that question should anyway be secondary.",5812
"Technology, Psychedelics , Bitcoin and Culture.

Meditation- taking a step back and observing your mind, without judgement.Becoming aware of your mind. Can bring up repressed trauma. Therapeutic rewiring

Psychedelics- effectively enhanced mental activity, breaking down the concepts and cognition we have evolved and with which we understand and navigate the world. Destroying the old so that the new can emerge. Trying to ‘see’ your mind with a cognition that has not quite yet reached the necessary stage of evolution to do that. Fast, intense rewiring. Profound learning

Steep learning curves- A viscerally felt rewiring , making new connections between seemingly unrelated concepts, new meaning, learning.

Technology- has profoundly impacted our psychological evolution since the first stone tools were used. Each instantiation of new tech has facilitated the ever increasing complexity of our psychology and thus how we organize our societies, communities, civilizations.

Bitcoin is the next iteration in the evolution of the technology of money. Money is a technology that facilitates cooperation between people. Understanding Bitcoin necessitates understanding money, thus facilitating awareness of behavior. Old concepts of money are deconstructed and re-assimilated, re-defining how value is denoted, even what value is (is value a specific kind of meaning?). Over time, the impact of technology on our behavior and thus culture can be compared to that of a psychedelic experience and steep learning curve, the destruction of old concepts for the emergence of new, and the connecting of seemingly unrelated domains and ideas, rewiring to incubate new meaning, inspire new behavior and insight.

This is a steep learning curve (The rabbit hole). Each major new technology has resulted in disruption then settlement and new culture formed. Technological evolution describes an exponential growth curve. The time between settlement and next new tech is shorter and shorter. Like a pulse of change on the timeline of human history. Each pulse is closer to the last and thus seems stronger. Much like a psychedelic trip. I guess you could call it the evolution of consciousness if we couch it in a global and (human) cultural context.

This post was prompted by a query to a reply I posted on twitter in response to a tweet from Tim Denning.",2349
"“The best way to predict your future is to create it”

— Abe Lincoln

Free version of the article available here

Big changes need small ones first.

Legalization of psychedelics is a massive undertaking, and yet we see progress in Denver, Oakland, Santa Cruz, Oregon, and DC.

Ken Burns’ excellent documentary “Prohibition” provides a great case study.

Two lessons stood out

Adversity helps: the Great Depression exacerbated the demand to re-legalize alcohol Single issue politics: sticking to one point was what got the ban on alcohol repealed

Two lessons, apropos now:

Adversity: COVID triggered a mental health crisis that accelerated demand for solutions Single issue: legalize psilocybin.

All due respect to other psychedelics, there’s only one Mona Lisa.

Psilocybin is safe, sustainable, and the most well-documented psychedelic compound.

So let’s wave a magic wand, imagine psilocybin achieves full legalization, and play this out.

Psilocybin legalized

What might this world look like?

Like most compounds, one-size-fits-all ain’t gonna cut it.

So, let’s approach this like an ER doctor that triages patients, or a tech entrepreneur that segments users into different cohorts.

Four groups

High risk e.g. treatment resistant mental illness Medium risk e.g. healthy, but struggling Low risk e.g. stable Recreational

High risk

Persona

This user/patient, let’s call him Tough-Time-Tommy, is quite unwell. Tommy (42 years old) is suffering from treatment resistant depression and bipolar disorder.

So what

Tommy requires significant care and supervision. Provide him with all the tools at our disposal for psychedelic use within the medical system.

This may apply to before, during, and after his entire multi-session psychedelic therapy.

Protocol attributes

There are many fantastic guide protocols (e.g. CIIS or Synthesis) that detail the nitty-gritty. A few high-level for Tommy:

5–10 hours of prep work with a trained professional

1–2 guides present during every session

5–10 hours of post journey integration

Medium risk

Persona

Let’s call this user Training-Wheels-Tiffany (29). She’s generally healthy, but not great, and she runs the risk of descending into more pathological mental illness.

So what

As they say, better to teach a man to fish.

The first couple psychedelic sessions are very different from subsequent journeys. Give Tiffany hands-on guide support for her first few experiences, then remove the training wheels.

A healthy individual that’s gone through a couple guided sessions will be more likely to safely conduct journeys on her own.

In addition, like with wave function collapse, observation changes the outcome. Some psychedelic experiences are better when you are completely alone.

Protocol attributes

Once Tiffany has gone through a couple sessions and gotten accustomed to the experience, provide the tools for her to continue work on her own.

2–3 hours of prep work with a guide or trained professional

1 guide present for only the first 2–3 sessions

2–5 hours of integration work per session

Tools for solo journeys

This will reduce costs for Tiffany and the burden on the medical community… and enable deep solo journeys.

Low risk

Persona

Let’s call this user DIY-Dave. Dave (37) either has previous experience or feels confident in his ability to handle the journey.

Dave feels healthy, strong, and doesn’t like being told what to do.

He wants to optimize cognitive abilities and find purpose.

So what

Don’t make Dave jump through hoops, because he won’t.

This is the cohort that breaks the regulated model. For the determined, it’s fairly easy to obtain or grow psilocybin mushrooms so DIY Dave is unlikely to ask or wait for a prescription.

Give him tools that make the experience safe, satisfying, and meaningful, without being onerous.

It’s similar to the debate around providing young adults with free condoms. We know they’re going to engage in sexual activity whether we provide condoms or not, so better to encourage safety.

We know Dave will engage in psychedelics with or without us, so let’s help him do it right.

Give him a tool that encourages robust prep, follow up, and integration… and gives him power over the process.

Protocol attributes

Dave likes things simple and efficient. Provide an elegant-UX tool that enables a few things:

Suggestions for set-and-setting

Post-journey integration

Connection with others

A feedback loop

Recreational

Persona

Let’s call this user Socialite-Stephanie. Steph (22) wants to explore psychedelics with her best friends.

She’s losing interest in alcohol and wants to explore a different state of mind.

So what

“Life, uh… finds a way,” and much like the sentiment from Jeff Goldblum’s Jurassic Park quip, Steph will find a way to use psychedelics recreationally.

People like Steph will often turn into future intentional users, so, rather than demonize her, better to help her engage safely.

Protocol attributes

Provide her with the same tools we offered DIY-Dave. In addition, emphasize that during her psychedelic experience:",5082
"7 Penny Stocks To Buy According To Analysts With Targets Up To 316% Joe Sirianni Jul 22·6 min read

Should you buy penny stocks today? If you like volatility, quick gains, handle high-risk situations and understand trading basics, I say, why not? But the stock market today isn’t like it was even just a year ago. The fact is, millions of new traders have flooded into the markets since the pandemic. The goal is making money without a 9 to 5, and so far, many are finding real opportunities to profit.

Over the last few days alone, we’ve seen seriously bullish action in the stock market. Today former penny stock ImmunoPrecise ( NASDAQ:IPA) was the latest to grab attention from retail traders. On Monday (7/19), IPA stock was traded around $5 with a volume of fewer than 60,000 shares. Fast-forward to today (7/22), and IPA stock reached highs of $16.47 before the afternoon session got into full swing, with more than 60 million shares traded.

[Read More] 3 Biotech Penny Stocks To Watch Seeing Explosive Gains In 2021

This wasn’t an outlier either. Other stocks this week like Chembio Diagnostics, Inc. ( NASDAQ:CEMI) and NeuroMetrix Inc. ( NASDAQ:NURO) made parabolic moves as well. CEMI stock jumped from lows of $2.20 to highs of $6.95, while NURO stock jumped from around $3 on Monday to over $40 today.

Should You Buy Penny Stocks?

Let’s get something straight; all penny stocks don’t make these types of moves. Many end up failing. But as I said earlier, if you know how to trade penny stocks, this volatility is something you seek daily. Retail traders aren’t the only ones looking for opportunities in the stock market. Today, we’ll look at a few names that Wall Street firms are actually following and giving price targets upwards of 316% right now. Will they end up on your list of penny stocks to buy right now or avoid entirely?

*Note: This is a Continuation Of Our Article “ Best Penny Stocks To Buy Now? Analysts See 85%-288% Upside In 3.”

Penny Stocks To Buy [or avoid] According To Analysts

Performant Financial Corporation (NASDAQ:PFMT) Kadmon Holdings Inc. (NASDAQ:KDMN) Seelos Therapeutics Inc. (NASDAQ:SEEL) 9 Meters Biopharma Inc. (NASDAQ:NMTR)

Penny Stocks To Buy [according to Craig Hallum]: Performant Financial Corporation (NASDAQ:PFMT)

Earlier this month, analysts at Craig Hallum initiated coverage on Performant Financial Corporation. The firm started PFMT stock with a Buy rating and a $10 price target. Based on the current price, that’s an implied upside of roughly 114%. Obviously, time will tell if this comes to fruition, but what are some things to look at right now?

Performant aids clients in the government and commercial sectors, helping them find ways to expand revenue models and streamline spending. Craig Hallum analyst George Sutton pointed out that “With a conservative near-term potential revenue opportunity of $700–960M and expectations for legacy revenue to be completely out of the picture starting in FY22, we believe investors should get involved before the market picks up on the opportunity…PFMT has built a massive data set covering >200M lives and invested in developing sophisticated AI to identify inaccuracies. These investments result in higher cost savings for clients, better quality scores, and better margins for PFMT.”

Echoing this bullishness is the PFMT stock chart. Year-to-date, this penny stock has quietly climbed from $0.88 to over $4.60 this week. Financial technology — Fintech — has received its fair share of attention thanks to the pandemic and things like cryptocurrencies gaining mainstream attention. One thing to focus on is the company’s exposure to opportunities in healthcare. In its last few earnings updates, management has stood behind its 2021 target range for healthcare revenue of $83 — $90 million and positive EBITDA.

Penny Stocks To Buy [according to Oppenheimer]: Kadmon Holdings Inc. (NASDAQ:KDMN)

Since we last covered Kadmon, shares have remained range-bound. KDMN stock typically trades between $4.22 and $3.50, with its 200-day moving average acting as a consistent level of resistance over the last few months. However, this week was the few times that shares broke above this upper 200-day moving average resistance level.

The initial catalyst for the recent trading action came after Kadmon announced that the FDA granted full approval of its REZUROCK treatment for Chronic graft-versus-host disease. Now we look to August as the next important time for Kadmon. This is when the company expects REZUROCK to be available in the US.

[Read More] Highest Volume Penny Stocks to Watch Today? Check These 4 Out

This wasn’t the only thing helping give KDMN stock a boost, either. Earlier this month, analysts at Oppenheimer gave an update to their current rating. The firm has an Outperform but upped its price target from $8 to $9 following the latest FDA news. That puts an implied upside of 117% based on current trading levels.

Analyst Mark Breidenbach explained, “We discussed Rezurock’s label and pricing with management, and we believe Kadmon is fully prepared to begin promoting Rezurock despite the early approval-thanks in part to a PDUFA date extension earlier this year…We expect Rezurock could take ≥three months to be added to payer formularies and will take time to gain market share relative to ruxolitinib, which is already widely used off-label in cGVHD.”

Penny Stocks To Buy [according to Guggenheim]: Seelos Therapeutics Inc. (NASDAQ:SEEL)

Seelos has long been on our lists of penny stocks to watch throughout the year. The company is part of the growing list of psychedelic stocks to watch. It’s also one of the larger holdings in the first psychedelic ETF, Horizon’s Psychedelic Stock Index ETF (PSYK). It currently has 5.71% weighting in the ETF (as of July 16th).

Thanks to its progress in its racemic ketamine trials, retail traders have taken notice. An update a few weeks ago revealed more details from its current ketamine study for acute suicidal patients. The company explained that part 2 of the study would be the basis for a registration trial for its SLS-002 racemic ketamine program. CEO Raj Mehra went as far as saying that SLS-002 has the potential for rapid onset and “aims to show a benefit as quickly as overnight.”

Wall Street analysts may also see potential in this company as well. Guggenheim was the latest firm to jump on the bullish bandwagon this month. Analyst Yatin Suneja started coverage with a Buy rating and $8 target, implying an upside of over 220% from current trading levels. Suneja explained that “The current data suggest that treatment effects with SLS-002 are rapid, robust and durable,” and if approved, “$450M in peak U.S. sales with additional upside in other indications.”

Penny Stocks To Buy [according to BMO Capital]: 9 Meters Biopharma Inc. (NASDAQ:NMTR)

Earlier this month, BMO Capital initiated coverage on 9 Meters Biopharma. Analyst Gary Nachman started NMTR stock with an Outperform rating and a price target of $5.00. Based on the current trading levels, that would imply a potential upside of 316%. 9 Meters focuses on developing treatments for gastroenterology. It recently acquired the global development rights to a humanized monoclonal antibody, LOB-0136, from Lobesity LLC. LOB-0136, now NM-136, aims to treat patients with Prader-Willi Syndrome.

“Continuing to advance NM-136 will be a very important step forward for patients currently living with PWS and suffering from this life-threatening disease,” said John Temperato, President and Chief Executive Officer of 9 Meters. “This meaningful therapeutic complements and broadens our pipeline of digestive disease assets, while providing us strategic options within our expanded portfolio.”

Aside from this, progress with its Vurolenatide short bowel syndrome candidate is grabbing interest. The company just began its Phase 2 VIBRANT Study on the treatment in June. Nachman also made mention of it in his commentary in tandem with his rating.

[Read More] 4 Best Penny Stocks To Buy As Musk Recharges Dogecoin & Ethereum?

“Vurolenatide offers unique mechanism for short bowel syndrome with promising earlydata and good visibility on Ph2/Ph3, and larazotide could be first approved treatmentfor celiac disease with Ph3 ongoing. Physicians are excited about having both productsavailable given lack of treatments. We conservatively forecast ~$700mm revenue in2030 with just modest penetration. Key catalysts over next 6–18 months should help unlock value.”

Are Analysts Right About These Penny Stocks?

It’s ok to question analysts and their recommendations. Sometimes they are outdated or don’t quite align with the current market sentiment. This list of penny stocks is made up of companies with very recent analyst ratings and, with that, could have a bit more relevance in the stock market today. But the ultimate decision to buy them or avoid them is up to you.

You can read about the rest of the 7 penny stocks to buy according to analysts: Best Penny Stocks To Buy Now? Analysts See 85%-288% Upside In 3",9070
"Pioneering The Next Generation Of Targeted Psychoactive Medications

PsyBio Therapeutics is developing biosynthetic psychoactive compounds that offer a new paradigm of treatment to reverse mental health issues. We are dedicated to exploring and pioneering the next generation of Psychoactive Medications. Our team is well-versed in drug discovery based on synthetic biology and metabolic engineering. There is extensive ongoing research for naturally occurring psychoactive tryptamines which were originally discovered in different varieties of hallucinogenic mushrooms. PsyBio, in partnership with Miami University, is in the process of preparing the initial molecules for clinical batch manufacturing and concurrently preparing an investigational new drug application to study the compounds in patients with cancer-related depression.

We are dedicated to uncovering breakthroughs for the treatment of mental health. The difference between psychoactive medications and the typical treatment is vast, and PsyBio wants to best benefit their client. Psychoactive medications treat the illness by rewiring the brain through contemplation and a change of perception, whereas standard treatment of mental illness, such as the use of SSRIs, SNRIs, MAOIs, NDRIs, etc, chemically treat symptoms which keeps treatment very surface level. Alongside our partners at Miami University, we are working to biologically synthesize psilocybin.

3 primary breakthroughs for the treatment of mental health issues:​

1. Discovered an efficient, patent-pending bacterial biosynthesis process for tryptamine production.​

2. Developing a portfolio of tryptamine-derived, psychedelically inspired molecules.​

3. Demonstrated the entourage effect of certain tryptamines in a laboratory-controlled environment.",1787
"What are Delegations Anyway?

The Cosmos Hub utilizes Tendermint as the underlying BFT consensus engine and uses a BPoS protocol for block proposer elections, where each proposer (validator) is weighted according to their relative total bonded stake and where a validator’s total bonded stake is directly correlated to their chance to propose a block and the amount of rewards they receive for doing so. The protocol supports a fixed number of bonded validators [1].

These validators can self-bond, meaning they can delegate ATOMs to themselves, and they can also receive delegations from any other atom holder. These bonded ATOMs acts as collateral and cause each delegate, including validators, to have “skin in the game” so to speak. If any equivocation or byzantine behavior by a validator were to be committed, the validator and its delegates would be slashed a percentage of their relative total bonded stake.

Delegates may have many reasons to delegate their ATOMs. First and foremost, delegates and validators receive rewards each block that are proportional to their total amount of bonded stake per each validator they’re bonded to. Secondly, it directly contributes to the security of the entire network. Of course delegates and validators themselves may choose to unbond their ATOMs for a variety of reasons.

However, it is important to note that these ATOMs are subject to what is known as the UnbondingPeriod [2], an on-chain parameterized period of time upon which all delegates, including validators, must wait for their ATOMs to become fully unbonded. Until the UnbondingPeriod has passed, the ATOMs are essentially locked. In addition, these ATOMs are still subject to be potentially slashed upon commitment of any byzantine behavior. The UnbondingPeriod ensures a variety of security measures in the network, such as accounting for network synchrony assumptions, providing a lower bound for the length of a long-range attack [3] and solving the “nothing-at-stake” problem.

Before we dive into some of the constraints and finer details to consider when delegating, we’ll need to also define what a redelegation is. A redelegation is simply a “rebonding” or moving a portion (or full amount) of your staked ATOMs from one validator to another. This can be beneficial for a number of reasons. For example, if a delegator is no longer happy with the services or commission rates the validator is providing, then they may decide to move their bonded ATOMs to another validator without having to wait the full unbonding period to make the switch. However, there are some limitations and we’ll discuss those below.

(Re)Delegation Constraints

So now that we have a decent understanding of the role (re)delegations play in the network and the UnbondingPeriod they are subjected to, let’s take a look at some of the finer details to consider before performing them. As we’ve mentioned previously, bonded ATOMs are subject to an UnbondingPeriod . In addition to this, there is another on-chain parameter of importance, namely MaxEntries [4]. The MaxEntries parameter pertains to the total number of:

Unbondings between a unique delegator and validator pair

Redelegations between a unique delegator, source validator, and destination validator tuple

The concept of “entries” are used in the Cosmos Hub state machine to hold information between a delegator, the unbonding amount, and the corresponding validator(s). From the perspective of a user, they are mostly abstracted away.

To illustrate this better, we’ll use the following example. Assume a delegator D has bonded N ATOMs to validator X. Of course D could have other delegations of varying amounts to other validators, but for this example we’ll only consider a single validator. Now delegator D wishes to undelegate some of her staked ATOMs from X, so D sends a MsgUndelegate tx unbonding M ATOMs. This creates an “entry” in the ledger’s state machine between D and X. Going forward if D wants to further undelegate some more staked ATOMs from X, she’ll send another MsgUndelegate tx. However, D can only do this up to MaxEntries times! Any further attempts will yield the following error:

too many unbonding delegation entries in this delegator/validator duo, please wait for some entries to mature

So delegator D will have to wait the full UnbondingPeriod before she can attempt to undelegate any more staked ATOMs from validator X.

For redelegations, the same concept applies. In other words, if delegator D wishes to redelegate some of her staked ATOMs from validator X to another validator Y, she can only do this up to MaxEntries times. Note, for redelegations this is per unique delegator, source validator, and destination validator tuple.

There is one other important key factor to note about redelegations and that is that “transitive” redelegations are prohibited. Transitive redelegations can be seen as “hopping” from one validator to another. For example, if delegator D redelegates to validator X and then wants to further redelegate from X to another validator Y. Delegator D would have to wait the full UnbondingPeriod period to do this. However, it is advised not to delegate to a single validator only, so this restriction should not be a problem in most cases. This is also not to say that transitive redelegations will always be prohibited. In the future they may be enabled but with a cap on the total number of “hops”. Perhaps this will come through a governance proposal?

Slashing

Another critical aspect to keep in mind when performing (re)delegations is slashing. As a delegator, any stake that you have in a validator that is either bonded or in the process of unbonding is liable to be slashed if the given validator performs an equivocation or any other punishable byzantine act during a period in which the infraction is “valid” [5]. For further details on slashing, please review the spec.

With regards to redelegations, there are a few important things to keep in mind. Given our previous example where delegator D redelegates some of her stake from validator X to another validator Y, delegator D is still liable to both validators! Expanding upon this example, assume some infraction occurred at height H₁ by validator X (the original/source validator) and was later discovered and submitted at H₂. Delegator D would only be liable to be slashed if both of the following conditions are true:

The redelegation was created after the infraction at height H₁

The redelegation is not mature (ie. the completion time is after height H₂)

If the destination validator, Y in this case, committed the infraction under the same conditions, then it’s treated no differently than a regular delegation slashing event as described above.

Hopefully you now have a decent understanding of (re)delegations, the role they play in the Cosmos Hub BPoS network and some of limitations and restrictions to keep in mind when performing them. Happy delegation!",6956
"Before diving into this post, if you haven’t already, you should read Part 1: The Dual Adoption Curves of Bitcoin. That post explains how Bitcoin adoption can be understood by exploring its dual adoption curves in the context of the Diffusion of Innovations theory.

In this post, I’ll explain the concept of ‘Crossing the Chasm’ and hypothesize that the Lightning Network is Bitcoin’s Crossing the Chasm Superpower.

Crossing the Chasm

In the early 1990s, Geoffrey Moore studied the relationship between the burgeoning tech startup ecosystem and the Diffusion of Innovations theory. In his book, Crossing the Chasm, Moore posited a minor, yet enormously consequential, tweak to the Diffusion of Innovations theory. He hypothesized that between the early adopters and the early majority existed a massive gap, the chasm, which any technology needs to traverse to go mainstream. Otherwise, if a product could not cross the chasm, it would wither in the doldrums of its niche market and never reach full, mainstream adoption.

The Chasm exists because of the major psychological and social differences between early adopters and the early majority. Early adopters are visionaries searching for revolutionary change. They see a problem with the status quo and they actively want to adopt technologies to fix that. They are willing to take big risks to drive an order of magnitude change. On the other hand, the early majority are risk averse pragmatists who are focused on the day to day problems that exist within their specific vertical. They are searching for incremental, reliable solutions from established players. The early majority is not at all influenced by early adopters. Hence, the chasm…

Moore spends the rest of the book outlining the key strategies necessary to effectively cross the chasm. To briefly summarize, the key to crossing the chasm is relentless focus on one specific market segment, a beachhead. Moore says, “Target a specific market niche as your point of attack and focus all your resources on achieving the dominant leadership position in that segment.” Within that target market, you need to build out a “whole product.” A whole product includes the core product, but also, whatever you need to achieve your compelling reason to buy. This can include additional software, hardware, systems integration, installation and debugging, training and support, standards and procedures, etc. Early majority customers expect a product to just work for their use case. Once a dominant market position is achieved in this beachhead segment, the product can expand strategically into adjacent markets using the reputational gains from the initial segment to convince other members of the early majority.

Airbnb is one of the best examples of successfully crossing the chasm. Originally, Airbnb started as a service where people booked travel accommodations during only the busiest moments in a specific city (i.e. during a major conference). But, it has since grown to become one of the most popular ways to book any accommodation (travel or otherwise). Airbnb accomplished this feat, in part, by focusing on delivering a “whole product” for its early hosts. For example, initially, growth was slow in New York. So, the founders went there to identify the problem. Although the accommodations were up to standards, the photos of the accommodations were terrible. So, they hired professional photographers to take pictures of the accommodations. This decision kick-started growth for the platform. Airbnb didn’t start off in the photography business, obviously, but in order to solve the needs of its initial early majority users, it created a “whole product” that incorporated a photography service. With decisions like this, Airbnb successfully crossed the chasm into the mainstream.

Bitcoin’s Crossing the Chasm Hurdle

As discussed in Part 1, for Bitcoin, overall, the initial adoption has been driven mostly by Bitcoin, the asset. Given the current point on the overall adoption curve along with the projected rate of adoption, it is clear that over the next 10 years, Bitcoin will start to move past those early adopters towards the early majority. To do so, Bitcoin will have to cross the chasm. The early majority, as risk-averse pragmatists focused on the day-to-day problems within their specific verticals, might struggle to grasp the narrative around Bitcoin as purely an investment asset. The early majority, when perceiving Bitcoin as exclusively an asset, will not comprehend how it can directly help them solve their day to day, vertical specific problems. So, in all likelihood, in order for Bitcoin to effectively cross the chasm, reach the early majority, and go mainstream, Bitcoin’s positioning will need to evolve. So, the pertinent question is: how will Bitcoin cross the chasm to reach the early majority?

To answer that question, let’s circle back to Moore who says the key to crossing the chasm is relentless focus on one specific market segment, a beachhead. Focus all resources on building a whole product for that specific segment. This strategy to cross the chasm does not fit neatly with most of the existing or previous narratives around Bitcoin. Those narratives encompass general use cases for broad swaths of the population. For instance, the narrative of a cheap payments network is exciting and interesting. Any business could use that, right? Well, given its broad scope, any business could use it. But, given the lack of a whole product incrementally solving a problem in a specific niche that just works, no business will use it. While early adopters don’t mind general narratives and half-baked use cases, the early majority will only adopt if there are specific use cases that incrementally solve their existing problems.

Bitcoin’s Crossing the Chasm Superpower

For Bitcoin to cross the chasm into the early majority, it will need to meet the unique needs of specific niche audiences who are focused on day to day problems within their business. These audiences want solutions that make their lives significantly easier and give them more time to focus on their core value propositions. If they’re pitched Bitcoin exclusively as an asset, based on their needs, they might not be compelled enough to explore and adopt. But, these early majority users could be very intrigued by the potential solutions enabled by Bitcoin, the network, or a better form of money.

Over the past few years, Bitcoin has developed a superpower in this race to cross the chasm. The Lightning Network will help transition Bitcoin from the early adopters who mostly use Bitcoin, the asset, to the early majority who will use Bitcoin, the network.

Before diving into that statement, let’s quickly zoom out and define the Lightning Network in the simplest terms possible. The Lightning Network is a software layer on top of Bitcoin that allows for instant, low fee, and high volume transactions. The network is based on a technology called payment channels. Payment channels allow for any two Bitcoin users to open up a channel with a defined amount of Bitcoin between themselves. Within a payment channel, these two users can exchange instant, nearly free payments with each other as long as the amount of Bitcoin in the channel meets their bi-directional payment needs. Lightning Network takes this technology and builds a network of payment channels in a fully decentralized, permissionless way. That network of payment channels allows for anyone on the network to pay someone else on the network in an instant, cheap way even if they don’t have a direct channel between them. For example, let’s say Alice wants to pay Bob. But, Alice doesn’t have a channel with Bob. However, she does have a channel with Carol who happens to have a channel with Bob. Alice can route the payment to Bob through Carol. And, because of cryptographic proof and the smart contracts inherent in the design of the Lightning Network, this transaction can occur without Alice needing to trust Carol.

It’s also important to highlight that the Lightning Network is a protocol layer built on top of the Bitcoin protocol. The Bitcoin protocol, powered by adoption of the early adopters, has become an almost trillion dollar, decentralized, global, always-on monetary network. So, the Lightning Network provides the ability to instantly, cheaply, and premissionlessly transact with Bitcoin, a nearly trillion dollar global monetary asset. And, the Lightning Network is completely open source. Anyone can contribute to it and anyone can build with it. Finally, as Lyn Alden discussed in her piece on Bitcoin network effects, the Lightning Network may have even stronger network effects than the Bitcoin base layer. The main constraint for the Lightning Network is liquidity. Namely, liquidity in the right places with a sufficient number of unique channels between unique nodes. As more and more liquidity is added and more channels are created and maintained, this will allow for more adoption, which kicks off a virtuous cycle.

So, Lightning Network is Bitcoin’s crossing the chasm superpower. It gives developers the ability to build on a stack that provides instant, cheap, and permissionless movement of value anywhere in the world at any time. With the Lightning Network and motivated entrepreneurs, Bitcoin, the network, unlocks the power of human ingenuity and optionality in its race to cross the chasm. It can harness the power of entrepreneurs to build compelling uses for niche audience segments leveraging a programmable, global and internet native money. These entrepreneurs will be able to use the open source protocol to solve their payment routing and cost needs. It provides Payment Infrastructure as a Service in the same way AWS provides Cloud/Hosting Infrastructure as a Service. So, instead of wasting valuable bandwidth solving a difficult general problem, entrepreneurs can focus on building useful tools for their specific audiences. With the Lightning Network, Bitcoin will get an almost infinite number of ‘shots on goal’ to build compelling products and use cases for the early majority.

The best historical example to demonstrate the power of human ingenuity and optionality in helping to cross the chasm comes from smartphones. In January 2005, RIM (Blackberry) reached 2 million subscribers marking a major milestone for smartphone adoption. However, it was only the very beginning of the adoption curve. RIM subscribers were the innovators. They were high-powered business people who needed to look at documents and send emails on the go. Then, the first iPhone launched in 2007 with the first Android phone coming a year later. The early iPhone and Android users were the early adopters. They saw the value in having the ability to listen to music, make calls, watch videos and access the internet over their mobile devices. But, smartphones didn’t actually reach the early majority of users until mid 2010. In order to get there, smartphones needed the launch of the App/Play Store in 2008/09. With that launch, developers were able to build on a stack that gave them a touch screen interface, internet connection, location data, and much more. Given that resource, developers built countless remarkable experiences that users wanted and, eventually, could not live without. The app ecosystem drove smartphones across the chasm into the early majority. The early majority didn’t need a smartphone to send emails or listen to music like the innovators or early adopters. But, they did need a way to get home from the bar (Uber). They did need directions to the new sushi restaurant to meet their friends (Maps). They did need a way to easily chat with friends who may not live in the same country (WhatsApp).

Potential Lightning Use Cases

So, what will early majority users need from Bitcoin, the network, an instant, cheap, global, always-on monetary network? Within just the past couple of years of development on Lightning, we’re starting to see glimpses of potential early majority use cases and solutions that might be just around the corner. I will outline just a few of those below. There are many more out there right now. And, even more to come that haven’t even been conceived of yet.

International Remittances

When someone from Country X wants to send money to a family member in Country Y, this is called an international remittance. International remittance payments cost, on average, 6.5% of the amount sent, with lower amounts actually requiring a higher percentage fee. Those prices can be even higher in countries with less formal financial infrastructure, which are often the places that most need remittances due to inflationary pressures and stagnant economies. Even at that cost, international remittances can take anywhere from days to a week to process.

Strike

On Lightning Network, Strike has built a product that allows users to send money instantly, with no fees, anywhere in the world. So, you can take dollars from your account in the United States and send that directly to a user in the UK who receives that money in pounds instantly with no fees. This transaction is possible because the USD is instantly converted to BTC sent to the UK and instantly converted back to GBP.

With this product, Strike immediately solves a real problem for millions of users. That’s a fact, not a hypothesis. In the past two months, Strike launched in El Salvador, the sixth highest ranked country in inbound remittance from the United States. 24% of the nation’s entire GDP is remittance based. After Strike’s launch, it became the #1 most popular app in the country after only 3 weeks. These users are using Bitcoin + Lightning, even if they don’t know it.

Card Rewards

Credit card companies use points as a user acquisition mechanism with the promise that the more you spend, the more reward points you’ll receive, which can be used for travel, experiences, and more. However, the industry’s dirty little secret is that ‘your’ points are at risk of inflation, devaluation, and, even, cancellation.

Enter Fold. Fold allows for users to earn Bitcoin on all of their spending. Rather than accumulating transitory credit card points, users are able to stack hard money with every single purchase. Just in the past few months, Fold has processed over $100M worth of transactions and distributed billions of satoshis to its users. And, in the future, they will launch a Bitcoin Rewards API so that other card companies can create credit cards which allow for users to receive Bitcoin back rather than points.

Fold, Earn Bitcoin on your spending

Creator Economy: Value for Value

In the tech and venture capital world, the creator economy has been a hot sector for years. Recently, Li Jin of Atelier Ventures, a prominent thought leader in this space, posted the following, which suggests that platforms impose an egregious tax on their most valuable users, those that create the content.

Platforms are able to impose these take rates because of their monopoly on user attention. That monopoly on user attention is driven in part by the creators of the content, but also, by the user needs around content discovery and the user unwillingness to pay for broad access to content. As creators grow their followings on these platforms, they are increasingly finding better ways to monetize those users off the platform through tools like Patreon or Substack. So, despite not being willing to pay for the initial discovery platform, once fans find their creators, they are willing to pay for content from that specific creator. That said, price discovery is still ongoing with regard to what fans are willing to pay creators for each type of content.

Sphinx Chat Streaming Payments

On Lightning Network, Sphinx.chat built a product which allows for creators to receive micropayments when they chat with or podcast for their fans. For podcasts, podcasters can set the cost of listening to each episode at a certain number of sats per minute. Additionally, users can tip additional sats while they listen. By creating this product with Bitcoin and Lightning, the payments can be extremely small (micropayments aren’t possible in the legacy financial system). So, users can tip as granularly as they want and hosts can set a very low cost per listen. And, creators can grow their paid following beyond their own borders as they don’t have to manage currency conversion. As the size of the audience builds, the streaming sats paywall along with tipping could actually drive meaningful revenue for creators. Users and creators could build a value-for-value model where users pay directly for the content itself. If this model gets traction amongst creators, it could start to crack the existing internet content model around monopolizing user attention and relying on ads to deliver value back to content creators.

Gaming + Metaverse

Since the invention of digital gaming, games have tried to mimic the use of money by creating unique tokens that can be earned in the game and exchanged for in-game products. Roblox is one of the most well known examples of such an experience. Games created these tokens as a way to reward users and encourage them to purchase status in the game (outfits, tools, weapons, etc.). In this way, countless games have built up these walled garden economies. Even though it would potentially drive more engagement, games are not able to incorporate real money into their experience due to the costs associated with transferring that money between players and from the game to the players.

On Lightning Network, Zebedee has built a platform which allows for game developers to incorporate real Bitcoin payments into their experience. Instead of points with no value outside of the walled garden of their games, game developers can easily incorporate Bitcoin payments. This platform allows for developers to add real skin in their game, which can make the experience much more engaging for end users.

Earning Sats

Stakwork is a Lightning based platform for permissionless microtasks. Through the platform, anyone, anywhere can accomplish small tasks and get paid in Bitcoin. In the last 6 months, workers on Stakwork have been paid for 3M tasks, totaling 171,000 Lightning payments. They have 20,000 workers onboarded in places like Argentina, Nigeria, Ghana, Turkey, and the Philippines with a waiting list about as long.

Real Time VPN

Impervious built a dynamic Lightning based VPN, currently in closed beta. Impervious generates cryptographically secure tunnels that ensure data remains private both at rest and during transit, while also shielding the source of data transmissions.

Bitcoin + Lightning Network = A Bet on Human Ingenuity

When the App Store launched, no one immediately knew that Uber, Doordash, SnapChat and Instagram were going to make smartphones irreplaceable. No one knew which specific experiences would resonate with end users in the early majority. But, Apple and Android bet on the power of human ingenuity to figure out compelling use cases on top of this new, powerful tech stack.

We find ourselves in a similar position with Bitcoin today. Bitcoin, the asset, has kickstarted the adoption curve of Bitcoin overall. And thus, we’re rapidly approaching the crossing the chasm moment. Bitcoin, the network, will help Bitcoin evolve to solve the specific problems of the early majority. The Lightning Network gives Bitcoin the superpower of relying on human ingenuity and optionality to do that. It lets developers and entrepreneurs build on top of a global, always-on, permissionless nearly trillion dollar network to create compelling products for their niches. What will those hit products be? No one knows. But, I’m betting on Bitcoin + Lightning Network. Because, at this point, that’s just a bet on human ingenuity.",19845
"These amazing friends are connecting hodlers and merchants across the world making the dream of crypto currency a reality via the Lightning network.

Founders of Lightning Nomads: Naser Dashti & Jake Senn

Lightning Nomads mission is to grow a community of Bitcoin merchants that accept payment via the lightning network and consumers who are willing to make purchases with Bitcoin to make Bitcoin a truly global currency.

There is a closely held historical relationship between people participating in the cryptocurrency evolution and digital nomads.

The crypto community is naturally decentralized and distributed across the globe because nationality doesn’t predict who will be attracted to new technology and ways of thinking.

So, as people begin looking for a community of like-minded individuals, they have to push past borders, getting to know people around the world, attending conferences together and sharing knowledge.

The lightning Bitcoin map across the world⚡

Merchant Maps Show You Where to Spend Your Bitcoin: which inturn serves as a network map to Lightning Nomads to expand their gateways

Interactive online maps are the most intuitive way to find anything these days, including merchants that are willing to take your crypto for whatever they sell. And while Google Maps can help you locate a few businesses dealing with cryptocurrencies near you, other platforms are far more specialized.

Coinmap.org is one of them, as its website allows crypto companies to share their coordinates for free. The map displays around 16,000 venues around the world that accept cryptocurrency payments. Users can filter these entries by multiple categories such as shopping, café, food, grocery, lodging, transport, sports, and nightlife. It will also show you ATMs where you can withdraw digital coins.

Bitcoin hotspots across the planet

The Lightning Network is a “layer 2” payment protocol designed to be layered on top of a blockchain-based cryptocurrency such as bitcoin or litecoin. It is intended to enable fast transactions among participating nodes and has been proposed as a solution to the bitcoin scalability problem.

Whether you’re most comfortable contributing time to help expand our network of ⚡ merchants or Sats to help us grow, we need you on our team. All spending will be accounted for supporting our mission. The QR code is a lightning wallet address. Make sure to send donations from a lightning wallet.

Lightning Network: Scalable, Instant Bitcoin/Blockchain Transactions for the Future

Instant Payments. Lightning-fast blockchain payments without worrying about block confirmation times. Security is enforced by blockchain smart-contracts without creating a on-blockchain transaction for individual payments. Payment speed measured in milliseconds to seconds.

Scalability. Capable of millions to billions of transactions per second across the network. Capacity blows away legacy payment rails by many orders of magnitude. Attaching payment per action/click is now possible without custodians.

Low Cost. By transacting and settling off-blockchain, the Lightning Network allows for exceptionally low fees, which allows for emerging use cases such as instant micropayments.

Cross Blockchains. Cross-chain atomic swaps can occur off-chain instantly with heterogeneous blockchain consensus rules. So long as the chains can support the same cryptographic hash function, it is possible to make transactions across blockchains without trust in 3rd party custodians.

How it Works

The Lightning Network is dependent upon the underlying technology of the blockchain. By using real Bitcoin/blockchain transactions and using its native smart-contract scripting language, it is possible to create a secure network of participants which are able to transact at high volume and high speed.

Bidirectional Payment Channels. Two participants create a ledger entry on the blockchain which requires both participants to sign off on any spending of funds. Both parties create transactions which refund the ledger entry to their individual allocation, but do not broadcast them to the blockchain. They can update their individual allocations for the ledger entry by creating many transactions spending from the current ledger entry output. Only the most recent version is valid, which is enforced by blockchain-parsable smart-contract scripting. This entry can be closed out at any time by either party without any trust or custodianship by broadcasting the most recent version to the blockchain.

Lightning Network. By creating a network of these two-party ledger entries, it is possible to find a path across the network similar to routing packets on the internet. The nodes along the path are not trusted, as the payment is enforced using a script which enforces the atomicity (either the entire payment succeeds or fails) via decrementing time-locks.

Blockchain as Arbiter. As a result, it is possible to conduct transactions off-blockchain without limitations. Transactions can be made off-chain with confidence of on-blockchain enforceability. This is similar to how one makes many legal contracts with others, but one does not go to court every time a contract is made. By making the transactions and scripts parsable, the smart-contract can be enforced on-blockchain. Only in the event of non-cooperation is the court involved — but with the blockchain, the result is deterministic.

Watch how common marketers, barbers and tattoo artists use Bitcoin via the lightning network:

A barber from the Lightning Nomads community uses the Lightning network to get his fees in Bitcoin

A tattoo artist from the Lightning Nomads community uses the Lightning network to get his fees in Bitcoin

Bitcoin Coffee via the lightning nomads network

Lightning is the future of Bitcoin: why?

In a nutshell, the lightning network allows participants to transfer bitcoins between one another without any fees using their digital wallets. Payment channels are created between the two users so that they can transact with each other — in other words, off-chain transactions. Lightning network is another layer added to Bitcoin’s blockchain so that it can process micropayments between participants.

The goal of the network was to create channels in which payments could be made between users without any fees or delays. By allowing the transactions to be done off-chain, the processing time and the number of transactions done via the on-chain network would be improved.

An example of workflow of Bitcoin transactions via lightning

It solves Bitcoin’s transaction fee Problem upto a viable extent : How and why?

Lightning Network is often touted as a solution to the problem of bitcoin’s rising transaction fees. Its proponents claim that transaction fees, which is one of the direct consequences of Bitcoin’s clogged network, will come down after the technology takes transactions off the main blockchain. But bitcoin’s congestion is one among several factors that influence its transaction fees. Besides, the cryptocurrency’s fee itself is a large component of the lightning network’s overall costs.

1. Opening and Closing Channel Costs

Specifically, there are two parts to their costs. The first part consists of a fee equivalent to Bitcoin’s transaction charges in order to open and close channels between parties. Although the lightning network allows payments between two parties, an opening transaction or deposit must be made via on-chain. The two parties then can process multiple transactions between each other, but once the bill has been settled, they need to record a closing transaction for the settled amount on the blockchain.

2. Routing Fees

Besides the transaction fees to open and close channels, there is a separate routing fee to transfer payments between channels. Since the fees for the lightning network are quite low, in theory, it should attract more participants. However, if the fees are so low for the routing of payments between nodes, there might not be any incentive for the nodes to facilitate the payments. Also, as businesses adopt the lightning network as a method of payment, they may also charge fees.

This problem contrasts with the approach being taken by other cryptocurrencies to increase their payments business. For example, Dash has free software plug-ins for merchants to download and use. Dash uses masternodes, who must have deposited 1,000 in Dash coins so that they can approve transactions very quickly. The fees for users are approximately two cents per transaction and Dash payments are available at more than 4,000 merchants.

While exchanging funds on the Lightning Network, users pay truly negligible fees. They only have to pay a full network fee when they close the channel.

3. Remaining Online At All Times Makes Nodes Susceptible

Nodes on Bitcoin’s lightning network are required to be online at all times in order to send and receive payments. Since the parties involved in the transaction must be online and they use their private keys to sign in, it’s possible that the coins could be stolen if the computer storing the private keys was compromised. However, cold storage of coins, which is considered the safest method for storing cryptocurrencies, is possible on a lightning network.

4. Offline Transaction Risk

Going offline creates its own set of problems on the Lightning Network. According to Dryja, it is possible for one of the two parties from a payment channel to close the channel and pocket funds while the other is away. This is known as Fraudulent Channel Close. There is a time period to contest the closing of a channel, but a prolonged absence by one of the parties could result in the expiry of that period.

5. Malicious Attacks

Another risk to the network is congestion caused by a malicious attack. If the payment channels become congested, and there’s a malicious hack or attack, the participants may not be able to get their money back fast enough due to the congestion.

According to Dryja, “forced expiration of many transactions may be the greatest systemic risk when using the Lightning Network.”

If a malicious party creates numerous channels and forces them to expire at the same time, which would broadcast to the blockchain, the congestion caused could overwhelm the capacity of the block. A malicious attacker might use the congestion to steal funds from parties who are unable to withdraw their funds due to the congestion.

6. Bitcoin’s Price Fluctuations

The advent of Lightning Network is also supposed to herald Bitcoin’s viability as a medium for daily transactions. Customers are able to open payment channels with businesses or people that they transact with frequently. For example, they can open payment channels with their landlord or favorite e-commerce store and transact using bitcoins.

However, Bitcoin still has ways to go before gaining mainstream traction. The increase in its transaction volumes is largely attributed to a rise in trading volumes. In other words, Bitcoin’s popularity is a double-edged sword since the increased attention garners investment but also attracts more traders increasing the volatility or price fluctuations in the cryptocurrency. The price volatility makes it challenging for companies to use Bitcoin as a method of payment when pricing their products to sell to their customers or to purchase inventory from their suppliers.

Bitcoin Fluctuations in 2021

For example, let’s say a company has to pay an invoice to their supplier of bitcoin. Typically, suppliers give their clients time to pay, such as 30 days. If bitcoin’s price has increased by 10% during the 30 day period, the business has to come up with another 10% worth of fiat currency or another cryptocurrency to convert to Bitcoin and pay the invoice to pay the supplier. This exchange risk exists because the business might be paid by their customers in a fiat currency and not Bitcoin. The exchange risk also exists for consumer transactions since the salary or wages for most individuals are not paid in Bitcoin, leading to transactions being converted from a fiat currency to Bitcoin.

As a result, the overall effect of Lightning Network on reducing Bitcoin’s transaction fees and building scale may be limited since the crypto has yet to be adopted as a method of payment.

The Future of Bitcoin’s Lightning Network

There remain challenges with Bitcoin’s Lightning Network and its ability to boost scale while simultaneously lowering transaction fees. However, the technology’s core team has incorporated new use cases and has been researching additional features. As a result, there have been significant developments that are due to improve the network in 2021 and beyond.

Larger Payments via Lightning Network

Lightning had initially limited channel size to a maximum of 0.1677 BTC but in 2020, it was announced that the constraints will be removed so that clients can have larger channels. These “Wumbo” channels are designed to increase the usage and utility of Lightning Network for consumers and businesses.

Larger payments too can be processed via lighting networ

Crypto Exchanges

One of the most promising initial use cases to emerge involves cryptocurrency exchanges. In December of 2020, Kraken exchange announced that it will begin supporting Lightning Network in 2021. At first, only withdrawals will be allowed as they get systems acclimated, but payment channels may become possible so that Lightning transactions can be done directly with the exchange.

Watchtower

Watchtowers are third parties that run on nodes to prevent fraud within Lightning Network. For example, if Sam and Judy are transacting and one of them has malicious intent, they may be able to steal the coins from the other participant. Let’s say Sam and Judy put up an initial deposit of 10,000 bitcoins and a transaction of 3,000 has taken place in which Sam purchased goods from Judy. If Judy logs off her system, it is open to possible fraud. Sam could broadcast the initial state, meaning they both get their initial deposits back as if no transactions were done. In other words, Sam would have received 3,000 BTC worth of goods for free.

This process of closing the channel based on the initial state versus the final state in which all of the transactions have been done is called fraudulent channel close. The watchtower or third party can monitor the transactions and help prevent fraudulent channel close.

The Bottom Line

The Lightning Network is an ever-evolving concept that is likely to make a significant difference to Bitcoin’s blockchain. However, the network might not be the solution to all of the challenges facing Bitcoin. Also, as new changes and improvements are made to the network, there’s the potential for new problems within the cryptocurrency’s ecosystem. Much will depend on the research and development of new technology in the future.

Connect with Naser on Instagram for the Best tips on earning bitcoins !

Naser’s short videos on earning Bitcoins is definitely worth it!",15039
"In my last article, I have explained what Blockchain is. By taking an example of the Bitcoin network, let us understand how the transactions are recorded.

Before that, let us understand the term Hash/Hashing that we used in the previous article.

What hashing does is it will take your transaction as an input of any length and creates an output of fixed size (64 digits and letters).

Use the website to understand it better. (Click on the text)

Bitcoin network uses SHA 256 Algorithm.

Let us see how transactions are recorded in the network.

1. We need a crypto wallet to make a transaction; after opening an account, load money into your wallet through the bank account and buy the bitcoin. Now your wallet will show the bitcoins that you are holding.

2.To make a transaction or to send someone BTC. We need the recipient’s public key, senders private key, BTC amount. Think of it as the recipient’s home address when you are sending a parcel (public key), and when you are making an online payment to someone, we need the password (private key).

3. Next, the system will do the Broadcasting of all the transactions in the network, and validators validate the transactions on two parameters.

a) Sender has the desired number of BTC in his wallet. b) Private key is valid.

Transactions are added to the memory pool once they are verified.

4. From the memory pool, miners will pick up the transaction to add to the block. (higher the transaction fees — higher the chances of transaction added in the block faster)

5.Miner will propose the next block, and other nodes will verify the same. (blocks are generated using proof of work consensus mechanism — will discuss in next article)

6. A New block is added to the existing Blockchain.

7. The transaction is complete.",1779
"4 Best Methods to Search Data in Your JavaScript Arrays

Photo by Amit Lahav on Unsplash

“What’s the best way to do this?”

This is a question I’m sure you have asked yourself a hundred times using JavaScript. And that’s because this language has so many different ways of doing anything.

If you’re working with arrays, you will often have to search through them. But what’s the best way to do that?

What if you need to know if a value is present in an array? What if you want to filter something out of your collections?

Turns out that in this sea of information, JavaScript has 4 amazing methods to do anything you may need when searching through your arrays.

Let’s start.

Use includes()

The includes() method returns a boolean telling you if the passed-in parameter exists or not in an array.

In my example, I’ve used this method to ask the question: “Hey array, do you contain this value, yes or no?”

An extra important thing to note about this method is that it doesn’t implicitly perform type coercion on the value you pass in. So basically, it won’t modify the type of parameter you pass in to confront values.

As you can see, a strict comparison is performed here, so the number 4 won’t be converted to the string '4' to test the condition.",1258
"Imagine you’re on Wall Street Journal and you want to read this article:

Kind of important in my opinion, but as soon as you read a few words, you’re blocked by this paywall:

And you would subscribe, but $29.99 per month is a little steep for basic coronavirus info and news.

So, today, we’re going to talking about bypassing subscription paywalls with JavaScript.

If you’re not up for spending 100+ hours coding a fully functional Chrome extension to do this, save time by just adding this free extension, called Hover. It got taken down from the Chrome Web Store, but you can still install it fully from Github (just scroll down to the Install instructions :) ).

It bypasses paywalls on pretty much all sites, including Medium, Business Insider, New York Times, Bloomberg, Quora, etc. and is the most effective method of bypassing paywalls on the market right now.

This is the extension in action, on Medium:

Hover Paywall Demo

We’ll go over how paywalls work, the three main ways of bypassing them, and the code to do so.

This is for educational purposes only. Please never violate the Terms and Services of any website.",1132
"By: Heather Roth, and Logan Patterson

The swift changes necessary to adapt in 2020 brought to light the growing need to pivot from an omnichannel back to a customer-centric approach. Retail leaders are already calling for a shift from the omnichannel perspective. Instead, they are considering the customer as the channel. In doing so, retail companies are looking to “embrace the blur.”

The convergence of digital natives with brick-and-mortar

The ethos of omnichannel at the start was simple — find more ways to adapt to customer searching and shopping habits by opening new and creative ways to get the brand and product in front of buyers.

Digitally native companies like Warby Parker, Casper, and Bonobos built branded storefronts or inked partnerships with brick-and-mortar retailers to meet more of their customers where they were and offer a full spectrum of channel options aligned with the brand. In contrast, brick-and-mortar retailers extended their brands through new online channels to keep pace with the growing number of digital retailers.

These moves followed the perceived and now proven ROI of making products available in multiple channels. Data shows that customers using multiple shipping and shopping channels tend to spend 4% more in brick-and-mortar locations and 10% more at online stores.

There can be too much of a good thing

Following the initial success of taking an omnichannel approach, companies enhanced their channel strategy to get in front of as many eyes, and in as many ways, as possible.

Yet as customers became more digitally savvy, expectations of high-quality, differentiated experiences across channels soared. In response, the number of Marketing Technology vendors (currently over 8,000 MarTech solutions) promising to collect, connect, and activate customer data, soared. Efforts in big data and the concept of a “Customer 360” has rarely provided any utility to the consumer at all. In fact, these efforts commonly led to a focus on processing high volumes of transactional data across channels and were often managed in silos. In other words, while there were many early wins, today’s omnichannel approach causes strain on retailers’ data collection-to-activation journey in many areas, including:

The lengthy, iterative, ‘test and learn’ mentality required for success;

Unnecessary silos resulting in slow response to changes in buying behavior, supply chain, and operations;

Digital fatigue and the changing customer desire for differentiated in-person and on-line experiences;

Lower than expected marketing performance from loyalty programs;

Adaptation to new and ever-changing privacy laws.

This leaves many retailers looking for answers about evolving their customer journeys and investing time, effort, and resources accordingly.

Putting the customer back at the center of retail

Retailers are increasingly blurring the lines between physical and digital and working toward letting the customer own their journey. This change to the “customer as the channel” approach will require a monumental shift in how the organization thinks about and uses customer data and how the organization is structured.

Key areas of attention to create customer focus

Rethinking performance metrics: A company’s culture is often dictated by the metrics that they use to run their days. Earlier omnichannel initiatives created performance-focused cultures, driven by clicks, conversions, and ROI. The new customer-focused model requires a shift to Return on Experience (RoX), bringing in data across the entire experience to monitor how other touch points drive outcomes. These data sources may include support data, digital behavioral data, social and review data, traditional Voice of Customer (VoC) data and customer support data.

Focusing on different outcome metrics will be critical for shifting the way companies think about performance. Adding or increasing attention on metrics such as customer retention, customer lifetime value, referrals, issue incidence, and time to resolution can transform how internal teams work together and focus on delivering for the customer.

Creating a customer-centered data unification strategy and taking an agile approach to data modeling: Shifting to an RoX model requires understanding customer personas, the customer journey, and the associated data collection points currently available. These data sources are critical to understanding behaviors, attitudes, or outcomes that drive RoX calculations.

The good news: many retailers already have much of the required customer data needed to build out a customer-centered model. Shifting to an experience-led strategy does not mean retailers have to rebuild their data models completely before transitioning to an RoX focus. Instead, retailers must focus on building a flexible model that starts to align existing data to the customer journeys so they can mobilize around the customer while they create backlogs for future data enrichment through integrations and other collection channels.

Building a flexible data infrastructure as agile as your customer’s behavior: Looking at the customer as the channel requires a more detailed and dynamic view of that customer, supported by holistic, unified, and democratized data. As digital channels took root, the data problem quickly shifted from one of scarcity to one of quality and accuracy, all while being efficient, real-time, and aligned with global/local privacy law. To meet all these criteria, organizations looked to technology to solve the problem before streamlining the underlying data collection-to-activation process. By taking a step back, identifying data sources, integration requirements, and the activation vision, a robust data strategy can be created and often unify the, historically, siloed organizations.

Operationalizing the customer through your people, process, and technology strategies: Companies can re-adjust technology, data models, and measurement, but the channel mindset will likely be challenging to abandon after a decade of working within these bounds. Business model and organizational transformation are critical components to successfully implementing updated data and cloud technologies. Defining the vision and gaining senior leader alignment and advocacy before telling the organization “what’s in it for them” are precursors to unlocking the potential benefits of such a change. To ensure success, it is as important to focus on change management and sufficient education and training of those responsible for implementation and execution as it is to focus on the technology needed.

Business transformation is needed to embrace customer as a channel

Strategic Execution: Leaders need to focus on building a Modern Culture of Data, where data guides every person at every level of your organization. Leading with experience-focused outcomes will be critical. Even more critical will be making data easily available and aligning these outcomes into employee KPIs to create data literacy and help employees connect their work to the overall outcomes.

People & Culture: Retailers need to rethink the channel focus of their organization structure to “embrace the blur” and focus teams on understanding the full journey experience of their customers versus a single marketing channel or domain of the journey. This requires a shift away from channel conversion rates, and more to metrics that monitor ease, relevance, trust, loyalty, and experience outcomes. It also means more horizontal organizational structures, rather than the traditional verticals.

Process: A shift towards a customer-focused retail model requires changes in supply chain strategies, delivery and warehousing, product planning, sales strategies — and all teams need to be looking at the same data story for this change to work. It is critical to ensure that the people, processes, and training are well thought out during implementation to ensure the success of the new technology and data approach.

Technology: Organizations will likely need to take a multi-pronged approach to implementing technology in order to make the most of the investment today and build on it in the future. Depending on where a company is in its data maturity, it might start with migrating data to the cloud and retiring outdated on-premises machines or creating new applications to utilize the data and technology available fully. When making these critical investments, organizations must ensure the return is directly connected to, and in support of, the strategy of putting the customer first, not solely concentrating on the technology.

The value of Google Cloud for retailers

When we think about our online experience, Google is ubiquitous with researching, discovering, and interacting with brands. Many of us start our journey with a Google Search, move over to Google Shopping to compare prices or perhaps turn to Google Maps to find the nearest store to make a purchase. When it comes to our online experience, Google’s broader ecosystem influences many of our decisions.

Google has invested heavily in its products and services to make it easier for retailers to drive tangible results. Of particular note, through Google Cloud Platform, the organization democratizes the ability of retailers to leverage the industry-leading technology that underpins many of Google’s leading consumer products.

If this wasn’t compelling enough — Google recently made accessible their top Search data as a new dataset in BigQuery. This data can be drilled down to the city-level. Now, more than before, retailers of all shapes and sizes can leverage Google’s streaming data analytics offerings to drive business insights in real time from in-store displays to refining campaigns in response to what customers are looking for. As Slalom has demonstrated with DSW, this can translate to informing real-time interaction with their 26 million loyalty customers.

Our experience has shown that customers need to partner with vendors who can future proof their technical investments. This constant focus on innovation is a key reason why Google Cloud was assessed to be the top cloud vendor for retail worldwide and we believe they represent an industry leader, particularly when it comes to industry solutions for retail.

Conclusion

In today’s environment, the promises of customer-centric, data-driven retailing are more of a reality than ever before. Still, they are reliant on a relentless effort to make an organization’s vision come true. From data collection to team organization and the technology required for execution, putting the customer first by leveraging rich insights is a journey and must be treated as such. Retailers can become savvy technology companies but cannot do it alone. Teaming with partners such as Google Cloud and Slalom can put your company one step ahead of the rest and ensure swift and successful adoption and lasting results.

To find out more, download our article on customer data strategy.

Resources

About the authors:

Heather is a Director in Slalom’s Global Customer & Digital Strategy team, with over a decade of experience in customer data strategy, marketing analytics, and experience technology transformation.

Logan is a Director in Slalom’s Global Customer & Digital Strategy team, a subject matter expert in Retail & Consumer Products, and has over a decade of experience in marketing, sales, and digital strategy & transformation.",11470
"Blockchain and Its Applications for Service Designers

Blockchain might be one of the most notable technological innovations of the last decade. It has recently gained immense popularity in innovator communities, as it can solve some fundamental challenges recurring in many industries. Among many things, blockchain can inject transparency in operations, address data privacy and security challenges and solve the issue of trust in transaction handling.

Considering this, it is particularly important for product, service and system designers to understand the technology and be aware of the opportunities that it provides.

In this article, I will try to explain the essence of blockchain technology and bring practical examples of its applications across industries.

Let’s go! 🤗

What is Blockchain?

Blockchain technology might seem complicated, but its core concept is rather simple. Blockchain is a type of database. To understand the technology, it‘s important to first understand what databases actually are.

A database is a collection of information that is stored electronically on a computer system. In databases, data are typically structured in a table format to allow for easy search and filtering.

Blockchain differs from a typical database in the way it stores information; blockchains store data in blocks that are then chained together. Blocks have certain storage capacities and, when filled, are chained onto the previously filled block, forming a chain of data known as the “blockchain.” All new information that follows that freshly added block is compiled into a newly formed block that will then also be added to the chain once filled. The data are chained together in chronological order.

Blockchain are inherently distributed databases, meaning that many parties hold copies of the ledger (a.k.a. peer-to-peer networks). In a blockchain, each node has a full record of the data that has been stored on the blockchain since its inception (this is done with something called timestamps based on the use of hash-based proof of work). If one node has an error in its data, it can use the thousands of other nodes as a reference point to correct itself. This way, no one node within the network can alter information held within it. This system helps to establish an exact and transparent order of events. It can be used in a decentralized way so that no single person or group has control over it — instead, all users retain control collectively. Immutability and transparency are not the only characteristics making blockchain technologies trustworthy, secure and innovative from technological perspective. Blockchain technologies use cryptographic hash functions and assymetric encryption, which uses a mathematically related pair of keys for encryption and decryption: a public key and a private key. If the public key is used for encryption (read — send), then the related private key is used for decryption (read — receive/access).

If you are scared at this point, note that as a designer, you do not need to understand, or implement the technical aspects of the blockchain, however, having an understanding of the basic concepts and the way they are used is essential for developing functional and innovative solutions based on the technology.

What value does blockchain offer and how can it be applied?

Blockchain’s core advantages are cryptographic security, decentralization, transparency, and immutability. The technology allows information to be verified and value to be exchanged without having to rely on a third-party authority.

Blockchain does not need to be a disintermediator to generate value. Benefits from reductions in transaction complexity and cost, as well as improvements in transparency and fraud controls can be captured by existing institutions and multiparty transactions using appropriate blockchain architecture. There is no singular form of blockchain — the technology can be configured in multiple ways to meet the objectives and business requirements of a particular use case.

So far, cryptocurrencies remain the most well-studies and common blockchain applications. In fact, Bitcoin, the most traded cryptocurrency created by Satoshi Nakamoto in 2009, was one of the first and probably the most well-known applications of blockchain technology. However, the real value of blockchain technology goes beyond cryptocurrencies and the financial sector, in general.

McKinsey has categorized blockchain uses into 5 main categories:

Static registry: distributed database for storing reference data

distributed database for storing reference data Identity: distributed database with identity-related data (it’s actually a particular case of static registry with an extensive set of applications).

distributed database with identity-related data (it’s actually a particular case of static registry with an extensive set of applications). Smart contracts: a set of conditions recorded on a blockchain triggering automated, self-executing actions when these predefined conditions are met.

🙌🏻 Smart contracts are my favorite category! I highly recommend to watch this TED talk about smart contracts and how to make the world a more beautiful, functional and just place

Dynamic registry: dynamic distributed database that updates as assets are exchanged on the platform.

dynamic distributed database that updates as assets are exchanged on the platform. Payment infrastructure: dynamic distributed database that updates as cash or cryptocurrency payments are made among participants.

There are also other edge cases that do not fit into the categories, or are combinations of these (e.g. Blockchain as a Service, ICO’s).

How is blockchain used by different public and private organizations?

Each of the categories mentioned by McKinsey can be broken down into specific cases based on industries or functions.

Supply Chain Management

Blockchain’s immutable ledger makes it well suited to tasks such as real-time tracking of goods as they move and change hands throughout the supply chain. Using a blockchain opens up several options for companies transporting these goods. Entries on a blockchain can be used to queue up events with a supply chain — allocating goods newly arrived at a port to different shipping containers, for example. Blockchain provides a new and dynamic means of organizing tracking data and putting it to use.

Food industry

For example, IBM and Walmart have partnered in China to create a blockchain project that will monitor food safety. Similarly, Louis Dreyfus Co, is experimenting with soybean import operation using blockchain.

Shipping industry

Another example is the case of logistics giant Maersk that has experimented with a blockchain-based project in the maritime logistics industry.

Mining industry

In the mining industry, The De Beers Group is using blockchain to track the importation and sales of diamonds.

Inventory management

In inventory management, Russian rail operator Novotrans is storing inventory data on a blockchain pertaining to repair requests and rolling stock.

Healthcare

As connected medical devices become common and increasingly linked to a person’s health record, blockchain can connect those devices with that record. Devices will be able to store the data generated on a healthcare blockchain and append it to personal medical records.

MedRec is one of the pioneering projects experimenting with storing data on the blockchain.

Government

Blockchain has a plethora of application in the public sector, in particular for transparent government operations.

Voting

Blockchain technology has the ability to make the voting process more accessible while improving the security. Hackers are no match to blockchain technology, because even if someone accessed the terminal, they wouldn’t be able to affect other nodes. Each vote would be attributed to one ID. This is particularly helpful for ensuring transparent and just elections

For example, in Switzerland, voter registration is being facilitates via a blockchain project spearheaded by Uport.

Taxation

Blockchain tech makes the cumbersome and human error-prone process of tax filing much more efficient with enough information stored on the blockchain.

For example, In China, a tax-based initiative is using blockchain to store tax records and electronic invoices led by Miaocai Network.

National Security

The immutable nature of blockchain, and the fact that every computer on the network is continually verifying the information stored on it, makes blockchain an excellent tool for storing big data.

For example, for the past two years, the US Department of Homeland Security has been using blockchain to record and safely store the data captured from its security cameras.

Insurance

In the insurance industry, blockchain-based smart contracts allow customers and insurers to manage claims in a transparent and secure manner. All contracts and claims can be recorded on the blockchain and validated by the network, which eliminates invalid claims, since the blockchain by default rejects multiple claims on the same accident.

For example, American International Group Inc. uses a smart contact-based blockchain as a means of saving costs and increasing transparency.

Energy

Blockchain technology could be used to execute energy supply transactions, but also to further provide the basis for metering, billing, and clearing processes. Other potential applications include documenting ownership, asset management, origin guarantees, emission allowances, and renewable energy certificates.

One example is the case of Chile. As Chile’s National Energy Commission seeks to update its electrical infrastructure, it has started to use blockchain technology as a way of certifying data pertaining to the country’s energy usage.

Another example is The Energy Web Decentralized Operating System (EW-DOS). It is an open-source stack of decentralized software and standards — including the Energy Web Chain, middleware services, and software development toolkits (SDKs). EW-DOS is a shared technology running on a decentralized network maintained by many respected energy companies. EW-DOS supports two primary use cases: 1) clean energy and carbon emissions traceability and 2) using distributed energy resources to increase grid flexibility. EW-DOS leverages self-sovereign decentralized identifiers, a series of decentralized registries, messaging services, and integrations with legacy information technology (IT) systems to facilitate transactions between billions of assets, customers, grid operators, service providers, and retailers.

Real Estate

The average homeowner sells his or her home every 5–7 years. With such frequent movement, blockchain is particularly useful in real estate market. It can make home sales more efficient by quickly verifying finances, reducing fraud thanks to its encryption, and offering transparency throughout the entire selling and purchasing process.

For example, Propy, in Kiev uses blockchain to complete real estate deals.

Payments

Blockchain offers a way to efficiently and securely create a tamper-proof log of sensitive activity. This makes it particularly relevant to international payments and money transfers.

For example, Banco Santander money transfer service “Santander One Pay FX,” uses Ripple’s xCurrent to enable customers to make same-day or next-day international money transfers. The blockchain ledger that Ripple uses had been latched into by a group of Japanese banks, who will be using it for quick mobile payments.

Environment

Protection of endangered species

In the environmental sector, the protection of endangered species can be facilitated via a blockchain project that records the activities of these rare animals.

One example is the Newton Project. Newton uses NewSensor technology consisting of small IoT devices for monitoring location, temperature, air quality, humidity, etc, and uploads that data to NewChain, Newton’s blockchain. By inserting a NewSensor under the skin of an endangered mammal, for example, a rhino or elephant, it is possible to track the location and basic behavior of that animal.

Fishing

One application in fishing is the provision of a transparent record of where fish were caught to ensure legal landing.

For example, The WWF project uses a combination of radio-frequency identification(RFID) tags, quick response (QR) code tags and scanning devices to collect information about the journey of a tuna at various points along the supply chain.

Carbon Offset

The monitoring of carbon offset trading is another environmental application of blockchain technologies

For example, IBM is using the Hyperledger Fabric blockchain in China to monitor carbon offset trading.

Waste Management

Waltonchain is using RFID technology to store waste management data on the blockchain in China.

Record Management

The encryption that is central to blockchain makes it quite useful for record management because it prevents duplicates, fraudulent entries, and the like.

Land Registry

Record management can be particularly useful in land registry. For example, in Georgia, in a project developed by the National Agency of Public Registry, land registry titles are now being stored on the blockchain.

Border control

In the Netherlands, Essentia has developed a blockchain-based border control system that allows customs agents to record and safely store passenger data from an array of inputs.

Blockchain as a Service

Enterprises also benefit from blockchain technology by developing models offering blockchain as a service solutions.

For example, Ethereum’s blockchain can be accessed as a cloud-based service courtesy of Microsoft Azure.

Similarly, Google is building its own blockchain, which will be integrated into its cloud-based services, enabling businesses to store data on it and to request their own white label version developed by Alphabet Inc.

A concluding note for service designers

In this article, I tried to explain the basics of blockchain technology and give some examples of its use cases in different service sectors. One of our functions as service designers is to devise new business models, processes and infrastructures to help stakeholders exchange value and achieve their goals in more mutually beneficial ways. On the other hand, we are responsible for anticipating the threats that blockchain-based products, services and systems might create. Considering this, it is crucial for us to understand the nuts and bolts of the technology and be aware of its use cases. I hope, I managed to motivate you to learn more about one of the most promising technologies of the last decade.

P.S. Note that the use cases that I covered in the article are non-exhaustive. There are many more blockchain applications that are worth scrutinizing. There are also a lot of threats associated with possible applications of the technology that must be understood. As a starting point, I highly recommend watching this documentary about the future of cities driven by blockchain and this talk about the future utopian and dystopian realities that blockchain might create.🤗",15179
"ScriptKiddie is an Easy machine listed on Hack The Box. It was released on February 6th, 2021. This box’s vulnerability was largely based on poor input validation, resulting in the upload of a malicious .apk file. While I could not find. CVE code for this vulnerability, the Exploit Database ID is 43905. It is also worth noting that keeping penetration testing tools on a public-facing system allowed me to escalate to root privileges almost immediately.

Part One: Enumeration

A SYN port scan picked up a number of open ports. Notably, a web server. This also showed me that the program Werkzeug was running. Banner grabbing is often promising on Hack The Box, so I research Werkzeug. What I find is interesting, but I am unable to make use of any of it right now.

This could very well be the Chekhov’s Gun that helps me pivot inwards, but putting this info on the back burner for the moment allows me to look for a more immediate pivot point.

Part Two: Checking For a Web-based Entry Point

It is an unfortunate reality that many Systems Administrators believe using non-standard ports will hide their web assets. That seems to be the case here, where a web page hosted on port 5000 features two common pentesting tools, and a file upload specifically for payload files.

Exploiting Werkzeug through this avenue does not immediately yield results, but the pull-down menu for payloads also has an Android option. So, I Google “android template injection apk” and the first option is a Rapid7 page offering the following instructions for making a Metasploit payload for such an occasion.

“msf > use exploit/unix/fileformat/metasploit_msfvenom_apk_template_cmd_injection msf exploit(metasploit_msfvenom_apk_template_cmd_injection) > show targets …targets… msf exploit(metasploit_msfvenom_apk_template_cmd_injection) > set TARGET < target-id > msf exploit(metasploit_msfvenom_apk_template_cmd_injection) > show options …show and set options… msf exploit(metasploit_msfvenom_apk_template_cmd_injection) > exploit”

This works exactly as it is supposed to.

Part Three: Getting The User Flag

Payload in hand, I start a Netcat server, and upload the malicious apk file. I am immediately given file access.

The Netcat shell has immediate access to the User Flag.

Part Four: Escalating to Root Access

A box named Script Kiddie seems like a good candidate for a quick pivot to a root flag.

It was worth a shot. Maybe a pty shell?

So the most immediate methods of privilege escalation don’t seem to work. Accordingly, I start looking through files, hoping to find something with root access, or the promise of getting root access later.

Since I know what this box is set up for, I am pretty sure I can find a pivot point using the tools on this box. Since Script Kiddie is something of a slur for people who overuse pentesting tools (like I do), let’s check for the big one.

Metasploit is installed, and file access is offered in Metasploit. However, my access limitations don’t seem to have changed. Using bash, I improve my shell access.

These attempts appear to work.

This is one of those situations where bothered comments such as those seen above are probably a good thing. As suspected, this allows access to the Root Flag.",3235
"The FBI released a statement in April 2021, warning of cybercriminals using fake job listings to target applicants’ Personally Identifiable Information (PII). In the COVID-19 era, over 16,000 people were reported to the FBI as scammed through fake job listings with losses totaling more than $59 million. There have been over 2,000 reports in 2021, the FBI reports.

How’s It Done?

Hackers advertise jobs the same way legitimate employers do, online (ads, job sites, college employment sites, social media), in newspapers, and sometimes on TV and radio. Technology makes these scams easier and more lucrative than ever for fraudsters. They promise you a job, but what they really want is your money and personal information. These employment scams occur when criminals deceive victims into believing they have a job or have a job lined up. Criminals leverage their position as “employers” to persuade victims to provide them with personally identifiable information (PII) or send them money.

The scammers will go to great lengths to get your information, even conducting a fake phone interview. They conduct fake interviews with unsuspecting applicants, requesting PII and/or money from these individuals. PII can be used for any number of malicious purposes, including taking over a victim’s account, opening new financial accounts in their name, or using the victim’s identity for another deception scam (fake driver’s licenses/passports).

How You’re Tricked

It can be quite difficult to spot tricksters, but this example of a LinkedIn user who reached out to KrebsOnSecurity to verify the scam might help:

On Monday, someone claiming to work with Gwin (LinkedIn ‘recruiter’) contacted Siegel and asked her to set up an online interview with Geosyntec. Siegel said the ‘recruiter’ sent her a list of screening questions that all seemed relevant to the position being advertised. Siegel said that within about an hour of submitting her answers, she received a reply saying the company’s board had unanimously approved her as a new hire, with an incredibly generous salary considering she had to do next to no work to get a job she could do from home. Worried that her potential new dream job might be too-good-to-be-true, she sent the recruiter a list of her own questions that she had about the role and its position within the company. But the recruiter completely ignored Siegel’s follow-up questions, instead sending a reply that urged her to get in touch with a contact in HR to begin the process of formalizing her employment. Which of course involves handing over one’s personal (driver’s license info) and financial details for direct deposit.

According to the FBI, the attackers request the same information as legitimate employers, making it difficult to identify a hiring scam until it is too late. Some indications of this scam may include:

Interviews are not conducted in person or through a secure video call.

Interviews are conducted via teleconference applications that use email addresses instead of phone numbers.

Potential employers contact victims through non-company email domains and teleconference applications.

Potential employers require employees to purchase start-up equipment from the company.

Potential employers require employees to pay upfront for background investigations or screenings.

Potential employers request credit card information.

Potential employers send an employment contract to physically sign asking for PII

Job postings appear on job boards, but not on the companies’ websites.

Recruiters or managers do not have profiles on the job board, or the profiles do not seem to fit their roles.

How To Protect Yourself

If you’re looking for a job or if you receive an enticing offer, it’s vital to do a little research. CyberHoot and the FBI recommend taking these actions if you receive a job offer of any kind through online interviews:

Conduct a web search of the hiring company using the company name only. Results that return multiple websites for the same company (abccompany.com and abccompanyllc.com) may indicate fraudulent job listings.

Legitimate companies will ask for PII and bank account information for payroll purposes AFTER hiring employees. This information is safer to give in person. If in-person contact is not possible, a video call with the potential employer can confirm identity, especially if the company has a directory against which to compare employee photos.

Never send money to someone you meet online, especially by wire transfer.

Never provide credit card information to an employer.

Never provide bank account information to employers without verifying their identity.

Do not accept any job offers that ask you to use your own bank account to transfer their money. A legitimate company will not ask you to do this.

Never share your Social Security number or other PII that can be used to access your accounts with someone who does not need to know this information.

Before entering PII online, make sure the website is secure by looking at the address bar. The address should begin with “https://”, not “http://”.

While these actions will help you stop the hackers from stealing your information in Employment Scams, there are certainly other actions you and your business should be taking to help secure your sensitive information.

CyberHoot’s Top 7 Cybersecurity Program Recommendations:

Take these seven actions to improve your company’s cybersecurity program:

Adopt two-factor authentication to prevent a password breach of your business’s VPN, email services, and any other critical service that is directly Internet accessible Adopt a password manager and adopt 14+ character length passwords across your company both personally and professionally for strong password hygiene Train employees monthly on a variety of cybersecurity topics, but with a focus on how to spot and avoid phishing attacks — the primary way cyberattacks occur Test employees with fake phishing test to help employees apply their training to spotting and deleting phishing attack emails Regularly backup data following the 3–2–1 backup method for backing up all your critical and sensitive data Govern your employees with a set of cybersecurity policies that outline requirements for the protection of your most company critical data Consider performing a Risk Assessment every 2–3 years against your companies administrative, technical and physical practices.

Sources:

FBI

KrebsOnSecurity

Additional Readings:

FTC — Job Scams Article

CNBC — Job Scams Increase As COVID-19 Puts Millions Out Of Work",6588
"Stateful VS Stateless architecture

Alright so you type a URL of a website and then authenticate yourself to the server that you are who you say you are, usually with a username and a password and then you view your profile. Right? simple isn’t it? It’s the most simplest form. That’s what a user sees but underneath there is a lot more going on, which brings us today to discuss stateful and stateless applications. Let’s begin with an example.

Stateful applications

Consider an application which has two pages or services “Login” and “ViewProfile”, and the user Bob sends a request to the server1 to the login page which looks like this http://server1/login?username=bob&password=bob123 now the server queries the database to validate these credentials and if they are, the server responds with a Status: 200 OK response and sets a session variable or a global variable in the server.

Now when the user Bob requests for the ViewProfile page the server remembers the session variable and responds with a 200 OK response. Usually there is an array or some sort of session variable that tells that this user is logged in.

Advantages of Stateful applications

The next time Bob makes a call to ViewProfile page the server won’t have to query the database thus storing the state and saving the trouble of querying the database twice. And maybe it will be faster at the backend.

Disadvantages of Stateful applications

So where does it go wrong?

Remember the session variable is stored in the memory, the RAM. So if i literally do have multiple servers and now if the user Bob tries to request the ViewProfile page to server2 it will fail. Why? because server2 does not have the session variable stored because Bob logged into server1.

This means we can not scale our network horizontally, and Bob has to authenticate again every time he connects to a different server.

Now you might say “I don’t go to server1 or server2”, what really happens is that there is another machine called a Load Balancer, what it does is simply balances the load/traffic.

Bob does not know about server1 or server2, he knows about LB. So he makes a login call to LB, and LB randomly assigns a server and let’s say it picks up server1 and we do the login call, the server queries the database and we get a 200 OK response. Now Bob calls for the ViewProfile page, if he gets lucky he might get server1 but what if the LB picks up server2? The call will fail because he is not logged into server2. Imagine logging into facebook.com and then you go to profile, and you have to login again.

Stateless applications

Now consider the same application with our two services “login” and “ViewProfile” with multiple servers and a Load Balancer and a database. Alice makes a call to the login page and authenticates herself using her credentials, everything is same as before, but now when the database authenticates those credentials it also sends a token (usually a pseudo-random token) to the user (in our case Alice), now with each request Alice makes, the token is send along with the request.

Now Alice request for ViewProfile page, if the LB decides to take her to server1, the server1 queries the database and validates that token and if the token that Alice has is same as the one sent by the database we get an 200 OK response. On the other hand if the LB decides to take her to server2 that’s fine too. the server2 will again query the database and validate the token. It does not have to be the same database, it could be another intermediate database.

Advantages of stateless applications

Unlike the stateful application we can scale out network horizontally, usually used in larger companies.

Disadvantages of stateless applications

As stated before “with each request the user makes the token is attached with it”, so it has an extra cost of querying to the database for each request. but this type of authentication is more soft.",3924
"Open sources of information are causing trouble for dictatorial governments Dgvaghela Aug 17·1 min read

In the 1990s and 2000s, it was hoped that the Internet would be the greatest force for liberalism and freedom. However, this did not happen. Dictatorships have strengthened their hold on technology. The war of information was used as a weapon. In the midst of this despair the age of open source of information has awakened new hope.

New sensors, cameras and satellites are monitoring the corners of the earth. Media like intellectuals, activist online community and Slack have presented a wealth of information against amateurs and experts. Private organizations and people have exposed China’s stockpile of nuclear missiles. The atrocities against the Uyghurs have also been exposed through this system. This intelligence has also revealed the secret of the downing of a Malaysian passenger plane by Russia. Researchers have discovered the secret missile program of many countries in their own way.

if like this article…please share….",1043
"You probably have heard a lot about them recently. Back in May, a ransomware attack shut down one of the top U.S. gas pipelines. The Colonial Pipeline Company was ransomed for 75 bitcoins which is approximately 4.4 million dollars. But the impact and damage of this ransomware went beyond just monetary. Since Colonial pipeline is a major artery responsible for delivering fuel in the east coast, the thought of possible fuel shortage led to chaos and a mass panic in many areas in the east coast which can be seen with massive lines out of gas stations.

Gas shortages following Colonial Pipeline ransomware attack

Following the attack, Colonial Pipeline also disclosed that the attack also compromised the personal information of nearly 6,000 individuals. If that weren’t bad enough, Kaseya, an MSP (Managed Service Provider) got hit with an even bigger ransomware attack that flooded hundreds of companies that relied on its managed software with ransomware as well.

So, what is ransomware?

Ransomware is a subset of malware which the data on a victim’s computer is locked typically with encryption and payment is demanded from the victim before the attacker decrypts the ransomware data and the access is returned. Unlike other cyber-attacks, the motive behind ransomware attack is usually monetary driven. The victim will typically be notified that an exploit has occurred, the data on the system is locked, and given instruction on how to recover from the attack. As seen with most ransomware attacks, payment is often demand in forms of virtual currencies (bitcoin is a popular choice) so that the identity of the cybercriminals stated hidden as well as making payments harder to trace. A popular form for ransomware to spread is often through malicious attachments and links found in emails or other infected malicious software apps. Once infected, ransomware does not require any additional user interaction to do damage.

So why should you care?

Now you must be thinking, these are big enterprises, no way its going to happen to me, right? Wrong, everyone is vulnerable to ransomware especially now where ransomware is more accessible to attackers than ever. In fact, ransomware is so lucrative that it is being offered as a service. Ransomware as a service (RaaS) is all about providing ransomware in the form of a software as a service (SaaS) model. The RaaS vendor operates all the back-end infrastructure for running the ransomware with some even providing chatrooms for potential buyer with live agents to answer questions. It is so easy that the time it takes to launch a ransomware attack is equivalent or less than that the time it takes to add an item to the shopping cart on an ecommerce website.

What you can do to prevent ransomware attacks?

To protect yourself against ransomware attacks and other forms of cyber extortion attacks, you should do the following:

· Avoid clicking on links in emails or opening email attachments from suspicious senders especially ones that have unusual addresses and from senders you don’t recognize. If it seems suspicious, more often than not, it is.

· Always keep your devices and software up to date especially your OS and antivirus software to ensure them have the latest security patches and updates.

· Back up your devices regularly.

· Utilize your antivirus software to scan downloaded files (compressed and archived files) for malicious content. Most modern antivirus software includes this feature out of the box or can be turn on in settings.

· Turn on multifactor authentication (MFA) or two-factor authentication (2FA) on your accounts. Please use multifactor authentication or two-factor authentication on all your accounts! This cannot be stressed enough. Most social media sites offer this feature which is as easy as going into your user settings and switching it on. Below are couple examples of MFA and 2FA from LinkedIn and Facebook:

LinkedIn Two-step verification",3950
"We have been trading for much of human history (as long as 150000 years ago — Peter Watson notes in Ideas: A History of Thought and Invention from Fire to Freud). Commodities and capital has been moving on trade routes for thousands of years and has played a major role in human civilization.

By Whole_world_-_land_and_oceans_12000.jpg: NASA/Goddard Space Flight Centerderivative work: Splette (talk) — Whole_world_-_land_and_oceans_12000.jpg, Public Domain, https://commons.wikimedia.org/w/index.php?curid=10449197

Fast Forward to today, our world is defined by consumer convenience. We, as consumers, have at our hands the ability literally at the tip of our fingers to browse from catalogs of products available from a number of suppliers globally and expect them to be delivered to our doorstep within a reasonable amount of time. Likewise, businesses engage with other businesses in transfers of goods and services virtually from anywhere to everywhere in the world. So when we order a batch of our favorite tea from an online store — under the hood are the workings of a complex supply chain network interlinking many local and international supply chains where information, people, products, modes of transports and even robots spring into action for us to enjoy the perfect cup of tea.

The modern consumer and industrial revolutions, mass production, and the degree of globalization in the modern world has had a proportional effect on the complexity of supply chains. Supply chains that serve us today are highly evolved complex systems that are a testament to human ingenuity and collaboration as businesses across national borders have partnered and arranged themselves in a way that serves our needs at the lowest possible cost. Where the end product should be produced, where the components should be sourced from and which partner engages in what specialized activity (assembling, distributing, retailing etc.) are subordinate strategic decisions under the overarching guiding principle of serving the customer at low costs.

The discipline of Supply Chain Management

In their book “Adaptive Supply Chain Management” Dmitry Ivanov and Boris Vladimirovich Sokolov describe the 1950s and 1960s as the era of mass production and economies of scale leading to a focus on quality management. Improved quality of products led to individualization of customer requirements launching the era of customer centeredness in the 1980s. Inventory management and reduction in production cycles was the focus of this period. The wide variety of products, the need to react to market changes with agility and cutting the time to market required businesses to optimize internal processes and increase collaboration with suppliers resulting in lean manufacturing and just in time paradigms. From then onwards, companies concentrated on competitive advantages, core competencies, outsourcing, innovation and collaboration fueled by rising globalization, information technology advances leading to the development of the supply chain management paradigm in the 1990s.

Ivanov, Dmitry & Sokolov, Boris. (2010). Evolution of Supply Chain Management (SCM)",3149
"The intent of this note is to explore how the jobs market has evolved over the course of the last decade with a host of startups “craigslisting” Linkedin, Indeed, and all other job boards. Settle in this is a LONG one …

Close to a decade ago we saw craigslist being unbundled and this now familiar picture reflects what happened with Craigslist:

Source : The Gong Show — The Spawn of craigslist Like most VCs that focus…

Of course as it turns out a lot of these products “died” primarily since such vertical marketplaces had several key issues:

From : A Crowded Space — The Rebundling of Craigslist

Death was not necessarily caused by verticalization of a subsection of an otherwise horizontal market but rather the fact that the frequency and size of such transactions made it an otherwise inferior market. There were not enough transactions or volume of transactions in order for the market to take hold, seed, add continuous value, or to wit, make money! Verticalized plays of the past did not add anything, incrementally in terms of the marketplace (tooling and componentization, price setting and such)

In labor markets specifically, this lead to groupedverticalization which has been the market for the past 15–20 years leading to HUGE winners such as Indeed, ZipRecruiter, Upwork. Shiftgig is interesting but not a winner, yet.

Vertical and Horizontal Markets

Vertical markets failed, back in the day, primarily because you needed either high frequency or high transaction size in order to make it work. At this point this is pretty much a moot point since a significant % of labor market now finds employment online and this is only increasing due to mobile proliferation, Millennials, Gen Y, Gen Z, Unbundled education, and of course COVID-19

From COVID-19 and the Great Rehiring, this is literally the most opportune time to create a market since there is a huge constituent of the workforce (around the world) that is unemployed and this is perhaps the right time to unbundle.

Clearly, these are not normal times. So we can’t rely on the “normal” tools we’ve previously used to help people find work: the LinkedIn and Craigslist searches, coffee chats, recruiters, staffing agencies, and so on. Those methods can’t hit the speed, scale, and placement quality that companies or candidates will need coming out of this crisis. But verticalized jobs marketplaces — platforms that focus on one industry, role, candidate type, or demographic — can and will. Now is the time to be building them.

The tools we currently have at our disposal — the horizontal jobs platforms like LinkedIn, Indeed, and Craigslist — are clearly inadequate for the enormity of the task. The average time to hire in 2018 was 38 days. Given the scale and urgency of the crisis, that’s far too long.

Too often, the existing online jobs platforms are a futile exercise in imprecise drop-down menus and trial-and-error search terms just to surface the jobs or candidates you’re interested in, let alone qualified for. These platforms feel like one of those all-encompassing, cross-cuisine diners with a 17 page menu: they offer a broad look at everything you might want, but don’t do any one thing particularly well. They require too much work to separate the signal from the noise, which impacts both the speed and quality of the process. It’s the classic story of a horizontal platform reaching its inevitable breaking point.

And while the typical analog avenues — networking, an endless string of “catch-up coffees,” working with a placement agency — might suffice in a regular economy, they can’t hit the scale or speed needed to meet the challenge now ahead of us. And the most efficient channels for doing so, like internal hires and recruiters, are difficult to scale.

Network theory has a concept called chicken-and-the-egg problem, which essentially means which side of the network can you “mobilize” fast/first in order to make the other side of the marketplace attractive. Essentially, the argument is that given the number of unemployed people today, building a vertical network has both the largest chance of success as well as the significant number of participants. This is not necessarily only true of jobs that have been lost but COVID is reshaping work and a significant % of the “elite” workforce is also rethinking their “job”. In short the best time to go to market is NOW since bootstrapping a network has never been this easy (Entire read is worth it)

This does not mean that one starts a company without thinking about the following:

Friction to Value

What friction are you removing? What value is created and who will be willing to pay for that value. Think of a company such as TripleByte, they add value by pre-interviewing candidates for a 1st round and narrowing the funnel. They do this with quizzes that standardize and rank and “surface up” the best candidates

Tool to Value

What kind of tool can you create that people can use and once they find value use the jobs section? Take Blind for example. No one comes for a job, they come to ask questions about a job or company and stay for the network

Create supply

One of the ways to solve this problem is similar to what Lambda School and Flockjay are doing. They are creating the “supply” and “supplying” highly qualified in-demand candidates to companies and arbitraging on income (ISA’s, though that is rife with issues as is expected in a new market)

Network → Jobs

The starting point is some kind of community or information. Lunchclub for example brings together tech professionals to start with. Once there is critical mass, they can layer in jobs, referrals and such. Blind in some ways already does this with the “can you refer me to x”. And of course this guy below wants to work everywhere!

In the jobs world, the “transaction” was matching. Everything else had to be done by the jobseeker/company. As ‘Deep’ Job Platforms and How to Build Them shows:

The key point in this story in almost all cases the broad horizontal verticals eventually break creating a path to “bundle” not just job matching but other key components into the platform there doing three things:",6160
"Working memory

There is an extensive classification for all the different types of memory.

For our purposes as a designer, working memory (part of short-term memory) is the most relevant. The shortest type of memory is known as working memory, which typically only lasts a couple of seconds during a task.

In other words, our working memory is responsible for the information we can hold in our heads while we engage with other cognitive processes.

Miller’s Law states that the average person can only keep 5–11 items in their working memory at a time. The working memory required to complete a task within your product is proportional to the MIC burden you impose on your users.

Conversely, at no point should your task require the user to hold more than seven items in their working memory at any moment.

In rare scenarios where you require the user to hold more than 11 items in their memory, use “chunking” to reduce their mental burden. Chunking is when individual pieces of an information set are broken down and then grouped in a meaningful whole.

For example, we remember phone numbers as XXX-XXXX rather than XXXXXXX. It’s easier to remember the number in two chunks rather than a series of seven individual units.

Another factor related to consider related to attention and memory is “Hick’s Law.” The law states, “the time it takes to make a decision increases with the number and complexity of choices.

The more choices there are, the longer it takes the user to make a decision.

Avoid overwhelming users with excessive choices, highlight the best option for them whenever possible. Break complex tasks into smaller steps, i.e., using progressive disclosure when appropriate.",1697
"Session - 4

It was the wrap up session for interaction design. So the class started with greetings after that students were asking questions regarding Interaction design as it was the last session so students were clearing their queries regarding this discipline. During this discussion we came to know how things are interacting with people. For example, some people like some specific games because that games have some kind of Interaction with them whereas others games don't have such kind of interaction that's why they are included in their favourite list. Even we came to know that if we are developing a product it should be perfect. It should have Interaction with customers then only the product will be in demand. So after clearing all the queries we came to a portion where we were assigned an assignment as per the house analysing map that we did in earlier class. Find out the most interacting place in your house and represent it in visual forms . It should interact with that place . So from this session we came to know how everything has some Interaction with people. People like such things which Interact with them most so Interaction plays an important role in all aspects.",1195
"How to select an Integration Platform for your Business — Part III Chanaka Fernando Follow Jan 15 · 8 min read

Let’s select the right integration platform for your business

This article is part III of a series of articles written on the topic. You can find the first 2 articles in the below links.

How to make your Integration platform cloud-native?

The integration platform that you choose should be future-proof and cloud-native or cloud-ready. The layman’s definition of the word cloud-native is “to reap the benefits of the cloud”. At a high-level, cloud computing offers you the capabilities like

High availability — available across the globe

Elasticity — can scale up and down as and when necessary

Scalability — expand to global scales

Cost savings — pay as you go

There are many other aspects, but we can consider these as the basic advantages of the cloud. Let’s see how we can align our integration platform to reap the benefits of the cloud. There is an entirely different set of advantages offered by container platforms and microservices architecture which is not mentioned here. Once you have the platform designed for the cloud, you can consider gaining those advantages also through the same.

One of the fundamental aspects of scalability and maintainability is modular architecture. If you have all the functionality baked into a single monolithic application, scaling becomes much harder. The concepts like microservices architecture have come up to address the problems which are surfaced with this monolithic architecture. Once the functional components are divided into compatible yet independent modules, the deployment becomes much more flexible. Let’s discuss how to define a cloud-native architecture for your integration platform.

Figure: Cloud-native, brown-field integration solution architecture

The above architecture is an extension of the API-led architecture that we discussed in earlier sections. We have introduced a couple of new components to this platform so that we cover the modern requirements like event-driven architecture and analytics. Let’s discuss the components in this diagram in detail below.

Messaging System — This component is required when there is a need to process a huge amount of asynchronous data points (events) coming into the system using an event-driven architecture. Apache Kafka and NATS are 2 of the best technologies available to fulfill this requirement. Depending on the levels of guarantees and the performance required for event handling, you can choose either Kafka or NATS.

— This component is required when there is a need to process a huge amount of asynchronous data points (events) coming into the system using an event-driven architecture. Apache Kafka and NATS are 2 of the best technologies available to fulfill this requirement. Depending on the levels of guarantees and the performance required for event handling, you can choose either Kafka or NATS. On-premise systems — These are the existing systems that are used in the enterprise which may have built with various protocols, standards, messaging formats. It is impossible to replace the capabilities of these systems with modern architectures like microservices or containers. Instead, these systems need to be integrated with the newly built architecture.

— These are the existing systems that are used in the enterprise which may have built with various protocols, standards, messaging formats. It is impossible to replace the capabilities of these systems with modern architectures like microservices or containers. Instead, these systems need to be integrated with the newly built architecture. Event processing system — This component is used to process the events received by the messaging system in real-time or in batch mode. This component can be used to transform, correlate, summarize, and store these events so that these results can be used for business operations as well as to improve customer experience. The processed results can be sent back to the messaging system so that other applications can receive these results.

— This component is used to process the events received by the messaging system in real-time or in batch mode. This component can be used to transform, correlate, summarize, and store these events so that these results can be used for business operations as well as to improve customer experience. The processed results can be sent back to the messaging system so that other applications can receive these results. System APIs (Microservices) — There can be many situations where existing applications and systems cannot fulfill the ever-increasing consumer demands and business requirements. The microservices style of development is better suited for this sort of service development.

— There can be many situations where existing applications and systems cannot fulfill the ever-increasing consumer demands and business requirements. The microservices style of development is better suited for this sort of service development. Process APIs (Integration platform) — When there is a plethora of applications and systems that need to work together, the integration platform is the glue that connects various systems. Instead of using a traditional, monolithic integration layer, using a microservices style of a lightweight integration platform is recommended. The above figure depicts the usage of “micro integration” runtimes that are designed to support container-based deployments.

— When there is a plethora of applications and systems that need to work together, the integration platform is the glue that connects various systems. Instead of using a traditional, monolithic integration layer, using a microservices style of a lightweight integration platform is recommended. The above figure depicts the usage of “micro integration” runtimes that are designed to support container-based deployments. Experience APIs (API platform) — Once the services are built as microservices or integration services, these services need to be exposed to various consumer channels. The API gateway acts as an intelligent gatekeeper that allows only the authorized requests while listening to all the requests. As depicted in the above figure, using a “Micro gateway” is beneficial in a container or cloud-native deployment.

— Once the services are built as microservices or integration services, these services need to be exposed to various consumer channels. The API gateway acts as an intelligent gatekeeper that allows only the authorized requests while listening to all the requests. As depicted in the above figure, using a “Micro gateway” is beneficial in a container or cloud-native deployment. Developer portal — Providing a developer portal where external parties can build their own applications and products on top of the business APIs exposed through the system is critical in expanding the business to new heights.

— Providing a developer portal where external parties can build their own applications and products on top of the business APIs exposed through the system is critical in expanding the business to new heights. Analytics platform — Business Intelligence or Business Analytics is becoming ever so important with the amount of data collected in enterprise systems as well as the type of competition in the market. An analytics platform with proper tools and minds can really take you to the next level of your business and keep your competition at bay.

— Business Intelligence or Business Analytics is becoming ever so important with the amount of data collected in enterprise systems as well as the type of competition in the market. An analytics platform with proper tools and minds can really take you to the next level of your business and keep your competition at bay. Identity and Access Management platform — This is where the centralized management of authentication, authorization, provisioning, SSO, MFA happens. There are distributed alternatives like Open Policy Agent (OPA) that exists for microservices and API security. But not all the applications and systems support those alternative approaches.

The selection of the underlying infrastructure layer is totally up to the respective organization. But it is the role of the architect to design the solution so that it can be deployed in any infrastructure without much issue. The above architecture supports well with any of the below-mentioned infrastructure choices.

On-premise (physical/VM)

IaaS (VM based, AWS EC2, Azure, GCP)

Containerized (AWS ECS, GCP, Azure)

Kubernetes (EKS, GKE, AKS)

Deployment model

Now we have a better understanding of what functional and non-functional aspects need to be considered when selecting an integration platform at a higher level. The next thing that we should consider is the deployment model of the platform. This is as critical as any other point we mentioned so far. There are 3 main deployment models available for integration platforms.

Self-managed deployment (on-premise, on-IaaS)

Vendor-managed deployment (SaaS, private-IaaS)

Hybrid deployment (vendor managed + self-managed)

Let’s discuss in detail what these deployment models offer to the users.

Self-managed deployment

This is the traditional on-premise deployment model supported by most of the integration (ESB) vendors from the past. This means that the user needs to manage the deployment of the integration platform by themselves with support from the vendor. The vendors provide support through ticketing systems like JIRA with different SLAs for different types of issues (incident, queries, development, etc.). With the popularity of IaaS providers like AWS, Azure, and GCP, more and more people started deploying these products on those platforms. Even in that sort of deployment on a cloud platform is considered self-managed if the user is managing the deployment-related activities like patching, updating, upgrading, and monitoring. This option is suitable for organizations having dedicated IT teams with enough resources to manage these deployments. With the usage of proper automation and monitoring tools, most of these management activities can be offloaded to systems. But that requires a significant effort to build such an infrastructure. One advantage of this approach is that the system has the highest level of flexibility in terms of customizations that are required for the specific customer.

Vendor-managed deployment

This is the option with the least management overhead. In this option, the vendor who offers the product takes care of the management of the deployment. In most cases, these solutions are provided as SaaS (Software as a Service) or PaaS (Platform as a Service) on the cloud. The users have to work with the solution architects from the vendor to identify the components and their respective sizes required for the integration project. Then the vendor will provision the required services on their cloud platform and give the users with access to those services. In most of the vendors, these services are running as multi-tenant, shared infrastructure with each user getting logically separated deployment. These solutions are commonly known as integration platform as a service or iPaaS.

One drawback with this approach is that the flexibility in terms of customization is very since this is a shared infrastructure. As a solution to that problem, some vendors offer the option of providing a fully-managed, private cloud deployment for users who are willing to pay some extra bucks. The deployment will be a dedicated one for the user on a given cloud platform (IaaS) and the vendor will take care of the maintenance and management aspects.

Hybrid deployment

This deployment model is designed to provide the best of both worlds in terms of flexibility and the management of the infrastructure. In this model, the vendor is managing the “control plane” or “management plane” related components that do not need to stay close to the user's systems. As an example, the API gateway is a component that needs to sit close to the process APIs and system APIs but the developer portal does not need to do the same. In such a case, the API gateway will be deployed in a customer-managed infrastructure (on-premise, IaaS) while the developer portal is deployed in a vendor-managed cloud environment. This model comes with a certain set of challenges as well. Integration from vendor deployment to user deployment needs to be set up in a proper manner and given the platform resides in 2 places, debugging issues can be difficult at certain times. But those challenges come with the price of flexibility and performance (latency) gains that users could achieve with this approach.

Each organization has its preferred ways of managing the IT systems and based on that preference, users can select a platform that supports that particular deployment model. One thing to consider here is that, if the platform is designed in a modular manner where functional components can be deployed independently, any of the mentioned deployment models is possible and future changes of directions can be accommodated.

Developer Experience

The next big thing that we should consider when selecting an integration platform is the developer experience with the platform. Depending on the level of exposure to the technology, the users (who build integrations) of an integration platform can be divided into 3 categories.

Developers (programmers) — This is the category of users who are most comfortable with writing code and building integrations with programming languages. They will be comfortable with platforms that provide enough flexibility to extend the functionality in case they cannot find an OOTB feature to support a certain use case.

Citizen integrators — This is the type of user who comes with a better understanding of the specific business domain (e.g. financial, retail, healthcare) but not always has the same level of expertise in writing code as the developers. They would prefer tools that provide higher-level abstractions like UI based flow designers.

Integration specialists — These are the users with experience in implementing integration projects using various tools and technologies and comfortable with both writing code as well as DSL or UI based approaches. Some of them come with specific domain knowledge as well based on their previous experiences but not as much as citizen integrators.

It is critical to select a platform that aligns with the level of experience and the skill set that is available in the organization or has plans to hire. In most cases, we could find engineers who can wear different hats based on the need of the company. But it is good to select a platform that most of your engineers are comfortable with than asking them to adapt to the new platform.

Summary

In this article, we covered the fundamental requirements of an integration platform and then moved to more modern requirements that have appeared with the advancements in the enterprise technology domain. We used solution architecture diagrams to explain the overall architecture of an enterprise IT platform that has many other components in addition to the integration platform that we are going to choose. We identified various functional requirements at each component level. If you are in the process of selecting an integration platform for your enterprise, you should look at the most related functional requirements and architecture considerations mentioned in the article.

Given below is a list of leading vendors in the enterprise integration technology market.

This concludes the series of articles written on the topic of “how to select an integration platform for your business”.",15743
"Docker the most demanded and used technology in the industry, docker not only makes it easier to create, deploy, and run applications by using containers but it also simplifies the process of building, running, managing, and distributing applications.

Usually, we get a terminal when we launch a container using images and we work on it, but we mostly interact with the terminal and we never use any GUI application inside the docker container.

Have you ever tried running a GUI application on docker? If not let’s do this today!!

For creating this type of scenario we are going to set up docker on top of RedHat Linux 8.

To install docker you have to install docker and start docker services by configuring the repo file.

after installation of docker and starting docker services we have to pull docker image for our use case, in my case I am using centos image in your case you can use another image.

now we have to start our docker container with some options for this use case which includes

e -> for specifying that container should use host display for running GUI application

net -> for specifying to use host net connection for connecting to GUI application like Firefox

name -> for specifying name of the container, if not used will specify name randomly

d -> for running container in background

i -> for providing standard input

t -> for getting terminal of container

after launching container we have to install Firefox in container by using docker exec <container name> <command> command

at last we have to run firefox by using docker exec <container name> <command> command

as you can see a GUI application running on a container.

Likewise, we can run a Jupyter notebook inside the container, firstly you have to install python inside the container, and then using the pip command you have to install a Jupyter notebook.

but while launching Jupyter notebook using the docker exec command you have to give root permission by the allow-root option

If you liked the above blog, please share it.

❗ LinkedIN Link :-",2042
"By Naveen Gavini, SVP, Products

When Pinterest first launched it provided a new way for people to collect and organize ideas from around the web. What started as a place to save though, quickly became a unique service for discovery where Pinners could draw inspiration from each other, brands and creators. In 2014, four years after Pinterest.com launched, a small group of engineers and designers asked, “what would a search engine look like if there wasn’t one right answer?”.

We realized Pinterest could be the service to help guide you to ideas when you aren’t quite yet sure what you want, and when you want to explore questions like, what to cook for dinner tonight? How to style your home? Where to go on vacation?. With so many possibilities, sometimes you don’t know what you want until you see it. This is when we kicked off our search journey in earnest.

Today, Pinterest powers global search through computer vision and recognizing billions objects through the Lens camera, image and within Pins, and a product search connecting to hundreds of millions of shoppable Product Pins.

In fact, there are now more than 5 billion searches on Pinterest every month.

As people prepare for a post-pandemic life, searches for outfits, vacations, and home renovations are at all-time highs, and searches for weddings have presumed pre-pandemic levels.

For Gen Z, the number of searches per Pinner have increased 31% YoY, while the number of searches within the audience has nearly doubled (+96%) YoY.

When it comes to shopping, product searches grew more than 20x YoY (at the end of Q1).

Search outside the box

As one of the first engineers at Pinterest, I had the opportunity to work on that first version of a search engine, called Guided Search. Guided Search allowed for further refining or narrowing a search. You could start with something generic like BBQ and end up somewhere more actionable like summer chicken BBQ recipes, or vegan DIY BBQ.

Search engines are great when you know what you’re looking for. But the internet was missing a way to explore, a way to start with a few keywords and then get help expanding upon that query to explore possibilities.

The design of Guided Search became used across the industry as a way to click through topics and find the best answer for you. The product paved the way for computer vision-powered visual search. In 2015 we began our work in object recognition within Pins and later brought the technology to camera search to make it possible to search and find recommendations for anything you see online or off.

Search at Pinterest over the years, from Guided Search to search with skin tone ranges and Lens camera to shop. High-resolution image here.

The future of search is visual

Throughout this evolution, search on Pinterest became more visual, and more useful in helping people go from inspiration to action. We brought together the worlds of text and visual search and even created technology to shop the look inside Pins and complete the look. Because people are taking the same image and putting it on different boards, we can learn the deeper semantics of an image. We can then train our systems to emulate the ways Pinners are categorizing images. For example, when you think of a sofa, it isn’t a bunch of words — it’s a mix of images and notions of what a sofa is — and we’re teaching the computer this. Every day we’re making improvements to relate images to how people think of them, through machine learning. Each time we improve our model we see engagement go up.

We’ve also launched the ability to shop within Lens and extended our visual search work to augmented reality with AR Try on for lipstick and eyeshadow.

Along the way, we made improvements to Related Pins, and it has become one of the most common ways people refine their search — by clicking on a Pin they like and then scrolling through (in itself a type of visual search).

A big focus for us has also been inclusive product development, to ensure every person feels represented on Pinterest. This has included skin tone ranges so people can filter beauty searches by their skin tone range. We’ve taken this technology to AR as well, and worked to diminish bias in AI by working with a diverse data set.

Ultimately, we want to help Pinners take action on inspiration — for example, making a purchase in the shop tab or learning a new skill through a creator’s Idea Pin. We’re continuing investments in building our in-house search framework to surface relevant Pins to the right person at the right time, as well as working to make search results as inclusive as possible, including showing diverse results by default, starting with beauty queries, and more to come!

*There are more than 5 billion searches on Pinterest each month (Searches is inclusive of text searches and visual searches, which includes zooming in on an object in a Pin to see visually similar Pins.) This stat reflects global searches over any 30-day period since April 2021.",4999
"I mentioned last week I would be combining our monthly reading recommendations and weekly news into a single weekly publication, while keeping the size of the email about the same (4 +/- min). The late March-like weekend weather here in Boston inspired me to move quicker than planned.

Our new format includes recommended articles in our Opinion / Analysis section. In our News section we have the full text of some of our edited news stories with links to the original source, and snippets of additional news items with links to our edited version.

This week we suggest articles by Benedict Evans and Daniel Tunkelang, and have news from Elastic, Librestream, Varada, Microsoft, Squirro, Fluree and Lead Semantics, and AtScale.

Always interested in hearing what you think!

Apple, Fedex and the cookie apocalypse

If you’ve been following the consumer privacy versus advertising discussions, you know that Apple’s and Google’s moves are a big deal, but what will result? What will the three-way push-and-pull between advertisers, publishers, and consumers look like? How will each change their model or behavior? What impact will the reactions of rent seekers and regulators have? Nobody knows. Benedict Evans reviews the current state of affairs, suggests some changes, and helpfully looks at the bigger picture.

As cookie-based ads go away, does the targeting move from the cloud to the client, or from the reader to the content? Does this make the strong stronger? Or does the money go to Fedex instead?

https://www.ben-evans.com/benedictevans/2021/5/25/apple-fedex-and-the-cookie-apocalypse

AI for query understanding

We know that Google, Amazon, Microsoft, and others are working hard to determine search intent to create better search results and assistant responses. There is also a lot of interest in replacing inverted indexes with neural information retrieval. Search expert Daniel Tunkelang argues for prioritizing the use AI tools on query understanding before using them to improve retrieval and ranking. He explains more in this short article, and provides useful links for those of us who might need a refresher on search technology terminology.

https://dtunkelang.medium.com/ai-for-query-understanding-d8c073095fff

Elastic adds features in Elastic Enterprise Search 7.13

May 27, 2021 — Elastic announced new Dropbox integrations, the beta release of the Elastic App Search precision tuning API, and enhanced custom source APIs across the Elastic Enterprise Search solution in the 7.13 release. Elastic Enterprise Search includes expanded content sources for Elastic Workplace Search, now supporting Dropbox Paper and complete document-level permission synchronization across all Dropbox content. Teams that rely on Dropbox to collaborate, create, and coordinate can use Workplace Search to find the content they need alongside relevant chat messages, wiki pages, PDFs, and other content.The new Elastic App Search precision tuning API, now available in beta, allows customers to adjust the recall and precision of their websites or application search results. Through a single API call or parameter at query time, search admins can easily unlock granular search optimization by configuring how broad or narrow their search results should be to suit the exact needs of their users.New custom source management APIs, also introduced in Workplace Search 7.13, simplify data ingestion and enable users to manage content from any source, including legacy and custom applications. The new APIs allow for programmatic, on-the-fly creation and operation of sources without the need for manual setup. Users can create high-quality ingestion pipelines for any source alongside other day-to-day tools, all with unified relevance.

https://www.elastic.co/blog/whats-new-elastic-enterprise-search-7-13-0

Librestream enhances knowledge-sharing with AI Connected Expert

May 27, 2021 — Librestream, provider of augmented reality (AR) and remote collaboration solutions, launched new capabilities for its Artificial Intelligence (AI) Connected Expert vision giving industrial workers access to the content, people, smart data and guidance needed to efficiently and safely perform their work.

The new components to the Onsight platform include natural language processing (NLP) with real-time translation capabilities to bridge language barriers amongst global workers, and an industrial-first AR experience for the Microsoft HoloLens 2 platform. The advancements build on the AI Connected Expert workforce vertically-trained computer vision, reducing human cognitive load, and enabling faster time to competency for employees, as well as IoT sensor data visualizations within the Onsight experience.

Onsight Translator, Librestream’s NLP capability, enables users to simply and securely translate “speech to text,” including live transcription (captions) displayed on-screen and live translation of calls from one language to another. Whether a field technician works with a subject matter expert on an asset repair task, communicates with a supplier in a different country or assists with a remote inspection, Onsight Translator’s speech recognition and machine translation bridges language gaps.

Onsight Connect for HoloLens 2 provides hands-free experience across collaborators, incorporating the mixed reality features of the HoloLens 2 such as holographic visual interface and support for gestures, voice-driven commands, and 3D audio experience.

https://librestream.com/press-releases/librestream-unveils-workforce-collaboration-and-knowledge-sharing-enhancements-as-part-of-ai-connected-expert-vision/

Varada delivers text analytics on the Data Lake for SQL Consumers

May 27, 2021 — Varada, a data lake query acceleration provider, announced its platform now supports text analytics workloads and helps data teams deliver faster time-to-insights on exabytes of string-based data. Varada’s solution for interactive text analytics, integrated with open source search engine Apache Lucene, works directly on the customer’s data lake and serves SQL data consumers out-of-the-box. Varada’s text analytics feature is deployed in the organization’s own environment, so the data is not duplicated and never leaves. Plus, it incorporates all data from any source without modeling with results that are both thorough and precise. Varada’s dynamic and adaptive indexing technology enables text analytics workloads to run at close to zero latency response time.

Varada’s adaptive and autonomous indexing technology leverages machine learning capabilities to dynamically accelerate queries to meet evolving business requirements. Varada indexes data directly from the data lake across any columns. Based on the data type, structure, and distribution of data, Varada automatically creates an optimal index from a set of indexing algorithms including text-optimized search and index (based on Apache Lucene) as well as bitmap, dictionary, trees, etc. Indexes also adapt to changes in data over time.

https://varada.io/

More recent news…

Microsoft Build — selected news

Lots of news this week at the annual Microsoft Build developer conference. They did produce a very helpful “Book of News” (at about 8,800 words) with a table of contents to cover it all. Below is a selection of announcements our readers are most likely to be interested in, followed by a link to the …

https://gilbane.com/2021/05/microsoft-build-selected-news/

Squirro launches new Squirro App Studio

Squirro, an Augmented Intelligence solutions provider, has announced the launch of its new Squirro App Studio, a no code / low code platform to build and set up AI-powered apps such as Cognitive Search quickly and easily. The platform enables users with no background in data science to build a Cognitive Search app, leveraging Artificial …

https://gilbane.com/2021/05/squirro-launches-new-squirro-app-studio/

Fluree and Lead Semantics announce TextDistil

Fluree, provider of an immutable semantic graph data platform, announced a technical partnership with Lead Semantics to provide an integrated solution, TextDistil, for enterprise data management teams building semantic-capable, secure data fabrics. A focus for the integrated solution includes regulated industries, with a greater magnitude and scope of requirements needed to prove compliance, including fintech, …

https://gilbane.com/2021/05/fluree-and-lead-semantics-announce-textdistil/

AtScale announces AtScale CloudStart

Semantic layer eliminates the friction of moving BI, artificial intelligence and machine learning workloads to the cloud. …

https://gilbane.com/2021/05/atscale-announces-atscale-cloudstart/

The Gilbane Advisor is curated by for content technology, computing, and professionals. The focus is on strategic technologies. We publish recommended articles and content technology news weekly. We do not sell or share personal data.

Get this weekly up in your mailbox",8940
"To view the updated DevOps course(101DaysofDevOps)

Course Registration link: https://www.101daysofdevops.com/register/

Course Link: https://www.101daysofdevops.com/courses/101-days-of-devops/

YouTube link: https://www.youtube.com/user/laprashant/videos

Welcome to Day 84 of 100 Days of DevOps, Focus for today is Introduction to ElasticSearch

ElasticSearch is an ultrafast distributed(fault tolerant)search and analytics engine powered by Apache Lucene Project. ElasticSearch is specifically designed to search an index of massive datasets in the order of Petabytes.

Installing ElasticSearch

Requirement

java >7

Installation

$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.4.2.zip $ unzip elasticsearch-5.4.2.zip $ cd elasticsearch-5.4.2 # To start ElasticSearch $ bin/elasticsearch

To test elasticsearch

Now if we want to test via GUI interface we need Kibana for that

$ wget https://artifacts.elastic.co/downloads/kibana/kibana-5.4.2-darwin-x86_64.tar.gz $ tar -xvf kibana-5.4.2-darwin-x86_64.tar.gz $ cd kibana-5.4.2-darwin-x86_64 $ vim config/kibana.yml Now go inside kibana.yml and uncomment this entry elasticsearch.url: ""http://localhost:9200"" # Start Kibana $ bin/kibana

# To access Kibana

http://localhost:5601/

Kibana

Now let’s try to put some index(using Dev Tools)(don’t worry if you don’t understand what index is)

Now try to get it

ElasticSearch Stack

Kibana: Data Visualization ElasticSearch: Store,Index,Search and Analyze data Logstash,Beats: Data Ingestion X-Pack: Additional Features

What is an Index?

The index is a logical namespace which points to one or more shards(container for data)in an ElasticSearch cluster and it serves as the place to store related data

# Adding Index $ curl -XPUT http://localhost:9200/testing {""acknowledged"":true,""shards_acknowledged"":true} # Getting all indices in a cluster $ curl -XGET http://localhost:9200/_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open testing Zkhf_-5pR2yPvjvqgdLlyg 5 1 0 0 650b 650b yellow open test rfTZJwiSTSmufAzzHWjSkg 5 1 0 0 650b 650b yellow open .kibana 8-5xFao6TjCqV3a_iNCFnw 1 1 1 0 3.1kb 3.1kb # Get a specific index in a cluster $ curl -XGET http://localhost:9200/testing?pretty { ""testing"" : { ""aliases"" : { }, ""mappings"" : { }, ""settings"" : { ""index"" : { ""creation_date"" : ""1498323861381"", ""number_of_shards"" : ""5"", ""number_of_replicas"" : ""1"", ""uuid"" : ""Zkhf_-5pR2yPvjvqgdLlyg"", ""version"" : { ""created"" : ""5040299"" }, ""provided_name"" : ""testing"" } } } }

NOTE:

When Index is created by default it’s assigned to 5 primary shards (fixed)

(fixed) There is a one replica shard(can be changed any time)

ElasticSearch VS Relational Database Analogy

Documents

As mentioned above Document can think of Row(an individual entry in ElasticSearch) and contained in Document is Field which we can think as a column.

Easiest way to understand this

index --> my_index

type --> my_doc

document --> 1

Fields --> ""name"":""testuser"",

""years"":1,

""date"":""2017-06-24""

Now to get the data back

Now to get a mapping

If we want to get information about a particular index

Now if we want to change the number of primary shards(not possible as they are immutable)and number of replicas, we can do it easily with the help of Kibana Developer Console

To verify it

As mentioned above a number of shards is immutable(i.e) we can’t change that value but we can change the number of replicas

So let’s try to change the number of replicas

But if we try to change the number of shards

To verify it

Mapping

Mapping describes the field properties of a document through the type. Mapping should be setup before the first document added as the system knows what type of data each field contains.

name: string date: date

NOTE: If you try to change the datatype after it’s been indexed(eg: from date to string) all that data will become unsearchable. The only solution to this problem is to re-index all the data.

A simple example of Mapping

Deleting an Index

First, let’s verify all index

$ curl -XGET http://localhost:9200/_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open testing Zkhf_-5pR2yPvjvqgdLlyg 5 1 0 0 650b 650b yellow open my_index upg2ElNdTHGpcCPCOMb6AA 5 1 1 0 4.4kb 4.4kb yellow open my_new_test qcwTmf7VSbiutHfZOJSlZw 1 2 0 0 130b 130b yellow open test rfTZJwiSTSmufAzzHWjSkg 5 1 0 0 795b 795b yellow open .kibana 8–5xFao6TjCqV3a_iNCFnw 1 1 1 0 3.1kb 3.1kb

Now to delete an index

Deleting an index effectively removes all documents and types associated with that index.

To delete multiple index(via developer console)

To check the cluster health

$ curl -XGET http://localhost:9200/_cluster/health?pretty { “cluster_name” : “elasticsearch”, “status” : “yellow”, # It shows yellow because it's single node cluster “timed_out” : false, “number_of_nodes” : 1, “number_of_data_nodes” : 1, “active_primary_shards” : 12, “active_shards” : 12, “relocating_shards” : 0, “initializing_shards” : 0, “unassigned_shards” : 13, “delayed_unassigned_shards” : 0, “number_of_pending_tasks” : 0, “number_of_in_flight_fetch” : 0, “task_max_waiting_in_queue_millis” : 0, “active_shards_percent_as_number” : 48.0 }

Adding a document

We can add a document without assign an id, ElasticSearch by default assign id for us(we need to use POST for this purpose)

Optionally we can assign id using PUT(But if the same id exist, ElasticSearch will throw 409 conflict error)

Let’s add a field to our document mapping, we can’t change existing mapping but we can add new mapping after the index has been created

To delete a document

To verify it

Bulk API

Allow us to index multiple documents at one time

To verify it

To use a bulk API to add external document

Looking forward from you guys to join this journey and spend a minimum an hour every day for the next 100 days on DevOps work and post your progress using any of the below medium.

Reference",6001
"TECHNOLOGY

Monetary innovation is the utilization of creative innovation to convey a wide scope of monetary items and administrations. It is planned to work with the multi-channel, advantageous and quick installment experience for the shopper. This kind of innovation is viable in various business sections, like portable installments, venture the board, cash move, raising support and loaning.

The quick development of monetary innovation has been exceptionally helpful for shoppers around the world, for example, the capacity to serve clients that were not recently taken care of, a decrease in costs, and an expansion in rivalry.

We should investigate a couple of the advantages identified with monetary innovation:

Better installment frameworks — this kind of innovation can make a business more exact and proficient at giving solicitations and gathering installment. Additionally, the more expert assistance will assist with further developing client relations which can improve the probability of them returning as a recurrent purchaser.

Pace of endorsement — numerous independent company adventures are beginning to utilize the elective moneylenders like those engaged with monetary innovation since it can possibly speed up the pace of endorsement for finance. Much of the time the application cycle and time to get the capital can be finished inside a time of 24 hours.

3 Best Practices To Follow When Generating Your Bulk QR Code

If you need to create multiple QR codes simultaneously, you are on the right page. Typically, employers need to generate several vCard QR codes for their workers. At times, you may need to organize an event and you may need to have all of the participants bring their unique QR codes with their name tags. In this article, we are going to talk about the best practices you have to follow when it comes to generating these in bulk. Read on to find out more.

1. Add an Appealing Call-to-Action

First of all, there should be something that will arouse the interest of people. Apart from displaying your QR code, you may want to add something eye-catchy. The idea is to attract the attention of the viewers.You can use different call-to-action phrases such as “scan to win” and “scan to find out more”. This type of phrase can make the call to action more engaging. Apart from this, the purpose of adding a call to action is to ensure your message is brief and concise apart from being interesting.

2. Put the code where It can be seen

These codes must be placed in the right location strategically. If the codes cannot be spotted, they won’t scan them. So, you may not want to make the mistake of placing it at the corner of printed mediums or posters.

So, you should not follow this practice and position the QR codes in a place where they can be easily seen. Besides, they should be big enough.

3. Design Matters

Typically, these are available in black and white. Therefore, people can treat these figures just like barcodes. They are there for technical purposes and common people have no use of them. Still the design of these codes is of paramount importance for a number of reasons. So, make sure you choose the best design possible before printing them.

When it comes to printing these in bulk, don’t forget the design and static aspects of these tools. If you go for the right code generator, there will be no such problem. For example, if you employ the best tool, you can add your desired logo, make alterations to the color and add many other features. This will make the image stand out and make it more interesting.

Since the generation of QR codes one by one involves a lot of effort, we suggest that you opt for the best generator to save you time and headache. Large companies hire the services of dedicated employees to generate QR codes for every service, web page, and individual product.

However, since we have code generators today, you can just click a button and the tool will create hundreds of codes in a few minutes.

Long story short, we suggest that you invest in a good bulk QR code generator and follow the best practices. After all, you cannot take the risk of compromising the quality of the codes.

The Introduction Of The Face Recognition System

According to news, the Saudi Ministry of Interior is planning to launch an Iris recognition biometric system across Saudi Arabia. This includes plans to cover major places, such as airports, sea and land. The purpose of installing the new systems is to make use of advanced technology for the identification of passengers. In other words, this state-of-the-art system is designed to make sure no intruders can enter a secure area.

The National Information Centre of the ministry will be responsible for installing the Iris recognition system. Basically, this system involves the application of mathematical recognition techniques that scan the iris of a person before allowing access to the facility.

Digital Transformation Is About People, Not Technology

According to an article published in The Economist, “the infusion of data-enabled services into ever more aspects of life”, shall be the most evident consequence of the enduring Covid-19 pandemic. Digital transformation is expected to have greater importance for companies in the future, very shortly.

A 2019 survey of CEOs, directors and some senior executives discovered that their #1 concern was risk involving the digital transformation. However, 70% of their initiatives towards this motion failed to meet its goals. Of a whopping $1.3 trillion spent on the new endeavors in 2019, unfortunately, $900 billion was wasted.

Why?

Fundamentally, digital transformation teams fail, despite the possibilities for growth and efficiency gains, as people lack the mindset to a shift. With flawed organizational practices, it is extremely difficult to transform completely. Moreover, digitization would magnify the flaws, only to make it appear bigger.

What is Digital Transformation?

When you bring in a new system into an organization, it is only obvious to get a little hyper with the plans for implementation, specification, and counting.

Digital change is one of the most critical processes today, which ensures organizations are relevant as well as profitable in this competitive market.

The process involves integrating innovative technologies and services into existing business practices and streamline operations. The idea is to improve and add greater value to the final product. This involves adding new tools and applications, storing data, recording information, and a lot of new techniques.

That’s of course, the digital aspect of things. But, if you spare a thought, we are talking about “transformation”, which means introducing innovative ways to work with the existing team.

Tricky, right!

Anybody would be willing to buy a new set of digital suites with the latest tools, but who would run it? The key here is to ensure that the talent, or people, onboard, and the company culture is prepared to adapt. A successful transformation is change management, and people can only make it happen.

Getting Your Team Involved

Any change is difficult. If you want to introduce major changes in your organization, you have to ensure everyone is with you, and not only your leadership team. Yes, you cannot let the team take big decisions for the organization, but involving your team in a process can give better results.

A McKinsey study showed that whereas 84% of the CEOs are dedicated to major transformation changes, only about 45% of the frontline employees agree. Obviously, connecting the dots is a primary obstacle to enact a successful strategy.

There are many ways to achieve this:

• Take feedback from the team about the changes you implemented

• Keep your team abreast of the implementation strategy

• Incentivize the team with internal marketing to convince new technology to the most reluctant team member

Transformation to the digital landscape can be potentially beneficial to an organization, but only if every single team member agrees and accepts the change. Make sure you have a positive digital transformation team that understands why it is important to adopt new technology and its benefits.

Invest & Train Your Team

Going digital would have hurdles. Some of the team members may not be as tech-savvy as others. However, you cannot leave them behind. To bring them up to that level, a lot of training is required to help them adapt to the latest technology and tools.

Remember, people have different ways of learning too, and speeds may differ. For instance, some team members may understand the concept in one demo session, whereas others may require multiple days of training to get a grip of the new technology.

Experiment with varied training materials, such as online courses and hands-on learning, and give them the flexibility to choose how they want to learn.

It may take some time to learn how to use new technology for better results, especially for team members who do not possess the natural inclination towards technology. Investing in training is a sure-shot way to leverage this transformation.

Digital Transformation Framework Doesn’t Change Everything

The digital transformation framework is not about changing everything at once.

When you start transforming the business, getting carried away is easy. However, it’s critical to know about the technologies to adopt. You may consider the one that employees would find easier to implement, and being selective to choose the best way.

Anything that glitters is not better. When you are planning to transform your business processes digitally, it is only to simplify the work process and facilitate your team members. So, do not make it complicated. If you have any doubt about the changes, consult the frontline staff.

For instance, if you want to adopt a new platform for online communication, but you cannot decide between Zoom, Teams, and Slack, consult your staff and take their opinion.

Broaden Your Vision

Do not have a myopic vision when it comes to a major transformation. Digital transformation services aim to make lives simpler and better. A successful transformation strategy is about introducing new changes into the business to make it more efficient and reduce employee workloads.

If executed properly, such a digital revolution can lead to improved working practices, increases value of customers, and lesser workload for the team. If your digital move is not ticking all the boxes, something is amiss.

Bring Change Right from the Top

The concept of grassroots change is intuitive. However, in reality, change is more likely to take place if driven right from the top. Again, that does not indicate a hierarchical or autocratic structure or a culture that breeds fear. It simply implies leadership, both transformational and transactional.

When digital change is concerned, the primary implication is that no major change or even an upgrade to the organization is possible unless you select and develop the top leaders to start. It’s very clear that leadership, both good as well as bad, flows down to affect every aspect of an organization. The single most factor that determines the effectiveness of the transformation of an organization is the CEO or the top leader of an organization. Of course, industry, culture, context, legacy, people, and real tech matter, just like other resources.

However, these things are rather too similar among competitors, while values, mindset, integrity, and competence of senior-most leaders stand out to be the primary differentiating factor. Needless to add, everything in an organization can be imitated, but not talent. So, invest in the best talent for greater impact, which is exactly where you would get the highest value.

Final Thoughts

Technology is all about doing a lot more with minimum resources, yet the arrangement is effective only when technology is paired with the best human skills. Like technological disruption leading to automation and eliminating out-of-date jobs, it has created more jobs. This is precisely why innovation is also called ‘creative destruction.’ Any creative facet of innovation depends on people. So, leveraging human adaptability to upskill and reskill the workforce can augment technology and humans simultaneously. Simply put: a brilliant innovation would be irrelevant if we do not have enough skilled workforce to implement it, and the most inspiring human minds would be least useful if they do not connect with technology. The core implication is — when leaders want to invest in new technology, they should consider investing in people who make technology useful.",12687
"Grafana is an open-source monitoring tool that can be used to monitor various kinds of things in an IT company.

Namely, it includes, server usage monitoring like CPU, memory, disk, network loads, endpoints, and many.

But the main reason of using this tool is that it provides a wide variety of visualization dashboard which makes very easy to understand for a non-technical person also.

Grafana offers many dashboards which includes System Overview, Elasticsearch logs, Prometheus monitoring, world ping, etc

Yes, if you want to use more features then you have to pay for Grafana Cloud which provides extended support and features.

Let us dive into the installation process for the ubuntu 20.04 version.

Along with Grafana we also have to install two tools that will have to save data and send data to the Grafana dashboard.

InfluxDB

Telegraf

Influx dB Installation (Ubuntu 15.04+. Debian 8+):

sudo apt-get update && sudo apt-get install influxdb sudo systemctl unmask influxdb.service sudo systemctl start influxdb

The config file of Influx dB will be located at /etc/influxdb/influxdb.conf

sudo systemctl status influxdb

The screenshot is taken by the author

By default, Influx dB runs on 8086 Port. To access the shell fire “influx” on the terminal.

CREATE DATABASE SHOW DATABSES USE <db_name>

Telegraf Installation

sudo apt-get update

sudo apt-get install apt-transport-https

sudo apt-get install telegraf

sudo systemctl start telegraf

sudo systemctl status telegraf

The screenshot is taken by the author

Now create an admin account for the Influx dB server Go to your influx shell and follow these queries

CREATE USER admin WITH PASSWORD ‘root’ WITH ALL PRIVILEGES

SHOW USERS

Then, to enable http authentication for influx dB Server, Head over to /etc/influxdb/influxdb.conf and edit the following lines.

# Determines whether HTTP endpoint is enabled.

enabled = true

# The bind address used by the HTTP service.

bind-address = “:8086”

# Determines whether user authentication is enabled over HTTP/HTTPS. auth-enabled = true

Configure HTTP authentication on Telegraf

Go to the configuration file of Telegraf, located at /etc/telegraf/telegraf.conf. Modify the changes as mentioned below

## HTTP Basic Auth

username = “telegraf”

password = “root”

Note: To perform the above step you need to create a user in Influx dB same as we have created for admin in the above steps.

sudo systemctl restart influxdb

sudo systemctl restart telegraf

sudo journalctl -f -u telegraf.service

Configure Telegraf for HTTP

# Configuration for sending metrics to InfluxDB

[[outputs.influxdb]] urls = [“https://127.0.0.1:8086""]

## Use TLS but skip chain & host verification

insecure_skip_verify = true

Grafana Dashboard Installation



sudo apt-get upgrade

sudo wget -q -O —

sudo apt-get install grafana sudo apt-get updatesudo apt-get upgradesudo wget -q -O — https://packages.grafana.com/gpg.key | apt-key add—sudo apt-get install grafana If you are using systemd, Grafana created a grafana-server service for you. To ensure it, run the following command:

sudo ls /usr/lib/systemd/system/grafana-server.service

cat /usr/lib/systemd/system/grafana-server.service

sudo systemctl start grafana-server

sudo systemctl status grafana-server

The screenshot is taken by the author

The default port for Grafana is 3000. The default login is “admin” “admin”

The screenshot is taken by the author

Your Grafana Setup is done. Now you can manage dashboards, alerts, and many more things as per your use case.

I will be adding more articles regarding Grafana in the future and in the meantime explore on your own and please give this article a clap so that it helps the algorithm.

Thanks.",3715
"Check the updated 101 Days of DevOps Course

Course Registration link: https://www.101daysofdevops.com/register/

Course Link: https://www.101daysofdevops.com/courses/101-days-of-devops/

YouTube link: https://www.youtube.com/user/laprashant/videos

Welcome to Day 5 of 100 Days of DevOps, On Day 2 I discussed the SNS https://medium.com/@devopslearning/100-days-of-devops-day-2-introduction-to-simple-notification-service-sns-97137b2f1f1e and in the current DevOps world, no one denies the importance of Notification especially in cases when something went wrong with your infrastructure(Server Down/CPU Utilization high…).In this tutorial, I will show you how to integrate CloudWatch with Slack, so that you will be notified and take an effective measure.

This can be achieved by following a few steps

Create an AWS Access key and Secret Key

Create an IAM Role

Deploy the lambda function

Create an SNS topic

Create a Cloudwatch Alarm

Step1: Create an AWS Access key and Secret Key

Go to IAM console https://console.aws.amazon.com/iam/home?region=us-west-2#/home

Click on Users → Particular user → Security Credentials

Click on Create Access Key

Save this Access Key and Secret Key as we need them later while configuring Lambda function

Step2: Create an IAM Role

Go to IAM console https://console.aws.amazon.com/iam/home?region=us-west-2#/home

Roles → Create role → AWS service → Lambda

Search for AWSLambdaBasicExecutionRole

Give your Role a name

Click on create Role

Copy the role arn, we need it for future configuration

Step3: Deploy the Lambda Function

For the purpose of this demo, I am using Public GitHub Repo

# Step 1

$ git clone https://github.com/assertible/lambda-cloudwatch-slack.git Cloning into 'lambda-cloudwatch-slack'... remote: Enumerating objects: 244, done. remote: Total 244 (delta 0), reused 0 (delta 0), pack-reused 244 Receiving objects: 100% (244/244), 668.48 KiB | 3.56 MiB/s, done. Resolving deltas: 100% (120/120), done. #Step 2

$ cd lambda-cloudwatch-slack/ #Step 3

cp .env.example .env

Now we need to perform some configuration at Slack End

Go to Slack, Apps section and click on Add apps

Search for incoming-webhook

Enter the channel name where you want to send a notification, also note down Webhook URL

Under .env file,enter the following info

#ENCRYPTED_HOOK_URL= you can use ENCRYPTED_HOOK_URL if you want

UNENCRYPTED_HOOK_URL=Step3

AWS_FUNCTION_NAME=cloudwatch-to-slack

AWS_REGION=us-west-2

AWS_ROLE=""Step2""



# You can get AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY here: https://console.aws.amazon.com/iam/home#/users

# Click on user -> Security credentials -> Access keys -> Create access key

AWS_ACCESS_KEY_ID= Step1

AWS_SECRET_ACCESS_KEY= Step1

After making these changes, execute the following command on the terminal

$ npm install added 136 packages in 4.039s ╭─────────────────────────────────────╮ │ │ │ Update available 5.6.0 → 5.8.0 │ │ Run npm i -g npm to update │ │ │ ╰─────────────────────────────────────╯

Finally, deploy it

$ npm run deploy > lambda-cloudwatch-slack@0.3.0 deploy /Users/plakhera/Documents/lambda-cloudwatch-slack > ./scripts/deploy.sh Warning!!! You are building on a platform that is not 64-bit Linux (darwin.x64). If any of your Node dependencies include C-extensions, they may not work as expected in the Lambda environment. => Moving files to temporary directory => Running npm install --production => Zipping deployment package => Zipping repo. This might take up to 30 seconds => Reading zip file to memory => Reading event source file to memory => Uploading zip file to AWS Lambda us-west-2 with parameters: { FunctionName: 'cloudwatch-to-slack', Code: { ZipFile: <Buffer 50 4b 03 04 14 00 08 00 08 00 20 10 4a 4e 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00 00 2e 65 6e 76 6d 90 5d 4f 83 30 18 85 ef f9 15 8d bb 5c 18 9b ... > }, Handler: 'index.handler', Role: 'arn:aws:iam::XXXXXX:role/cloudwatch-to-lambda', Runtime: 'nodejs8.10', Description: 'Better Slack notifications for AWS CloudWatch', MemorySize: 128, Timeout: 60, Publish: false, VpcConfig: { SubnetIds: [], SecurityGroupIds: [] }, Environment: { Variables: { UNENCRYPTED_HOOK_URL: 'https://hooks.slack.com/services/XXXXXXXX' } }, KMSKeyArn: '', DeadLetterConfig: { TargetArn: null }, TracingConfig: { Mode: null } } ^C ╭─────────────────────────────────────╮ │ │ │ Update available 5.6.0 → 6.7.0 │ │ Run npm i -g npm to update │ │ │ ╰─────────────────────────────────────╯

If everything looks good, you will see the new function on the lambda page

Step4: Create an SNS topic

Go to https://us-west-2.console.aws.amazon.com/sns/v2/home?region=us-west-2#/home

Click on create a topic and enter Topic name(eg: cloudwatch-to-lambda-sns-topic)

Click on newly create a topic and then from Actions drop-down Subscribe to topic

Click on Create subscription, using AWS Lambda as Protocol

Step5: Create CloudWatch Alarm

Go to CloudWatch home page https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2

Alarms → Create Alarm → Metric → Select Metric → EC2 → Per Instance Metric

Select CPU Utilization

Fill all the necessary details

In order to replicate the scenario, I am using stress tool, which is available as the part of RedHat epel repo

# stress --cpu 10 --timeout 300 stress: info: [15259] dispatching hogs: 10 cpu, 0 io, 0 vm, 0 hdd stress: info: [15259] successful run completed in 300s

You will see the notification like this

Looking forward from you guys to join this journey and spend a minimum an hour every day for the next 100 days on DevOps work and post your progress using any of the below medium.

Reference",5634
"Top 10 Robotic Process Automation Consulting Company in 2020 Christopher Jun 1·3 min read

CoSourcing Partners, a full-suite Robotic Process Automation (RPA) Services Provider, has been named one of the “Top 10 Robotic Process Automation Consulting/Service Companies of 2020” by CIO Applications.

By offering a full suite of RPA consulting services, CoSourcing Partners has been able to differentiate themselves by “teaching their clients how to fish,” at any stage of their automation journey. The strategic opportunity assessment begins with training clients on how to identify processes that make good candidates for automation.

CoSourcing Partners will assess current systems, map a plan with optimized processes (including business cases, opportunity prioritization, and development road map) and put in place a system to measure the success of the programs. CoSourcing Partners also recommends RPA software products to find the product(s) that best suits their business needs.

CoSourcing Partners also performs 15 Day Automation Opportunity Assessments. The first week onsite and the second week off-site. On the 15th day the customer will have a completed work product that includes 20 of their processes, modeled and ranked according to a payback model that outlines (ROI) return on investment.

Recent research indicates that 83% of US companies are going to be making investments in RPA technology over the next year. As RPA technology becomes more ubiquitous within enterprise environments it’s important to make sure your implementation partner has proven RPA experience within the four pillars of RPA Infrastructure, Solutions Architecture, Business Process Analyst, and Development. By partnering with only the top RPA software providers located in the upper right hand corner of Gartner’s magic quadrant such as UiPath, Kryon Systems, NICE, Blue Prism, and Automation Anywhere, CoSourcing Partners is equipped with the expertise needed to ensure successful process automations get developed and put into production.

“Being named as a Top 10 RPA Service Company by CIO Applications is an honor. It reinforces the value that we bring to clients who have either already made an investment in RPA solutions, or are thinking of bringing RPA into their environment,“ says Rita Brunk, Executive Vice President of RPA, “The main differentiator is that we don’t just do development, but coach, mentor, and train while partnering with the client. In other words, we teach them how to “fish. “

About the CoSourcing Partners

CoSourcing Partners™ is a Chicago-based Robotic Process Automation and IT Services Company. Our portfolio of professional RPA solutions and IT services enables our partners to maximize their investments and capacity in the achievement of business goals and the realization of rapid returns. We are a dedicated team of professionals who value long-term relationships and take pride in achieving customer satisfaction. The company has quickly developed a track record of highly successful automation and IT projects by delivering top talent and employing RPA proprietary tools to build and maintain workforce performance.

About the CIO Applications

CIO Applications magazine stands out with its unique approach of learning from industry leaders offering professionals the most comprehensive collection of technology trends. We are determined to propose a myriad of additional services that can help customers deal with issues related to this industry.

Check This Out :",3503
"Aceyus, a World’s Top 10 Contact Center Solutions Provider, has been recognized by CIOReview as one of the most promising contact center technology solution providers of 2020.

CIOReview is a leading technology magazine aiming to guide enterprises with the most up-to-date news on solutions and services. Each year, they publish a listing of 20 companies that “are at the forefront of providing Contact Center Tech solutions and transforming businesses.”

Aceyus, selected as one of these companies in the forefront, provides a modern platform to help customers gain visibility and actionable insight into their data. With robust dashboards, reporting and analytics tools, Aceyus brings you an easy way to centralize all of your contact center data, improve the customer journey with real-time dashboards with adaptive technology designed to enhance contact center efficiency.

While already incorporating these factors into our sleek product, Aceyus is also looking to the future and identifying trends critical to adapt to and fold into upcoming solution needs.

“Aceyus is now planning to incorporate the power of AI into its solutions. This allows the company to better analyze huge volumes of data and help clients identify and resolve issues associated with the customer journey.”

Read This Original Article Here …

Check This Out:",1338
"Marketpath is honored to have Marketpath CMS, our easy to use website content management system, named as one of the industry’s Top Web Content Management Systems by CIOReview Magazine. Last month, CIO Review selected Marketpath as one of their 20 Most Promising Web Content Management Solution Providers, based on extensive evaluation standards. Marketpath excelled during the evaluation process based upon an internally developed set of evaluation criteria.

“Our list has been compiled by the CIOReview editorial board and a panel of expert advisors for the purpose of recognizing and highlighting the companies demonstrating outstanding performance in the market today,” said Jeevan George, Managing Editor, CIOReview.

Marketpath has traditionally rated highly with and focused on the small to mid-sized businesses and organizations, with simplicity for non-technical users being what we are known for. But with the launch of Marketpath CMS 4.0 later this year, Marketpath will be introducing a new version of our software that not only meets the needs of small business, but also appeals to developers and agencies as well.

CIOReview Magazine (www.cioreview.com) is a technology magazine for corporate and enterprise IT decision makers. They present expert information for technology professionals, in both print and digital formats, on how to succeed with technology, sharing innovative solutions from established and up and coming technology companies.

View the full press release here or visit CIOReview Magazine and see their list of 2016’s website content management system providers. If you’d like to learn more about Marketpath CMS or view a Demonstration of our solution, please visit our contact page

Check This Out :",1735
"Can Artificial Intelligence (AI) Help Deal With The Problem Of Toxins and Toxic Waste?

NOTE: This was curated by Medium and chosen for further distribution.

Photo from Danish Technological Institute of Robots with Artificial Intelligence Sorting Hazardous Waste. Copyright Fair Use Provision.

Artificial intelligence probably won’t help much with stopping the production of nuclear and chemical toxins, except if used to monitor industries that tend to pollute.

It’s the job of governments to regulate polluting activities and then enforce those regulations. However, as far as cleaning up the chemical and nuclear toxic waste that already exists, there’s a role that AI (i.e., robots) can play by taking over from humans the arduous and dangerous hazardous waste clean-up tasks. AI-enhanced robots can do this work much more efficiently and safely by using AI-enhanced.

A primary focus should be on batteries.

“Technological items often have one thing in common: that they use some form of battery. Batteries are used in our smartphones, laptops, remote controls, etc. — useful but dangerous, and just as dangerous when discarded.” — Danish Technological Institute

We’ve all been guilty of carelessly tossing used batteries into regular trash receptacles, with the batteries then winding up in landfills. The batteries then slowly release their deadly toxins into the soil, and those toxins can then seep into the environment and negatively affect our health.

“Toxic materials are poisonous byproducts as a result of industries such as manufacturing, farming, construction, automotive, laboratories, and hospitals which may contain heavy metals, radiation, dangerous pathogens, or other toxins. Toxic waste has become more abundant since the industrial revolution, causing serious global issues. Disposing of such waste has become even more critical with the addition of numerous technological advances containing toxic chemical components. Products such as cellular telephones, computers, televisions, and solar panels contain toxic chemicals that can harm the environment if not disposed of properly to prevent the pollution of the air and contamination of soils and water. A material is considered toxic when it causes death or harm by being inhaled, swallowed, or absorbed through the skin. The waste can contain chemicals, heavy metals, radiation, dangerous pathogens, or other toxins. Even households generate hazardous waste from items such as batteries, used computer equipment, and leftover paints or pesticides. Toxic material can be either human-made and others are naturally occurring in the environment. Not all hazardous substances are considered toxic. With the increase of worldwide technology, there are more substances that are being considered toxic and harmful to human health. Some of this technology includes cell phones and computers. Such items have been given the name e-waste or EEE, which stands for Electrical and Electronic Equipment. This term is also used for goods such as refrigerators, toys, and washing machines. These items can contain toxic components that can break down into water systems when discarded. The reduction in the cost of these goods has allowed for these items to be distributed globally without thought or consideration to managing the goods once they become ineffective or broken.” — Wikipedia

This problem is human-made and should be stopped at the source by humans taking responsible actions to dispose of batteries and other toxic materials.

And suppose we discard this kind of toxic material in a manner that results in potential danger to our health. In that case, we must take proactive measures to eliminate or at least reduce that danger. That’s where the use of AI comes in. Using AI for such tasks can provide a great benefit to all of us.

Many people, including some experts in the field, have expressed concern that AI could develop in a way that presents an existential threat to humankind. Some see those concerns as justified, while others see them as an overblown exaggeration, sort of a Chicken Little “the sky is falling” worst-case scenario.

In any case, AI is not going away and is here to stay. So we must implement proper safeguards and ethical standards for developing and using AI and vigorously monitor how we use it. But we must also take the best advantage of what AI has to offer, including helping us deal with the severe problem of dangerous toxins and hazardous toxic waste.

NOTE: The author is a retired corporate executive who spent over 35 years working in the information technology (IT) and services industry. He has a Bachelor’s Degree in Business Administration from Sophia University, Tokyo, Japan, and a Master’s Degree in Management from Webster University, St. Louis, Missouri.

_________________

Thanks for reading. (Copyright Terry Mansfield. All rights reserved.)

_________________

*Recommended resources for writers:

_________________

These are Terry’s most popular pieces on Medium:

— — — —

*Aff",5017
"Battery Algorithms

Shivaram N V, Hari Vasudevan

A battery management system or BMS is core to the functionality of an EV. While much has been documented, written and talked about the mechanical, electrical and software architecture of a BMS, not much has been written about the algorithmic architecture of the BMS. Of-course a relevant question that could be asked at this point is - is the time spent in creating an algorithmic platform worth it? Here at Ather, developing and deploying such an architecture has helped us immensely in creating a robust and reusable platform. In this post we are going to detail the basic algorithmic structure of the Ather BMS and how it has helped accommodate the many different use cases of the 450X.

Algorithm Fragmentation

A cursory search of BMS algorithms often yields a number of various algorithms, dealing with charging, protection and discharge. Take charging for instance, even the very basic CC-CV charging flow that is common to most chargers has a number of caveats related to temperature, voltage and charging currents. What makes this trickier is that a variety of such chargers are encountered by the vehicle. For eg, fast chargers have different charging current capacities, temperature limits and protections when compared with a slow home charger. In addition to this the EV will most likely encounter multiple generations of these chargers each with its own rules. In fact since Ather continually upgrades the public charging infrastructure to improve the access and reliability of the fast charging network, it causes much (happy) consternation internally as now the algorithms will need to track these changes also. Similarly, discharge of a battery pack also needs to be handled by the BMS algorithms. Discharge control of the battery pack needs to ensure that we are able to maintain the right cell temperatures, avoid drops in individual cell voltages and gracefully transition to a lower power mode at low States of Charge (SOC). The BMS also needs to ensure consistency in performance even as the battery pack ages as part of natural degradation in cell capacity. Last but not least, the BMS also needs to compute the right SOC for display to the user.

Optimality & Robustness

The most commonly known charging algorithm like the CC-CV belongs to a class of heuristic algorithms that gets the job done but not necessarily in the most optimal way. In the case of charging, we’re chasing minimum charging time while adhering to battery safety constraints. Similarly take the case of SOC - there are quite a few heuristic methodologies to estimate the SOC to a “decent” extent. But to maximise the usable energy consumption from a battery, it is necessary to have an algorithm that is not only accurate but also robust to a whole bunch of noise factors that a Battery Management System is exposed to.

Accommodating all of the above functionality with a multiple different algorithms paired up with the corresponding hardware or vehicle platform or scenario will eventually lead to fragmentation of the code. Finally the maintainability of the code-base will also take a hit as verification and validation start to consume increased time and effort. To get around these issues on the 450X we have developed a scalable BMS algorithm architecture (Figure 1 below) that attempts to provide a framework wherein all the above described optimality criteria and variations can be accommodated.

Figure 1: Battery Charge & Discharge Vehicle Application Architecture

SOC & The Kalman Filter

Literature on the SOC estimation of a cell or battery pack is available in plenty. In fact a cursory glance through literature on this leads to various formulations of the Kalman Filter for the estimation of SOC. Literature is peppered with various Kalman Filter formulations for SOC such as the Extended Kalman Filter (EKF), or the Sigma Point Kalman Filter (SPKF) and further varieties of these with each providing added benefits for added complexity. However often literature on SOC estimation is the end in itself and further development of the Kalman filter towards an LQG (Linear Quadratic Gaussian) controller for discharge/charge regulation is overlooked. Here at Ather, we have developed a framework that can further put the state estimate obtained as part of the Kalman filter structure to create a flexible and powerful framework to accommodate most charging and discharge functions of the BMS.

Cell Dynamics & State-Space formulation

A number of cell models are generally used for formulation of the Kalman Filter. In Figure 1 below we present a simplified lumped parameter model that is in widespread use. A description of these parameters is given in 1. It is important to understand that all the RC parameters in this model are functions of SOC. Estimation of the RC parameters is a topic in itself and will be the contents of a subsequent blog post.

Additionally we would like to point out that our model is a second order model, whereas higher model orders may be used for higher accuracies with the trade off of complexity. Given the above model and parameters, the equations that define this system are as shown below. The voltages V₁ and V₂ are described by the dynamic equations 4, 5, while SOC dynamics by 6. The OC voltage Voc is linearized (in equation 2) and re-written to cast it in a form suitable for LTI state-space analysis. This assumption is valid as long as the SOC does not change very fast, or in other words the rate of change in SOC is an order of magnitude different from V₁ and V₂ dynamics, which thankfully is the case.

Figure 2: RC parameter Model

Using these above equations we are now able to formulate a state-space model for an individual cell. This formulation is described in the following equations, with the state equation 7 and output equation 8 formulated as shown.

State Estimators

In the prior section we have a model for cell dynamics, we now explain the basis for formulating a state estimator. Of-course the way we have described the cell dynamics model is by no means the only way to describe the dynamics. Ours is a current input and voltage output system, and there exist equally valid models with voltage in and current out. Furthermore, we have not included the Voc hysteresis in this model, which for certain cell chemistries can be dominant.

All state estimators function via a ‘plant’ model that mimics the dynamics of the system under observation and a ‘correction’ term which is usually a measurable output of the system. Eventually the plant model and the correction term are fused together to arrive at a state estimate that is more accurate than either relying on the model or the sensor alone. The beauty of the Kalman filter is of-course that it estimates the least squares optimal estimate of the state even under varying noise conditions. A basic state estimator and its formulation is described below in equations 9 and 10.

In state estimator design of course the main question is how we design the correction gain ‘L’. Whether we are designing a Luenberger or a Kalman Filter, there’s tons and tons of literature on how best to pick ‘L’. We would like to refer the reader of this blog to the excellent webpage of Dr. Gregory L. Plett, Professor, University of Colorado. A thorough description of Kalman Filter equations is described in his course notes on Battery management and Control.

Charge & Discharge Controllers

At this point if our intent was to only describe a better way to estimate the State of Charge (SOC), we would be done. However, while the state estimator is a great way of arriving at the true SOC of the cell, it can also be put to other uses namely in the charge and discharge control of the battery pack.

Charging Controllers

The most widely used charging controller for EV is the CC-CV controller, meaning that the charger charges at a constant current until it hits a terminal cell voltage and then transitions to a CV charging mode, which applies a constant voltage until the cell voltage rises to equalize this voltage and the charging current drops to zero. This transition from CC to CV on chargers is what mostly causes much of the grief in creating a scalable architecture.",8239
"Thanks so much for all your feedback on my first blog post.

Today I’ll respond to that feedback, as well as some input from the press, and provide more detailed answers about Toyota’s Electric Vehicle strategy.

Has Toyota dragged its feet on Battery Electric Vehicle (BEV) investment because of its investment in Hydrogen Fuel Cell Electric Vehicles (FCEVs)?

No. Here are the facts:

Toyota spends about $10 billion per year on R&D.

That pays for a lot of R&D on fuel cells, but it also pays for over a thousand scientists, including over a hundred in the US, working on battery R&D. And so far they’ve generated one of the largest battery patent portfolios of any company, including far more patents on solid state batteries.

So you aren’t dragging your feet on BEVs?

2012 Toyota Rav4 EV — built in partnership with Tesla

No. Not only was Toyota an early developer of our own BEV, we were also an early investor and joint developer with Tesla. Until now, high battery costs have prohibited a more widespread effort, but by 2030 we expect that:

approximately 20% of our global vehicle production will be BEVs; and

approximately 70% of US vehicle production will be electrified (BEVs, PHEVs, FCEVs, or HEVs)

Toyota is ALL IN on the Biden Administration’s goal of 50% of US vehicle production being BEVs, PHEVs, or FCEVs by 2030.

The 2021 Toyota RAV4 Prime

But why pursue four different types of drivetrains? Isn’t that wasteful of R&D investment?

Toyota is known for being scrupulously efficient in manufacturing and has helped other companies become more efficient using concepts from the Toyota Production System (TPS).

But being efficient about R&D means managing the risk of lost opportunities if we make decisions too soon. And it also means developing products for diverse customers, not just a category of customers.

This boils down to two principles:

The solution to uncertainty is diversity, and What is best for the average person is not best for every person.

Let’s talk about each of these principles in turn:

1. The solution to uncertainty is diversity.

This is a profound, fundamental lesson from nature.

Sure, it feels more satisfying to make bold predictions and decisions right away, and put all of our eggs in one basket. But making decisions too soon is foolish.

We know that total carbon emissions in transportation must be drastically reduced, and we know that BEVs are part of the solution.

However, we do not know (nor, in truth, does anyone know) whether BEVs are the entire solution.

Why is that?

It is because the solution to long term carbon emissions will involve large changes to many of the world’s energy and production systems, and tomorrow’s technology will be different than today’s.

We can of course calculate the marginal costs and benefits of making changes to current energy and production systems with today’s technology. But optimal incremental strategies now do not accurately predict the best strategy in the future.

Faced with this uncertainty, our strategy is to place several bets on several promising pathways, including BEVs, and adapt our strategy as we learn.

This is the core idea of Toyota’s philosophy of Kaizen and the approach of every investor who diversifies their portfolio to manage uncertainty.

It is also the strategy nature has used so successfully for evolving life on earth.

2. What is best for the average person is not best for every person.

Everyone understands this principle when it comes to clothing: maximizing a group’s total comfort isn’t achieved by making everyone wear size “medium.” The same is true for mobility.

As battery technology improves and the carbon emissions from the electrical power grid decrease, long range BEVs will become more attractive, both from a practicality and environmental point of view, to the average customer.

Does this mean that in 2030, the greatest net carbon reduction would be achieved by making every customer buy a long range BEV?

No, it does not.

For some customers, particularly those that live in areas with low carbon intensity electrical power generation and easy access to rapid (level 3) charging infrastructure for long trips, replacing a gasoline fueled car with a long range BEV will be the best way for them to contribute to carbon emission reduction.

But for other customers in different circumstances, the best way to contribute to carbon emissions reduction will be different — perhaps a PHEV or an FCEV. For example, for a customer using their car mostly for commuting in an area with fewer high speed chargers, a PHEV would make more sense.

The animated chart below, produced by our open-source, peer-reviewed simulation tool, illustrates this point. It compares net lifetime carbon emissions for several Toyota Vehicles to current BEVs made by other manufacturers. It examines both the average US electric power generation carbon intensity and the lower electric power generation carbon intensity of California.

This chart shows that even with the lower carbon intensity electric power grid of California (a reasonable estimate for what the overall US grid might be like in 2030), diverse types of EVs have lifetime carbon emissions that are similar to BEVs.

Note that the scale of the Y axis is arbitrary — the relative results would be the same regardless of the time period chosen. In this case, five years was selected to give meaningful scale because it is the average duration of US ownership for new vehicles.

This means that in 2030, more people in more circumstances will be able to contribute to carbon reduction, and greater net carbon reduction can be achieved, if we provide customers with a diverse EV portfolio of choices that includes both BEVs and other options.

But isn’t it true that PHEVs don’t achieve their potential high carbon reduction in real world use, particularly in Europe?

We know that PHEV owners with low-range PHEVs, poor access to overnight charging locations, or company-provided gas credit cards (that disincentivize electric charging) more frequently refuel with gasoline than necessary.

But these factors are reasons to fix those problems — not reasons to dismiss PHEVs!

Modifying company “gas card” policies, using slightly larger batteries in PHEVs (such as in the RAV4 Prime), and installing more overnight charging infrastructure with some rapid chargers is a more practical way to reduce carbon sooner than building only BEVs and installing the many more rapid chargers an all-BEV approach would require.

Doesn’t your argument about “right sizing” battery packs depend on a shortage of battery supply?

No. “Right sizing” battery packs into diverse types of EVs, and limiting the ramp-up speed of global battery production, is a good idea even if battery supply is not otherwise limited.

TNGA-Powertrain Units — Lithium-ion Battery

Batteries are continuously improving. Compared to current technology, future battery technology (including possibly Toyota’s solid state batteries) will be produced with less carbon emissions, reduced environmental impact, at lower cost, with higher performance, and with fewer recycling problems.

By spreading today’s battery cells across a diversity of EV types now, we can maximize carbon reductions immediately. Then, as battery technology improves, we can ramp up production of more efficient, less carbon intensive batteries.

Here’s an analogy: Cell phone technology allowed developing countries to leapfrog wired telecommunications infrastructure. It would have been a waste of time and resources for these countries to have invested heavily in wired telephone infrastructure when cell phone technology was just around the corner.

We believe there is a corollary with the evolution of battery technology.

Summary

Can Toyota meet aggressive, results-oriented, quantitative limits on how much carbon may be emitted over a vehicle’s entire lifetime?

Yes, we can.

Will Toyota meet this challenge by producing millions of Battery Electric Vehicles (BEVs)?

Yes, we will, and we will also offer other drivetrains for customers in circumstances where those vehicles are a better carbon reduction choice.

We believe the best approach, as the Biden Administration has proposed, is for policymakers around the world to insist on results, and allow innovators like Toyota and our competitors to create diverse solutions to achieve those results.

In years past, when other pollutants were of greatest concern, governments established limits on the amount of emissions permissible and necessary fleet fuel efficiency, and then allowed the private sector, including Toyota, to innovate catalytic converters, engine control systems, and new types of drivetrains to meet those limits.

We believe this proven approach of driving innovation by focusing on outcomes — rather than prescribing particular technology solutions — is the best way to reduce carbon emissions.

Carbon is the enemy, not a particular drivetrain.

Battery Electric Vehicles are wonderful ways to reduce carbon for some customers. Other types of electrified drivetrains are best for other customers, especially in the near future.

I believe we should use all the tools in the toolbox.",9206
"The High-Performance Series

How to design a system to scale to your first 100 million users

In order to keep up with emerging techniques, I would like to update you on this story throughout the year. Last updated on 2021 Jun 28

Photo by Kirill Sh on Unsplash

It is not easy to design a system that supports hundreds of millions of users. It always is a big challenge for a software architect (but it’ll be easy from today after reading my article 🤣)

Here are some topics covered by me in this article.

Start with simplest: all in one.

The art of scability: scaling out, scaling up.

Scaling a relational database: master-slave replication, master-master replication, federation, sharding, denormalization, and SQL tuning.

Which database to use: NoSQL or SQL?

Advanced concepts: caching, CDN, geoDNS., etc.

Today I don’t want to discuss general terms in high-performance computing such as fault tolerance, reliability, high availability., etc.

Keep calm, let’s start now!

Start from scratch

In the figure below, I would like to start by designing a basic app with some users. The simplest way is to deploy the entire app on a single server. This is probably how most of us get started.

A website (include APIs) is running on a webserver like Apache¹ (or Tomcat²).

A database like Oracle³ (or MySQL⁴).

We have both the webserver and the database server on the same physical machine

But there are the following disadvantages with the current architecture.

If the database fails, the system fails.

If the webserver fails, the entire system fails.

In this case, we don’t have failover and redundancy. If a server goes down, all goes down.

Using a DNS server to resolve hostnames and IP addresses

In the above figure, users (or clients) connect to the DNS⁵ system to obtain the Internet Protocol (IP) address of the server where our system is hosted. Once the IP address is obtained, the requests are sent directly to our system.

Every time you visit a website, your computer performs a DNS lookup.

Usually, the Domain Name System (DNS) server is used as a paid service provided by the hosting company and is not running on your own server.

The Art of Scalability

Our system may have to scale because of many reasons like increased data volume, increased amount of work (e.g., number of transactions), and grown users.

Scalability usually means an ability to handle more users, clients, data, transactions, or requests without affecting the user experience by adding more resources.

We must decide how to scale the system. In this case, there are the following two types of scaling: scale-up and scale-out .

scale up vs scale out

Scaling up: add more RAM and CPU to the existing server

This is also called “ vertical scaling ”, it refers to the resource maximization of a system to expand its ability to handle the increasing load — for example, we increase more power to our server by adding RAM and CPU.

If we are running the server with 8GB of memory, it is easy to upgrade to 32GB or even 128GB by just replacing or adding the hardware.

There are many ways to scale vertically as follows.

Adding more I/O capacity by adding more hard drives in RAID arrays.

Improving I/O access times by switch to solid-state drivers (SSDs).

Switching to a server with more processors.

Improving network throughput by upgrading network interfaces or installing additional ones.

Reducing I/O operations by increasing RAM.

Vertical scaling is a good option for small systems and can afford the hardware upgrade but it also comes with serious limitations as follows.

“It’s impossible to add unlimited power to a single server”. It mostly depends on the operating system and the memory bus width of the server.

When we upgrade RAM to the system, we must shut down the server and so, if the system just has one server, downtime is unavoidable.

Powerful machines usually cost a lot more than popular hardware.

Scaling up not only applies to hardware terms but also in software terms, for example, it includes optimizing queries and application code.

Do you need more than one server?

With the growth of the number of users, one server is never enough. We need to think about separating out a single server to multiple servers.

With the growth of the number of users, one server is never enough

With this architecture, there are some advantages as follows.

The webserver can be tuned differently than the database server.

A web server needs a better CPU and a database server thrives on more RAM.

Using separate servers for the web tier and data tier allows them to scale independently of each other.

Scaling out: add any number of hardware and software entities

It also described as “ horizontal scaling ”, we add more entities (machines, services) to our pool of resources. It’s harder to achieve horizontal scaling than vertical scaling since we need to consider it before the system is built.

Horizontal scaling often initially costs more because we need more servers for the most basic but it pays off at the later stage. We need to do trade-offs.

Increasing the number of servers means that more resources need to be maintained.

The code of the system also needs changes to allow parallelism and distribution of work among multiple servers.

Using load a balancer to balance the traffic across all nodes

A load balancer is a specialized hardware or software component that helps to spread the traffic across a cluster of servers to improve responsiveness and availability of the system (include but not limited to applications, websites, or databases).

Use load a balancer to balance the traffic across all nodes

Typically a load balancer sits between the client and the server accepting incoming network and application traffic and distributing the traffic across multiple backend servers using various algorithms. So, it can also be used in various places, for example; between the Web Servers and the Database servers and also between the Client and the Web Servers.

HAProxy and NGINX are two popular open-source load balancing software.

The load balancer technique is a fault tolerance assurance methodology and improves availability as follows.

If server 1 goes offline, all the traffic will be routed to server 2 and server 3. The website won’t go offline. You also need to add a new healthy server to the server pool to balance the load.

When the traffic is growing rapidly, you only need to add more servers to the web server pool and the load balancer will route the traffic for you.

Load balancers employ various policies and work distribution algorithms to optimally distribute the load as follows.

Round robin : in this case, each server receives requests in sequential order similar in spirit to First In First Out (FIFO).

: in this case, each server receives requests in sequential order similar in spirit to First In First Out (FIFO). Least number of connections : the server with the lowest number of connections will be directed to the request.

: the server with the lowest number of connections will be directed to the request. Fastest response time : the server that has the fastest response time (either recently or frequently) will be directed to the request.

: the server that has the fastest response time (either recently or frequently) will be directed to the request. Weighted : the more powerful servers will receive more requests than the weaker ones underweighted strategy.

: the more powerful servers will receive more requests than the weaker ones underweighted strategy. IP Hash: in this case, a hash of the IP address of the client is calculated to redirect the request to a server.

The most straightforward way to balance requests between multiple servers is to use a hardware appliance.

Adding and removing real servers from shared IP happens instantly.

Load balancing can be done as desired.

Software load balancing is a cheap alternative to hardware load balancers. It operates at layer 4 ( network layer ) and layer 7 ( application layer ).

Layer 4: the load balancer uses the information provided by TCP at the network layer. At this layer, it usually selects a server without looking at the content of the request.

the load balancer uses the information provided by TCP at the network layer. At this layer, it usually selects a server without looking at the content of the request. Layer 7: the requests can be balanced based on information in the query string, cookies, or any header we choose as well as the regular layer information including source and destination addresses.

Scaling a relational database

With a simple system, we can use an RDBMS like Oracle or MySQL to save data items. But relational database systems come with their challenges, especially when we need to scale them.

There are many techniques to scale a relational database: master-slave replication , master-master replication , federation , sharding , denormalization , and SQL tuning .

Replication usually refers to a technique that allows us to have multiple copies of the same data stored on different machines.

usually refers to a technique that allows us to have multiple copies of the same data stored on different machines. Federation (or functional partitioning) splits up databases by function.

(or functional partitioning) splits up databases by function. Sharding is a database architecture pattern related to partitioning by putting different parts of the data onto different servers and the different user will access different parts of the dataset

is a database architecture pattern related to partitioning by putting different parts of the data onto different servers and the different user will access different parts of the dataset Denormalization attempts to improve read performance at the expense of some write performance by coping of the data are written in multiple tables to avoid expensive joins..

attempts to improve read performance at the expense of some write performance by coping of the data are written in multiple tables to avoid expensive joins.. SQL tuning.

Master-slave replication

The master-slave replication technique enables data from one database server (the master) to be replicated to one or more other database servers (the slaves) like in the below figure.

All update made to the master

A client would connect to the master and update data.

The data would then ripple through the slaves until all of the data is consistent across the servers.

In practice, there is still a bit of a bottleneck here.

If the master server goes down for whatever reason, the data will still be available via the slave, but new writes won’t be possible.

We need an additional algorithm to promote a slave to a master.

Here are some solutions to implement only one server can handle update requests.

Synchronous solutions : the data modifying transaction is not committed until accepted by all servers (distributed transaction), so no data lost on failover.

: the data modifying transaction is not committed until accepted by all servers (distributed transaction), so no data lost on failover. Asynchronous solutions: commit -> delay -> propagation to other servers in the cluster, so some data updates may be lost on failover.

Keep in mind that if synchronous solutions are too slow, change to asynchronous solutions.

Master-master replication

Each database server can act as the master at the same time as other servers are being treated as masters. At some point in time, all of the masters sync up to make sure that they all have correct and up-to-date data.

All nodes read and write all data

Here are some advantages of master-master replication.

If one master fails, the other database servers can operate normally and pick up the slack. When the database server is back online, it will catch up using replication.

Masters can be located in several physical sites and can be distributed across the network.

Limited by the ability of the master to process updates.

Federation

Federation (or functional partitioning) splits up databases by function. For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.

Federation splits up databases by function

Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality. With no single central master serializing writes you can write in parallel, increasing throughput.

Sharding

Sharding (also known as data partitioning), is a technique to break up a big database into many smaller parts such that each database can only manage a subset of the data.

In the ideal case, we have different users all talking to different database nodes. It helps to improve the manageability, performance, availability, and load balancing of a system.

Each user only has to talk to one server, so gets rapid responses from that server.

The load is balanced out nicely between servers — for example, if we have five servers, each one only has to handle 20% of the load.

In practice, there are many different techniques to break up a database into multiple smaller parts.

Horizontal partitioning

In this technique, we put different rows into different tables. For example, if we are storing profiles of users in a table, we can decide that users with IDs less than 1000 are stored in one table, and users with IDs greater than 1001 and less than 2000 are stored in the another

we put different rows into different tables

Vertical partitioning

In this case, we divide our data to store tables related to a specific feature in their own server. For example, if we are building an Instagram-like system — where we need to store data related to users, photos they upload, and people they follow — we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server.

we divide our data to store tables related to a specific feature in their own server

Directory based partitioning

A loosely coupled approach to this problem is to create a lookup service that knows your current partitioning schema and keeps a map of each entity and which database shard it is stored on.

We can use this method when a data store is likely to need to scale beyond the resources available to a single storage node or to improve performance by reducing contention in a data store. But keep in mind that there are some common problems with Sharding techniques as follows.

Database joins become more expensive and not feasible in certain cases.

become more expensive and not feasible in certain cases. Sharding can compromise database referential integrity .

. Database schema changes can become extremely expensive.

changes can become extremely expensive. The data distribution is not uniform and there is a lot of load on a shard.

Denormalization

Denormalization attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins.

Once data becomes distributed with techniques such as federation and sharding, managing joins across data centers further increases complexity. Denormalization might circumvent the need for such complex joins.

In most systems, reads can heavily outnumber writes 100:1 or even 1000:1. A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.

Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent.

Facebook’s Ryan Mack shares quite a bit of Timeline’s own implementation story in his excellent article: Building Timeline: Scaling up to hold your life story⁶ by using the power of Denormalization.

Which database to use?

In the world of databases, there are two main types of solutions: SQL and NoSQL. Both of them differ in the way they were built, the kind of information they store, and the storage method they use.

SQL

Relational databases store data in rows and columns. Each row contains all the information about one entity and each column contains all the separate data points.

Some of the most popular relational databases are MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB.

NoSQL

It also called non-relational databases. These databases are usually grouped into main five categories: Key-Value, Graph, Column, Document, and Blob stores.

Key-Value stores

Data is stored in an array of key-value pairs. The ‘ key ’ is an attribute name that is linked to a ‘ value ’.

Well-known key-value stores include Redis, Voldemort, and Dynamo.

Document databases

In these databases, data is stored in documents (instead of rows and columns in a table) and these documents are grouped together in collections. Each document can have an entirely different structure.

Document databases include the CouchDB and MongoDB.

Wide-column databases

Instead of ‘tables,’ in columnar databases we have column families, which are containers for rows. Unlike relational databases, we don’t need to know all the columns upfront and each row doesn’t have to have the same number of columns.

Columnar databases are best suited for analyzing large datasets, big names include Cassandra and HBase.

Graph databases

These databases are used to store data whose relations are best represented in a graph. Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities).

Examples of graph databases include Neo4J and InfiniteGraph.

Blob databases

Blobs are more like a key/value store for files and are accessed through APIs like Amazon S3, Windows Azure Blob Storage, Google Cloud Storage, Rackspace Cloud Files, or OpenStack Swift.

How to choose which database to use?

When it comes to database technology, there’s no one-size-fits-all solution. That’s why many businesses rely on both SQL and NosQL databases for different needs.

Look at my below guidance!

which database to use?

Scaling the web tier horizontally

We have scaled the data tier, now we need to scale the web tier too. To make this, we need to move the data of user sessions (states) out of the web tier by storing them in a database such as the relational database or NoSQL. It also called stateless architecture.

a stateless system is simple

Don't use stateful architecture.

We must choose stateless architecture whenever possible because the implementation of state limits scalability, decreases availability, and increase the cost.

In the above scenario, the load balancer can achieve maximum efficiency because it can select any server for optimal request handling.

Advanced Concepts

Caching

Load balancing helps you scale horizontally across an ever-increasing number of servers, but caching will enable you to make vastly better use of the resources you already have, so that the data may be served faster during subsequent requests.

If data is not in cache, get it from database then save it to cache and read from

By adding caches to our servers, we can avoid reading the webpage or data directly from the server, thus reducing both the response time and the load on our server. This helps in making our application more scalable.

Caching can be applied at many layers such as the database layer, web server layer, and network layer.

Content Delivery Network (CDN)

The CDN servers keep cached copies of the content (such as images, web pages, etc.) and serve them from the nearest location.

The use of CDN improves page load time for users as the data is retrieved at a location closest to it. This also helps in increasing the availability of content, since it is stored at multiple locations.

The use of CDN improves page load time for users as the data is retrieved at a location closest to it

The CDN servers make requests to our Web server to validate the content being cached and update them if required. The content being cached is usually static such as HTML pages, images, JavaScript files, CSS files, etc.

Go Global

When your app goes global, you will own and operating data centers around the world to keep your products running 24 hours a day, 7 days a week. Incoming requests are routed to the “best” data center based on GeoDNS.

when your app go global

GeoDNS is a DNS service that allows domain names to be resolved to IP addresses based on the location of the customer. A client connecting from Asia may get a different IP address than the client connecting from Europe.

Putting it all together

Applying all these techniques iteratively, we can easily scale the system to more than 100 million users such as stateless architecture, apply load balancer, use cache data as much as you can, support multiple data centers, host static assets on CDN, scale your data tier by sharding., etc.

Scaling is an iterative process

Which topics will be discussed later?

There are many ways to improve scalability and high performance as follows.

Combining Sharding and Replication techniques.

Long-polling vs Websockets vs Server sent events.

Indexes and Proxies.

SQL Tunning.

Elastic Computing.

Easy, right?",21342
"Layer 2, also known as off-chain solution, is a way to solve scalability and congestion issues for blockchains with heavy traffic and high gas fees. There are several Layer 2 solutions, however, each of them bears its own limitations. By possessing the pros of Layer 2 solutions, such as high throughput and confirmation times, but not the cons, such as inherent dependency issues on the main chain, or additional risks and opportunity costs that come with developing new solutions, ThunderCore offers a better solution. Being a fast, high-performance EVM-compatible blockchain, it now has increased its interoperability with Binance Smart Chain, making cross-chain asset transfers possible with lightning-fast speed.

Whether it is in the eyes of individuals, institutions, or governments, the adoption of blockchains and cryptocurrencies is inevitable. But there are underlying issues, such as scalability, that inhibit such progress. This is why many blockchains deploy Layer 2 (L2) solutions. This article will introduce what L2 means, give a few examples, and explain why ThunderCore is a better solution and more viable for mass adoption.

What is Layer 2?

Layer 2 solution, also known as off-chain solution, describes a protocol that overlays on top of an original, existing blockchain. Why is a secondary network built on top of an existing one? The reason is to solve scalability and transactional speed issues. Take Ethereum (ETH) for example. It is the biggest and most established blockchain so far, but it has significant problems. When the ETH chain undergoes heavy traffic and congestion, users will become victims of high gas fees.

On top of its low transactions per second (TPS), this marks a big underlying issue for ETH by greatly affecting its ability to scale. If you can’t properly scale in the blockchain/crypto industry, your competitors will.

Layer 2 Examples: State channels and Side chains

There are 2 main types of L2 solutions: State channels and Side chains.

State channels are basically two-way channels between participants that allow them to interact with each other in a sealed-off area of a blockchain. Once the interaction is complete, the channel will then report it back to the blockchain. This solution, usually used for payment purposes, removes the need of transacting on a blockchain and the dependency on a miner, thus drastically improving the transaction speed and throughput. Bitcoin’s Lightning Network is a great example.

Side chains, as the name suggests, are secondary chains that will interact with the main chain, forming a relationship like that of a parent and a child. The main chain will then delegate certain tasks to the side chain, therefore, reducing the load for itself. The design space of side chains could be further categorized into two general categories: (1) Plasma style side chain and (2) Rollups, which comes in two flavors: optimistic and zero-knowledge.

Plasma is a generic framework for non-custodial sidechains. Each plasma chain is a smart contract designed for a specific function and can operate independently from the main chain as well. The OMG Network is a great example. However, Plasma suffers from the data availability problem hence Rollups aim to solve this issue.

Polygon (previously known as Matic Network) is another good example of an L2 solution. Polygon aims to establish itself as a platform that is able to launch and host interoperable blockchains. You can think of it as an L2 aggregator where a multitude of side chain technologies are used to solve the Ethereum scaling problem.

Why ThunderCore is Better

There are a few similarities between L2 solutions and ThunderCore. L2 is meant to provide faster transaction speed and lower gas fees; ThunderCore does just that and more. With 4,000+ transactions per second (TPS) and sub-second confirmation speed, it makes it hard to compete with even for other L2 solutions. This clear performance-centric edge makes ThunderCore the optimal space to deploy DApps, allowing developers to reap more profits due to better throughputs and cheaper costs. Along with our Jumpstart Program, developers can bootstrap their DApps to a growing base of daily active users (currently at 70K DAU) and enjoy proper exposure and awareness that is rare elsewhere.

Moreover, L2 solutions such as Plasma derive their security from the main chain (Ethereum), whereas ThunderCore is secured by rigorous mathematical proofs of the underlying consensus protocol and not dependent on Ethereum.

The key difference between ThunderCore and other L2 solutions is that we are a Layer 1 blockchain, making us impervious to dependency issues L2 solutions face. Along with our newly launched cross-chain service with Binance Smart Chain (BSC), ThunderCore now supports assets between three blockchains. With the trifecta of high-performance throughput, low fees, and interoperability, L2 solutions are no longer necessary.",4941
"We have taken time to talk about 5 best free Microsoft Office alternatives for Windows 10 in this article.

Microsoft has played a major role in the significant workplace accessibility for office workers and remote workers. Even in the last decade when their competitors are becoming so tough on them, they are still very significant in their features and expectancy for their office suite software. Since they have had a very proficient portfolio si portfolio existence, people still rate them very high. However, there is a bit of an issue attached to their existence which is their price tag. Of course, everyone wants something easily accessible and very familiar with their work pace but no one is ready to pay the price for it.

This article will be a revelation to the clear alternatives to the Microsoft Office suite but before we proceed, let’s look at the most important Microsoft Office alternatives to the Microsoft Office suite in recent times.

The Microsoft Office suite known as the Microsoft 365 is a very reliable and solid suite that can save and mimic office work even faster than the normal office. It contains Word, PowerPoint, Excel, Outlook and so on which performs almost I’m an office functi9n.

The only difference is that files are changed digitally. Whereas it comes with an extreme advantage that almost every professional wants to work with. Every feature you will need for prime office work is situated ok the Microsoft office suite. This makes the value attached to a higher price point. For about every task you carry out in your office, a price should follow but this has been brought down to something relatable and higher than what everyone is seeing in their alternatives. Especially if someone just needs to use a special part of the features and not all.

No matter the issue you have with the Microsoft office you will still need this alternatives to work with. Even if you just want to step out a bit from this Microsoft office suite to another office suite as well. This is the reason we have written the article for you to choose which of the most significant alternatives to the Microsoft Office suite to choose.

You would also like: How to use 2 WhatsApp Accounts on A Phone (3 Best Methods)

5 best free Microsoft Office alternatives for Windows 10

This is one of the 5 best free Microsoft Office alternatives for Windows 10. This Google workspace is the main reason why Microsoft is never stopping in its plan. Although this is not available offline like Microsoft office suite, it still offers a great advantage over what is expected. Another fact is that they are available for free if they are divided into their sectors.

Let’s point out that the Microsoft Office suite is available for free as well if they are divided into their sectors. For Google workspace, Google docs, Google Sheets and Google slides are free as well as the same applications you find on Microsoft Office suite which are word, excel and PowerPoint respectively.

Let’s delve into the details for google workspace which is a very great alternative to Google workspace. Google workspace contains Google docs, sheets, forms, slides and many other which works only in place of very good internet access. Another way to look at it is the opportunity for an online meeting through google meet. The tools work well in any modern web browser, which makes deployment across an organization a cinch. Unlike any other Microsoft office suite, google workspace is also available in a variety of mobile and computer operating systems.

This is a very important suite that allows for familiar working collaborations with team members on certain projects without any problems. With $4.20 per month, you are free to enjoy all the features of google docs all through the day.

This is another office suite that works well with the Linux OS and has been a very great alternative to Microsoft office suite and google workspace. Although it may lack a bit of the strong portfolios held by both Google suite and Microsoft office workspace, it is still the most used free office suite application. It is very reliable and responsive to anything being carried out. It has a closer feature to that of Microsoft Word.

You don’t have to work too hard on yourself when you have a reliable and most importantly free and open-source Libre office. Its files can be converted to Microsoft files and pdf files without any problem. This makes it easily used and adaptable to any kind of operations in any system. It just works anywhere except that it’s not mobile-friendly.

Even if you have never used a computer system or you don’t have any deep knowledge about office files and documents, WPS still got you covered with its reliable and easy to use features. It is one of the 5 best free Microsoft Office alternatives for Windows 10.

The main significance to point to is that it does not need any technical effort to use. You can always use the free premium for anything you want but if you require more features, then you might as well need to fall into the premium sector. It’s one of the alternatives Microsoft office but it might run slow and still be very easy to use.

let’s delve into a cloud-based office suite that makes work easy, fast and efficient. It is available on Windows, Linux and more importantly mobile Android and iOS. This means it is very adaptable to any kind of setting it is which makes it very easy to use. The only office is an exciting alternative to the Microsoft Office suite and has been specifically coated free. Just like Google workspace, it also requires a standard online source for all its operations.

This is specifically created for Apple and Apple products. It works well on the iOS and Mac OS as well. The office suite of Apple is another competitor it is bringing to the table for the can’t competitor which is Microsoft. A great effort has been put into these to satisfy a healthy working and file management environment for Apple users. Work is very easy to use and has different features such as word processing, spreadsheets, excel and so on but still has a few setbacks to the set-up of the Microsoft office suite. However, what is more, unique for Apple users is iWork and it is proving its purpose all the time.

If you enjoyed our article about 5 best free Microsoft Office alternatives for Windows 10, you would also like: 5 Best VPN Services For Tech Lovers",6409
"Here’s how blockchain innovations bring untapped value to WFP’s humanitarian operations

By Gulia Rakhimova

WFP actively seeks new ways of delivering humanitarian assistance by exploring cutting-edge technologies and innovations, and has done so since its earliest days. Photo: WFP/Jörg Koch.

Those who champion blockchain suggest that it is the next big disruptor after the internet that could transform every industry. The technology is still emerging, and with enormous potential.

The United Nations World Food Programme (WFP) is the world’s largest humanitarian organization, and the one providing the largest blockchain-based cash transfer assistance. As an early adopter of blockchain in the humanitarian and development sector, WFP is exploring further opportunities to trailblaze on this front, to help in our work towards Zero Hunger. Here’s some of our recent blockchain projects that can change the game for good.

So, what’s so special about blockchain that can be transformative for humanitarian work?

Blockchain technology

Essentially, blockchain is a type of database that records information in chronological order by adding a new block of data to an ever-growing chain of data blocks. These blocks are all linked together, they reference each other, and the records are both traceable and irreversible at all times.

What makes blockchain truly disruptive is the ability to ensure data is secure and valid without any central authority. Participants of the blockchain can add records to the blockchain, but cannot alter the blockchain because it exists in multiple computers around the world. In fact, nothing in the blockchain can be added, removed or changed without the visibility of the blockchain’s participants. This community visibility and management make it unnecessary for a centralized party to hold authority over the blockchain, offering a new avenue for transparency and peer-to-peer collaboration.

For WFP, blockchain technology can help us trace food produce from their source, farms, to an end point, such as markets or retail shops. Blockchain can accelerate financial inclusion for food-insecure communities, by recording disbursements of cash based assistance to ensure equitable distribution. Blockchain can also coordinate various humanitarian organizations’ assistance packages that are provided to a population (such as food, protection, shelter, skills development, etc.) which together can ladder up to a better and more sustainable future.

Let’s take a closer look at different blockchain use cases at WFP.

1. Smallholder farmers use blockchain for quality control

Despite producing much of the world’s food, smallholder farmers tend to be food insecure themselves — growing and earning just enough income to make it to the next harvest. Decapolis is a blockchain-powered solution that tracks the quality of the food produce as it passes through the entire supply chain. From the farm to the marketplace, this quality control benefits both the smallholder farmer and the end consumers at markets.

Since the pilot’s launch in February 2020, 100 smallholder farmers have been onboarded, trained and are now using the Decapolis platform in Jordan. Photo: WFP/Mohamad Batah.

Decapolis is an early stage startup that came through the WFP Innovation Hub in Jordan and tailored its pilot plan at WFP’s Innovation Bootcamp. Decapolis is now operational in Jordan under WFP’s Sprint Programme.

Smallholder farmers use the Decapolis platform to register quality assessments such as lab test results, during each step of its production. Because the blockchain is tamper-proof, smallholders can refer to these records to certify their products’ compliance with local and international food safety standards. On the Decapolis platform, Retailers can verify this product quality, can trace the food produce through the supply chain, and can ultimately be assured that they are purchasing certified premium crops.

This is a step in creating access for smallholder farmers to large food market places, as they compete with large scale industrial food suppliers. Blockchain is a significant differentiator here, enabling a stronger buyer trust and the opportunity for smallholder farmers to access bigger markets, generate better incomes, and support local food systems.

2. Blockchain promoting financial inclusion for the unbanked

The EMPACT innovation project connects young people from food insecure communities to the digital economy. In Kibera, Africa’s largest urban slum located in Kenya, the students are trained to perform routine IT tasks via freelance microwork platforms. In doing so, they can earn income online, afford to buy food or even pay for their own housing, when there are no opportunities open in the local job market.

A consistent issue that the EMPACT project faces is that young people in Kibera are “unbanked” — that is, they don’t have bank accounts to receive payments. It is estimated that up to 75 percent of Sub-Saharan Africans are financially excluded and don’t have access to formal financial institutions, credit, or even basic savings accounts. Cash transfer platforms are an alternative, but they incur high costs per transaction that can make receiving payments for microwork prohibitively expensive.

The WFP Innovation Accelerator is working with a startup to explore the use of a blockchain based payment system to solve this issue. “The blockchain would provide students with access to a digital account and significantly reduce the transaction fees for bite-sized microwork payments,” says Gustav Stromfelt, New Ventures Consultant at WFP Innovation Accelerator. To date, the startup has worked with 40 Kenyan university students to pilot an initial concept which they brought to the WFP Innovation Bootcamp in March 2021.

3. Ramping up co-innovation and knowledge sharing with blockchain

The humanitarian and development sector is increasingly applying blockchain solutions in field projects. Sharing knowledge and building on each other’s experience goes a long way to creating new blockchain applications that can advance work towards the Sustainable Development Goals. This is what The Atrium aims to do.

Intended for experimentation, the Atrium is a collaboration platform allowing participants to plant and grow new ideas in a “sandbox” environment, and explore each other’s blockchain applications. The Atrium consists of three components: a web-based community platform with access to learning content and a forum, a GitHub repository of UN blockchain applications, and the underlying infrastructure to set up and run blockchain prototypes.

WFP co-created the Atrium platform with UNICEF and the United Nations Innovation Network (UNIN) to stimulate learning, collaboration, and innovation around blockchain applications across the UN. It is now open to any interested UN staff. Over 30 UN agencies representing 20 blockchain projects and more than 200 users are currently on the Atrium.",6986
"Photo by Joel Muniz on Unsplash

The KitaJaga community is well-known for its peer-to-peer organization and its commitment to achieving food security, stable basic supplies, and provide financial aids. This alleviates the COVID-19 pandemic’s hardship faced by the people. The platform hopes to save lives by allowing individuals who are motivated to help others on their own terms and comfort effectively and aid directly within their communities. We believe in the grassroots movement from the small scale of individuals helping one another, and slowly build communities to tackle bigger issues faced by our society. Our vision is to empower individuals and communities to help one another and build a better future for all of us.

The figures above show the rate of hunger in Malaysia according to the Department of Statistics Malaysia and Global Hunger Index (GHI)

One out of every nine people on the planet still does not have enough to eat. According to the Global Hunger Index, Malaysia was placed 59th. We have a hunger rate of 13.5 percent in 2020, which was deemed to be at a ‘moderate’ level. It is projected to rise by the end of 2021 and reach a ‘serious’ level as a result of the effects of COVID-19, which is now in effect.

Aside from the hunger issue, the current crisis has resulted in high levels of unemployment across the globe as a result of the global economic downturn. Since 2020, the rate of unemployment has risen dramatically, and it is set to continue to rise through the end of 2021. Consequently, most businesses are unable to do business as normal and must lay off part of their employees to alleviate the financial strain placed on the organization. Malaysia, on the other hand, is likewise dealing with the same problem as well. The unemployment rate has risen, and many employees are unable to maintain or feed their families as a result of the lack of a consistent source of income. Because of this, KitaJaga has taken the necessary steps to enable people to assist one another in a peer-to-peer manner by giving nutritional assistance and medicines, as well as establishing a path to peace, stability, and prosperity for those who have survived the recent pandemic, COVID-19.",2212
"How limited WFP seed funding grew the Telecommunications Security Standards (TESS) project into a powerful inter-agency service

By Emma Wadland

On a sun-baked hill overlooking Kigali, Rwanda, workers from dozens of humanitarian agencies set up telecommunications repeaters side by side, dotting the drumlin with equipment. It was 1994 and the country was embroiled in genocide.

Despite the heightened security risks, there were no defined security communications standards or procedures keeping humanitarians safe.

In fact, it was typical to see United Nations agencies and non-governmental organizations show up to emergencies, as they did in Rwanda, with their own equipment and protocols, working in parallel.

“Without guidelines and uniform standards, all the technology is useless,” says Peter Casier, Telecommunications Security Standards (TESS) Senior Programme Manager for the World Food Programme. He was also on that hill in Kigali.

Checking to make sure security communication is up to TESS standards in the Central African Republic, 2020. Photo: WFP/ETC

The TESS project arrived on the scene in 2018, fuelled by seed funding from the World Food Programme (WFP) and a belief that clear procedures and standards can stop the humanitarian sector from repeating security communications mistakes and duplicating efforts at enormous cost.

Mobile phones, for example, are used more and more in field operations but there were no criteria for employing them as a security communications tool.

UN radio rooms, now known as Security Operations Centres (SOCs), never had clear guidelines for what is expected from them even though they provide a critical security service, managing complex data, systems, and applications.

“These people are core to operations: but it was never clear what we expect from them?” Peter says.

Starting with missions in Mauritania, Guinea-Bissau, and Senegal in 2018, TESS demonstrated that proposed upgrades to VHF radio networks were unnecessary and wouldn’t improve efficiencies. And just in these three countries, this observation led to savings of about US$ 1.2 million.

Beyond savings, TESS became synonymous with “inter-agency collaboration” tackling complex technical and procedural issues in technology and security.

In a whirlwind over the next two years, the TESS project –supported by WFP’s Fast IT Telecommunications Emergency and Support Team (FITTEST) and in close collaboration with the WFP Technology Division and the Emergency Telecommunications Cluster (ETC) — standardized security communications infrastructure designed for humanitarians in an astonishing 62 countries, saving the common UN system a tenfold of the annual TESS costs.

Burkina Faso is one of these countries. When terrorist insurgencies increased there in 2019, the UN system opened five new field offices, where under the guidance and support of TESS, the interagency team coordinated by WFP put the new TESS standards to work.

Each of these field offices would have their local Security Operations Centers run from Ouagadougou, the capital city, rather than in field locations. This not only saved the common UN system about US$ 35,000 per month, but also improved efficiency and reduced the number of required field staff.

A watershed moment in June 2020 saw the United Nations Department of Safety and Security (UNDSS) and the Interagency Security Management Network (IASMN) asking TESS to lead an interagency working group that eventually, in less than a year, defined all the UN’s security communications procedures, guidance and standard operating procedure (SOPs). Suddenly, TESS went from a purely technologist service provider role into a position to contribute a critical part of UN security policies.

In another first, TESS and its interagency partners were one of the initial parties to conduct satellite testing of the new Starlink network with SpaceX, well before this service became publicly available. Equally, TESS partnered with other key security telecommunications providers, to help design and test prototype equipment and services, keeping abreast of all newer technologies.

Cargo planes, technicians, logistics and time are all needed to set up communications connectivity in an emergency. This new technology, which TESS is testing, will “dramatically change how we will provide connectivity to operations, and drastically reduce the costs of communications,” Peter says.

Imagine, for instance, being able to pack a satellite the size of a pizza in your suitcase and set up the self-configured solution in a matter of minutes?

While for the first two years, TESS was financed by WFP seed funding, as of January 2020, TESS services have been fully covered by interagency security funds, managed by UNDSS.

Based on TESS’ success, UNDSS and the IASMN requested to convert the project into a permanent service called {TESS+}, which started in January 2021, keeping one eye on field security and the other on innovation and agility.

In addition to being the definitive source on interagency compliance with UN security communications standards, {TESS+} saves WFP US$ 1.76 million per year on investment.

The priceless bottom line, though, are the humanitarian lives protected in operations all over the world by smart security protocols and standardized equipment. As {TESS+} has proven, a little bit of seed funding can go a long way.

To learn more about TESS, go here.",5430
"UNHCR, the UN Refugee Agency, marks its 70th anniversary this year. During the past seven decades, UNHCR has assisted millions of refugees fleeing conflict and persecution on all continents. It has stood the test of time by making the most of new technologies as new challenges arose. This article dives into the past to shine a spotlight on seven technologies that have shaped our work from 1950 to 2020.

1) 1950s — Telex

In the 1950s, Telex, the ancestor of the fax, facilitated the resettlement of tens of thousands of European refugees.

A telex operator in a telegraph station in Stockholm, Sweden in the 1950s. © telehistoriska

The first decade of UNHCR’s work was marked by the crisis in Hungary. As Soviet troops invaded Budapest in 1956, tens of thousands of Hungarians crossed the border to neighboring Austria and Yugoslavia.

This is where Telex played a key role. For about 150 years, one of the faster media for long-distance communication was the telegram, and Telex machines appeared in the 1930s to make them even faster. Telex sent telegrams by electricity and radio signals.

In November 1956 UNHCR sent an appeal by telegram to 20 States: “In our and Austrian Governments opinion extremely effective help would also be provided if governments sympathetic to the trials of Hungarian people would agree to give at least temporary asylum to the greater possible number of refugees STOP.”

Within days of the exodus, an extraordinary movement of solidarity arose to shelter and help resettle the refugees. As a result, 180,000 Hungarians found refuge in 37 different countries.

2) 1960s — Off-road vehicles

In the 1960s, off-road vehicles helped provide assistance to refugees affected by new conflicts amid decolonization in Africa

Refugees in Elisabethville, Congo (today Lubumbashi, DRC) gather around a vehicle from which a UN official announces the names of refugees to be repatriated. © UN Library, 1962

During the 1960s, UNHCR’s focus shifted away from Europe to Africa. UNHCR was called to provide assistance to millions of refugees fleeing the violence sparked by wars of independence in Algeria, Congo, Angola and Nigeria.

To assess the needs of and provide assistance to refugees across vast territories with poor road infrastructure, heavy-duty off-road vehicles became essential. While in the beginning UNHCR mostly purchased Land Rovers, it switched in the 80s to Toyota Landcruisers, deemed more suited to deep field conditions.

Today, UNHCR’s fleet is made up of 6,000 light vehicles — 85 percent are off-road cars. These cars include as few electronics as possible — with the exception of a radio and a tracking system — to minimize the need for repairs.

3) 1970s — Polythene sheet

In the 1970s, polythene sheeting allowed for more resistant emergency shelters

Instructions to assemble a temporary shelter presented in Oxfam’s technical guide on Plastic sheeting (first edition 1973) © Oxfam

In the 1970s, UNHCR became involved in large-scale relief operations in Asia. UNHCR expanded its activities to protect refugees fleeing Vietnam, Bangladesh, Laos and Cambodia. In 1971, India, UNHCR and other aid organizations set up 800 camps to shelter part of 10 million Bengali refugees. Crowded conditions led to serious cholera outbreaks.

Providing better temporary shelters became a priority. In the 70s, aid organizations, which until then mostly relied on reinforced cotton fabric — expensive and prone to rotting — started using polythene sheeting to shield refugees from extreme weather and the spread of disease. The polythene sheeting used in the 70s was mostly agricultural film, which remained fragile.

Shelter technology evolved a lot in the 90s when UNHCR and Médecins Sans Frontières (MSF) wrote their own specifications for manufacturers. An MSF logistician discovered that black fibres coated in white were more resistant and reduced the temperature inside shelters. Hence the infamous white humanitarian tarpaulin.

4) 1980s — First computers

In the 1980s, UNHCR started shifting from paper to computers, revolutionizing the way we plan, communicate and manage information

An aid worker in Thailand types information on a computer. © ICRC/ Thierry Gassmann, 1986

In the 1980s, the population of displaced persons continued to grow. UNHCR received the 1981 Nobel Peace Prize in recognition for its work on different continents. The beginning of the decade was also marked by the Soviet invasion of Afghanistan, which displaced 6,3 million Afghans to Iran and Pakistan.

As UNHCR’s work became more complex, computers were introduced in the 80s first to manage payroll and later on, to improve emergency planning and statistics. The first software used by UNHCR were WordPerfect (a word processing application), Lotus 1–2–3 and MultiPlan (spreadsheet programs).

The organization took some time to make the move towards computers. In a memo from 1976, a UNHCR consultant advised against “rushing the process” while noting that: “In the long-run, the advantages are on the side of the computer, provided it is intelligently used.”

5) 1990s — Satellite phones and high frequency radio

In the 1990s, satellite phones and HF/VHF radios transformed UNHCR’s communications as it upscaled operations in war-torn countries

A UNHCR colleague in a radio room in Bosnia gives a call over a satellite phone while smoking a cigarette. Other technologies in the room include: a HF radio, a VHF radio, a laptop, a teleprinter and a fax. © UNHCR/ Anneliese Hollmann, 1994

In the 1990s, the end of the cold war created new challenges. Sadako Ogata, then the UN High Commissioner for Refugees, declared: “On the one hand, a new spirit of international cooperation has fostered opportunities for conflict resolution. On the other, new sources of tensions and conflicts are emerging.”

In Bosnia, Somalia and Rwanda, UNHCR mounted relief operations in the midst of ongoing wars.

In the 1990s, good communications became vital to manage programs and ensure staff security. To send messages between UNHCR’s 200 field locations and headquarters, staff used shortwave radio transmission.

By 1995, UNHCR had equipped some 1,200 vehicles with high frequency radio transceivers, allowing staff to maintain contact with their base within a 1,000 km perimeter.

Portable satellite phones became particularly useful in emergency conditions. But in 1995, the sets were still limited to a single voice channel (one person at a time) and were very expensive to use (from $ 6.50 to $ 10.00 per minute).

6) 2000s — Biometric registration

In the 2000s, biometric technology enhanced refugee registration and paved the way for a more efficient delivery of aid

A UNHCR colleague in Ethiopia takes the fingerprint of a young Somali girl in a screening centre. © UNHCR/Petterick Wiggers, 2009

In the 2000s, displacement levels initially declined before escalating again due to a series of conflicts, including the invasion of Iraq.

In a 2003 report to the General Assembly, UNHCR highlighted the challenge of mixed movements — where refugees and economic migrants travel the same routes — and its concern that asylum policies might become subordinate to migration control.

To enhance refugee registration and protection, UNHCR started using biometric technology in 2002. This consists of recording refugee’s fingerprints and running iris scans. Biometric registration enabled more secured identity documents. It also became an entry point for the delivery of other kinds of assistance, most notably cash transfers.

7) 2010s — Digital education

In the 2010s, internet connectivity became a lifeline for refugees and offered access to transformative services, such as digital education

Young refugees use tablets provided by the Instant Network Schools © INS

The 2010s were marked by the war in Syria, which displaced 12 millions, and the South Sudanese civil war, which broke out two years after the country’s independence, displacing 4 millions. As both conflicts prolonged over years, millions of refugees were left in limbo in neighboring countries.

A 2016 study by UNHCR and Accenture, revealed that refugees see connectivity as a critical survival tool. In the last decade, providing mobile networks and internet to refugees became a priority of UNHCR. Among other innovations, internet connectivity has enabled digital education services.

The Instant Network Schools, set up in 2013 by UNHCR and Vodafone Foundation, aims to give 500,000 refugees and teachers access to digital learning in marginalized communities in Africa by 2025. Other initiatives like the Connected Learning in Crisis Consortium bring together universities which enroll refugees in online Bachelor and Master’s programs.

2020s — What’s next?

In 70 years, technologies have changed a lot but UNHCR’s mission to help refugees survive and thrive remains. What iconic technologies will shape UNHCR’s work in the next decade? Artificial intelligence, robotics and smart devices may be on the podium. But while technologies have and will continue to facilitate UNHCR’s mission, all credits go to UNHCR colleagues — the human brains and hearts behind our work.

***

With thanks to:",9193
"Before we start let’s check the first pain point that when we create an automation project to assure our integration testing (microservice/API) with an environment dependency (like using development, staging or something like that). We’ll have a lot of problems and I can give some examples about that, let’s see:

A colleague changes something, pushes that code into that environment and then break our tests. One dependency isn’t working correctly (API gateway, database, queue stuff etc). We won’t believe anymore in our tests because we have a lot of flaky tests (they sometimes pass and sometimes fail). We need a lot of uptime in our environments (if we have to run at 3 a.m. that automation, will the environment dev and staging be up for that or do I have to be up for that environment manually this time?). We have a lot of problems into the pipeline (CI/CD) because of all the points that I’ve mentioned above.

So, my colleague, our problems have finished. We can build and simulate a whole environment that our application needs with TestContainers, some like databases, queues tools, keycloack or any other authentication tool and an infinity of containers that we can use to make all tests to assure our endpoints/resources/behavior/responses from our microservices. And when that has finished we can stop all containers at the end of the execution. The main point here is that we can use this concept with any programming language (Java, NodeJS, Python, C#/dotnet and others). Let’s see a code example structure:

Code example using Java with Spring Boot Application:

Starting dynamo database with TestContainer using the image amazon/dynamodb-local:latest:

2. Starting the Microservice (Spring Boot Application) with the database that we started using TestContainers and with Sprint Boot Test Property passing the dynamos’s endpoint:

3. Before starting the test we must setup the dynamoDB with some index or elements (in this case we used a NoSQL Database). There is a class that I use AWS library to put the necessary structure DynamoDataBase.createDynamoDataBase:

4. So, now in the test method using JUnit or other unit testing tools we can make HTTP request using any library (RestAssured, RestTemplate) that the programming language provide us and test our microservice/API.

Finally, we can easily concern some good points using these TestContainers approaches:

We won’t spend money on maintenance of any environment uptime 24 hours per day. We have our own environment all time that we want. The developer will help us more frequently to assure that the tests will pass because whole test structure will be in the same repository. We can setup a mock to external integrations in an easily way. (using WireMock for example). We will trust much more in our tests into the pipeline or in any other execution.

Some points that we really need to pay attention to using pipelines with this TestContainers approach:

The agent pipeline will start all containers locally that we need for our tests, but imagine a lot of projects starting and finishing this kind of thing. So we can sometimes have a problem trying to start a container with the same port or the agent pipeline doesn’t have a lot of memory to parallel jobs, it can be a problem.

Link to TestContainer: https://www.testcontainers.org/",3319
"Are you benchmarking your organization in the best way possible? Are you tracking almost the exact metrics everyone else is tracking? Are you consistently measuring, monitoring and communicating your Key Performance Indicators (KPI) that drive performance in your blood center? Do you counterbalance your KPIs for maximum effect?

As we work with blood centers to digitize their operations, we have observed that they have some KPIs that are measured, with the majority of these leaning toward donor acquisition and related activities. To gain maximum value from your bench-marking efforts, we recommend you follow a few best practices that we’ve learned over the last fifteen years while working with organizations in and outside of the blood industry.

Choose your KPIs that are related to your specific center’s key goals and have KPIs that support those critical goals for each department and business unit. KPIs can vary based on your size, competitive landscape, specific areas that need attention, and they need to correspond to the culture you want to nurture. Pick 2 or 3 KPIs that can be consistently measured and monitored, and that are easy for your organization to understand and implement. By starting with a few simple KPIs, and gaining buy-in across the organization, you can create a culture driven by numbers. Implementing KPIs can often be met with resistance from members in the organization who might feel they are under the microscope but we’ve found that once implemented and well communicated, employees grow to appreciate having clear, measurable goals. Lack of ambiguity leads to higher productivity and better decision making as well. For every KPI you implement, counterbalance it with another KPI. If you measure the number of defects, deviations or other quality related metrics, we recommend that you also estimate the cost of preventing quality issues. KPIs such as Cost of Quality as a % of Annual Revenue, or Average Training Hours per Employee, will provide a needed counterbalance to the cost of defects. A defect or a waste typically translates into hard expenses for your organization, but your blood center also invests money and resources to prevent such issues. Is this investment tracked? If not, why not? We are big fans of combining both lagging and leading metrics. This way you can almost predict with a level of certainty where you are going and adjust before you get there as needed using the leading indicators. Implementing KPIs is only the first step. KPIs gain more value over time as you collect data, and unless you drive consistency and communicate the value of your metrics to everyone in the organization, there is a good chance that your gains will be short-lived. Communicate the importance of these metrics to your organization, and don’t settle for anything less than 100% compliance in bench-marking activities and they will become embedded in your culture. It is essential to use the right tools to record and communicate your KPIs to the rest of the organization. Business intelligence software and workflow systems, if set up correctly, are beneficial. We are sharing our latest experience with the City of Tyler, Texas where we helped them track the complete life-cycle of KPI scorecards and quality standards, at the next ASQ World Conference in May 2019.

Here is a list of common KPIs that our blood centers can track. Some can be assigned to departments, and others are attached to the blood center as a whole and managed by the leadership.

Cost Per Unit Collected (CPUC)

DASH rate (donations per staff hour)

Percent of waste or discarded products

Cost per donation

Blood product demand period

Number and percentage of adverse donor reaction

Percent of reactive units

Percent of individual product type vs. total inventory

Percent of defect-free processes

Cost of quality as a percentage of annual revenue

Number of deviation and percentage of deviation vs. total number of processes

Overall turnaround time from donation to product completion (internal)

Deferral percentage

Cost per mobile donation vs. cost per fixed-based location.

We would be keen to hear about other KPIs that your center is currently tracking. Also, there are a couple of KPIs that we believe that every blood center should monitor.",4290
"From proof of concept to proof of complexity

Our team is thrilled to announce that we have produced the world’s first cell-cultured human milk outside of the breast. We find ourselves at the frontier of lactation science, helping to advance the entire field by putting mothers and babies at the forefront.

According to UC Davis Human Milk Researcher & BIOMILQ Technical Advisor Dr. Jennifer Smilowitz, “human milk is tremendously complex in both composition and structure which has made it impossible to replicate outside of the lactating parent.” Until now.

In just 11 months, we have moved the field of lactation science from proof of concept to proof of complexity, producing milk compositions that look more like a symphony than a concerto. We’re a HUGE step closer to empowering parents with another infant feeding option, one that provides much of the nutrition of breastmilk with the practicality of formula.

Dr. Leila Strickland and Michelle Egger stand outside BIOMILQ HQ proudly

Our Co-Founder and Chief Science Officer Dr. Leila Strickland, a cell biologist who has been envisioning milk production outside the body since struggling to breastfeed her own children, shares BIOMILQ’s approach: “Our core hypothesis has always been that milk is greater than the sum of its parts, which all work together as a dynamic system. Our latest work demonstrates that much of the complexity of milk can be achieved by replicating the intricate relationship between the cells that produce it and the conditions they experience inside the body during lactation.”

We’re not creating a handful of components like many players in this space; like many human milk researchers, we believe human milk is a complex biological system and should be brought to the world as a whole milk product that maintains the integrity of it’s incredible evolutionary origin.

We can now confirm that BIOMILQ’s product has macronutrient profiles that closely match the expected types and proportions of proteins, complex carbohydrates, fatty acids and other bioactive lipids that are known to be abundantly present in breastmilk.

This gives us the confidence to say ‘we’re making milk!’

Breastmilk, which is rich in human milk proteins, bioactive lipids, and human milk oligosaccharides (HMOs), is known to support better immunological development in a child. After reviewing BIOMILQ’s latest results, Dr. Smilowitz shares that “BIOMILQ has shown it can create a product that contains a comprehensive suite of human milk proteins that not only function to nourish but also protect infants.” BIOMILQ, unlike infant formula, also contains polyunsaturated fatty acids (PUFAs) that are known to have anti-inflammatory qualities that promote and sustain healthy development. In our proprietary lactation system, our cells are making numerous HMOs, including those unexplored, such as one that was recently found to correlate with speech acquisition.

Who knows what benefits we will discover with our breakthrough mammary biotechnology?

But let’s be clear, it’s not bio-identical to mother’s milk — we’re not confident it can be. Hormonal changes, baby’s cues, skin-to-skin contact, and environment all affect the dynamic complexity of breastmilk. Our product will not display the real-time composition dynamics that occur during feeding in response to the intimate relationship between mother and child and their unique environment. But, because BIOMILQ’s product is produced outside the body in a sterile controlled environment, our milk will be free from the environmental toxins, food allergens, and prescription medications that are often detected in breastmilk.

We’re not making breast milk. We’re making BIOMILQ, produced by human mammary cells, rich with the nutritional profile most suitable for supporting human life and absent of the environmental damage created by the dairy industry. Michelle Egger, Co-Founder and CEO, shares: “We have no intention to replace chestfeeding, so we’re comfortable with the differences between our product and breastmilk. Instead, we intend to offer parents another supplemental feeding option to nourish healthier babies, empower parents through choice, and contribute to a healthier planet.”

Zakiyah Williams, Lactation Services Manager at Stanford Children’s Health, IBCLC, and Technical Advisor to BIOMILQ, is thrilled about what our latest data confirms: “It is amazing that there will soon be a product to support families and provide an alternative to formula. It is not a replacement for their milk, but has implications for other families in which their milk is not an option. From adoptive families, babies with allergies and birthing parents with contraindications to providing their own milk, this is a game changing option!”

We know many parents hope to nourish their babies with our product — but, we’ll admit, we’re only in our first trimester.

We still have many exciting challenges to work through to further optimize our nutritional profile and confirm which additional bioactives, and their benefits, exist in our product. We’ll need to scale our production to make our product accessible to any parent looking for another feeding option, regardless of their circumstances. We have extensive safety and efficacy testing to prove to our team, government regulators, and our consumers that our product is a safe alternative to bovine-based infant formula.

We, as pioneers unlocking human potential with breakthrough mammary biotechnology, take doing right by people and planet both seriously and personally.",5552
"Ombori on Creating Operational Efficiency

All businesses strive for some level of efficiency within their company, whether that means ensuring that owners are not doing everything themselves or using the right technology to streamline processes. The leaner and more efficient your company is, the more likely it is to succeed over the long term.

While efficiency is a good overall goal, defining what that means for your company can be tricky. What is very efficient for one company may not work well at all for another business. Individual companies have to define what operational efficiency means for their unique company, industry, and goals. Once these metrics have been defined, business owners and operators can work toward creating operational efficiency.

The High-Level Overview: What is Operational Efficiency?

At the very basic level, operational efficiency is the relationship between an organization’s input and output. Operational efficiency is the goal to produce a high-quality product with as few resources as possible. Operational efficiency generally leads to increasing revenue by cutting down on unnecessary costs.

Distinguishing Between Productivity and Efficiency

Productivity and efficiency are often used interchangeably in business. However, they are very different concepts. Productivity essentially means that you are doing more with the same resources. Efficiency, on the other hand, means doing more or achieving the same level of production with less or fewer resources.

Productivity really focuses on performance. It might mean increasing costs, such as by improving current equipment or asking employees to go through more training. Although there is some “upfront” investment, the cost is justified because the overall output increases.

Efficiency, on the other hand, generally does not make any upgrades. Instead, it focuses on producing at the same (or increased) level, but with fewer resources. The most common example is in the manufacturing context: producing the same amount of product with fewer machines.

The Importance of Focusing on Efficiency First

It can be tempting to try to focus your efforts on both productivity and efficiency at the same time. After all, they can both lead to increased revenues and a more attractive bottom line. However, if you want to improve the overall performance of your company, focusing on efficiency first will be a better use of your time and resources.

By creating as much as you can with the resources you have, you create an optimally efficient environment that can be improved upon later. Going extremely lean first will help you use resources effectively. Then, you can increase productivity by building upon an already highly efficient operation.

Start by Creating a Baseline of Operations

Your baseline of operations includes all of the functions that must be in place for your company to run. This requires an in-depth understanding of what each department does and how it contributes to the overall goals and functions of the company. In smaller businesses, the roles of each department may overlap quite a bit.

Creating the baseline of operations requires an understanding of:

The purpose of each department (or person)

Responsibilities of each department

Steps that each person or department must carry out to fulfill their role within the company

Once you have this high-level overview of understanding, you can move on to actually calculating efficiencies.

Creating Metrics to Measure Efficiencies

All of the understanding of operations and cost-cutting in the world cannot do any good if companies do not have a meaningful way to measure relative efficiencies. This step is necessary to have the means to evaluate efficiency. Without some type of metric to assess efficiency, you cannot tell whether improvements are actually being made or measure the overall health of an organization.

To start, you must decide which input and output measures are the most relevant for your company. Every industry and individual company are different, and many efficiency measures will not make sense across more than one industry.

Key performance indicators might include things like:

Cost of goods sold

Hours necessary for production

Average transaction values

Overtime or labor hours generally

Transportation costs for products

Gross margins on investments

Conversion rates

Cost of raw materials or producing specific component parts

Foot traffic and digital traffic

Inventory turnover

Reviewing metrics from other comparable companies might be a helpful starting point. Review annual reports or quarterly updates to shareholders of companies in the same industry. Determine what they see as performance indicators and what kind of measurement they think is good or bad. There are also many resources available that provide industry-wide averages for various indicators that might be helpful for your company as well.

As you work through measurements like time and quality indicators, you can begin to spot bottlenecks that make processes slower.

Addressing Operational Bottlenecks

Bottlenecks in operations are critical points where efficiency can be improved. They might be unnecessary tasks or points in the product lifecycle that can be improved by altering current processes.

Consider an example. Imagine that workers have to have two quality approvals from different department managers before a final product can be shipped to a customer. Getting these approvals is time-consuming and delays getting products out the door by at least a day each time a batch is ready.

To address this bottleneck, you might be able to eliminate one of the approvals required. Alternatively, perhaps you could ensure that one approval happens as part of an earlier quality check and not as part of the final check. As a third option, you could incorporate a quality control worker at the end of the line whose job it is to instantly do a quality review, instead of moving the product from one location to another or waiting for specific people to come to the product.

Creativity is often very helpful in addressing bottlenecks that curtail operational efficiencies. You should also keep in mind that removing or reducing bottlenecks will generally require working closely with a particular department or employee to develop a new process or strategy to accomplish the same goal.

Once you have addressed a bottleneck, be sure to continue measuring key performance indicators to check for improvements. Continue communication with your team to make efficiencies even better as you work through implementing the new solution.

Implementing Technology to Increase Operational Efficiency in Employees

Technology can play a huge role in increasing efficiencies. Although implementing new technology may be seen as a move toward productivity rather than efficiency, it can actually increase both concepts. Using the right type of technology can not only help increased output, but it can cut down on costs as well.

Although operational efficiency is often viewed from the perspective of manufacturing or similar industries, the retail industry can benefit significantly from increasing efficiencies even at the tail-end of the product cycle. Increasing employee efficiencies is perhaps the easiest place to start improvements in retail.

Scheduling and Appointment Setting

Implementing technology that allows employees (and customers) to be more organized can have a profound impact on efficiency and productivity. A simple switch to a scheduling software or appointment setting technology allows employees to not only track their schedules better, but they can often communicate with customers better as well.

Curbside Pickup and BOPIS Services

Customers get a lot of value out of services like curbside pickup or BOPIS (Buy Online, Pickup In-Store) options. However, employees can also significantly increase efficiencies in serving customers when clients use this service as well. It allows them to coordinate services in a way that best fits the customer’s needs. It also allows business owners to schedule more effectively to meet client demands.

People Counting Systems

People counting technology allows business owners to know exactly how many people are in their brick-and-mortar store at any given time. Over time, companies can spot trends, including situations where more workers are necessary to meet demand. The workforce can be decreased on slower days as well.

Real-time people counting technology can also help companies spot bottleneck customer service areas and address them immediately. Over time, you can see trends that create bottlenecks and stop them from occurring before they cause any additional waiting time for retail customers.

Queue Management

An effective queue management system will allow your team members to work much more efficiently. Instead of herding clients and dealing with angry customers who have had to wait, a good queue management system will stop bottlenecks before they happen and allow your team to address client needs proactively instead of reactively.

Using Ombori Technology to Increase Operational Efficiency

If you are looking for ways to make your company more efficient and productive, Ombori may have the solution for you. The team at Ombori will work with you to develop the personalized system you need to decrease costs and increase outputs. Learn more by contacting Ombori today.",9458
"CosmWasm is a new smart contracting platform built for the cosmos ecosystem. If you haven’t yet heard of it, please check out this intro. The purpose of this article is to give a deep dive into the technology for developers who wish to try it out or integrate it into their product. Particularly, it is aimed at Go developers with experience with the Cosmos SDK, as well as Rust developers looking for a blockchain platform.

CosmWasm was originally prototyped by Team Gaians at the Berlin Hackatom 2019. In particular, Aaron Craelius came up with the architecture, especially avoiding reentrancy, Jehan Tremback led the rust coding, and Ethan Frey led the go side of the implementation. After the successful prototype, the Interchain Foundation provided a grant to Confio to implement a robust version that would work in an adversarial environment. This article introduces developers to the output of that grant work, and lays out possible future directions.

How to use CosmWasm

Photo by Rob Lambert on Unsplash

CosmWasm is written as a module that can plug into the Cosmos SDK. This means that anyone currently building a blockchain using the Cosmos SDK can quickly and easily add CosmWasm smart contracting support to their chain, without adjusting existing logic. We also provide a sample binary of CosmWasm integrated into the gaiad binary, called wasmd , so you can launch a new smart-contract enabled blockchain out of the box, using documented and tested tooling and the same security model as the Cosmos Hub.

You will need a running blockchain to host your contracts and use them from an app. We will explain how to set up a local “dev net” in the tutorial. And plan to soon release a hosted testnet, to which all developers can simply upload their contracts, in order to easily run a demo and to share their contract with others.

Once you have a CosmWasm-enabled blockchain, you can deploy a custom contract. This is similar in principle to Ethereum, but there are a number of differences in the details. Since most people are familiar with that flow, let us look at some of the main similarities and differences:

Comparison with Solidity Contracts

Ethereum and Solidity are what most developers think of when they hear “Smart Contracts”. Since most readers have some experience using this system, I will highlight the similarities and the differences.

First of all, the deploy-execute process consists of 3 steps rather than 2. While Ethereum was built around the concept of many unique contracts, each possibly custom-made for any bilateral agreement, the reality seems to show that writing a bug-free contract is harder than originally thought, and a majority are copies of standard templates like OpenZepellin. With that in mind, and conscious of the overhead of uploading and validating wasm code, we define the following 3 phases of a contract:

Upload Code — Upload some optimized wasm code, no state nor contract address (example Standard ERC20 contract)

Instantiate Contract — Instantiate a code reference with some initial state, creates new address (example set token name, max issuance, etc for my ERC20 token)

Execute Contract — This may support many different calls, but they are all unprivileged usage of a previously instantiated contract, depends on the contract design (example: Send ERC20 token, grant approval to other contract)

Just like Ethereum, contract instantiation and execution is metered and requires gas. Furthermore, both instantiation and execution allow the signer to send some tokens to the contract along with the message. Two key differences are that sending tokens directly to a contract, eg. via SendMsg , while possible, does not trigger any contract code. This is a clear design decision to reduce possible attack vectors. It doesn’t make anything impossible, but requires all execution of the contract to be explicitly requested.

Contract Security

Code correctness is paramount in blockchain smart-contracts. As the source code is public and there is no “admin” to fix things, there must be no bugs in the execution paths that can be exploited to produced undesired (or undefined) behavior. Many teams look to use Turing incomplete languages to perform deeper analysis and remove many classes of bugs. We choose to use a Turing complete language, like Solidity, but with security as a primary goal. This includes limiting possible attack surface, but also very strong in-built unit test capabilities to easily shake out bugs before deployment.

While CosmWasm development has a steeper learning curve than Solidity (it definitely takes more work to get your first sample contract running), it is designed to be more productive for devs with a couple weeks of experience who wish to produce production-ready code. And we hope to have an architecture and tooling to avoid the need for a whole industry of “CosmWasm smart contract auditors”, as it should be easy to just “do the right thing”.

One major class of attacks in Ethereum (and probably the most infamous due to the DAO hack) is reentrancy. CosmWasm was architected to eliminate the possibility of this class of attacks.

Much like Ethereum, we take resource usage seriously. As such, we provide hard limits on memory usage, and gas-based limits on CPU and Storage.

For those who take security very seriously, you can read a longer list of how CosmWasm stacks up against all known Ethereum attack vectors.

Getting Started with CosmWasm

If you are anxious to get started, you can jump right in with our first tutorial. This will walk you through modifying an existing contract, compiling it, deploying it to a local “dev net” and running the contracts via a command line tool. If you want more theoretical background, you can read up on the architecture design.

Writing Contracts (Rust)

Writing your own contract is quite easy if you have a working knowledge of rust. If you don’t, it should still be relatively straightforward to make minor changes to existing contracts, just picking up syntax on the fly. We do walk you through the basics of rust and explain editing a contract in the tutorial, but if you are an advanced rust developer and want to dive into the code (and review the quality), here are a few pointers to navigate around the CosmWasm code base.

cosmwasm is a library providing all modular code needed for building a contract. And cosmwasm-template contains a starter pack to quickly set up a minimal contract along with build system and unit tests, so you can start writing custom logic directly. Both of these libraries offer deeper documentation on how to build them. If you want to write you own contract, follow the instructions on cosmwasm-template and just start editing contract.rs .

To get a feel of how a contract can be built, take a look at the code for a simple escrow. State is what is persisted in the database. InitMsg is sent once to create the contract from the generic code. This contains info on the parties to the escrow, as well as the timeouts. HandleMsg is an enum containing all possible messages that can be sent. Rather than calling functions directly, we can match on the enum to execute the proper logic for each call. Benefits here are the easy ability to serialize the call, as well as a clear definition of which functions are public. Finally, QueryMsg provides an enum to allow multiple ways to query the state of the contract (each potentially executing code on a read-only store).

The entry points are defined in lib.rs . They handle some standard translations between rust types and the wasm external FFI interface, but maintain no real logic there, just allow you to work with Vec<u8> and Result<Response, Error> rather than raw pointers and manually serializing error messages over the FFI boundary. The real logic is in your contract.rs file. init is the entry point to construct a new contract from this code, and should define all configuration options. handle loads state and matches over all supported enum values to execute an action on the contract. After which we can try_approve to release the funds to the beneficiary, or try_refund to return the funds to the original sender, if the escrow has expired.

Note that one can write complete unit tests in pure rust, running with the entire test toolchain and full backtraces, for ease of development and testing. cargo unit-test will do just that in the escrow example. You can also compile to wasm with cargo wasm and run a full suite of integration tests on the resulting wasm bytecode. The integration tests have a similar syntax and ease of development as unit tests, but lack stack traces and some other instrumentation on the contract code.

Deeper Integration with your Chain (Go)

Now that we have covered developing custom contracts in Rust, let us turn to the potential extensibility on the Go side for experienced Cosmos SDK developers. The provided cosmwasm module is a minimal, unopinionated implementation of bindings between the SDK and the smart contract VM. It is embedded in wasmd , which is a fork of gaiad with x/wasmd added. It takes care of all the implementation details, but leaves the field open for you to fork this repo and add custom business logic around it. Below are some ideas on how this could be customized:

Add Permissioning or Fees — Are you building a platform where anyone can upload a contract? Or do you intend to use this feature to let on-chain governance add new features without organizing a full chain upgrade process? Consider modifying the handler to deduct fees when uploading code or instantiating a contract. Or maybe just make uploading code a governance handler (proposal type). Current implementation allows anyone to upload code and instantiate contracts for free — great for testnet, not so great for mainnet.

Add storage limits — Current gas limits in the SDK limit how many reads and writes can be performed in one Tx (or one block). However, they do nothing to limit total storage. A contract could eg. write 20 chunks of 500 bytes to disk. And next time another 20, and so on. Since you pass in the KVStore to the contract, you could wrap it with a layer to provide some limitations. Like only one write (or one write to a new key) per contract execution. Or maybe a total limit of keys stored in the contracts KVStore over all executions. Or maybe the creator needs to prepay for storage space (buy or rent) and this defines the limits. All this business logic can be written in go without any changes to the underlying contracts (except preventing some that violate these limits).

Support OpaqueMsg — The current CosmWasm spec allows returning an OpaqueMsg variant. This is a message type that is never parsed or created by the smart contract code, just passed through from client to contract to SDK. You can use this for eg. multisigs, where the client proposes some message (maybe a staking issue), which must be approved to be executed with the permissions of the contract. Just as the contract then can dispatch a SendMsg , it can dispatch such an OpaqueMsg as well. This requires no changes in the VM or contracts, but a clear format that the SDK module parses out and then a router with multiple modules to dispatch it to. And then some client side support to construct (unsigned) messages in that format as part of the body of the contract calls. Is it the go-amino json representation of a sdk.Msg implementation? Base64-encoded go-amino binary representation? Or some completely different encoding? As long as your module and your client agree on the format, it is totally opaque to the CosmWasm VM. The current implementation leaves it as a TODO, for chain developers to customize how they want.

Pretty much all the crypto-economic and governance design decisions can be implemented by forking the Go module. If you have ideas, please open an issue, or just fork the code and implement it. We’d love to review your PR.

Future Work

CosmWasm, both the VM as well as the platform, is at a usable alpha-state now and we are working on refining the last issues to make it production-ready, with your feedback. Smaller fixes needed to make it ready for mainnet are being tracked in this project. In addition to that, we want to build tooling around it, and iterate on new features, ideally focused on the needs of real users. Some of the main points on the current roadmap are:

Launch and maintain a public testnet, so anyone can experiment with contracts.

Add support for existing cosmos tools, like block explorer and wallet on this network.

Build collection of standard contracts to inspire development (like OpenZeppelin), with documentation and tutorials to help onboard new developers.

Create a website to verify rust code behind wasm byte code, like etherscan. The build system is already built, but a web frontend is needed.

Set up a registry to share contracts (or provide a nice way to use crates.io).

Provide simple JS APIs to instantiate and execute contracts to enable dApp development, along with a demo dApp.

Provide integrations with IBC (when golang implementation is stable) to allow multi-chain interactions by smart contracts.

We are also very eager for feedback from the community to influence some designs, like queries, precompiles, and even a second language for contract development . As well as concrete use cases and apps that would motivate deeper integrations with the standard Cosmos SDK modules.

Contact Us

Please read the docs, and play with our starter kit. If you have any questions, or if you want some advice on integrating CosmWasm with your app, or building on top of it, please don’t hesitate to get in touch.

You can email us at cosmwasm@confio.tech or join the CosmWasm Telegram group. Also visit confio.tech on the web, or follow our blog to see what other projects we are up to.",13844
"Originally posted on blog.knoldus.com by Aman Verma

WebAssembly, generally called wasm, is a compact, fast and portable code that can run on most browsers, making it a technology of the future. Let us learn more about it in this blog.

Introduction

""WebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications.""

This is the definition that I got from the WebAssembly official web page. Let us break this definition up and understand it. It states that WebAssembly is a binary instruction format. This binary instruction format is similar to byte code in java. This code can not be understood by us but by a virtual machine which is next in our definition. A stack-based virtual machine is a virtual machine that considers the operands of all the instructions as they are on a stack. The next part of the definition tells that wasm is a compilation target for many programming languages. This is indeed true, many languages compile their code to wasm which makes it easier to transfer on the web for deployment purposes.

Understanding WebAssembly

To understand WebAssembly, we have to understand it from the very beginning when the computers were invented. At that time, to interact with the big-sized computers we needed to code in machine language , i.e., the language of 0s and 1s. It was quite difficult to code in that.

So we started coding in Hex symbols which were not quite helpful as it was also very difficult to code in Hex . We then started using assembly languages which made it somewhat easy to code as it was understandable by us. For example, LDX stood for load in the X register, similarly LDY for load in the Y register. So it was easier but for different species of microprocessors, we needed a different assembly language .

So we switched to high-level languages like C, which did the same task but it was highly dependent on OS. To solve this we came up with the concept of virtual machines. Like in Java, we have a JVM(Java Virtual Machine). The Java code is compiled to a byte code that is understandable by JVM which runs it.

Now, WebAssembly is the same concept but for web browsers.

A java code or any other code is compiled to a WebAssembly code(.wasm file) which can run on a virtual machine in the browsers. It enables most browsers to run any kind of code and use the output. This is the beauty of WebAssembly codes.

Here is a picture that shows how a wasm file interacts with the DOM.

In this image, the main.wasm file cannot directly interact with the DOM, but it can interact with the app.js file through the VM. The app.js file uses the main.wasm file to instantiate the module and use the functions that the module exports. It then uses the output of the wasm file to print on the webpage. This is how wasm architecture works.

What WebAssembly is not

WebAssembly is not –

A transpiling target for JavaScript

A replacement for JS

A Programming Language

Let us understand each one of them. Firstly, WebAssembly is not a transpiling target for JavaScript. Transpiling means converting a high-level language to another high-level language like transpiling typescript to JavaScript. Though typescript can be compiled to WebAssembly.

Second, WebAssembly is not a replacement for JavaScript . Though many people say, it is the end of JavaScript , but you need JavaScript to host WebAssembly in the browser.

Third, WebAssembly is not a programming language. Though there is a textual format of wasm, writing it on its own will take far more time.

Textual Representation of wasm

wasm is a binary instruction format code that is not understandable by us, so there is a .wat file that is the textual representation of the wasm file. Here is a sample .wat file that adds two numbers.

add.wat

(module (func $add (param $lhs i32) (param $rhs i32) (result i32) get_local $lhs get_local $rhs i32.add) (export “add” (func $add)) )

There is a module declaration in the first line and then there is a function named add with two parameters lhs and rhs and an i32 return value. The parameter declaration is done in this format (param $parametername datatype) . It is called an S-expression.

Then there is a get_local call that retrieves the local variables and puts them on the stack. So, lhs and rhs are put on a stack and then add function for integers is called and the addition is done for the top two elements in the stack and the result is pushed back. The result is returned because it is at the top of the stack.

The add function is exported with the name ‘add’ to be used by other programs.

So this was an introduction to wasm. This is indeed the technology of the future. So, that was all from my side.",4833
"I have recently written about how major changes in storage technology are changing conventional knowledge on how to deal with storage I/O. The central thesis of the article was simple: As fast NVMe devices become common place, the impact of the software layer gets bigger. Old ideas and APIs, designed for a time in which storage accesses were in the hundreds of milliseconds should be revisited.

In particular, I investigated the idea that Buffered I/O, where the operating system caches data pages on behalf of the user should always be better than Direct I/O, where no such caching happens. Once we employ modern APIs that is simply not the case. As a matter of fact, in the example using the Glommio io_uring asynchronous executor for Rust, Direct I/O reads performed better than Buffered I/O in most cases.

But what about writes? In this article, we’ll take a look at the issue of writes, how it differs from reads, and show that much like credit card debt, Buffered I/O writes are only providing the illusion of wealth on cheap money. At some point in time, you still have to foot the bill. Real wealth, on the other hand, comes from Direct I/O.

How do reads and writes differ?

The fact that reads and writes differ in their characteristics should surprise no one: that’s a common thing in computer science, and is what is behind most of the trend towards immutable data structures in recent years.

However, there is one open secret about storage devices in particular that is nothing short of mind-blowing:

It is simply not possible to issue atomic writes to a storage device. Or at least not in practice. This stackoverflow article does a good job summarizing the situation, and I also recommend this LWN.net article that talks about changes some Linux Filesystem developers are discussing to ameliorate the situation.

For SSDs the situation is quite helpless. For NVMe, it is a bit better: there is a provision in the spec for atomic writes, but even if all devices implemented it (which they don’t), there’s still a big contingent of devices where software has to run on where this is simply not available.

For this reason, writes to the middle of a file are very rare in applications, and even when they do happen, they tend to come accompanied by a journal, which is sequential in nature.

There are two immediate consequences of this:

Append-only data structures vastly dominate storage writes. Most write-optimized modern stores are built on top of LSM trees, and even workloads that use more traditional data-structures like B-Trees will have a journal and/or other techniques to make sure data is reliably written. There is usually a memory buffer that is used to accumulate writes before they are passed into the file: this guarantees some level of control over the state of the file when the write happens. If we were to write directly to an mmap’d file, for instance, flushes could come at any time and we simply would have no idea in which state the file is in. Although it is true that we can force a maximum time for a sync with specialized system calls like msync the operating system may have to force a flush due to memory pressure at any point before that.

What this means is that coalescing, which is the usual advantage of buffering, doesn’t apply for writes. For most modern data structures, there is little reason to keep a buffer in-memory waiting for the next access: likely what is sent to the file is never touched again, except for future reads. And at that point the calculations in my read article applies. The next write is likely for the next position in the file.

This tips the scale even more in favour of Direct I/O. In anticipation of using the recently written pages in the future, Buffered I/O may use an immense amount of memory in the operating system page cache. And while is true that this is cached memory, that memory needs to be written to the device first before it can be discarded. If the device is not fast enough, we can easily run out of memory. This is a problem I have written about in the past.

Since we can write an entire Terabyte-large file while keeping only a couple of kilobytes in memory, Direct I/O is the undisputed way to write to files.

But how much does Direct I/O cost?

Much like reads, you need to make sure you are measuring the right thing to realize the advantage of Direct I/O. And how to do that that is far from obvious.

Recently, one of our users opened an issue in our Github page, in which he noted that despite what we advertise, Direct I/O writes consumed a lot more CPU than buffered writes. So why is that?

The reason is: Buffered writes are like a loan: you can get your asset for cheap now, but you then have to pay it back in the future, with interest. When you issue a Direct I/O write, you are paying most of the costs related to the transaction right away, and in the CPU that dispatched the I/O — which is predictable. The situation is different for Buffered I/O: the only cost to be paid immediately are the very cheap memory writes.

The actual work to make the data persistent is done in kernel threads. Those kernel threads are free to run in other CPUs, so in a simple system that is far from its saturation point, this can give the user the illusion of cheaper access.

Much like a loan, there are certainly cases in which this can work in your favour. However, in practice, that will happen at an unpredictable — and potentially inconvenient time in the future.

Aside from this unpredictability, in order to make the right decision one needs to be at least aware of the fact that the total cost of the loan may be higher. More often than not it can be the case that at or close to saturation, all your CPUs are busy, in which case the total cost is more important.

If we use the time command to measure the Direct I/O vs Buffered version of the same code provided by the user, and focus on system and user times, we have:

Direct I/O:

user 0m7.401s

sys 0m7.118s

And Buffered I/O:

user 0m3.771s

sys 0m11.102s

So there we have it: all that the Buffered I/O version did was switch user time to system time. And because that system time is consumed by kernel threads, which may be harder to see, we can get the illusion that buffered writes are consuming less CPU.

But if we sum up user and system times, we can clearly see that in reality we’re eventually paying interest on our loan: Buffered writes used 1.7% more CPU than Direct I/O writes. This is actually not very far from current monthly interest rates on my credit card. If this is a shocking coincidence or a big conspiracy, is up for you, the reader, to decide.

But which is faster?

Many users would be happy to pay some percentage of CPU time to get faster results. But if we look at the real time in the examples above, Direct I/O is not only cheaper, but faster.

You will notice in the example code that the user correctly issued a call to close. By default, Glommio’s stream close imply a sync. But not only that can be disabled, most of the time in other languages and frameworks this is not the case. In particular, for Posix, close does not imply a sync.

What that means is that even after you write all your buffers, and close your file, your data may still not safely be present in the device’s media! What can be surprising, however, is that data is not safely stored even if you are using Direct I/O! This is because Direct I/O writes the data immediately to the device, but storage devices have their own internal caches. And in the event of a power loss data can still be lost if those caches are not persisted.

At this point it is fair to ask: if a sync is necessary for both buffered writes and Direct I/O, is there really an advantage to Direct I/O? To investigate that behaviour we can use Glommio’s example storage benchmark.

At first, we will write a file that is smaller than memory and not issue a sync. It is easy to have the impression that Buffered I/O is faster. If we write a 4GiB file in a server with 64GiB of DRAM, we see the following:

Buffered I/O: Wrote 4.29 GB in 1.9s, 2.25 GB/s

Direct I/O: Wrote 4.29 GB in 4.4s, 968.72 MB/s

Buffered I/O is more than twice as fast! That is because since the file is so small compared to the size of memory, it can just sit in memory for the whole time. However at this point your data is not safely committed to storage at all. If we account for the time-to-safety until our call to sync returns, the setup costs, lack of parallelism, mapping, and other costs discussed when analyzing reads start to show:

Buffered I/O: Wrote 4.29 GB in 1.9s, 2.25 GB/s

Buffered I/O: Closed in 4.7s, Amortized total 642.54 MB/s Direct I/O: Wrote 4.29 GB in 4.4s, 968.72 MB/s

Direct I/O: Closed in 34.9ms, Amortized total 961.14 MB/s

As we can see, Buffered I/O loans provided us with the illusion of wealth. Once we had to pay the bill, Direct I/O is faster, and we are richer. Syncing a Direct I/O file is not free, as previously noted: but 35ms later we can predictably guarantee it is safely stored. Compare that to the more than 4s for Buffered I/O.

Things start to change as the file gets bigger. That is because there is more pressure in the operating system virtual memory. As the file grow in size, the operating system is no longer able to afford the luxury of waiting until the end to issue a flush. If we now write 16 GiB, a 32Gib, and a 64Gib file, we see that even the illusory difference between Buffered and Direct I/O start to fade away

Buffered I/O: Wrote 17.18 GB in 10.4s, 1.64 GB/s

Buffered I/O: Closed in 11.8s, Amortized total 769.58 MB/s Buffered I/O: Wrote 34.36 GB in 29.9s, 1.15 GB/s

Buffered I/O: Closed in 12.2s, Amortized total 814.85 MB/s Buffered I/O: Wrote 68.72 GB in 69.4s, 989.7 MB/s

Buffered I/O: Closed in 12.3s, Amortized total 840.59 MB/s

In all the cases above Direct I/O kept writing at around 960MB/s, which is the maximum throughput of this particular device.

Once the file gets bigger than memory, then there is no more pretending: Direct I/O is just faster, from whichever angle we look at it.

Buffered I/O: Wrote 107.37 GB in 113.3s, 947.17 MB/s

Buffered I/O: Closed in 12.2s, Amortized total 855.03 MB/s Direct I/O: Wrote 107.37 GB in 112.1s, 957.26 MB/s

Direct I/O: Closed in 43.5ms, Amortized total 956.89 MB/s

Conclusion

Having access to credit is not bad. It is, many times, crucial for building wealth. However we need to pay attention to total costs, make sure the interest rates are reasonable, to be sure we are building real, and not illusory wealth.

When writing to files on modern storage, the same applies. We can write them for cheap at first, but we are bound to pay the real cost — with interest, later. Whether or not that is a good thing, is certainly situational. But with high interest rates and a potential for memory spiralling out of control if you write faster than what the device can chew, Buffered I/O can easily become subprime. Direct I/O, with its fixed memory usage, and cheaper CPU costs, is AAA.

I hope this article will empower you to make better choices so you can build real storage wealth.",11135
"130 million people give birth around the world every year. Up to 50% of these will have surgical intervention. Medical procedures require consent — but how? Femtech startup founder Amy Haderer is on a mission to make sure that millions of birthing parents are having their most basic human rights resepcted — with technology as the solution.

Amy Haderer is a mother of six, a doula (i.e. personal birth attendant), fine artist, stage set designer, and, but not least, the co-founder of a Femtech startup Motherboard Birth.

Amy explains her journey to entrepreneurship. “My first daughter made me a mother which completely rearranged my cells and made me a completely different being. My second daughter made me a doula and that was an amazing opportunity to serve people during their birth experience. My third daughter made me an artist.”

All of the Motherboard brand and UX imagery has been created by fine artist and doula Amy Haderer.

Motherboard is the big tech baby that’s making her a startup founder.

Working one on one wasn’t giving Amy the widespread impact she wanted. “I really wanted to move from one family, one birth, to ‘What can we do to go to scale and change birth from the inside out systemically?’” This is what she and her team are doing now.

Many birthing parents will have heard of a birth plan — most may not know that they are not well received by care providers. Some care providers even joke that a Birth Plan is a fast track to the operating theatre.

The colourful icons differentiate the Motherboard birth plans from the usual long text format, making it easy to see and understand for health care providers.

Amy’s vision is to return the balance of authority back over to the birthing person. “We can reclaim what birth plans are meant to be. Birth plans get a bad rap. 65% of care providers believe they actually create worse outcomes. Really, I wanted to turn birth plans back into the communication tool they are supposed to be.”

Amy’s is now achieving this, with the Motherboard Birth just passing its 1st birthing (there may have been cake and candles), and birthing parents and their care providers around the world now able to access its tools to create easy to read and understand visual birth plans.

But how can anyone know what they want to choose if they doesn’t know the risks, benefits, or potential consequences?

To ensure informed consent, all of the Motherboard birth plan icons are matched with in-depth birth education modules compiled from Amy’s experience as a birth keeper.

That’s where the real genius of Motherboard comes out to play. Amy’s solution is to link her online visual birth plans with easy to understand birth education.

“Most families don’t know how they feel about foetal monitoring, or even what foetal monitoring is. They don’t know the different ways you can get induced or the benefits and risks that come with induction. They don’t know what happens when somebody breaks your water.”

Birth is a complex landscape to navigate at the best of times — any birth involves choices about tests, ultrasounds, induction, birthing positions, vaccines, monitoring, interventions, when to cut the cord or not, when put the baby on the chest, and even how to birth the placenta. How can a birthing parent ever hope to communicate all that to their care provider in the exact instant when the care provider needs that information if the birthing parent may be in no way able to express themselves?

Rather than just verbal explanations, Amy has made sure all aspects of the birth plan are accompanied by visual how-tos.

Yet Motherboard’s simple and colourful icons, paired with matching educational in-depth walkthroughs, takes the mystery out of birth choices and the urgency out of communication. Parents can learn as little or as much as they want about each procedure and option, pop their choice on the board, and the care provider can quickly refer to it during birth, at a glance understanding just what’s most important to them.

To craft such a simple and yet beautiful end-user experience has been an epic adventure that’s taken Amy 6 years, and couldn’t have been achieved without her previous years of experience as both a doula (a birth assistant) and parent, knowing from the other side of the table exactly what a birthing parent is looking for to make their experience a positive and empowering one.

Birthing parents are customers of the birthing experience, and yet, because of the barriers to expert knowledge, at this most exciting and expressive period of a woman’s life, a mother in the medical system may be subject to feeling she is being treated like a little child — with untold psychological consequences of trauma, disempowerment and depression.

Amy’s mission has been to return the dignity and the beauty of the birthing experience to parents, first as a doula and then with her art, and with the Motherboard App she achieves that.

Amy Haderer, Motherboard App founder.

Nevertheless, as a non-technical founder with a technical platform necessary for the execution of her vision, her path to tech founder hasn’t been without bumps in the road. Like any founder, she’s had to stretch beyond her prior knowledge as a sole business owner, but also claims she’s been tasked with navigating “all the patriarchal bullshit that goes along with that.”

“I would interview developers that would only want to talk to my male co-founder,” Amy says, rolling her eyes.

“It’s been kind of a wild ride over the last year, lots of ups and downs.” Until only a couple months ago, Amy on her own without a team. If she was complemented by others for the work her team had done, she says she’d be like “yeah… my team…”

The Motherboard App also has B2B facing side, providing educational materials that health care providers can use to educate clients.

Just about all the exquisite and well-executed educational content that you see now on the Motherboard website has been produced alone by Amy, from her background as a fine artist and doula. Her intentions are to produce education materials for all the birth care providers who use it with their birthing clients. From the perspective of the author, it’s some of the best I’ve read and seen.

“If you are to boil down what I’m really passionate about it’s education and communication and also informed choice….childbirth ed and doulas are only half of the equation. I’m really passionate about working on the other size of the equation: Care providers, birth teams and health care systems.”

Doulas are an essential guide at any birth, however, few birthing parents use them, sometimes out of lack of awareness, and sometimes due to expense. A doula is like a cushion between the birthing parent and the hospital system, explaining procedures and practices to them that the medical practitioner might not have time or incentive to do. They work for the family, and thus are seen as a buffer between them and a faceless, over-bearing medical system with its ticking clock and standard measurements.

Motherboard is like a doula in your pocket. Without it, “Informed choice is at best very inconsistent. At worst it’s absent or manipulative,” says Amy.

“A provider is just going to say what you need to say to get the job done and it really opens them up to a lot of liability.”

iDecide provides an interface for maternity care providers to display educational modules in hospitals to patients to ensure informed consent.

Amy’s company is also working on the tool called “iDecide”, a clickthrough informed choice tool helping care providers give timely, evidence-based information at the bedside. When it becomes clear that a medical decision needs to be made, doctors can hand over the iDecide tool so the patient can go through the module alone, allowing the doctor to come back and discuss the patients’ options on a more level playing field atop that education.

“We’re taking some of the weight off providers to educate. This allows medical staff to instead spend their time answering patient-specific questions and building relationships.”

“I think that’s the best application of tech — when you use it to build relationships.”

In modern times, the emphasis has been placed on making the woman’s body a birthing machine — she’s on the clock and under pressure to perform to arbitrarily set standards of what normal birth looks like. In contrast, Amy’s technology puts the automation back where it should be — away from the mother, and into the systems to support her and inform her.

“I would like to help health care providers recapture the joy that they experienced and the reasons why they got into perinatal health. Compassion fatigue is a real thing. Secondary trauma is a real thing. So we need to automate and take the weight off them as much as possible.”

Amy talks about how it can take two decades for new evidence-based research to be integrated into clinical practice. “We need better ways to automate evidence-based information and how it influences clinical behavior.”

Posterior labour can often be long and uncomfortable — much like birthing a tech baby!

Amy compares entrepreneurship to childbirth. “I tell people I’ve been in labour for six years. Prodromal labour, start and stop labour for about five of those.”

When she got to the point of pitching tech partners, that was the start of active labour. “When we were beta testing, we were pushing,” laughs Amy.

Amy’s reaction to launching surprised her. “I launched November 5, and then I cried for a week.”

Bringing her big tech baby to the world made her feel vulnerable and exposed. “I felt like all my insides were on the outside, I felt so raw and vulnerable. It’s like those moments as a new parent.”

“Now I’m parenting this enormous tech baby. I’ve had to natural births, and I think birthing this tech baby was harder! It was definitely longer!”

Certainly, I agree that giving birth to any business as an entrepreneur can be like giving birth to a baby — and I’ve even spoken to other birth professionals like Jane Hardwicke Collings and Mechell Turner who feel the same!

Just as all the elements need to align for an easy birth — mentally, emotionally, and physically — for our passage to entrepreneurship to be smooth, we need to be personally ready to bring a new idea into the world at the same time that the world is willing to receive it — and pay for it!

Amy recommends startup founders read Big Magic, to access the kind of profound inspiration necessary for changing world paradigms.

Amy recommends we read the book “Big Magic” — “It’s about living creatively. The author talks about how she believes the world is not only inhabited by people, plants, and animals but also by ideas.”

“I’m first and foremost an artist and I found tech to be creative in a way I didn’t expect.”

For Amy, Motherboard birth was an idea that was ready to be born, and she was ready to do the work to bring it into the world, fully formed.",10965
"Let’s start by talking about the evolution of battery usage in the past decades. It has been implemented in numerous devices and has revolutionized to reach better performance at the same time increase efficiency. The Royal Swedish Academy of Sciences awarded Nobel Prize in Chemistry, 2019 to John B. Goodenough, M. Stanley Whittingham, and Akira Yoshino for the development of lithium-ion batteries, batteries, which were developed in the ’90s. It has not lost its charm, rather the direct opposite has happened, the revolution it has created is tremendous and today from iPod shuffle to electric trucks everything carries lithium in its pocket. Recent customers of lithium are the electric vehicles, transportation running on the same principle as of a battery toy car.

Why EV manufacturers chose lithium batteries over other battery technology?

The beauty embodied in lithium battery packs is the energy they can carry in the required volume. The high energy density of lithium batteries makes it perfect for an electric vehicle, lighter and efficient. Charging is always a concern for any EV owner and with a faster charging rate of lithium, packs outshine other technologies. Lithium charges at a faster rate and reduces the refuelling time of the vehicle.

Moving on to the construction of lithium cell -the chemical structure, solid electrolyte, and stable encapsulation make it suitable for application prone to mechanical vibrations and impacts. The lithium cells are found performing at the same efficiency in both stationary and mobile applications. like every coin has two sides, Lithium brings a good amount of constraints and restrictions in its usage.

Why do we need BMS?

Thermal instability has always remained the greatest obstacle in a lithium cell’s design and application. The efficiency plummets considerably at higher operating temperatures. Though lithium brings higher energy density, faster-charging rate, low self-discharge, and longer life cycle but these factors are maintained only when it is provided by a manager. Overcharging is another prime concern with lithium batteries usage therefore charging needs to be closely & accurately monitored. memory effect is also a benefactor to its disadvantages.

So, what is meant by a manager? it definitely is not a physical entity, and here goes the answer: BMS. Lithium battery packs are comprised of multiple cells connected in series and parallel configuration, hence cell balancing is the important aspect for efficient power delivery. Imbalance cell causes internal loading of battery reducing the state of charge and increasing failure rate. A cell balancing mechanism is required for proper battery pack operation and higher cell life. A battery management system helps lithium batteries maintaining and enhancing their performance with protection against hostile conditions to prevent irreversible failure.

What is Battery Management System?

“To make a best lithium battery pack just manage its voltage, current & temperature and this is what a BMS does”

As the name is self-explanatory, a system designed to manage and control a battery unit for proper operation. Initial designs only incorporated protection components i.e. fuses, FET switches with analogue comparators sensing deviation, and pulling the complete battery unit down in case of abruptions. With due course of time, lithium batteries found their way into several electronic devices, and hence, simultaneously the need to design an efficient manager to take care of its working increased. A modern BMS comprises multiple blocks integrated to build a complete lithium battery unit:

Power Delivery Unit (PDU): Comprises of FET switch and gate driver circuit which control the outgoing and incoming power in the battery. Cell balancing with individual current bypass FETs is also part of the PDU. Few specific PDU also manages regenerative braking power Sensing and measurement: Comprises of on-chip or external ADCs to compute cell voltages, junction temperature, and charge/discharge current. SAR-based ADC provides a high sampling rate and resolution. The modern BMS chip has SAR-ADC-based AFE integrated into it. Communication block: Comprises of CAN transceiver, RS232 transceiver for communicating with parallel battery packs or with the vehicle control unit Central processor: Manages the above blocks with an embedded algorithm deciding as per inputs and battery state. It also manages the external and internal communications of BMS.

What are the functions provided by BMS?

Battery Management System (BMS) performs three primary functions:

It protects the battery pack from being over-charged (cell voltages going too high) or over-discharged (cell voltages going too low) thereby extending the life of the battery pack. It does this by constantly monitoring every cell in the battery pack and calculating exactly how much current can safely go in and come out of the battery pack without damaging it. These calculated current limits are then sent to the source and load (motor controller, power inverter, etc), which are responsible for respecting these limits. It calculates the State of Charge (the amount of energy remaining in the battery) by tracking how much energy goes in and out of the battery pack and by monitoring cell voltages. It monitors the health and safety of the battery pack by constantly checking for shorts, loose connections, breakdowns in wire insulation, and weak or defective battery cells that need to be replaced.

The secondary functions that the BMS performs:

Balances all the cells in the battery pack by intelligently bleeding off excess energy from cells that are charged more than others. This provides the maximum amount of usable energy (capacity) from the battery pack since the pack is only as strong as the weakest cell.

Monitors the temperature of the battery pack and controls a battery fan to regulate the temperature of the pack. Additionally, it constantly monitors the output of the fan to make sure it is working properly.

Provides real-time information and values to other devices such as motor controllers, chargers, displays, and data loggers using several different methods.

Stores error codes and comprehensive diagnostic information to aid in fixing problems with the battery pack should any issues arise.

What are the basic components of a BMS?

Fuse: When a violent short circuit occurs, the battery cells need to be protected fast. Fuse is meant to be blown by the overvoltage control IC in case of overvoltages, driving it to the ground. The MCU can communicate the blown fuse’s condition, which is why the MCU power supply has to be before the fuse.

Current Sensing: Keeping a time reference and integrating the current over time, basically a sensor used to sense the current flow and report it to the MCU, we obtain the total energy entered or exited the battery, implementing a Coulomb counter. In other words, we can estimate the state of charge by using the following formula:

Thermistors: Temperature sensors, usually thermistors, are used both for temperature monitor and for safety intervention. It blows the fuse, when the temperature goes above the required rating without MCU intervention, leading to no time delay.

Balancer: Battery cells have given tolerances in their capacity and impedance. So, over cycles, a charge difference can accumulate among cells in series. If a weaker set of cells has less capacity, it will charge faster compared to others in series. The BMS has to therefore stop other cells from charging, or else the weaker cells will get overcharged.

Conversely, a cell can get discharged faster, risking that cells going under its minimum voltage. In this instance, a BMS without a balancer has to stop the power delivery earlier

In Figure 7, you can see a thermistor that controls the input of the overvoltage control IC. This artificially blows the SCP (the fuse shown in Figure 5) without MCU intervention.

In conclusion, a proper Battery Management System can lead to the optimal usage of a lithium-ion battery in any available electronic device. Its main features are utilized and amplified by BMS, also it acts as a catalyst for the work done by the battery by improving overall battery life and efficiency.",8276
"Out of 150 applicants, Hub71, Abu Dhabi’s global tech ecosystem, selected 16 start-ups from 11 countries and seven industries for its first cohort of 2021.

Following the pioneering Emirates Mars Mission and the recent selection of the UAE’s first female astronaut, Hub71 selected its first start-ups from India, Saudi Arabia, Turkey, Mauritius, and South Africa, in addition to the first SpaceTech start-up, with an 11 percent success rate.

Hub71 has chosen start-ups that have raised a total of AED274 million ($75 million) in funding from internationally renowned institutions such as Goldman Sachs and Saudi Aramco Entrepreneurship, as well as world-class investors such as Y Combinator, Draper Associates, 500 Startups, Plug and Play, and Techstars.

Start-ups in the cohort are working on innovative innovations like the world’s first blockchain satellite constellation and the world’s first AI/machine learning-based ideation recommendation and sourcing platform for event venue facilities, services, and food and beverage.

Following Hub71’s achievement of attracting more than 100 start-ups in less than two years, applications for the cohort were received from 34 countries, with the majority of applicants hailing from the UAE, the United States, and the United Kingdom, according to a statement.

Applicants were primarily from start-ups specialised in FinTech, software, e-commerce and artificial intelligence.",1424
"Prerequisite

MySQL installed on your machine Background on MySQL Database The ability to simplify is always a plus 😃

What is MySQL Event Scheduler?

The MySQL Event Scheduler is a tool that manages the scheduling and execution of events that are so-called tasks that run according to a schedule. MySQL introduced the Event Scheduler in version 5.1.6. When we create an event then, actually we are creating a named database object containing one or more SQL statements to be executed at one or more regular intervals, beginning and ending at a specific date and time. The idea of this event scheduler is similar to the idea of the Unix crontab (also knows as a “cronjob”) or the Windows Task Scheduler.

When to use MySQL Event Scheduler?

Suppose you manage your database which consists of two tables. One table consists of the data related to user's information like Name, Age, DOB, Gender, etc. and in the other table, we want to enter the data only once a day in which we push the number of user entries created for each day.

Now at first, this problem statement looks easy to you! Let's first go with the cronjob approach. We can write one script which will fetch all the user entries created for each day and insert the result into another table. And we will execute our script using cronjob by setting the scheduled time using crontab expression.

But wait! Let's re-iterate over the approach once.

We have to write the block of code which will handle our business logic.

Then, we need to add a cronjob which will trigger our script with a specific interval.

Now we are consuming two machines here, the one in the database and the other is the machine where we are setting up our cronjob.

Don’t you think it's a wastage of resources? Can we do it using the single machine? Say thanks to MySQL Task scheduler!

We can use MySQL Event Scheduler when our task is purely data-centric or complements the functionality that is already built into the database. We can do data clean-up, log certain events, aggregate certain data, etc using this event scheduler.

MySQL Event Scheduler over CronJob

There are a few use-cases where we can prefer MySQL Event scheduler over Cronjob. But we cannot make a decision based on these use-cases and make a statement about which is the best. The following are the advantages if we use MySQL Event Scheduler:

It is directly written in MySQL Server.

It is the platform-independent approach. So if our application is written in any language it does not matter we just need to know MySQL.

Whenever there is a database update or cleanup required at regular intervals we can using this event scheduler approach.

There is no need to compile queries every time so it improves the performance.

MySQL Event Scheduler Configuration

MySQL uses a special thread called the event scheduler thread to execute all scheduled events. We can view the status of the event scheduler thread by executing the below command:

SHOW PROCESSLIST;

If the event scheduler is not enabled, we can set the event scheduler system variable to enable and start it using the below command:

SET GLOBAL event_scheduler = ON;

Now if we Execute the first command we can see the state of the event scheduler thread:

Creating New MySQL Events Syntax

Let's create our first MySQL event using the below basic syntax:

CREATE EVENT [IF NOT EXIST] event_name

ON SCHEDULE schedule

DO event_body

In this syntax:

First, we specify the name of the event that we want to create with “CREATE EVENT” keywords. The event names must be unique within the same database. Second, specify a schedule after the “ON SCHEDULE” keywords.

If the event is a one-time event, we use the below syntax:

AT timestamp [+ INTERVAL]

If the event is a recurring event, we use the EVERY clause:

EVERY interval STARTS timestamp [+INTERVAL] ENDS timestamp [+INTERVAL]

Third, place SQL statements after the DO keyword. And we can call a stored procedure inside the body of an event. In case we have compound statements, we can wrap them in a “BEGIN END” block.

Implementing MySQL Event for our Use-Case

Let’s create a table that consists of user’s information like Name, Age, Gender, DOB using the below command:

CREATE TABLE USERS (

id INT PRIMARY KEY AUTO_INCREMENT,

Name VARCHAR(20) NOT NULL,

Gender VARCHAR(1) NOT NULL,

AGE INT NOT NULL,

CREATED_AT TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,

UPDATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP

);

Now we will create another table that will store the count of user entries for each day and create this table using the below command:

CREATE TABLE USERS_COUNT_DAY_WISE (

id INT PRIMARY KEY AUTO_INCREMENT,

USER_ENTRIES_ON DATETIME,

COUNT INT

);

Here comes the time for our MySQL Event Scheduler. We will create an event that fetches the number of user entries for each day from the “USERS” table and insert the result into the “USERS_COUNT_DAY_WISE” table. And we will execute this task once a day. Using the below command we can create this event and schedule it for a specific time interval:

CREATE EVENT USERS_ENTRY_COUNT

ON SCHEDULE EVERY 1 DAY

STARTS (TIMESTAMP(CURRENT_DATE) + INTERVAL 1 DAY + INTERVAL 1 HOUR)

DO

INSERT INTO USERS_COUNT_DAY_WISE (USER_ENTRIES_ON, COUNT) SELECT NOW(), COUNT(*) FROM USERS WHERE CREATED_AT >= DATE_SUB(NOW(), INTERVAL 1 DAY);

To check our executed event, we can use the below command:

SELECT event_schema, event_definition, status, starts, last_executed FROM INFORMATION_SCHEMA.EVENTS

Greetings! 🥳

We have successfully created our first MySQL Event which will take care of a specific task and we also envoke this task for a specific interval of time.

As a Page of victory let's check if our scheduled event is added to the process list or not.",5774
"We live in a time frame where there is no limit in exploring technology space. Innovations happening everyday makes you understand how fast the world is moving around you. Blockchain is one technology that boomed back in 2016 and has paved a path for many startups to emerge.

Business to Customer startups have come with great innovations to help consumers overcome their daily problems. Here are some 6 strong startups which caught my eye that are focused completely addressing the consumer front.

Circle

A Boston based company that has been helping consumers to transfer funds from anywhere in the world without charging a nickel on exchange fee. They also run a platforms for startups to get their seed money from global investors

BuzzShow

BuzzShow is a completely decentralized video platform, where every single contributor — right from the viewer to the content creator gets incentivized. Users have complete retention towards privacy and control over their space unlike how other platforms capitalize our data for ads.

Salt Lending

Lending platform that allows one to leverage his or her cryptocurrency. It basically allows users to pledge their coins for amounts starting from $5000. Their current interest rate starts from 5% which is less than most of your indian banks which starts ranges from 12–15%(personal loans).

Basic Attention Token — BRAVE Browser

This is a technology built on ethereum with a goal to distribute advertising money between publisher and content creators. Focus here is to build a safer space for user and also share limited data with advertisers. Other browsers actually don’t pay users for sharing our data pattern to advertisers, This eliminates that function and incentivizes all ends.

NanoVision

Nano Vision helps the global community to lend their efforts in access to medical research and development. Citizens, Doctors or Scientists would be eligible to procure any data stored that has been recorded on Nano vision platform

Tradove

This is a blockchain social media platform for B2B users, this focuses on connecting buyers and sellers with their products and at the same behaves as a communication platform for potential prospects. Some big players like IKEA, Amazon use this platform as part of their BAU.",2263
"Image credits: RazorRobotics

Robotics is an interdisciplinary field that combines Electrical & Electronics, Computer Science, and Mechanical engineering. Robotics engineers make everything from automatic floor cleaners to robots that perform gymnastics like Atlas (Boston Dynamics).

How do these different disciplines come together ?

To understand how the different fields come together let us look at an example. Nuro (a silicon valley startup) does autonomous delivery with self-driving vehicles. On the surface, Nuro autonomously drives and delivers products, but on the inside there is a complex interplay of different engineering systems.

First, let us discuss the cameras, Lidar, and other vision sensors involved in Nuro. These sensors provide information on Nuros’ surroundings. These systems fall under digital control systems/electronics.

Once this data is received computers run algorithms to detect its environment and to localize itself on the map. As Nuro travels it maps its path and localizes (locates) itself on the map. All of this comes under the computer science branch.

Image credits: Nuro

Once the robot perceives its location, orientation in space-time and plans its path, Nuro will have to control the car and move to the goal. This is where control systems come into the picture. A control system manages, directs, and regulates the way that the system functions. Based on the current location and final destination it decides the speed of movement.

Mechanics is the backbone of the robot from structure to design. All control algorithms are written based on the mechanical design of Nuro. It also involves other aspects such as; aerodynamics, materials used, and other thermodynamic considerations.

Mechanics plays a crucial role in robots such as Kuka (collaborative robot arm) where advanced mechanical engineering equations are used to determine the angles of the joins to reach for the arm to reach its desired point in space. And it also involves designing the entire robots physical structure

Image credits: Kuka

All of these individual disciplines are in rapid development and when they come together they create wonders.

As you might know, the Robotics market is on the constant rise (Check out my previous article).

So we should expect more from the robotics industry. As Boston Dynamics CEO Marc Raibert puts it:

“I happen to believe that robotics will be bigger than the Internet” — Marc Raibert

Is there any other discipline with so much collaboration among other disciplines? Often roboticists take inspiration from the life sciences and the way nature works. This is a field that I believe will change the world drastically in the coming few years. And as a person interested in this field I would strongly suggest you try out searching more about this field and learn more about it.

Hope you found this article interesting. Reach out to me on: pramotharun.github.io to talk more about this and more!",2956
"Saying that most people spend their favourite time on social media platforms would not be an exaggeration, especially in today’s world.

Over the time social media applications like Facebook, Instagram, Messenger, Twitter and more have become an integral part of people’s lives and daily routines. People have become so addicted to it, that the first thing they are dying to do after waking up is to check their social media feeds.

With billions of registered accounts and trillions of annual active users, social media apps have emerged as the bright opportunity for businesses to stay connected with their targeted audience.

Today, where the covid-19 pandemic has pushed businesses to doom and people are in isolation, traditional old-fashioned yet in-person communication has become almost impossible. And that’s where social networks and social media applications outshined as the most potential platform for emerging entrepreneurs to generate better business opportunities.

If you also belong to one of them, then you are at the right place. Here we are going to answer all of your burning questions and walk you through every step you need to take to make it happen.

Here’s what we’ll cover:

Market Statistics: The Growing Social Media App Landscape

Why Do Businesses Should Invest in Social Media Apps?

What Type of Social Media App You Should Develop and Its Cost?

Tech Stacks That You Can Use To Develop A Social Media App

Current Trends That You Can Integrate Into Your Social Media App

Must-Have Feature To Consider When You Create a Social media App

Step-By-Step Guide To Create a Social Media App

How Much Does It Cost To Create a Social Media App?

How To Make Money From a Social Media App?

So before you get straight into the process of hiring the best mobile app development company, it is worth getting deep into each point and understand these parameters closely…

Market Statistics: The Growing Social Media App Landscape

Since developing an app is like starting another business that requires the same level of effort and budget. So before making something groundbreaking, it is worth evaluating the market and analyzing the leaders.

Here are a few statistics reflecting the growing landscape of social media apps:

According to the studies, the landscape of social network and social media applications is swiftly expanding and it has been observed that social media users have grown by 10% over the last year in 2020 and taking the global total to 3.96 billion in the first quarters of 2020.

On average, users are spending 2 hours and 33 minutes socializing online mostly on Facebook, YouTube, Messenger, Whatsapp, Instagram, Twitter or Snapchat in 2020.

According to the eMarket survey report, 11.8 million US user growth for the app in the next four years and expected to reach 138.1 million users will access Facebook Messenger in the US alone by 2022.

In the nutshell, it is fair enough to say that for the first time, more than half of the world’s population now uses social media with 99% of total social media users accessing these platforms via smartphones at some point.

And this provides the huge opportunity to grow your community and monetize your targeted audience with your own unique social media applications. While it’s true that social media apps are a great platform for communication and staying connected. But how can it benefit your businesses?

Why Do Businesses Should Invest in Social Media Apps?

Initially, social networking services are started with the aim to create a platform that helps users find each other and communicate online. But today, social media apps are not left just a medium of communication between friends and relatives. Let’s understand the relevance of social media apps with new era businesses.

Here are the few benefits that emerging bootstrappers can leverage with social media app development:

Easier To Reach Your Targeted Audience!

100% enables you to reach your targeted audience as you can use multiple search filters with the choice of location, gender, age, category, interest and more while posting any content on the app.

Build a Direct and Deeper Relationship With Community!

Customised social media apps can help in creating direct yet deeper relationships and interactions with users. Unlike traditional marketing campaigns, App owners can directly interact with the community and are able to make alterations in their services or products as per the user’s needs.

Get Access To Full Data And Keep Users Engaged With Your Content!

Get a deeper understanding of your community as existing social media platforms only give access to a fraction of your communities’ data. But when you are the owner of your own app, you will have the power to access full data and understand the interest areas of the people and what kind of social activity they like the most in your app which is a great lead for your business.

Bring Better Monetization Opportunities!

Social Media apps can make better profits by providing the opportunity to build digital marketplace options right under your app.

Say Goodbye to Clever Algorithms!

When promoting services on your own app, you don’t need to chase those tricky algorithms to navigate or news feed spaces to fight for. It’s simply you and your audience. All you need is to hire an app developer to create a full-featured app. But when you promote it on existing social media platforms, there are thousands of people, creators and businesses to compete with and algorithms would decide which content to show and to who.

Stay Connected With the Users 24/7!

A Social media app development builds a channel for your business to promote a certain product, brand or services and stay in touch with the users 24/7.

Create a Private Community of Users!

It’s a unique yet leading opportunity to get brand enthusiasts together at your app and form a circle with like-minded people.

In the nutshell: By creating your own social network, you will free yourself from all such hassles and grab the attention of the users of your products and services. By hiring a software developer you can easily create a social media app that can add an edge to your business. But the central question is, what type of app should you build for your business?

Let’s answer this question in the next section!

What Type of Social Media App You Should Develop and Its Cost?

Before you head straight into the app development process, let’s first understand what are the major categories that all social media apps are divided into. This will help you decide which app type will better suit your business goals and bridge the gap between your success.

Here are the common types of social media app that you need to know about:

Media Sharing Networks: This is a type of app or a platform that allows users to share all types of media files including photos, videos, GIF files and more. The perfect examples to consider are Vimeo, YouTube, Snapchat, or Instagram. An average cost of app development can be $15,000 to $25,000+, depending upon the features and functionalities of the app.

Social network Apps: Applications like Facebook, Twitter or Linkedin are the perfect examples of personal and professional social networking apps. And with millions of monthly active users of Facebook are strongly influencing business to hire mobile app developer to build this type of application. The estimated cost of app development can be starting from $20k+ and go to any expensive price.

Networks for Consumer Reviews: Such types of applications are used for customers to verify reviews and ratings of the businesses they’ve had experience with. And Yelp is one of the most prominent examples of this category. The average price of creating this type of app can be starting from $15,000 to $18,000+.

Community and Discussion Forums: This can be a great platform where like-minded people come to the same platform for asking questions, receiving answers, sharing news, ideas, insights and experiences. Quora and Reddit are the titans in this category of social media apps. The starting price to build this type of app can be ranging from $15000 to $20,000+.

Blogging and Publishing Platform: Applications like Medium can be a perfect example of this type of social media app that allows users to create their blogs and publish their content and update them about any technology, services, products or anything every day. The average budget you need to build for this app can’t be starting from $12,000 to $15,000+.

As you notice, there are many types of social media applications available to choose from. And there are various applications that are leading in their domain. So no matter what type of app category you choose to develop, it is important that you hire the best app development company that can translate your app idea into a robust, scalable and flexible digital solution under your limited budget.

Tech Stacks That You Can Choose For Cross-Platform Social Media App Development(iOS/ Android)

If you are interested in creating a social media application that runs on multiple platforms but behaves like native apps, then here are the few tech stacks including Flutter, React Native, and more that you can use to build a cross-platform application

Since we are aiming to build a social media app with React Native, so are here some tools that you can use to build a full-featured social media app.

Since React Native is one of the top choices of the framework, though most IT companies are inclining towards React Native for mobile app development solutions. RN is not only easy to fit the budget but also empower developers with a choice of features that make the development process 10x faster and cut down the impactful amount on the budget.

Now the question is, what kind of modern trends can you integrate into the social media application while developing with React Native?

Current Trends That You Can Integrate Into Your Social Media App

With each passing year, the Social Media platform brings the latest trends to keep their users engaged and addicted to their applications. For some of us, this topic seems to be threadbare, but it is not in fact because developing an application with old tired and boring functionalities will only bring failure.

So here are the few trends to watch out for before you hire a cross-platform app developer for building an outstanding social media application:

Artificial Intelligence and Chatbots

AI and chatbots are playing a major role when it comes to delivering excellent customer supper on social media apps. Businesses running over social media can use AI-powered chatbots to answer the queries of their customers in no time. AI-integrated chatbots can efficiently conduct conversations with the consumers and provide them with the solution right away by understanding their query.

AR Powered Face Filters

Like Snapchat and Instagram, you can also be a trendsetter in the field of social media apps. People still enjoy the funny filters and visit the app frequently to check new updates every day.

Live Video Streaming

This could be the most engaging and attractive feature of your social media app as users love watching live videos or reading comments below on social media.

Uploading Video Content

In comparison to Images, video content is most popular to attract the attention of the users and keep them stay on your post for longer. But since everyone has a fast-paced life, keep it simple, short, and engaging to make your users come back to your post regularly.

24 Hours Story Post

It is a type of trend that allows you to post a story that stays live for 24 hours on your feed like Instagram and after that, it will disappear. People widely use this feature to keep their friends updated with their everyday life activities.

Sharing and Publishing Content On Social Media

While more and more people want to share their content with as many users as possible. Though it’s good to provide your users a platform whether they can publish the content and expand the user base.

Once you have decided upon your niche and learned trends to follow to make a social media app, it’s time to hire cross-platform app developer and check out the rough estimation of time and cost it will take for your project.

As of now, you know the trends that you can integrate to make your social media app working smoothly. But what are the basic features that should have in your app in case you are going with MVP?

So here, we have jotted down the list of must-have features along with the estimated development cost for iOS and Android.

Must-Have Feature For When You Create a Social media App

Building a social media app is no simple task. So to make your app function, you may be wondering what features you should consider for an internet-based network?

So here’s an infographic list of features along with the estimated development cost and time that it will take to develop. To make it develop, all you need is to hire an app developer experienced in customizing social media apps with detailed features.

Note: These are the rough estimations mentioned here by simply evaluating the market survey reports. The real estimations can vary according to the needs of the businesses and the type of application you are interested in building.

How To Build a Social Media App in 2021?

While building a social media app is a complex task that requires pro skills to code this app idea. So it is worth that the coding job will be left with the expert app development company and you’ll focus on aspects of building an app for iOS and Android. To make sure that reading this section will actually be worth it, therefore we are keeping things simple and help you assist in setting all the things before the developers get their hands on the project.

Now is the time to find out all the ins and outs of making a social app for Android and iOS:

Phase 1: General Information on Social Media App Development For Android/iOS

Architecture: MVVM (Model-View-viewmodel) architecture pattern.

MVVM (Model-View-viewmodel) architecture pattern. Programming Language: Earlier we used only Java to develop Android native but now developers are using Kotlin for new projects. For iOS Native apps, you can use Swift or Object C++. But to make it work on the web, Android and iOS with the same programming language, you can choose React Native for the app development.

Earlier we used only Java to develop Android native but now developers are using Kotlin for new projects. For iOS Native apps, you can use Swift or Object C++. But to make it work on the web, Android and iOS with the same programming language, you can choose React Native for the app development. Framework: In the case of Android, Google Play Services are mainly used as a Framework as it provides the access to a wide choice of Google Services including in-app purchase, Geolocation, Cloud Messages, Firebase and more. For iOS, Dip Framework is used as a service locator.

In the case of Android, Google Play Services are mainly used as a Framework as it provides the access to a wide choice of Google Services including in-app purchase, Geolocation, Cloud Messages, Firebase and more. For iOS, Dip Framework is used as a service locator. Library: RxJava2 and RxSwift for Android and iOS respectively.

Phase 2: Technology Stacks to Be Used on UI layer of the App

RecyclerView: It is a native Android component that is used to show scrollable content in the form of a list to a user and allow interactions with items on the list. Whereas, DTTableViewManager/DTCollectionViewManager are used for building type-safe table views and collection views in iOS apps.

It is a native Android component that is used to show scrollable content in the form of a list to a user and allow interactions with items on the list. Whereas, DTTableViewManager/DTCollectionViewManager are used for building type-safe table views and collection views in iOS apps. Fragments: It’s a native Android container that contains other views and widgets and they have their own lifecycle. And in iOS, LoadableViews are used for creating reusable view components.

Phase 3: Technical Implementations are Required For the Network Layer

For Android Native Apps

Retrofit for network requests

GSON for JSON parsing

Glide for loading images and caching

For iOS Native Apps:

TRON/Alamofire for building network abstraction.

Codable/SwiftyJSON for parsing JSON responses.

AlamofireImage for loading and caching images from the network.

How Much Does It Cost To Create a Social Media App?

Now when you understand the important technical things that are required for social media app development, let’s speak about the cost. No matter whether you are a startup or an entrepreneur, the financial side always matters to you!

So, we will provide you with the calculation of three app versions to help you decide which app version will best suit your business needs and budget:

1: Basic Version, is the best option for the first app version that can be launched with the basic functions and feature set.

2: Full Product with advanced features and additional functionals that make your app fit the growing needs of the market.

3: Modern Large App with complex structure and features to add a competitive edge to your product.

Note: Before you hire a mobile app development company, it is vital to understand that these price estimates are based on market surveys. It can differ from company to company and developer to developer.

But the main question still remains unanswered: how will you make money from the app?

How To Monetize Your Social Media App in 2021?

If you are wondering that how can you monetize your social media app and make a good profit from your investment, then here are the few strategies that you can try to achieve this:

Paid Subscriptions: As long as your content serves the real value to the community, members will be happy to pay for a subscription. You can offer a free tier with good content and for in-depth and high valued content, you can offer a paid tier. In addition, Apple and Google Payment systems make subscribing easier and simpler.

As long as your content serves the real value to the community, members will be happy to pay for a subscription. You can offer a free tier with good content and for in-depth and high valued content, you can offer a paid tier. In addition, Apple and Google Payment systems make subscribing easier and simpler. Physical Purchase: From innovative face filters to cool merchandise with snappy slogans, you can sell direct stock to the users. So it can be a great way to use this platform to earn money.

From innovative face filters to cool merchandise with snappy slogans, you can sell direct stock to the users. So it can be a great way to use this platform to earn money. Sponsored Content: If your app is leveraging great community members, then you can provide opportunities to other businesses to reach them out with the paid sponsorship program.

If your app is leveraging great community members, then you can provide opportunities to other businesses to reach them out with the paid sponsorship program. Events and Activities: Promote events and online experience through your community platform with the paid ticketing and start earning the money.

Let’s sum up this blog!

Conclusion

So here we are ending this blog! Still, if you are sitting and thinking that it will take a hefty budget and ages to complete the project, then we recommend you to hire a mobile app development company that always comes up with the best alternatives. Still, we have covered everything you need to know about building a social media app right from the scratch- the technicalities, features, functionalities, costing, monetizing strategies and more. We try to help you get started with the digital connection with no strings attached anywhere.

If you have any doubt anywhere, then you can drop a query in the comment box! Experts will answer it with their expert advice.

More content at plainenglish.io",20096
"#1 Timing

Some of you might argue that idea or money is the most essential for a startup’s success but according to Bill Gross’s research, timing is one of the most important attributes for any startup success as it accounted for a whopping 42% of the difference between success and failure.

“The timing is the most important key element for the success of any startup, idea, or a business” — Bill Gross

We all know, not every moment is perfect to enter the market. Choosing the perfect timing is like choosing the best moment for taking off because one really needs to figure out if the market is ready for his/her idea or whether it is too way too early.

For instance, when YouTube came into the market, there were already dozens of video-streaming platforms, but because YouTube was launched at a perfect moment — after high-speed Internet became the norm — it became a major success.

Airbnb also had its timing on its side as it was launched during the recession when people needed extra cash. And coming to Uber, it came into the market when drivers were looking to pick up some extra money to supplement their income.

These companies took off because they were launched when the time was right — when the people were ready to accept and adapt their technologies.

“The best way to really assess timing is to really look at whether consumers are really ready for what you have to offer them” says Bill Gross.

Choosing the right time to launch a product or service makes sure that the market is ready and willing to use it, and consumers can recognize what they are being offered as a solution to their problem.",1622
"API 📗 Swagger

In this article, we will learn how to document our already existing APIs with .Net and.Net Core. Many developers absolutely deplore writing documentation. It’s boring to put all these things together in a document by manually writing them. To overcome this we have Swagger to implement documentation and it’s an open-source API documentation tool.

Packages required to configure Swagger,

Swashbuckle.AspNetCore (latest version) — This package adds Swagger and Swagger UI and other libraries to make it easy for us to create API documentation.

Step 1

After installing the package to the respective project, here I am sharing the screenshot for your reference if you are new to Swagger in ASP.Net Core

Fig-1

Step 2 -Setup

We need to enable our project to generate XML comments. The comments come from Triple-slash (///) comments throughout the code.

First, in the project properties, check the box labeled “Generate XML Documentation”

Right Click on the Solution and click on the Properties

Fig-2

You will probably also want to suppress warning 1591, which will now give warnings about any method, class, or field that doesn’t have triple-slash comments.

Fig-3

Step 3 -Configure Swagger

Startup.cs

launchSettings.json

XML Comments

In our XML Comments for methods.

WeatherController.cs

Here are XML nodes in use:

Summary: A high-level summary of what our method /class/field is or does.

remarks: Additional detail about the method/class/field

param: A parameter to the method, and what it represents

returns: A description of what the method returns.

Output — View Swagger

Fig-4

Here is a clear description of what is what in Swagger UI

Fig-5

Thank you for reading, please let me know your questions, thoughts, or feedback in the comments section. I appreciate your feedback and encouragement.

Move your project on next level by hiring skilled and seasoned ASP.NET core developers.

Keep Learning and Exploring…. !!!",1957
"Windows 10 for Maximum Productivity: The Ultimate Guide

How to customize your install and choose the best add-ons to reorganize Windows 10 for utmost efficiency Mario Chamorro Jun 4·15 min read

Photo by Tadas Sar on Unsplash.

Windows 10 is great and it has really improved a lot from previous versions. But some default features prioritize ease of use or visual appeal instead of productivity. Luckily, with some improvements, you can get the ultimate Windows 10 productivity machine.

Table of Contents

· Taskbar and Desktop

∘ Reduce the taskbar size for increased working space

∘ Don’t merge elements for moving faster between apps and open files

∘ Don’t use the desktop for file management to avoid chaos

∘ Disable notifications for better focus

∘ Activate clipboard history

∘ Hide Cortana

∘ Other recommendations

· Windows File Explorer

∘ Use Windows Explorer Quick Access to efficiently browse folders and files

∘ Set Downloads as default opening folder to save time

∘ Set “Details view” as default for better file management

∘ Use Windows PowerToys for light-speed file management

∘ Windows Explorer shortcuts you must learn

· Hardware configuration

∘ Increase mouse/trackpad sensitivity for faster work

∘ Disable the FN key to work properly

· Productivity software

∘ DisplayFusion for multi-screen management

∘ Snagit for screenshots

∘ RoamResearch for notes management

∘ Other notes management tools: Evernote, Notion, and OneNote

∘ TextExpander for faster typing

∘ Logitech G-Hub to be productive with your mouse

∘ Ccleaner for Windows maintenance

· Chrome

∘ Set up your bookmarks efficiently

∘ Install productivity extensions

∘ Assign shortcuts to extensions

Taskbar and Desktop

Reduce the taskbar size for increased working space

The taskbar is the small bar usually located at the bottom of the screen where all the active and most used (pinned) apps are shown. It is one of the most important and most used Windows features and it is essential to quickly move between apps, folders, and any content you are using.

Unfortunately, since Windows 7, Microsoft decided that the taskbar should be three times bigger than it should be, and most of the users don’t care to change it or don’t know how to reduce its size. As a consequence, they spend their life with a huge ugly bar that takes about 10%-15% of the limited screen size (even worse for laptops, and even even worse for widescreen laptops) and doesn’t offer any advantage.

Also, with the default XXL taskbar, icons are huge but only one icon per app is shown. This means that, with the standard 5–7 pinned icons and open apps, most of the taskbar will be empty. Totally wasted precious productive space.

To fix that, you must reduce the taskbar size and you will have a little functional and efficient taskbar that only takes about 3% of the screen size, with the remaining 97% of the space for you and your productive active windows. To fix the taskbar:

Right-click in the taskbar > Taskbar settings > Use small buttons

Don’t merge elements so you can move faster between apps and open files

You also want to be able to see all the open files at the same time. To do that:

Right-click in the taskbar > Taskbar settings > Combine buttons: When taskbar is full

This way, the taskbar will be reduced and icons won’t be combined unless there is no more available space (something that rarely happens if you use an external display higher than 24 inches).

You don’t want to combine buttons. If you do so when you have two Windows Explorer windows or two Excel files open at the same time, only one icon will be shown and you will need to first click on the icon, and then locate the file you want to open. Much less efficient than quickly seeing all the open tabs and simply making one click on the desired one.

Don’t use the desktop for file management to avoid chaos

Nothing feels more frustrating and distracting than a messy desktop with thousands of files.

Simply compare this:",3993
"The Dichotomy of Silicon Valley

“It is no measure of health to be well-adjusted to a profoundly sick society.”

― J. Krishnamurti sumarius Jul 15·6 min read

Over the past ten years, Silicon Valley has infamously been a dreamer’s haven. A place where seemingly impossible ideas came to life in a matter of years or even months. Anyone could do it, all it took was an idea, a laptop, a strong belief and a shabby chic living room to affect change and disrupt an industry. Everyone, investors and employees alike, put their time and money to get in early and be a part of the next Google. Millions of dollars were invested in silly apps like Yo, only to disappear without a trace after a few months. Movies like the internship only cemented what a capitalist fairytale would look like to an average middle class parent. Why focus on reality when the promise of a unicorn exists?

We fell for this fantasy too. As young graduates, we weren’t interested in the ugly truth of life. We were more than willing to shell out half of our paycheck to live in a tiny 1 bedroom for a chance at this modern roulette. The allure of becoming a visionary is what got us hooked on Silicon Valley. We idolized leaders like Steve Jobs who convinced the market that they needed a smartphone and in doing that, completely changed the tech landscape. He got ‘it’ while the old suits didn’t. We loved him for his defiance to authority and the status quo. We were not the only ones who put him up on a pedestal. Silicon Valley built a mythology around his story and elevated the template that Jobs created.

The public was hungry for a narrative like this in the late 2000s. The country had just experienced the worst man-made financial crisis and people were losing trust in the financial system. Greed was aspirational in the 90s but the 2010s needed a heart. Silicon Valley presented that change. Companies were focused on changing the world and making it a better place to live. ‘Do no evil’, proclaimed the biggest giant in sharp contrast to Wall Street’s “Greed is good”.

Ten years later, the valley has lost some of its luster. The honeymoon period is coming to a close and we are now faced with having to make the marriage work. Interestingly, the pundits have been crying for the end of Silicon Valley for a while now but the party never seems to end even if the party goers are burned out, sick and addicted to substances. How did the valley sell us on the promise of a unicorn? As longtime residents, we can shed some light on the strange addiction and the dichotomy that exists.

Health vs wealth — the allure of an exit

We came to Silicon Valley and joined a hyper-growth startup. We immediately started spending 80+ hours a week delivering on key asks. However, despite the long hours, the work was fun and the chaos was addictive. We were no longer in a boring, corporate job. We had a chance to be a part of the next big IPO, a badge of honor in the Valley. The crazy hours were reinforced by rewards. At the end of the quarter, we learned how much closer the company was to raising the next round of funding while glossing over or omitting challenges the company faced in growth.

The effects of it, we left our health at the altar for the greater good, not of society but that of the company we work for. We put in crazy hours hoping for a payout and ignored the effects of the mounting stress on our mental and physical health. For example, waking up every morning to a veggie smoothie and a run and ending the night with four drinks after a long, stressful work day. Ultimately, we made choices to superficially take care of our health while, in parallel, ruining it — in other words, living our lives as a dichotomy.

The Infantilized Engineer

It is hard to distinguish Google’s campus from a nursery for grown ups. Employees are treated like children in the Valley with a fun office and exciting toys to engage our creativity but much like children, it also meant painting the world around us in the most positive, simplistic light — hiding problems from us. Let’s examine the ever increasing presence of the so-called listening parties — a modern way for the big bosses to pacify their employees. Everyone gets a chance to air their grievances and be heard. You can almost be convinced that companies care about our concerns and well-being up until when it actually matters.

In reality, this is just to protect the company’s interest. With all the PR efforts to promote women in technology, Google protected and paid Andy Rubin after sexual conduct allegations. From a business perspective, it made sense. The victim (a woman) in question was not a key hire. A town-hall was organized to soothe the anger. The employees were heard. Everyone promptly moved on to the next hot topic like children on their best behavior.

Nothing about this is particularly surprising. There is something cathartic about airing out concerns to an authority figure. Many religions have formal rituals surrounding this activity. It makes us feel like we are solving the problem. That illusion of progress is addictive even if it is just an illusion. What we forget to focus on is whether there is follow through.

The promise of changing the world

Millennials joined the workforce around the 2008 crisis when there was mounting criticism of Wall Street. Silicon Valley smartly marketed itself as the protagonist to the next generation of dreamers.

The valley felt like a place to achieve a higher level of consciousness. Rather than a stoic industry with a lot of red tape, we got to be part of something that was constantly at the brink of innovation. The novel business model of online advertisements made information free and abundant. The ever changing twitter feed made the morning newspaper obsolete. Nobody cared about what you wore or what car you drove because people were busy working on more impactful projects that were solving real problems.

This mindset continues to permeate the fabric of Silicon Valley to this day. There is still chatter on how we are morally superior to Wall Street. When faced with criticism, we remind the detractors that we freed the world’s information and birthed the iPhone.

We refuse to face the reality that we are hitting the messy and complicated part of human nature. Integrating technology with the more sacred aspects of humanity requires careful consideration of its impact on the community — the antithesis of the “move fast and break things” philosophy that worked so well for many years. As an industry, we shy away from answering these difficult questions because the nuanced challenges we are facing cannot be disrupted with a neat algorithm. Or maybe because it conflicts with the ability to make money quickly. The dichotomy of Silicon Valley is people are unwilling to see through the bullshit when it is coated in gold just like our Wall Street brethren.

Conclusion

Money and success have changed what the valley is but as an industry, we continue to perpetuate a vision of utopia. Employees compromise every aspect of their lives in hopes of hitting the jackpot — a successful exit or stumbling on the next Google. Companies prey on idealistic graduates by promising a chance to change the world for the better. In reality, the social good initiatives are often just PR stunts. As an industry, we continue to market our contributions to humanity. However, when faced with real decisions we shy away from taking any meaningful steps under the pretense of being just a technology platform.

Silicon Valley is not dead. It has afforded companies the chance to push boundaries and question norms for so many years. There is an immense amount of novel ideas, talent to execute on these ideas and money to fund these ideas. However, we must learn to separate fact from fiction. It’s time to realize that a horse painted white with a horn is not a unicorn.",7896
"How to focus on digital transactions and keep customers away from cash withdrawal Markswebb Team Follow Aug 27 · 6 min read

It is hard to motivate customers to carry out more non-cash transactions especially if the bank’s geography historically shows vast cash circulation. To do so, it’s important to look into detail of the bank’s processes, explore customers’ journeys and their behaviors, and review the functionality and convenience of online channels. However, the most insightful tool is in-depth customer research.

In this article, we show how we solved that kind of task for a large Russian bank.

The in-depth interviews and the field research are the sourcessource of insights

The primary samples of customers are designed with formal indicators we got from the bank’s internal systems:

Customers who instantly withdraw all the money they got paid with. Customers who actively use a bank card for purchases. Customers who applied for a card themselves (not for salary). Customers who make all of their payments in a mobile banking app. Customers who make their payments with the internet-banking instead of the mobile banking app. Customers who lately applied for a loan or a credit card.

Conversations with clients helped us find out how they keep and handle money, transfer it and make purchases. The very first interviews showed the link between how customers get money and how they spend it. Customers don’t seek to replenish their cards or withdraw cash with no significant need; and they prefer spending money the way they get it.

The results of the interviews were supplemented with our field research: we made purchases in local stores and public eating places, had a chat with retailers, made payments and money transfers, used digital services, and observed customers and the bank’s staff in its branches.

Why do people still use cash

The vast cash circulation can be explained by the nature of the region where a lot of small and micro businesses are represented. These businesses prefer using cash: taxi drivers, private tour guides, small vendors, street sellers, etc. It can also be explained by the lack of habitual digital services as the region is poorly developed digitally. The bank can affect most of these factors only indirectly by teaching customers and forming new digital habits.

People store cash because there are a lot of places that only accept this type of payment. The travelers withdraw cash in advance, and the locals don’t seek to replenish their bank cards as they don’t want to take extra action with money. The interest on the deposit doesn’t provide any benefits and cannot motivate people to store money in a bank account as most people live paycheck to paycheck and don’t have much money. A lot of customers don’t know that non-cash payments are easy and convenient. Financial institutions refuse to tell people about the possibilities and convenience of digital services, and they don’t use onboarding within digital services. Customers don’t understand the profit provided by the loyalty program as the bank explains its details very poorly, the discounts can’t be accumulated, and the amount of cashback isn’t that big. Customers can only wonder how much they can save. Small businesses aren’t interested in cards acquiring and using POS -terminals. The free-of-charge acquiring conditions are very high, and this type of payment and its pros are alien to entrepreneurs as customers continue using cash. Customers aren’t used to transferring money to friends as it’s easier and more convenient to exchange cash. Some people even hesitate and think that money transfers can attract too much attention from the bank and the tax authorities. Furthermore, wire transfers take more time than exchanging cash. The bank doesn’t make it clear for customers that bank cards can be used for online shopping. Usually, customers get cards from other banks which they can surely use for online purchases. Customers don’t think that the mobile banking app is easy and convenient to use for regular tasks. For example, one can’t fill in the description of the money transfer or add comments for recipients. Some important services aren’t available in digital channels. Online payments and services are a crucial driver for the use of digital banking but customers couldn’t use these services because of some UX problems.

The audience thresholds for using non-cash payments remain because of the region’s nature, low financial literacy and inconvenient digital banking services.

Framework for digital transactions’ growth

To retain customers from withdrawing cash and motivate them to use digital channels, we needed to solve 3 global tasks:

Make more payment features, and make payments easier and more convenient — for those who already use digital channels.

Make it possible to switch to digital channels, increase motivation and lower the entry thresholds — for those who prefer using cash.

Establish conditions throughout the region to involve as many participants ininto digital transactions as possible.

Here are some particular initiatives that banks can take:",5122
"Thank you to the 6 industry experts (and moderator Nina Muhleisen!) for sharing your hot predictions for tech in the next 3–5 years.

A Top Tech Trends Debate 2021 event report

Avion has always been a forward-thinking agency, passionate about emerging trends. For two years, I acted on the board of The Churchill Club — a collective dedicated to discussions about technology with strong ties to Silicon Valley.

I stepped down from my board position in 2019 to move to the US but have kept an interest in its events. The Churchill Club hosted its annual flagship event last night, The Top Tech Trends Debate. The premise? A panel of experts fight it out to decide what will be the next big thing in tech in 3–5 years, and the audience votes on a winner. Curious? Let me give you an overview of what was said, along with a few personal insights.

2021 tech trend: Higher education transformation

By panellist 1: Jodie Imam, co-founder and COO of Tractor Ventures

Jodie believes that in the next 3–5 years, future students will have more opportunities to engage in hybrid models of learning that are recognised by industry. They will offer an immersive mix of digital content and in-person teaching, and improve access to education in poorer nations.

Jodie backs this trend with a statement by Michael Batko, CEO of Startmate, who says:

“Education is still a one size fits all model that trains future generations to be a cog in the wheel of the old economy.”

Research shows the best learning is mostly ‘by doing’, followed by coursework and mentoring. People are already pursuing their own professional development with fast-growing platforms like Masterclass, featuring Gordon Ramsay and James Cameron as teachers.

My thoughts: Disagree

I love Jodie’s argument as a top tech trend, however I disagree that it will be the next big thing. I feel shaking up the Australian education system has been brewing for many years-in fact, this theme was the winning argument at the Top Tech Trends Debate in 2017 ( see the PWC wrap-up) — yet traditional institutions are still lagging to catch up. Further, while people are hungry to co-create curriculum and design their own future, it’s going to take a lot more than 3–5 years for some industries (for example, medical) to recognise certifications outside the university system.

2021 tech trend: Voice interfaces

By panellist 2: Gavin Appel, founder, Ignition Lane

Gavin believes the way we are interacting with voice technology is changing, and that voice interfaces are the next frontier. The market for voice assistants will double to 8.4 billion by 2024 and he says they will disrupt every industry like the smartphone has done before. Beyond the home, we will see voice used in medical practices to speed up reporting.

We’ll also see it closing cross-border business deals, where leaders will speak different languages but understand each other in real time, thanks to Google closed captions or Skype automatic translation.

Gavin’s reasoning that voice interfaces will explode in the next 3–5 years? It’s already a natural mode of communication. Using voice is instinctive, it’s not a new experience. Voice is a natural behaviour-it’s already second nature and therefore can be adopted quickly and easily.

My thoughts: Disagree

I do not believe that voice will explode in 3–5 years. It’s already here. While its application and use has accelerated dramatically, there are still technical challenges around natural language programming to overcome before it becomes a fluid part of our everyday lives. Yes, voice is already incredibly competent at booking an appointment, but it still lacks the intelligence to see through sarcasm and contextual awareness to understand user intent. You can learn more in my 2019 LinkedIn article: 5 things copywriters needs to know about Ai and conversation design.

2021 tech trend: SME technology

By panellist 3: Kate Pounder, CEO of Technology Council of Australia

Kate believes the next big thing in tech will be ‘SME tech’-defined as tech solutions that address pain points for small to medium businesses. Well known examples of SME tech companies from Australia and New Zealand include Canva, Xero, and MYOB.

The reason Kate thinks this will explode in 3–5 years is COVID has forced this market group to become more digitally literate. And SME tech is now more accessible thanks to cloud-based products; before we only had ‘sexy tech’ that wasn’t made for this sector.

Kate says 98% of people in Australia work for SMEs. Her own research shows if SMEs started using more than one app, this would inject $7-billion dollars in the economy.

It would also have a positive social impact by helping automate the mundane, allowing SME workers to have better work-life balance.

My thoughts: Agree

As a SME owner, I am constantly relying on cloud-based tools to run my business-and the number that are available to me continue to grow by the day. Anecdotally, I’ve seen COVID result in many talented professionals rethinking their careers, and whether by redundancy or personal choice, they are leaving corporate companies in droves and starting their own businesses too. This widens the market even more and I think there is a lot of potential for SME tech to become more useful and more integrated, driving socioeconomic change.

2021 tech trend: Going off-grid

By panellist 4: Reeta Dhar, Former National Head of Emerging Industries Team at Westpac

Reeta believes that citizens will be flocking off-grid to achieve total energy independence. Why? Because we are tired of living in the same economic system that hinders positive change and has let us down time and time again.

Reeta cites the COVID toilet paper saga as an example of how highly centralised systems perform under stress.

Due to growing mistrust in government combined with the crisis of climate change, Reeta says in 3–5 years we will have had enough and want to take more control-starting with the freedom to store and discharge electricity through means like solar panels and electric vehicles. Such technology has developed rapidly and is now more accessible, ready to scale.

My thoughts: Agree

Solar panels and electric vehicles are not exactly new, but I believe Reeta makes a very good point about such technology becoming more readily available. Furthermore, younger generations have a feeling of urgency of not only wanting, but needing to do something about it because of the government’s continual incompetence. Plus, the relatively new concept of storing and sharing energy within communities is better understood. Some panellists argued otherwise-that going off-grid isn’t achievable for lower socioeconomic groups-but I still agree with Reeta that in the next 3–5 years, people will start taking energy into their own hands where they can.

2021 tech trend: Proteomics

By panellist 5: Dr Andrew Webb, Head of Proteomics Research Laboratory (Walter and Eliza Hall Institute of Medical Research) and CSO/cofounder at Mass Dynamics

Andrew believes that the science of proteomics will transform our knowledge of disease and shape the future of personalised medicine.

But what exactly is proteomics? It’s the study of the 23 million proteins in every cell, which has the potential to take personalised healthcare to a whole new level.

In 2004, we unlocked the entire human genome, and this has become a gateway towards learning so much more.

We’re starting to understand biological differences in a whole new light and proteomics will change the game of how we diagnose and treat disease. Andrew says $1.4 billion has already been invested in proteomics in the US, excluding academic initiatives, and this means we’ll start seeing a lot more impact from its research in the public domain.

My thoughts: Disagree

While there’s no dispute that proteomics research is progressing at lightning speed, I believe the adoption and acceptance of personalised medicine is much farther away than 3–5 years. Hesitation with COVID vaccines has proven that labs can move fast, but people’s mindset cannot. In addition, personalised medicine requires centralised records. In Australia, My Health Records was launched in 2009 with more controversy than uptake despite its positive intent.

2021 tech trend: Augmented intelligence and democracy

By panellist 6: Barbara Sharp, CEO of Plaetos Group

Barbara believes that augmented intelligence is set to disrupt democracy for the better. How? By empowering policy makers to make more informed decisions, faster. Rather than working solo, people and AI will come together in a human-centred partnership, where we have the power to reduce reporting time from 7 hours to 7 minutes-something that Barbara’s company Plaetos is already doing.

According to Gartner research, ‘support augmentation’ will surpass all other AI initiatives by 2030.

As such, organisations like the G7 and OECD will be able to make decisions based on richer sets of information that include both scientific data and social data (like sentiment), leading to better outcomes.

My thoughts: Disagree

I’m not across the research to agree or disagree with Barbara here, but my instinct tells me that no matter how smart AI will get, policy makers will always want to be in charge. As Reeta explained in her argument, there are too many egos in government that often leave policy makers stuck in a ‘juvenile sandpit’. Look at the way they’ve struggled to juggle health and science advice during COVID. Is it really the amount or speed of information that’s the problem? I would love to think AI will disrupt democracy for the better, but I’m unsure-particularly in a timeframe of 3–5 years.

The winner is… proteomics!

The Top Tech Trends Debate is an audience-driven event. After votes were cast, the winner was announced: Dr Andrew Webb on proteomics and personalised medicine. I had actually disagreed with this trend, but never argue with a scientist, I suppose! Thank you for organising the event, Churchill Club. I look forward to reflecting on all six topics in 3–5 years’ time.",10074
"Part 9: Meet Ranjith Kumar

Welcome to week 9! We’re one post away from our final Employee Spotlight, but it’s not the end yet!

Last week you met Pole, and on week 5 you met Vidhya, who are both parts of the QA Team. This week you’ll meet our QA Team Lead, Ranjith, who shares a more in-depth conversation about his role and experiences.

He also gives valuable advice to anyone starting out in the industry!

Ranjith Kumar | QA Team Lead & QA Automation Engineer

“Aequilibrium is definitely different from any company I’ve worked for — there’s a very friendly environment.”

Ranjith showing some quality in his style too!

Tell us a bit about what you do at Aequilibrium.

I joined Aequilibrium in November 2020 as the QA Team Lead, checking the quality of applications. For example, we are creating a user and we have to think in multiple ways how we can test that page, such as positive and negative scenarios.

How was your first six months?

I’ve only been here for a bit more than 3 months. So far, all of my priorities are getting to know the automation and tools we use. How we can improve the tools of QA testing, for instance. I currently have four projects and my work is more focused on manual testing rather than automation testing.

What inspires you the most about your role?

What inspires me the most is the quality of the product. I believe this is an essential step of every project, it doesn’t matter what we’re working on. I like to keep a customer perspective in mind when I perform my work. I love what I do.

What is it like to work from home?

With the pandemic, I believe everyone should have a good work-life balance and take care of their health. Working from 9–5 works for everyone and at Aequilibrium our schedules are flexible depending on the situation. In my previous work, I worked remotely and it was all new to me, but I really got habituated when I joined Aequilibrium. I was already used to working from home. We can skip the traffic and spend some time travelling, so definitely some positive aspects.

Ranjith at the gondola in Banff, Alberta

Tell us about your career path in the professional services sector.

I was a basic QA and worked with multiple applications. Most of my work was contract and I focused on that for a while. It was really great working in different places and I learned a lot. For example, I was doing automation in Java and later I got the chance to learn C#. In every project, I learn something new and each project has a different learning curve. Once I joined Aequilibrium, I learned that I should have a ‘stopping point’ and grow from there.

What is different about working in the professional services sector?

Aequilibrium is definitely different from any company I’ve worked for. In other companies, I worked exclusively for that company, for their own projects. Here, I have the chance to work with different projects and clients, talk directly with the clients and get those projects done. There’s a lot of communication. The main difference is the client-facing aspect of the role, which is challenging but exciting. I’d be happy if in the future more people join the QA team and I get to lead them, although right now is also great.

What is different about working in the bank and finance sector?

When dealing with banks, there is a mathematical aspect of projects that we don’t see in other projects. Most of the financial services use test-driven development. There are more things to be tested to produce higher quality applications and software.

Ranjith enjoying a kayak day in Squamish, BC

What is great about your team?

As a QA, I work alone but I support Backbase engineers. If they have any questions they come to me and sometimes I use their help in testing applications. I had a lot of interaction with all team members in different teams and whenever I have a question they are very responsive and I’m very happy to have that. I felt a very friendly environment different from other organizations.

What advice would you give to someone who is starting their career in the bank and finance/professional services sector?

They should be focusing on the requirements they get from the developers and be specific on what they are testing. My advice is to understand the exact requirements for the testing. When we focus on those aspects it will be more profitable to the end customer.

Interested in joining the Aequilibrium team?

Check out our Careers Page and don’t forget to stay social with us!

LinkedIn | Twitter | Instagram | Facebook

Aequilibrium is a digital consultancy focused on product design, development, and systems integration services in Vancouver, BC.

Since 2012, we have helped companies create award-winning digital experiences to help them grow and compete effectively while minimizing risks associated with digital transformation.

We apply expertise in retail, financial services, and health sciences. Using design thinking, platform, and data-driven strategies with an Agile approach we design, build, and run scalable end-to-end solutions for clients across North America.",5094
"Data Security and IAM to Optimize your IT Security Sennovate Follow Jul 15 · 2 min read

Yes, it is important and critical to safeguard your IT security. Business assets reside in the form of data and are rendered vulnerable in the event of a breach. Data breaches are happening just like that, and organizations are rendered helpless.

Recently UniCredit, the bank and financial services organization revealed that the data breach had close to 3 million records that were exposed, revealing the names, telephone numbers, email addresses, and cities where clients were registered.

Such breaches are said to have an impact over social media, rather than any real serious banking transaction, however, it is paramount that a breach had occurred. It is interesting to note, that the UniCredit had made substantial investments in upgrading their IT security systems, yet the breach took place.

In Risk-Based Security research, 2019 Midyear QuickView Data Breach Report, states the first six months of 2019, had seen more than 3,800 publicly disclosed breaches exposing 4.1 billion compromised records.

Legacy of breaches

It is important to read along, that similar breaches occurred in July 2017 reason was data theft due to third party provider accessing data without authorization, and in the latter part of 2016 as well.

Then why breaches?

Shockingly, such breaches are not happening to poor budgets or lack of security awareness. It can be identified as:

Lack of proper security training

Un-authorized access to systems /services

Exposure of sensitive data

Insider actions — both malicious and accidental

In such contexts, one cannot blame the solution provider, it is the organization that must ensure such breaches do not occur, bring more security process during selecting a vendor and decide on what type and at what level information can be accessed. Greater hours of security training programs must be pushed for.

To know more and learn about IAM,

Have questions about finding an Identity and Access Management consultant?

Call +1 (925) 918–6618 the consultation is free.

About Sennovate

Sennovate delivers custom identity and access management solutions to businesses around the world. With global partners and a library of 1000+ integrations, we implement world-class cybersecurity solutions that save your company time and money. We offer a seamless experience with integration across all cloud applications, and a single price for product, implementation, and support. Have questions? The consultation is always free. Email hello@sennovate.com or call us at: +1 (925) 918–6618",2602
"Image Credit: Bamyx Technologies

Today’s web applications aren’t even close to being as quick and responsive as native desktop apps, but what if they could be? WebAssembly makes this commitment.

WebAssembly is a low-level assembly-like language with a small binary structure that operates in web browsers at near-native speeds. At the same time, WebAssembly provides a portable compilation target for programming languages like as C/C++, C#, Rust, Go, Kotlin, Swift, and others.

WebAssembly is supported by Google, Mozilla, Apple, and Microsoft in their browser engines, and is hailed as a means to increase online application speed while also allowing languages other than JavaScript to be utilized in the development of browser programs.

WebAssembly has spawned a slew of new technologies, including even new programming languages, that make use of its capabilities. The following are 12 language initiatives that have staked a significant amount of money on WebAssembly.

Binaryen

Binaryen is a WebAssembly compiler toolchain infrastructure library. Binaryen is a C++ library that makes compiling to WebAssembly simple, effective, and quick. WebAssembly is supported by Google, Mozilla, Apple, and Microsoft in their browser engines, and is hailed as a means to increase online application speed while also allowing languages other than JavaScript to be utilized in the development of browser programs.

WebAssembly has spawned a slew of new technologies, including even new programming languages, that make use of its capabilities. The following are 12 language initiatives that have staked a significant amount of money on WebAssembly.

Binaryen is a WebAssembly compiler toolchain infrastructure library. Binaryen is a C++ library that makes compiling to WebAssembly simple, effective, and quick.

Blazor WebAssembly

Blazor WebAssembly is a framework for creating interactive, client-side, single-page web apps in.NET and hosting them in modern browsers (including mobile browsers) using the WebAssembly runtime. There are no plug-ins or code recompilation into other languages necessary. The runtime allows.NET code to use WebAssembly’s JavaScript APIs to access browser functionality.

When a Blazor WebAssembly program is executed in the browser, C# code and Razor files are compiled into.NET assemblies, which are downloaded along with the.NET runtime to the browser. The.NET code is further protected against malicious operations on the client system since it is run in WebAssembly in the browser’s JavaScript sandbox. Blazor WebAssembly apps can be run on their own or with server-side assistance.

Cheerp

Cheerp is a web-based enterprise-grade C/C++ compiler from Leaning Technologies that compiles C and C++ up to C++ 17 into WebAssembly, JavaScript, or a combination of the two. Cheerp is built on top of the LLVM/Clang framework, with specific optimizations aimed at improving efficiency and reducing the size of the compiled output. Cheerp is mostly used to convert existing C/C++ libraries and applications to HTML5, but it may also be used to create web apps and WebAssembly components. Cheerp is available in both open source and commercial versions.

Cheerp is available for download at leaningtech.com.

CherpJ

This LLVM-based compiler, dubbed “the Java compiler for the web,” turns any Java client program into WebAssembly, JavaScript, and HTML, allowing them to operate in modern browsers. To access the DOM from Java, CheerpJ uses three components: an AOT (ahead-of-time) compiler, a runtime in WebAssembly and JavaScript, and JavaScript DOM interoperability APIs. The AOT compiler can be used to compile JAR archives with CheerpJ. CheerpJ does not necessitate any server-side assistance.

CheerpJ, like Cheerp, is a product of Leaning Technologies. It’s available for download at leaningtech.com.

Emscripten

This open source compiler toolchain converts C and C++, as well as any other language that uses LLVM compiler technology, to WebAssembly for web deployment, Node.js, or a Wasm runtime like Wasmer. (The Emscripten compiler, emcc, also produces JavaScript, which gives the built programs API support.)

Emscripten was used to convert a number of real-world codebases to WebAssembly, including commercial codebases like the Unreal Engine 4 game engine and the Unity 3D platform. C and C++ standard libraries, C++ exceptions, and OpenGL/WebGL graphics commands are all supported by Emscripten. The Emscripten SDK may be used on Linux, MacOS, and Windows to install the Emscripten toolchain (emcc, LLVM, and so on).

Forest

Forest is a WebAssembly-compiling functional programming language. Forest’s purpose, according to developer Nick Johnstone, is to provide a language that makes it easier to create big, interactive, and functioning online programs without the conventional overhead of that approach.

Forest has static typing, pattern matching, immutable data structures, numerous syntaxes, and automatic code formatting, and is currently defined as “pre-alpha, experimental, conceptual research software.” Elm and Haskell inspired the first syntax in development.

The Forest language’s design principles include ease of cooperation, as painless testing as possible, and agreement on structure and semantics while agreeing to differ on syntax. Forest is designed to be quick enough for constructing sophisticated games while also being “blazing fast” for conventional web apps, according to Johnstone.

Forest is available for download on GitHub.

Grain

The Grain language combines aspects from academic and functional languages to create a language for the twenty-first century, the project website states. Grain can run in the browser, on the server, and potentially anywhere because it is compiled to WebAssembly using the Binaryen toolchain and compiler infrastructure. There are no runtime type errors, and no type annotations are required. The Grain toolchain is a single binary that includes a CLI, compiler, runtime, and standard library. To build Grain from source, developers will need Node.js and Yarn, and binaries are available for Linux, MacOS, and Windows.

You can find instructions for getting started with Grain at grain-lang.org.

JWebAssembly

JWebAssembly is a Java bytecode to WebAssembly compiler by I-Net Software that accepts Java class files as input and outputs WebAssembly binary format (.wasm file) or text format (.wat file).

The goal is to use WebAssembly to run natively in the browser. JWebAssembly can theoretically compile any language that compiles to Java bytecode, including Clojure, Groovy, JRuby, Kotlin, and Scala.

JWebAssembly is not yet ready for production use. Although all of the requirements for the JWebAssembly 1.0 release have been met, testing still has to be completed. A Java bytecode parser, a test framework, and a Gradle plug-in are all in the plan for version 1.0. JWebAssembly 1.0 is expected to be released this year by I-Net Software.

JWebAssembly is available for download on GitHub.

Pyodide

Pyodide compiles Python and the Python scientific stack to WebAssembly, bringing the Python 3.8 runtime, NumPy, SciPy, Matplotlib, Scikit-learn, and dozens of other packages to the browser, which was just spun out from Mozilla. Pyodide allows Python to access web APIs and provides transparent object conversion between JavaScript and Python. Pyodide was launched in 2018 as part of the Iodide data science in a browser effort. Pyodide can be tested using a browser-based REPL.

Instructions for obtaining and using Pyodide can be found at pyodide.org.

TeaVM

TeaVM is a Java bytecode compiler that generates WebAssembly and JavaScript to operate on the web to run in the browser. It should be noted, however, that WebAssembly functionality is currently in beta. TeaVM, like its cousin GWT (Google Web Toolkit), allows programmers to design Java applications and deploy them as JavaScript. TeaVM, unlike GWT, operates with compiled class files rather than source code. TeaVM also uses existing compilers like javac, kotlinc, and scalac to build Kotlin and Scala code in addition to Java. TeaVM is primarily a web development tool; it is not intended for converting big Java or Kotlin codebases to JavaScript. Flavour, a TeaVM subproject, is a framework for creating single-page web apps.

On GitHub, you’ll find instructions for downloading and using TeaVM.

Uno Platform

Uno Platform is a UI platform for.NET teams to build single-codebase applications for WebAssembly, the web, Windows, MacOS, Linux, iOS, and Android using C# and XAML, as an alternative to the Xamarin mobile app platform. Uno uses the.NET 5 Mono-WASM runtime to run C# code in all major web browsers, and it also acts as a bridge for WinUI and UWP (Universal Windows Platform) apps to run natively on WebAssembly. Developers can use Visual Studio or Visual Studio Code to create web apps with Uno.

The Uno Platform website has instructions for getting started.

Wasmcloud

Wasmcloud is a Cosmonic application runtime that uses WebAssembly to create composable, portable apps that can be used in multi-cloud, edge, and browser environments. Security is handled by a WebAssembly sandbox, and an actor model separates business logic from specific underlying capabilities, according to the company. Developers can create microservices once in their preferred language and then deploy them anywhere. Rust, TinyGo, and AssemblyScript are among the languages currently supported. Wasmcloud has been accepted as a Sandbox project by the Cloud Native Computing Foundation (CNCF).

wasmCloud installation instructions can be found at wasmcloud.dev.

Don’t forget to give us your LIKES today",9660
"Earth, the year 2021, our planet is slowly dying from the exploitation and pollution we humans generate. At the time of writing this article as well as in every moment that follows, the fate of our planet and ourselves is at stake. There has been a lot of talk about this recently, but are we aware of what is going on right over our heads in this matter?

Not many people seem to be aware of what the situation in the earth’s orbit looks like at any given moment.

But let’s start from the beginning…

SOME HISTORY

The first artificial satellite sent into space was the Russian Sputnik, its task was to send telemetry data to Earth. Based on them, the density of the atmosphere was measured and the propagation of radio waves in the ionosphere was analyzed.

The whole event took place in October 1957. It was this event that started the Space Race between the United States and the Soviet Union.

Competition between the two countries lasted until 1975 during which time, twenty-four objects entered Earth orbit, twelve of which belonged to the United States, ten to the Soviet Union. The missing two were sent by the Canadians. This competiton has been ended by a joint manned mission by a Soviet-American ship called Soyuz-Apollo.

Since then, the number of objects sent into orbit has increased significantly from year to year. To date, the total number of all satellites sent into orbit is 9920, of which 6542 are still physically in space and only 3372 are fully functional. To compare, last year those numbers were around 5000 for all that are now in space and 2000 of which are fully funcitional…*

What does this prove? We have started to create another garbage dump in space and we have no intention of stopping…

WHAT EXACTLY IS SPACE JUNK?

As defined, junk is old or discarded articles that are considered useless or of little value.

Consequently, space junk is nothing more but unnecessary equipment and its debris left by man in space. Trash that was not worth bringing back to earth…

Space junk is seen in several different categories. They are usually distinguished by size. They can be small, millimeter-sized pieces of old equipment sent into space by man, but also entire satellites that have long been non-functional and are merely drifting in Earth’s orbit, slowly breaking apart to form more and more space junk of various sizes.

FILTH IN NUMBERS

If, after reading the first three parts of this article, you have begun to wonder what scale of the problem we are facing, below I bring you up to date on the current amount of space junk drifting in the orbit of our planet:

3372 operational satellites in Earth orbit

3170 dead satellites in Earth orbit

38,000 pieces of space junk larger than 10 centimeters

130 million pieces of space junk larger than 1 millimetre

17,500 mph, speed of every piece, roughly 10 times the speed of a bullet

500,000 pieces of space junk bigger than one centimeter

These numbers alone should arouse great concern in us, considering the fact that our adventure with the conquest of space began less than 65 years ago. No more and no less testifies on what scale we use – not only our own ecosystem, but also the outer space.

It may be immeasurable, and there may not be evidence of any other life beyond planet earth and in space, but that doesn’t give us the right to exploit these areas unlimitedly in the way we do now. Until we change our thinking, we will not change anything, ever.

THREATS AND HOW TO GET RID OF THEM

At the moment, space junk poses little danger to space exploration itself or to humanity living on earth, but the situation is changing every day.

Larger debris is colliding with each other and other satellites, creating more and more space junk. The whole phenomenon is called the Kessler Syndrome, the meaning of which is explained in the next paragraph.

Nevertheless, space debris is a big problem for space stations and satellites orbiting the Earth. To avoid collisions the path of most debris must be constantly controlled by specially appointed for this purpose NASA units or various external companies. And that causes another generation of costs.

Collisions themselves do not happen very often, currently it is a problem of a sporadic nature. The last case of collision occurred in 2009 when a larger piece of space debris collided with a satellite and led to its destruction (which of course led to the generation of another amount of space junk)

Fortunately, scientists and related outside companies, under the command of the United Nations, are working to try and solve the problem. Any company that owns dead satellites currently orbiting the earth has 25 years to get rid of them from space. Will this be a short enough period of time to prevent the dead satellites from turning into more space junk?

At the moment, two methods are being developed to fight debris in space. The first is to use a giant harpoon with a magnetic net that will catch the satellite and pull it down into the Earth’s atmosphere, burning it as it falls to the ground.

The second solution is to use a laser that will heat up the object to increase its atmospheric drag so that it falls out of orbit.

However, this does not change the fact that both methods are only effective for larger objects such as dead satellites. The problem of smaller debris which there are most in our orbit, is still an open question to be solved, but at the moment nothing promises to change.

KESSLER SYNDROME

In 1978, NASA scientist Donald Kessler came up with a thesis about a chain reaction caused by too much debris in space.

According to this thesis, more and more debris will collide with each other, creating new debris, until the Earth’s orbit becomes inaccessible to humans. This is a very disturbing but realistic prediction, which one day may come true.

THE SH*T GOT REAL

What would you say about 50,000 new giant satellites orbiting the Earth over the next few decades? That’s the idea being considered by two of the largest companies, Amazon and SpaceX, explaining it as a desire to make the Internet accessible anywhere on Earth. A seemingly noble action, but is it really necessary?

SpaceX’s Starlink satellites are among several planned mega constellations of satellites © SpaceX (CC BY-NC 2.0), via Flickr

50,000 new satellites around the earth will change into millions of new collision opportunities, making attempts to deal with space debris nearly impossible.

Prudent or frivolous? Decide for yourself.

WE STARTED WITH EARTH, WE WILL END WITH SPACE

In the previous paragraphs you could see with your own eyes how serious the problem of space debris is already, and how great the danger of its presence in space is, not only for the astronauts themselves, but also for our future and development.

The collision of even a small piece of space debris with a working satellite can lead to its complete destruction, and thus critically affect such fields of infrastructure as navigation, satellite communications, reconnaissance and national security.

It is also necessary to consider the undeniable fact that man is simply littering another space, this time beyond the planet. In a few hundred years we may be confronted with the fact that there will be a huge garbage dump in space, which will make it much more difficult for mankind to carry out space missions, explorations and reconnaissance. Are we really ready to slow down the development of our species in this way?",7465
"Why Redux Is More Relevant Than Ever Today

Photo by Hadija Saidi on Unsplash

Over the last year or so I have encountered several opinions suggesting that the React ecosystem is recommending the deprecation of Redux in their projects in favor of moving towards “modern React” using the Context API. A few episodes on React Podcast have alluded as such (with such notions as “have you migrated your Redux over to Context yet?”) and there have been multiple articles published that exemplify this viewpoint. I’ve heard from bootcamp graduates whose instructors have rushed through the Redux content of their curriculum because it was “too complicated” and “people don’t use it anymore anyway.” Additionally, I’ve heard this sentiment spread to a brand new self-taught developer as he was starting on a new project at our company:

“I thought people were kinda moving away from Redux in favor of Context…“ — Anonymous self-taught developer

I myself even suggested that perhaps Thunks, a tool for making asynchronous action calls, were obsolete — which I no longer think thanks to Redux Toolkit and some inconveniences that I have since experienced when using only custom hooks in projects. Honestly, I think we all kind of just got excited about new React features and thought “This is the future!” They are cool, and they empower a lot of useful things. But getting rid of Redux is not one of them.

Perhaps the sentiment has arisen from a handful of controversial headlines without a lot of really in-depth analysis, but Redux is not dead. In fact, I think it is more relevant than ever. Of course, I don’t mean to come off officiously on the subject. It might not work for every project. In fact, Dan Abramov, the creator of Redux, has suggested that perhaps you don’t need Redux in your project in an article back in 2016. But for my organization, I have found several reasons why we will continue to use Redux for the foreseeable future. Hopefully new react developers will consider these as well instead of simply going along with the recent sentiment surrounding the Context API.

The nature of my work allows me to build a lot of greenfield applications with varying levels of complexity. This gives me a perhaps somewhat unique perspective on what is useful and what is not. I took the opportunity at a time when I had a lot of new projects hit my queue all at once to test varying implementations of state management to help determine for my organization where we were over-investing our time and what was truly useful for catching bugs and building reliably architected solutions. Those variations included building apps with:

Only useState hooks (trying to architect in such a way to avoid as much prop-drilling as possible).

hooks (trying to architect in such a way to avoid as much prop-drilling as possible). A completely vanilla implementation of Redux.

Only the Context API.

An implementation of Redux Toolkit.

What I found was that, even in my applications that were supposed to be small, scope creep and edge cases inevitably arose that ultimately made them larger and more complex. One of the main benefits that have been argued for the Context API is that it is simpler and makes more sense with smaller applications. But after this experience, I would argue that there is no such thing as the mythical “small app.”

Additionally, developers on my team who picked up Context for the first time started running into bugs related to implementing it with mutable variables. Refactoring their code to make it safely immutable led to basically re-implementing the Redux API (via reducers functions), but without the benefit of the awesome Redux Dev Tools. Seriously, the Redux Dev Tools are so good, I never need to use a breakpoint or a console.log when I’m debugging. The ability to retrace each action that has been dispatched, manually dispatch new actions in a live app, trace exactly where the action was dispatched from in your code, and see the full state and the state diff for each dispatch are not worth giving up for the “simplicity” of the Context API.

On the point of simplicity — the main reason the Context API is said to be more simple than Redux is due to the reduction of boilerplate code. However, a safely implemented Context application does not have that much less boilerplate than Redux. And these days, Redux Toolkit has significantly cut down on the boilerplate. Redux Toolkit is a big differentiator in this debate. I probably stumbled upon it later in the game than I should have, but its relative simplicity over a vanilla implementation of Redux basically shuts down the Context API argument completely.

TL;DR

There is no such thing as a “small app,” which is where the Context API is meant to shine. Redux Dev Tools are not worth giving up for a marginal reduction in boilerplate code. Redux Toolkit reduces the complexity of a Redux implementation to be comparable with Context API complexity.

Conclusion

Redux, in my opinion, is not only not dead — it is more relevant than ever. With the addition of Redux Toolkit, the state management capabilities surrounding Redux have gotten better than ever. New features introduced in Redux Toolkit, like RTK Query, seem pretty killer (I haven’t tried it yet, but I really want to). The folks contributing to Redux Toolkit have done such a good job that my organization will be paying close attention to that library for the foreseeable future as we see them as thought leaders in the industry.",5492
"Quickly Test and Deploy Vector Search Solutions with the Milvus 2.0 Bootcamp Milvus Jul 15·6 min read

With the release of Milvus 2.0, the team has revamped the Milvus bootcamp. The new and improved bootcamp offers updated guides and easier to follow code examples for a variety of use cases and deployments. Additionally, this new version is updated for Milvus 2.0, a reimagined version of the world’s most advanced vector database.

Stress test your system against 1M and 100M dataset benchmarks

The benchmark directory contains 1 million and 100 million vector benchmark tests that indicate how your system will react to differently sized datasets.

Explore and build popular vector similarity search solutions

The solution directory includes the most popular vector similarity search use cases. Each use case contains a notebook solution and a docker deployable solution. Use cases include:

Quickly deploy a fully built application on any system

The quick deploy solutions are dockerized solutions that allow users to deploy fully built applications on any system. These solutions are ideal for brief demos, but require additional work to customize and understand compared to notebooks.

Use scenario specific notebooks to easily deploy pre-configured applications

The notebooks contain a simple example of deploying Milvus to solve the problem in a given use case. Each of the examples are able to be run from start to finish without the need to manage files or configurations. Each notebook is also easy to follow and modifiable, making them ideal base files for other projects.

Image similarity search notebook example

Image similarity search is one of the core ideas behind many different technologies, including autonomous cars recognizing objects. This example explains how to easily build computer vision programs with Milvus.

This notebook revolves around three things:

Milvus server

Redis server (for metadata storage)

Pretrained Resnet-18 model.

Step 1: Download required packages

Begin by downloading all the required packages for this project. This notebook includes a table listing the packages to use.

pip install -r requirements.txt

Step 2: Server startup

After the packages are installed, start the servers and ensure both are running properly. Be sure to follow the correct instructions for starting the Milvus and Redis servers.

Step 3: Download project data

By default, this notebook pulls a snippet of the VOCImage data for use as an example, but any directory with images should work as long as it follows the file structure that can be seen at the top of the notebook.

! gdown ""https://drive.google.com/u/1/uc?id=1jdudBiUu41kL-U5lhH3ari_WBRXyedWo&export=download""

! tar -xf 'VOCdevkit.zip'

! rm 'VOCdevkit.zip'

Step 4: Connect to the servers

In this example, the servers are running on the default ports on the localhost.

connections.connect(host=""127.0.0.1"", port=19537)

red = redis.Redis(host = '127.0.0.1', port=6379, db=0)

Step 5: Create a collection

After starting the servers, create a collection in Milvus for storing all the vectors. In this example, the dimension size is set to 512, the size of the resnet-18 output, and the similarity metric is set to the Euclidean distance (L2). Milvus supports a variety of different similarity metrics.

collection_name = ""image_similarity_search""

dim = 512

default_fields = [

schema.FieldSchema(name=""id"", dtype=DataType.INT64, is_primary=True, auto_id=True),

schema.FieldSchema(name=""vector"", dtype=DataType.FLOAT_VECTOR, dim=dim)

]

default_schema = schema.CollectionSchema(fields=default_fields, description=""Image test collection"")

collection = Collection(name=collection_name, schema=default_schema)

Step 6: Build an index for the collection

Once the collection is made, build an index for it. In this case, the IVF_SQ8 index is used. This index requires the ‘nlist’ parameter, which tells Milvus how many clusters to make within each datafile (segment). Different indices require different parameters.

default_index = {""index_type"": ""IVF_SQ8"", ""params"": {""nlist"": 2048}, ""metric_type"": ""L2""}

collection.create_index(field_name=""vector"", index_params=default_index)

collection.load()

Step 7: Set up model and data loader

After the IVF_SQ8 index is built, set up the neural network and data loader. The pretrained pytorch resnet-18 used in this example is sans its last layer, which compresses vectors for classification and may lose valuable information.

model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)

encoder = torch.nn.Sequential(*(list(model.children())[:-1]))

The dataset and data loader needs to be modified so that they are able to preprocess and batch the images while also providing the file paths of the images. This can be done with a slightly modified torchvision dataloader. For preprocessing, the images need to be cropped and normalized due to the resnet-18 model being trained on a specific size and value range.

dataset = ImageFolderWithPaths(data_dir, transform=transforms.Compose([

transforms.Resize(256),

transforms.CenterCrop(224),

transforms.ToTensor(),

transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])) dataloader = torch.utils.data.DataLoader(dataset, num_workers=0, batch_si

Step 8: Insert vectors into the collection

With the collection setup, the images can be processed and loaded into the created collection. First the images are pulled by the dataloader and run through the resnet-18 model. The resulting vector embeddings are then inserted into Milvus, which returns a unique ID for each vector. The vector IDs and image file paths are then inserted as key-value pairs into the Redis server.

steps = len(dataloader)

step = 0

for inputs, labels, paths in dataloader:

with torch.no_grad():

output = encoder(inputs).squeeze()

output = output.numpy()

mr = collection.insert([output.tolist()])

ids = mr.primary_keys

for x in range(len(ids)):

red.set(str(ids[x]), paths[x])

if step%5 == 0:

print(""Insert Step: "" + str(step) + ""/"" + str(steps))

step += 1

Step 9: Conduct a vector similarity search

Once all of the data is inserted into Milvus and Redis, the actual vector similarity search can be performed. For this example, three randomly selected images are pulled out of the Redis server for a vector similarity search.

random_ids = [int(red.randomkey()) for x in range(3)]

search_images = [x.decode(""utf-8"") for x in red.mget(random_ids)]

These images first go through the same preprocessing that is found in Step 7 and are then pushed through the resnet-18 model.

transform_ops = transforms.Compose([

transforms.Resize(256),

transforms.CenterCrop(224),

transforms.ToTensor(),

transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) embeddings = [transform_ops(Image.open(x)) for x in search_images]

embeddings = torch.stack(embeddings, dim=0) with torch.no_grad():

embeddings = encoder(embeddings).squeeze().numpy()

Then the resulting vector embeddings are used to perform a search. First, set the search parameters, including the name of the collection to search, nprobe (the number of the clusters to search), and top_k (the number of returned vectors). In this example, the search should be very quick.

search_params = {""metric_type"": ""L2"", ""params"": {""nprobe"": 32}}

start = time.time()

results = collection.search(embeddings, ""vector"", param=search_params, limit=3, expr=None)

end = time.time() - start

Step 10: Image search results

The vector IDs returned from the queries are used to find the corresponding images. Matplotlib is then used to display the image search results.

Learn how to deploy Milvus in different enviroments

The deployments section of the new bootcamp contains all the information for using Milvus in different environments and setups. It includes deploying Mishards, using Kubernetes with Milvus, load balancing, and more. Each environment has a detailed step by step guide explaining how to get Milvus working in it.

Don’t be a stranger",8039
"Photo by Brett Jordan on Unsplash

Short answer: any!

We’ve ran the School of Code Bootcamp helping more and different types of people into tech and helped hundreds of people change career. In our experience, we’ve found it doesn’t matter where you come from, so long as you have motivation, humility, and a love of learning. Don’t believe us? Here’s an A-Z list of backgrounds for people who’ve been on the School of Code journey and changed careers into tech have come from:

Actor

Admin Worker

Aerospace Engineer

Air Hostess

Artist

Automotive Engineer

Baker

Bank Worker

Bar Staff

Barber

Barista

Barrister

Beautician

Bicycle Repair

Call Center Worker

Carer

Carpenter

Casino Croupier

Charity Worker

Chef

Chemist

Civil Servant

Community Manager

Complaints Handler

Construction Worker

Council Worker

Dentist Manager

Design Engineer

Designer

DJ

Engineer

Entrepreneur

Factory Worker

Fashion

Fast Food Worker

Finance Worker

Football Coach

Foreign Language Teacher

Forensics

Forklift Driver

Geologist

Healthcare Assistant

Higher Education

Hospital Porter

Hospitality Worker

Human Resources

Insurance

IT Support

Jeweller

Kitchen Designer

Landscape Gardener

Librarian

Lifeguard

Lorry Driver

Marketing

Medical Researcher

Mental Health Nurse

Metallurgist

Musician

NHS Worker

Operations Manager

Palaeontologist

Personal Trainer

PhD Researcher

Police Officer

Primary School Teacher

Probation Officer

Project Manager

Quality Checker

Recruiter

Refugee

Retail Worker

Return To Work Parent

Sales Person

School Leaver

Secondary School Teacher

Social Care Worker

Social Entrepreneur

Social Worker

Special Educational Needs Teaching Assistant

Teacher

Temp Worker

Theatre Director

Translation

Unemployed

University Graduate

Volcanologist

Waiter

Warehouse Worker

Xerox Machine Operator

Yacht Worker

Zumba Instructor

Cool eh?! 😎

You can get into tech from any background!

Join our transformational Bootcamp and change your life today: schoolofcode.com",2025
"“Will AI take my job?” This is one of the most asked questions on the internet. Before answering this question we have to understand what AI actually is and how it works and the history of technological innovations in general. Then it will help us to understand if there is any worry.

As its name suggests artificial intelligence is intelligence like humans has but it is created artificially. Before the age of AI, computers didn’t have their own intelligence. Programmers had to program manually for every permutation and combination of a task. They couldn’t do it on their own, so something like face recognition was impossible because every face has its own characteristics and programmers would have to write code to recognize every face individually. But now with the help of advanced mathematics and the increased computational power of computers, it is possible.

AI is nothing but another technological innovation like TV or mobile phone or the internet. It is the history of every technical innovation that when it is in its baby steps it creates a lot of buzz and a lot of skepticism. Like with the TV everybody said that it’s an idiot box and it will kill radio but radio is still alive and with the internet, people said it will destroy the world but here we are, our online lives have become as important as our offline lives.

It’s our history, whenever there is a groundbreaking change in our society, we adapt to it. The moral of this short history lesson is that the same will happen with AI.

As AI is an intelligent machine, there is one thing machines are very good at and that’s doing repetitive tasks more faster and accurately than human beings. So there are some jobs that are bound to change like factory workers on an assembly line or receptionists at a desk of the office, jobs like these will be done by a robot with AI. And that’s inevitable. Now if you are doing a job which consists of repetitive tasks, then it will be replaced by AI sooner or later.

There are some jobs that will not be replaced by an AI or not at least in the near future. Jobs that require human creativity or human emotion or imagination or any other human ability that nature has only given us. Jobs like writers, psychiatrists, scientists, lawyers, HR managers, or the same people who created AI, the software developers.

Now what’s the solution, if your job lies in nonreplaceable jobs then you don’t have to worry but if it is on the first list you have some options. If you are in the very early stage of your career you can always shift your job but if you are in later stage of your career, there is a history lesson in this regard. All those people who lost their job to automation just didn’t become unemployed. The nature of their job changed. Factory workers who worked on manual packaging lost their job to assembly line machines. They changed their job from packaging to operating those same machines, bank tailors whose job was taken by ATMs found new jobs in the same banks.

So the answer to the million-dollar question “Is AI going to take my job?” is “NO”. You will not be unemployed just the nature of your job will change.",3149
"I’ve been using VS Code as my main IDE for many many years. Though I’m going to be honest, it took me a few years before I began USING it. In my earlier years as a developer, I was simply leveraging the defaults in VS Code, the syntax highlighting, code suggestions, etc. Then one day after I updated it, I finally took the time to read the Getting started. I wish I had done that previously, and here is why.

Code Snippets — DRY and example storage

Code snippets are templates for code that you can reuse across projects. You create code that you repeat often and make them appear as code suggestions. If you often create a loop to iterate arrays, then you can create a snippet for that so you never have to type it again. You can create snippets for local projects, global across all projects, and for certain programming languages. You can also find snippets in the marketplace in VS code.

I use snippets to avoid repeating common patterns over and over, and also to help me remember HOW to do things. Start building your library as you code and you will thank me.

Searching for @category:""snippets"" in the extensions will search for only snippets, you can also specify certain keywords after. This is How I search for Go specific snippets @category:""snippets"" go

I find creating my snippets the most efficient way though. You can create one very easily. Enter the menu at the top and go to File->Preferences->User Snippets . Select the snippet you want to create, either local or global, and then name it. I will name my snippet go-snippets since it will contain all my snippets related to Go and be used globally.

Let’s start with a super simple example, in Go you write the following statement very often to see if an error occurred.

if err != nil {

return err

}

We can create a snippet so we never again have to write it again. Enter the following into the generated go-snippets file.

Some values in the snippets that are used are

Scope — A comma-separated list of all the languages to enable the snippet in. In this case, I only use go

— A comma-separated list of all the languages to enable the snippet in. In this case, I only use Prefix — The name of the snippet

— The name of the snippet Body — The lines of code to generate, we can even input parameters here, we will see that in example 2

— The lines of code to generate, we can even input parameters here, we will see that in example 2 Description — A text description to show in the code completion tab

VS Code snippet for generating an error check in Go

After you have inserted that, save the snippet file, if you then open any .go file and type Err you should see the snippet as a suggestion.

Showing how the Error check snippet in VS Code generates a nil check-in Go

Now that example is simple, you can even have input parameters for more advanced snippets.

I use a lot of dependency injection in my projects, and I usually have a Service struct that holds all repositories. In all projects, the creation of this service is the same, and I always retyped it, and I always forget how. The result is that I always waste a few minutes to bring up an old project and see what I did.

Let’s take a look at creating a snippet for this. Look how I add $1 in the body, that’s how you can add input values, if you need more values then add +1 to the number, $2 , etc.

NewService Snippet that generates a type, function, and constructor

An example of how I generate a constructor function, a type, and an alias function in a matter of seconds using snippets

See all that code and how fast it fast generated? Snippets are probably the feature in VS Code that I love the most. I tend to use it not only to store common code pieces but as a memory collection of how to perform certain algorithms. It is saving me time, money, and frustration.

LogPoints — Never again fill your logs with unforgotten Prints

Over are the days where you had to remember to remove your debug prints to the terminal.

Yes, I’m guilty, I do use the debugger, but sometimes printing a certain value, or a simple IM HERE is very effective and easy when debugging. And many times we forget to remove these debug prints, right?

Well, with Logpoints you can print things without using code. VS Code allows us to set these logpoints, just as we set breakpoints. Logpoints even allow expressions to be logged, which means you can run a function or more when a logpoint is hit. You can set logpoints by right-clicking on the panel where you set breakpoints. Select Add logpoint and select between the three options

Expression — Only log when the certain expression is true

Hit count — Break when hit count condition is met, good for loops or long-running services where bugs start occurring after X runs.

Log Message — Always log the message

Sadly, Logpoints are not available for all languages (sobbing, no love for Go due to an issue) You can find all supported languages here. In the GIF below you can see how I set a logpoint in python that prints the returned result from a function call. Expressions are placed inside {}.

Setting a log point in Python and seeing it print the output of a function

Select and Rename (Select + F2) — Smooth renaming

I HATED renaming variables and functions in VS Code. Why? I used to perform renaming of my functions etc by using the way suggested in the VS Code Basic editing guide on how to find and replace. I never liked the find UI in VS code.

Imagine my surprise when I learned that F2 is a rename command. It even works across the whole project. Simply select the word you want to change, and press F2 and then write the new correct word.

Look at how fast I can change the Service from the Code snippets into TestService instead. This has saved me a lot of time as well, and I wished I knew it before.

Renaming occurrence of the word across multiple files, simple find and replace

Notice that comments are not changed, only the code instances. One thing that the Find and Replace suggestion by the VS Code Basic editing tutorial supports that this does not is that you can review each replacement and accept them. In my experience the F2 does a flawless job of finding the right pieces though.

Multi Cursor Selection (ALT+Click) (Shift+ALT+UP/Down on Keyboard)

VS Code has something called Multi Cursor Selection. What this means is that we can have multiple cursors appearing at the same time.

You can write at two places at once, so bust out that second keyboard and double code, double your speed!

OK, hold your horse, that was a joke. Sadly we can only use one keyboard, but we can edit many places at the same time.

Holding down alt and click on the locations you want to edit and you will type at all the marked cursors.

Hold down ALT to create multiple cursors in VS Code

Now this feature alone has never impressed me, I find it hard to find use cases. Then why did I bring it into the list?

Simple! This multi cursor allows other commands to be used in combination for great power. The next item on the list will showcase this.

Select all occurrences of a word (Select + (CTRL+SHIFT+L))

This is yet another refactoring tip. Notice how the F2 replacement only worked on code instances?

Sometimes we want to change comments, or simple text files, etc. Gladly you recently learned about Multi Cursor Selection in Vs Code. We can combine multi cursor with selecting words.

Select the word you want to modify and then press ctrl+shift+L and VS Code will spawn a cursor at each instance. This can be used to change the name of a variable for instance, even in comments and more.

Showing how you can spawn Multi cursor at a selected word and replacing all occurrences

Go to in File (CTRL+Shift+O)

For some unknown reason, the outline function in VS Code is default collapsed. It is a very nice feature and I love having it open.

An outline is a visualization of the code in a file. It can be nice to get a fast sense of what’s in a file without scrolling through the whole thing.

Outlining in VS Code shows the structure of code in a nice render.

Using the ctrl+shift+o we can jump between these outlined items. This is useful for fast navigation in large codebases or files.

Pressing the shortcut command will open a search prompt which you can use to search for the wanted function, structure, more depending on the language of choice.

To make it even easier you can add : after the @ in the search to group by category. This makes viewing the code structure in a file even easier. I tend to use this very often in new codebases to get a sense of what's present in a file, instead of scrolling blindly through.

VS Code outline jumping — a time saver when navigating code

Fast Scrolling by holding ALT

This trick does not require a big explanation, it’s just a small feature that once I learned it, I remembered all the times I needed it in the past.",8881
"Bored of VS Code? Try Lite-XL

Visual Studio Code vs. Lite-XL, a cover designed by the author with Canva.

I was a die-hard fan of Visual Studio Code for three years. But I started using a lightweight alternative called Lite after Visual Studio Code started behaving similar to Visual Studio by taking all the resources that other processes wished to take. Lite is a minimal code editor written in Lua and C. It is indeed implemented minimally as much as possible. The Lite editor core is an application that consists of a multi-line textbox made with the SDL graphics library. All the other modern code editor features, such as syntax highlighting, are made as plugins. It just takes one megabyte in your disk and consumes around 20 megabytes of physical memory.

However, it doesn’t offer all the required features for all developers. The maintainer of the Lite project mentioned that the project aims to provide something practical, pretty, small, and fast implemented as simply as possible — easy to modify and extend, or to use without doing either. In other words, the Lite editor itself may not deliver any features further, and if someone needs more features, they have to fork the source repository and extend.

Lite-XL is an actively maintained fork of the Lite editor, and it offers almost all basic productivity features that Visual Studio Code has. Three months ago, I wrote a story explaining how Lite-XL technically performs better than Visual Studio Code. In this story, I will take you through Lite-XL’s new features that make it better than Visual Studio Code.

Problems With Visual Studio Code

If Lite-XL is just a code editor that does the same job as your favorite Visual Studio Code, why should you try an alternative? Well, there is a considerable technical difference between both. Visual Studio Code is built on top of the Electron framework that lets developers build cross-platform desktop apps with web technologies. Visual Studio Code is a web application that runs inside a frameless native window.

On the other hand, Lite-XL is a native desktop application built with the SDL graphics library. Lite-XL works on Linux, macOS, and Windows because SDL is a cross-platform graphics library like Google’s Skia. SDL doesn’t render elements to a Chromium webview like Visual Studio Code, and it renders graphical elements natively via OpenGL or DirectX. One Lite-XL instance typically takes around 10 megabytes of physical memory — while one Visual Studio Code instance takes more than 400 megabytes. Visual Studio Code is adding new features to the editor core every day. Now it takes around 300 megabytes of disk space without any extensions, and we won’t wonder if it takes one gigabyte after several years.

Ever heard of VSCodium? Even though Visual Studio Code’s source code is MIT-licensed, Microsoft makes releases with a different non-FLOSS (Free/Libre and Open Source Software) license by adding a kind of commercial flavor that includes telemetry (tracking). The VSCodium project releases the latest binary builds with the MIT-licensed codebase. However, VSCodium is technically the same Visual Studio Code which consumes above-average resources.

In the worst-case scenarios, you may run multiple Visual Studio Code instances with other Electron-based hybrid desktop apps and a web browser. Then you might blame your computer’s hardware, but in reality, your computer became a playground for modern bloatware.

The following story addresses this modern bloatware issue further:

How To Customize Lite-XL As Visual Studio Code

As mentioned earlier, Lite-XL’s features (even the context menu and tree view) typically come as plugins. However, Lite-XL core includes several crucial features such as the status bar, command executor, and file search. Lite-XL is just a text editor without any plugin, as shown below:

Lite-XL core, a screenshot by the author.

It looks like this if we customize it similar to Visual Studio Code.

Lite-XL is customized similar to Visual Studio Code, a screenshot by the author.

The memory usage will never go above 15 megabytes even after these customizations.

Let’s begin the Lite-XL customization process. First of all, make sure to download the latest Lite-XL version from GitHub releases. After that, open the preferences file ( init.lua ) and add the following line to enable Visual Studio Code’s default theme.

core.reload_module(""colors.vscode-dark"")

Every Lite-XL release has pre-installed plugins such as auto-complete, tree-view, context menu, syntax highlighting for some languages, etc. But you may need to install the following plugins to make it more like Visual Studio Code. Installing a Lite-XL plugin is a piece of cake. You can copy the plugin to the data/plugins directory and restart the editor to get a particular plugin activated. You can restart the editor with the command executer by pressing Ctrl + Shift + P.

The restart command in Lite-XL, a screenshot by the author.

Now, install the following plugins with the above method. All plugin source files are available here.

indentguide

The indent guide plugin draws a vertical line per each indentation, similar to Visual Studio Code.

minimap

This plugin renders a visual map of the source code on the right side of the editor, similar to Visual Studio Code. Visual Studio Code renders the source code map via an HTML canvas, but Lite-XL renders it natively. Therefore, the resources usage won’t go up when you work with larger files.

Additional syntax highlighting support

Lite-XL doesn’t include syntax highlighting support for all supported programming languages by default. For example, it doesn’t offer you JSX and TypeScript syntax highlighting support right after the installation process. Therefore, you need to install syntax highlighting plugins as you wish.

Conclusion

Visual Studio Code is backed by Microsoft and has a larger developer audience around it. But Lite-XL is new and still has a small developer audience (Still less than hundreds of members on Discord). Nowadays, native application development is so underrated and getting replaced by hybrid application development created by Electron. Hybrid application development frameworks motivate developers to make native-like hybrid apps so fast by hiding performance issues with modern hardware. However, frameworks/libraries like Flutter and SDL offer better performance-first solutions to develop cross-platform applications.

Also, cross-platform frameworks like Tauri and Neutralinojs try to give the Electron-like development environment with lightweight architectures. Lite-XL is built with SDL and is a truly native desktop app. Protect apps like these by using them because this could be the final era of native desktop apps.

Thanks for reading.",6781
"Analytics in Inventory Management

Tracking is a critical competent of inventory management for retailers. It is dependent on a number of factors — from changes in weather to changing client demands. The evolution of technology has enabled businesses to adopt to a more sophisticated method of inventory tracking. New technologies like big data and predictive analytics make tracking and updating records seamless and simpler at the same time. Businesses can make real-time decisions related to orders, supply, customer feedback and damage. Let me put down some of the major ways in which analytics can help simplify inventory management in the retail space.

Forecasting demand for businesses promotes cost cutting.

According to me, one of the biggest advantages of using predictive analytics is the need to be able to forecast demand. Analyzing past trends and future variables allow retailers to arrive at a dependable median. This lets businesses stock up sufficiently to meet customer demands. It also prevents wastage. Not only in terms of excess, but also in terms of blocking precious capital that could have been used productively.

Viewed from the other end of the spectrum, the demand forecasting exercise draws attention to the need for clean, accurate data sources. Trust me, the more reliable the data source the better would be the forecasting.

Improved forecasting has several benefits for retailers.

· Lower spending because of accurate availability of stocks.

· Infrequent clearance sales.

· Better understanding of market trends, which means better strategies for businesses.

· Efficient warehouse operations.

Reaping the benefits of reducing inventory costs.

Cost of housing excess inventory is one of the major concerns for retailers. Stocks which haven’t been sold or shipped inflate inventory costs considerably. This can happen despite having inventory control in place. Three major heads contribute to rising inventory costs.

· Capital costs can lead to wastage of businesses using the capital to fund productive activities like innovation and research.

· Holding costs are costs like rent, utilities, storage space and taxes.

· Handling costs are usually the costs related to labor organizing, moving and packing goods.

Predictive analytics helps with reducing costs, improve frequency and accuracy of ordering cycles. This increases economy in inventory management even if the costs cannot be eliminated.

Analytics supports reduction of inventory shrinkage.

Shrinkage is a real problem faced by retailers globally. All goods stocked do not get sold because of various reasons like damage, reaching expiry dates or even shoplifting. When shrinkage data is tracked and analyzed, manufacturers & retailers can analyze the potential reasons with causes. Statistics from various sources show that the biggest cause of damage is faulty packaging. Retailers can use the intel obtained from analytics to ensure sourcing from reliable suppliers and reduce inventory shrinkage to a great extent.

Better inventory management increases customer satisfaction index.

While customer satisfaction might not be one of the leading factors affecting inventory management for retailers, it is still important. Access to predictive analytics allows retailers to stock inventories at ideal levels. Hence customer demand patterns translate to lower costs in the long run.

Trends identified by a good analytics tool will also allow retailers to be aware of spikes or dips in demand levels as per the seasonality & interest. This allows them to reduce the wastage of excess stock. At the same time retailers can prevent in-demand items from going out of stock. Stock-out items are a direct loss of revenue and a challenge in the supply chain as well.

Retailers should use a personalized strategy.

The lack of an organized inventory process will reflect directly on the bottom line, productivity losses and unhappy customers. Big data and predictive analytics can be used effectively to monitor both market and customer trends, enabling retailers to reap harvests of an efficient inventory management system.

However, each business has its own priorities and focus areas. Hence, the best approach to using analytics for inventory management should be customized per business needs. What do you think? Let me know your thoughts in the comments.

About Author:

As the CTO of Rishabh Software, Srinivasa Challa (a.k.a CS) defines and drives the technology solution roadmap for global customers. CS is responsible for the seamless execution of the company’s technology strategy, development & cross-functional delivery. With 25+ years of experience, he is instrumental in creating CoEs across domains like Healthcare & Fintech. CS is a constant learner who loves to stay updated on technology, healthcare, digital transformation and customer experience design and more.",4887
"We’re excited to share today that we’ve expanded our Eat offerings to include Frozen Eats, featuring sweet treats from Coolhaus, Chloe’s Pops, Marco, and Dalci, alongside savory foods from Chef Bombay, Nuggs, Cappello’s, and more.

Frozen Eats speaks to our mission of simplifying your life by bringing you the most coveted premium brands we know you will love — FastAF.

To celebrate the launch of the new category and National Ice Cream Day, we’re hitting the streets this weekend (Saturday, July 17 — Sunday, July 18) with the FastAF #ChillSummer Ice Cream Truck Tour. For a $50 flat fee, you can select from three curated assortment of goodies including That’s Bold (artisanal flavors), FastAF Faves, and No Moo (dairy-free, plant-based), all delivered to your door by one of our trucks. Don’t wait, as spots are limited.

You can also order curated and fantastically-themed sundae bundles this weekend via our normal delivery method through the FastAF app, including Vacation Vibes ($25), Coffee Obsession ($30), and the Ultimate Sundae ($35). Each bundle comes with two ice cream pints, toppings, and a kit of add-ons to indulge in.

Just download the app and shop Frozen Eats to enjoy in LA, NY, SF & Miami.

Continue to check the app for the latest Frozen Eats to keep you cool this summer!",1298
"Distributed Ledger Technology- DLT:

Image from: shell

Hi folks 😊, Here is my new blog series on Blockchain. Hope you learn a lot from it. Subscribe me for getting notification for the blogs I publish in this series. In the Day-1 of Blockchain. We are going to know about Distributed ledger technology, it’s benefits, DLT’s effect on business and the difference between Distributed Ledger Technology and Traditional Technology.

Distributed ledger technology (DLT) is a digital system for recording the transactions of digital asserts. These are stored multiple times in multiple different distributed nodes at different locations. Distributed ledger technology (DLT) has great potential to change the financial sector, making it more efficient, resilient and reliable. Bitcoin’s blockchain is an example of peer-to-peer Distributed ledger technology.

Image from: unicsoft

In DLT Database spread across multiple computers. so it reduces the risk of Central storage. Using DLT we have different types of services like Blockchain, Tempo, Directed Acyclic Graph (DAG) etc.

Tempo is based upon subsets of ledger. In Tempo the person conducts the transaction and one who has access to store, can only store the transactions. While in blockchain everyone can store the transactions. In DAG’s , the transactions are verified by building a new transactions on top the existing transaction.

Benefits of DLT:

Distributed Ledger Technologies have many potentials.

Remove Middlemen: Distributed Ledger Technology reduces the need for an intermediary middleman to transfer information and goods between buyers and sellers.

Fast Transactions: It has potential to speed up the transaction by removing middleman. Removes cost taken by middlemen.

Reduces Costs: Cost taken by middle man are cut off in Distributed Ledger technology.

More Secure: Distributed Ledger technology is much more secure. In this Each node hold the record. That creates a system that creates system that is more difficult to manipulate the the system.

Transparent: Distributed Ledger is a much more transparent way of handling records, because the information is shared between different nodes and spread across the network.

How DLT affect Business?

Distributed Ledger Technology creates a permanent, decentralized, trustless ledger of records. Accessible to all across globe without the help of third-party to carryout the transactions.

Like in crypto markets, cryptocurrencies are not their product. It is more like creating a new economic systems with tokens as the medium of exchange and Distributed Ledger technology gives the rules and regulations for the users of this system.

These effects can be divided into three major components

Transforms Internal Processes

Transforms Business Models

New Opportunities

Transforms Internal Processes:

This includes DLTs potential along internal processes and interaction with business’ value network. It can be in the form of:

Payments

Assert Tracking

Data Sharing

Identity Management

Transforms Business Models:

This includes looking beyond technology and including the value potential related to new types of customer interactions and innovative ideas for business models like

Customer Engagements

Micro Transactions

Creating New Markets

New Opportunities:

Distributed Ledger Technology might create new opportunities which are neither part of the core vale chain nor the core business model:

Funding

Access to Data

Crowd Collaboration

Self Governed Organizations

Difference between Distributed Ledger Technology and Traditional Technology",3581
"George Gilder, one of the preeminent economic and technological thinkers of the past four decades, believes that decentralization via technologies like blockchain will disrupt cloud computing. In his latest book, Life After Google (2018), he forecasts the decline of closed ecosystems that are based on big data, centralized cloud storage, and machine intelligence. Alternatively, these “walled gardens” will be displaced by the rise of decentralized blockchains — open-source systems with distributed storage. Not only does distributed data via the blockchain offer increased security and an immutable record that protects one from coercion, but it will unleash a new era of innovation by magnifying the power of human ingenuity, to a level that’s impossible in a closed system.

Cloud Computing is Reaching It’s Limits

Just a decade ago, cloud computing was the hot new technology. It ushered in a new era of software innovation, shattering limitations related to data storage, flexibility, and cost. But according to George Gilder, cloud computing is reaching its limits. Improvements in parsing big data are incremental at this point. Due to physical constraints, big data storage centers are experiencing diminishing returns.

In his book, Life After Google, Gilder spends some time describing the Google Data Center up in The Dalles, Oregon. He makes the case that despite the misconception that cloud computing is this ethereal thing, its not. Cloud computing, in actual fact, is very energy intensive. Google’s data center in The Dalles, for example, is a huge, industrial-scale operation. There is no coincidence that it was built up in Oregon, next to a cheap source of hydroelectric power and an ample supply of glacial water for cooling. Like all other factories, cloud data centers are extremely energy and resource intensive. And like all other factories, they eventually run into the law of diminishing returns. Going forward, its going to be difficult for big data centers like The Dalles to get enough power. It’s going to be difficult to cool them adequately. Big data centers, as it turns out, are bumping into physical limitations on their size and efficiency.

Bell’s Law — The Birth And Death of Computer Systems (Computer Classes)

Another indicator that cloud computing and big data centers are nearing the end of their life cycle, is Bell’s Law. Gilder points to the premise that — according to Bell’s Law — roughly every decade, a new and lower priced computer system (computer class) forms, based on a new programming platform, network, and interface. This results in the establishment of a whole new industry.

Bell’s law of computer classes formulated by Gordon Bell in 1972 describes how types of computing systems (referred to as computer classes) form, evolve and may eventually die out. Partially correlated with Moore’s law (which states that the number of transistors per chip double every 24 months), Bell’s Law says every 10 years, technology improves between a hundred and a thousand fold, and it entails an entirely new architecture. New classes of computers create new applications resulting in new markets and new industries.

Cloud Computing (Centralized Data) — On It’s Way Out

George Gilder believes that cloud computing, which was a great triumph for its time, is now reaching the end of the line. If you do the math, from the first time Gilder wrote about the cloud (in an article in Wired in 2006), stating it would dominate the next Bell’s Law phase, we are now 15 years removed from that time. In other words, cloud computing is already in the mature stage of its life cycle.

Blockchain (Distributed Data) — The New Kid On the Block

But what will take its place? If the cloud is at the latter stage of its life cycle, that means that there is already a new architecture in the early part of its evolution. And in fact there is. Shortly after the Great Financial Crisis of 2007–2008, a new decentralized digital currency emerged, one verified by network nodes through cryptography and recorded on a public distributed ledger called a blockchain. In 2009 Bitcoin was born, ushering in a new age of blockchain technology. Since that time, the blockchain — distributed ledger technology — has seen a meteoric rise.

In keeping with Bell’s Law, it takes up to a decade to understand how a new computer class forms, evolves, and is likely to continue. So blockchain is right on track to disrupt the existing class — to take the place of cloud computing and centralized data storage. George Gilder says that this new architecture, one that has availed itself to the developer community, provides superior security, has the ability to prevent corruption and manipulation of transaction data, and the power to magnify innovation through its decentralized nature.

Distributed Data on a Blockchain Provides Superior Security to Centralized Storage

One of the biggest challenges of centralized storage is security. With centralized cloud computers, there is always going to be the risk of losing data or having your data fall into the wrong hands. What happens if a data center go down? It basically means you are down as well, since you no longer have access to your data. Or even worse, what happens when the data center’s security is compromised, and it is hacked? Well, then someone has access to your data, and maybe, even your assets. So while centralized storage on the cloud affords us many great benefits, users also need to weigh the risks of the product.

Fortunately, distributed data on a blockchain solves many of the cloud’s most troubling security risks. First, blockchain technology is inherently decentralized, meaning it isn’t vulnerable to a single point of failure like centralized cloud storage. Second, with distributed ledger technology (DLT), each transaction added to a blockchain is validated by multiple computers in the network, ensuring that a single system cannot add invalid blocks. This alone means that blockchain is much more resistant to attack. But there’s more. For additional security, when a new block is added to a blockchain, it is linked to the previous block using an immutable cryptographic signature called a hash. This ensures the chain is never broken and that each block is permanently recorded, since the only way to alter past transactions in blockchain is to alter all subsequent blocks first.

Distributed Data on a Blockchain Provides the Ability of Attestation

Apart from superior security, there is another disruptive aspect of distributed data on a block chain that isn’t getting enough recognition. And no, its not privacy. While the blockchain can provide a certain level of privacy, this isn’t nearly as critical, according to George Gilder, as being able to prove that you didn’t do something.

Blockchain cryptocurrencies are often compared to cash because they seem to allow anonymity. But it’s really better than cash. It’s a major step forward beyond cash because, not only does it allow you to conduct anonymous transactions, it also enables you to demonstrate your behavior and your transactions unimpeachably if you have to — to the IRS, to a prosecutor, or to whoever it may be. The blockchain gives you an immutable record that allows you to document your behavior.

So, for instance, the government wants to charge you with an unsubstantiated offense. With all transactions recorded on the blockchain, you now have an immutable record that is irrefutable. So the blockchain provides the ability of attestation, thereby eliminating coercion by government, big business, or from any kind of frivolous litigation. This is a huge step forward in personal freedom, one that’s not only sorely needed in developing nations with high levels of corruption, but more and more so, in western nations like the United States.

Distributed Data — Like the Distribution of Human Minds — Magnifies Innovation

As we move into a new era of decentralization via distributed data on the blockchain, George Gilder believes that human ingenuity and technological innovation will be magnified. He argues that the reason blockchain — the first fully decentralized data system — succeeds, is the same reason that human minds and human cultures triumph through their decentralized distribution around the planet.

Innovation triumphs to the extent that it captures the genius of human beings. And human beings — all human minds — are not all converged in some data center somewhere. Rather, human minds are separate and distributed. So technological progress will ultimately mimic the distribution of human minds.

The blockchain is what the cloud was always supposed to be — a truly transparent and interconnected network that eliminates the need for a centralized authority. The blockchain distributes personal data rather than concentrating it into the hands of a few big data collectors (like Google or Facebook). Whereas these “walled gardens” concentrate your information, then force you to petition to their centralized database for the right to access your own data, blockchain distributes all the personal information across the network. So rather than simply collecting giant data clumps in the cloud, blockchain mimics the way human intelligence and ingenuity is distributed, and how innovation is amplified. And in essence, this is Blockchain’s true genius.",9359
"Disclaimer: Radix’ codebase currently does not support relaying connections to validators in order to correctly enable sentry node architecture. Running a validator in such a topology could lead to missed consensus rounds and corresponding loss of emissions — getting increasingly worse as more people use the pattern. It is strongly recommended by the Radix team that node-runners use a standard topology using a backup full node pattern to mitigate DDoS attack until a sentry node-like architecture is supported in the Radix node.

Introduction:

Radix is a new DeFi platform with seemingly unlimited abilities to scale. The multi-sharded distributed ledger applies a new Byzantine Fault Tolerant (BFT) consensus algorithm (“Cerberus”) and recorded an astonishing 1.4M tps in a proof of concept. On 28/07/2021 the network launched v1 of its “Olympia” mainnet, which utilizes an un-sharded version of the later to come sharded Cerberus codebase.

Sentry Node Architecture is an infrastructure example for DDoS mitigation on validator nodes and is very successfully implemented by the majority of validators on a number of DPOS networks, including cosmos (& cosmos-sdk chains), binance smart chain or polygon.

To divert possible direct attack vectors on validator nodes, multiple distributed, non-validating full nodes (sentry nodes) are deployed in cloud environments. These sentry nodes each establish a private, direct connection to the validator node (through VPC or static routing), enabling the validator to be hosted in a very restrictive environment.

Validator nodes peering solely to sentry nodes can either be hosted in the same remote data center as the sentry nodes (less secure), or utilize private connections via vpn or direct routing to be run in a more secure environment of a private data center.

basic sentry node topology. source: forum.cosmos.network

Many well-established tendermint validators use this mixed architecture of private servers in regional data centers, supported by cloud nodes spread across several cloud service providers in different regions. If a sentry node fails or is brought down by a DDoS attack, new sentry nodes can easily be booted to replace the compromised node, making it harder to impact the validator. Furthermore, validator operators can utilize this topology in various ways, combining the power of physically accessible hardware (keystorage, hardware firewalls…) with high connectivity and availability of distributed data center nodes.

Test:

In our effort at CryptoCrew Validators to build the most-secure validator systems possible, we recently tested basic sentry node architecture on an active validator setup on Radix-stokenet (testnet).

For this test we used a simple combination of 2 sentry nodes (non-validating full nodes with port 30000 enabled to the public for gossiping connections), as well as one validator node with port 30000 enabled exclusively for the private (internal) IP addresses of the sentry nodes — and with the node-software bound to it’s internal eth1-interface address.

system monitoring

Tests began with an average stake of 0.56 % voting power and were run for 2 days before increasing the voting power of the validator to 3%. The validator node seemed to behave fine with only one dropped consensus proposal opposed to over 2000 proposals made. Even during the stress-test with high validator-consensus- participation no further proposal got dropped.

Radix Sentry Test 1: consensus proposals made vs. consensus proposals missed; sync difference

Radix Sentry Test 1: bft metrics

Problem: Currently Radix’ core code-base doesn’t support correctly relaying connections through full nodes. Validator nodes being run in such a topology will not allow any inbound connections. Instead, the validator node is making outbound connections to other peers in the network and that’s what’s being used for communication. Other nodes can’t connect to that validator (as initiator), but they re-use the outbound connection initialized on the validator’s side.

Peer-connection output of another radix-stokenet node during the test.

Conclusion:

The only seeming way to avoid this is if sentry nodes are actually able to act as proxies in order to relay consensus messages to the protected validator node. In our test, our public full nodes were serving as private bootstrapping nodes, which is not actually fully serving the sentry purpose. To achieve “real” sentry node architecture there has to be a built-in solution in the node’s core-codebase.

During our test, the Radix core dev-team has considered options for providing a more complete proxy solution, but did not disclose any concrete plans as for now.

In the meantime It is strongly recommended that any node-runner incorporates a standard validator topology using a backup full node pattern to mitigate DDoS attack until a sentry node-like architecture is supported in the Radix node.

Summary:

It was very interesting to experiment with Radix’ network topology and we’re happy that we were able to acquire some valuable insight. Of course, our results lead to the conclusion that for our “Olympia” mainnet-validator we choose a standard topology, utilizing fully redundant double-oversized (32 GB RAM + 8 cores) nodes, hosted in Tier 3+ data centers.",5299
"Photo by Martin Sanchez on Unsplash

COVID-19 is the largest public health emergency in recent memory—and some of the world’s most innovative minds are developing blockchain-based solutions to help.

During the first wave of the pandemic, the International Association of Trusted Blockchain Applications (INATBA) launched the COVID-19 Solutions Task Force. In collaboration with the European Commission and University College London, the initiative brings together key players in the blockchain sector to address the social, governmental and commercial challenges brought about by the coronavirus crisis.

Similarly, researchers throughout the world have investigated how the central characteristics of blockchain technologies — decentralization, immutability, transparency and traceability — can be used to surpass the obstacles caused by COVID-19.

Here are some of the ways that blockchain is being used to fight the pandemic.

Contact Tracing

Contact tracing is a process that identifies individuals who have been in close contact with people who have tested positive for COVID-19. It’s one of the most common methods being used to control the spread of the virus.

While several solutions have already been proposed, adding blockchain to the mix can increase accuracy and reliability. It can also offer anonymity — which has been a major talking point when it comes to centralized contact tracing systems.

Nodle.io, a decentralized internet of things (IoT) network, launched Coalition, a “privacy-first” contact tracing app. The platform uses blockchain, Bluetooth and cryptographic technologies to trace, identify and notify potential carriers of COVID-19 while maintaining their privacy and anonymity.

The Coalition app promises to protect users’ privacy.

Through an open-sourced protocol called Coalition Whisper Tracing, the app creates an anonymous ID for each user. When users get sick, they declare their COVID-19 status through the app, which then sends the anonymous ID to the cloud. Other users’ phones download these IDs to compare them to those that have been stored locally on their phones. If there’s a match, the app sends a notification to the user.

Donations and Fundraising

Many nonprofits and NGOs are using blockchain to facilitate transparency in fundraising. This can allow them to save costs, limit financial corruption, remove the need for middlemen, and increase trust between their organization and the general public.

HyperChain, a digital assets management company focused on blockchain and decentralized projects, is using its platform to help governments and healthcare organizations securely collect, track and verify donations to help coronavirus-related medical supply efforts.

Through its system, HyperChain allows donors to transparently monitor where their money is going. The company also provides “proof of need” and “proof of receipt” to let users know that the intended party has received the funds.

Vaccine Passports

As the vaccine rollout gains momentum and life inches back to normal, “COVID passports” and immunity certificates have been proposed to certify vaccination status. While some regions have no plans to enforce mandatory vaccination, others have made these health passes obligatory for anyone who wants to travel, eat at restaurants or even enter public spaces.

As the need for COVID passports has increased, so has the black market for counterfeit certificates — demonstrating the need for a reliable, immutable system.

The E-HCert App on the VeChainThor blockchain has been used in Cyprus to certify vaccinations for doctors and nurses at the Mediterranean Hospital of Cyprus. The app also verified about 8,000 PCR tests for people entering the country in June 2020.

While it remains unclear whether blockchain will be implemented into more official COVID passports, global teams of researchers are analyzing the possibilities. Hasan et al. have developed a blockchain-based solution for vaccine passports, and have publicly published the codes to their smart contracts on GitHub. Eisenstadt’s team has also created a protocol for a COVID passport mobile app on a consortium blockchain.

Supply Chain and Logistics

COVID-19 vaccines and supplies need to be stored, administered and used under strict conditions. Unfortunately, supply chain mistakes happen. Millions of doses of the Johnson & Johnson vaccine had to be thrown out due to human error. Thousands of vaccines are unusable because they were stored at the wrong temperature. And early in the pandemic, scammers tried to sell unverified masks and supplies to unsuspecting victims.

Two hospitals in England are using blockchain programs to monitor their COVID-19 vaccines using Everyware and Hedera Hashgraph. The systems maintain a record of temperature, note any irregularities in storage, and send the information to an unchangeable digital record on the Hedera Hashgraph platform.

When proper face masks were in short supply, blockchain supply chain platform VeChain teamed up with Chinese and American companies to limit the use of counterfeit masks. All of the boxes that were sent from China featured a VeChain NFT (non-fungible token) chip and a two-factor authentication QR code. This verified the masks and included important information about the supplies, such as when and where they were produced.

Clinical Trials

Governments, medical institutions and pharmaceutical companies rely on clinical trials to test the safety, efficacy and possible side effects of the COVID-19 vaccines. Adding blockchain to the process could facilitate access to clinical data in real time, improve privacy and safety by creating an audit trail, and increase regulatory compliance through data transparency.

Several companies have been developing blockchain-based clinical trial solutions over the past few years, including IBM and Boehringer Ingelheim, ACL Digital and Sphereon. In these initiatives, the companies are addressing various aspects of the clinical trial process, such as patient retention and clinical document management.

All of these efforts make it clear that blockchain is much more than just a trendy tech term—it’s a powerful asset that can be used to increase transparency, trust, safety and ultimately health around the world.",6281
"Distributed Ledger vs Blockchain Technology: Do You Know the Difference?

Blockchain is increasing in popularity because of bitcoin and other cryptocurrencies. Many traditional centralized bodies such as governments and banks are starting to take an interest in blockchain technology.

A new term that is starting to make waves in the cryptocurrency space is the distributed ledger technology. However, many people usually confuse distributed ledgers with blockchain and vice versa. In this article, we will highlight everything you need to know about distributed ledger vs blockchain.

What is a Distributed Ledger?

A distributed ledger is a database that can be found across several locations or among multiple participants. However, most companies still use a centralized database with a fixed location. Unlike a centralized database, a distributed ledger is decentralized, which helps to remove the need for a central authority or intermediary for processing, validating, or authenticating transactions.

Furthermore, these records will only be stored in the ledger after the parties involved have reached a consensus.

What is Blockchain?

A blockchain is a form of distributed ledger that has a specific technological underpinning. Blockchain creates an unchangeable ledger of records maintained by a decentralized network after a consensus approves all the records.

The significant difference between blockchain and DLT is the cryptographic signing and linking groups of records in the ledger that forms a chain. Furthermore, there is a chance for the public and users to determine how a blockchain is structured and run based on the specific application of blockchain.

What is the Difference Between Distributed Ledger and Blockchain Technology?

Although both blockchain and distributed ledger sounds similar, there are some differences between the two. Blockchain can be categorized as a type of distributed ledger, but you cannot classify every distributed ledger as a blockchain.

We have listed some of the unique aspects of blockchain and distributed ledgers to help you better understand the DLT vs blockchain technology comparison.

1. Block Structure

The first difference between blockchain and distributed ledger technology is the structure. A blockchain usually comprises blocks of data. However, this is not the original data structure of distributed ledgers. This is because a distributed ledger is just a database that is spread across several nodes. But you can represent this data in numerous ways in each ledger.

2. Sequence

All the blocks in blockchain technology are in a particular sequence. However, a distributed ledger does not need a specific data sequence.

3. Proof of Work

In most cases, blockchains usually use the proof of work mechanism. However, there are other mechanisms, but they typically take up power. Distributed ledger, on the other hand, does not need this type of consensus, which makes them more scalable.

Blockchain is just a subset of distributed ledgers, and it has additional functionality aside from the traditional DLTs scope. Proof of work adds a significant difference between distributed ledger vs blockchain.

4. Real-Life Implementations

Implementation is an essential point to consider when understanding the differences between blockchain and distributed ledger. Blockchain has many implementations in real life as it is more popular, and many usages are developed in due course of time. Since a lot of enterprises are adopting the blockchain nature and are slowly integrating it into their systems, you will also find big giants like Amazon, IBM, etc., that offer good blockchain as a service solution.

In comparison, developers recently started to dive deep into the distributed ledger technology core. Although there are several types of DLTs in the tech world, there are few real-life implementations. However, they are still being developed, and we will start to see real-life implementations very soon.

5. Tokens

There is no need for tokens or any currency in a distributed ledger technology. However, you may need tokens to block and detect spam.

Anyone can run a node in blockchain technology. However, running a full node requires a considerable network that may be difficult to manage. Furthermore, there is usually some token economy, and it takes a fundamental role in blockchain technology. However, modern blockchain technology is looking for a way to leave the cryptocurrency shadow.

Blockchain vs Distributed Ledger Comparison Table

Here’s a quick comparison showcasing the difference between distributed ledgers and blockchain technology,

Advantages of Using a Distributed Ledger like Blockchain

Using blockchain technology offers a secure and efficient way to create a tamper-proof log of sensitive activity. Blockchain has the potential to give an organization a safe and digital alternative to banking processes.

We can use distributed ledgers like blockchain for financial transactions as they help reduce operational inefficiencies and save money. Since distributed ledgers like blockchains are decentralized in nature and the ledgers are immutable, they offer greater security to the organization.

Distributed Ledger Technology Beyond Blockchain

Although the popularly known distributed ledger technology is blockchain, the distributed ledger technology future will depend on the collaborative effort of the two technologies.

According to James Wallis, the Vice President of Blockchain Markets and Engagements for IBM, the uses of DLT will be greater than what we can think of today, but it will require a level of sharing that does exist before.

Furthermore, if DLTs become standard, they can easily revolutionize the Know Your Customer (KYC) process. For those who don’t know, KYC is a business process to identify and verify its clients' identities. It will then help make broader identity management much more straightforward.

You may also like,",5958
"Tech CEO to workforce: Shut up, get back to work, and ignore those annoying critics

Software company Basecamp created a self-made controversy when co-Founder and CEO Jason Fried published a “Changes at Basecamp” memo on April 26, declaring “no more societal or political discussions” at work, “no more paternalistic benefits”, “no more committees” and other “no more” decrees.

While the company received an onslaught of criticism mixed with some applause for basically telling their employees to shut up and get back to work — and over a third of its 57 employees resigned — another change that has received far less public criticism and at first glance may not seem so egregious but really pushed my buttons was titled “No more lingering or dwelling on past decisions.” It reads:

“We’ve become a bit too precious with decision making over the last few years. Either by wallowing in indecisiveness, worrying ourselves into overthinking things, taking on a defensive posture and assuming the worst outcome is the likely outcome, putting too much energy into something that only needed a quick fix, inadvertently derailing projects when casual suggestions are taken as essential imperatives, or rehashing decisions in different forums or mediums. It’s time to get back to making calls, explaining why once, and moving on.”

I do not know which decisions the paragraph refers to, but I do know it reeks of an arrogance born, in part, of an environment that has long convinced the tech industry that they are smarter and more important than the rest of us and should not be bogged down with such “precious” ideas as responsibility, accountability, or duty of care.

Do I think Basecamp has the potential to cause societal harm in the way a company like my former employer Facebook has? No. But a tech CEO deciding to use his public platform, where he will undoubtedly influence other tech companies’ thinking, to make a public statement like this is not just negligent, it is downright dangerous. Having it then amplified with a tweet from Coinbase’s Brian Armstrong — with over a half million followers — as a courageous decision and asking who will be next makes it even worse. It sends the signal that tech companies are tired of the critics, tired of those pesky outside voices, tired of being pressured to worry about potential unintended consequences of their products and want to return to their core culture: move fast, break things, deal with the consequences later, and most importantly, don’t worry about having to answer for mistakes, no matter how they may affect society.

I know this culture all too well from my time at Facebook, where I was hired to do exactly what Fried seems to be saying will no longer be allowed at Basecamp: think critically about how to ensure bad actors do not use the platform with malicious intent. One of the many reasons I did not even last six months at Facebook was that I wanted to dissect the problem and then ensure that the company not only built programs and tools to protect users and — in the case of my particular role and entire life’s work, democracy — but that we do so after thinking through the myriad potential consequences, good and bad, of what we were about to throw out into the world. Yes, this means slowing down; this means asking tough questions; this means “assuming the worst outcomes” and building with those in mind.

In fact, one of the only criticisms of my actual work from my Facebook manager was that I was “too slow” to respond to emails. This was in regards to a question that would require an operational decision that had the potential to affect another country’s election, and I wanted to ensure I checked with everyone working the issue before giving an opinion. (Mind you, I responded the same day, just not within 5 minutes.) This is why Fried’s statement that “It’s time to get back to making calls, explaining why once, and moving on” rings so familiar, and in my opinion is one of the truly dangerous attitudes in parts of tech culture.

This is what I was trained to do as a CIA analyst: look around corners, anticipate the worst, identify signposts that would help evaluate over time if the worst is coming, constantly re-evaluate and be willing to update my analysis and recommendations as realities change on the ground. This is what those of us in the “risk” or “protection” space do, and yes, it builds unwanted friction into a system where speed and scale are often viewed as more important than the consequences.

As I interpret Fried’s post, he seems to be tired of having to think about any of this. He seems to be annoyed that the public has begun paying attention to how tech companies affect everyone’s lives, about questions being asked, about calls for more scrutiny, about the public wanting the tech industry to live by the same set of standards as the rest of the country. It reads as a backlash to the so-called “techlash”.

It is tempting to say that Basecamp isn’t Facebook and should not have to suffer the downstream effects of public anger at that company. I agree, to a point. And I certainly don’t put all tech companies into the same bucket and in fact work with a number that I admire. But after reading Basecamp employee Jane Yang’s “open letter to Jason and David” and her description of the so-called “Moral Quandaries” cases she worked on, I feel far less forgiving.

I have spent enough time in the tech industry to understand that culture is one of the key areas not explored deeply enough when trying to re-align technology with greater societal issues. Culture determines how a company will handle difficult moments, how they will hold themselves accountable for their mistakes, and how they will react to this pivotal moment in our country’s social history. Retreating to the “we just build stuff” and “we’re not the problem”, head-in-the-sand mentality while actively discouraging employees’ efforts to build safer practices (again, Jane Yang’s letter) perpetuates a culture and environment where tech companies insulate themselves from outside criticism and continue building sometimes-harmful products under the guise of “we’re not responsible for how bad people engage with our product.”

I have written before on the tension between the “move fast” tech culture and the “slow down” mentality of those in the risk space. As I wrote in my piece The Tech Giants’ Cultures Are Incompatible With Fixing the Societal Problems They’re Causing: “Scalability, quantifiable impact on growth, speed and competitive edge are what define success in a company like Facebook. The employees who meet these metrics…are revenue generators, and as such they occupy the top tiers of both financial reward and decision-making power. This encourages a culture where tech problems can only be solved by more tech. The roles that people like me fill in trust, safety, risk assessment and harm mitigation are… generally second tier in companies’ power structures…We’re the ones asking the company to slow down, challenging assumptions, analyzing multiple potential scenarios… that is just not what most of today’s fast-paced tech companies, or the investors who back them, value.”

I know this culture will not change overnight, and there will always be people who find those of us who say things like “but have you considered how that might affect….” simply a nuisance. But to see tech CEOs openly scold their employees for trying to have a more balanced approach, after all the public conversation about tech and society in recent years, is simply astonishing and, to say the least, hugely disappointing.",7611
"Your UMB tokens can be staked in the Umbrella Network’s staking portal to earn you rUMB rewards. The rUMB rewards can be redeemed 1:1 to UMB tokens on the achievement of distinct business goals or the passage of a fixed duration of time.

Uniswap Contract Address: 0x7a250d5630B4cF539739dF2C5dAcb4c659F2488D

rUMB1 Contract Address: 0x1b17dbb40fbed8735e7fe8c9eb02c20984fadfd6

Minted 3,402,778 rUMB1 tokens

Sent to contract: 3,244,292 tokens

Uniswap AMM Pool: 158,486 tokens

ETH: 17.777

Approximately $0.20 per rUMB

rUMB1 sent to contract is for 29 days. The shortened month only applies in month 1, after which will be normal months. You won’t have to renew every month.

This example uses Metamask as the wallet, but you can use any other web3 wallet as well.

To stake your tokens, you need to first connect your Metamask wallet to the Umbrella Network’s web3 staking platform. Get the Metamask Chrome extension here. Get the iOS app here and the Android app here.

Once done, head over to https://staking.umb.network (verify that you’re on the secure site)

(The presence of the lock icon on the address bar means that you are on the secure site)

Then, click on the ‘Connect Wallet’ button.

On a successful connection, your Metamask status shall change from ‘Not Connected’ to ‘Connected’ as shown in the image below.

Additionally, on the staking portal secure site, you’ll see a green dot on the top right with the word ‘Connected’ right next to it and your wallet address to its right within square brackets to its right.

Ensure that the wallet address shown in your Metamask wallet and on the staking portal is the same.

Now, you’ve successfully connected your Metamask wallet to the Umbrella Network’s staking platform and are ready to stake your UMB tokens and earn rewards.

Staking Your UMB Tokens

On the same page, you’ll see the headings — ‘Stake’, ‘Withdraw’ and ‘Your Wallet’ as bold and clickable buttons.

Click on Stake and it will start to glow Blue as depicted above.

Your UMB Balance shall show up under the ‘Balance’ section, right next to your wallet.

Click on ‘MAX’ to stake all your tokens at once OR enter a number within the limit of your Balance.

Next, you will be asked to provide an ‘Allowance’

This is your UMB token amount that will NOT be staked and stay in your wallet.

Click on the ‘Staking’ radio button and click ‘Okay’ to commence staking (depicted below).

To skip the ‘Allowance’ step, click on the ‘Trust contract forever’ checkbox and click on the ‘Stake’ button to commence staking

Withdrawing Your UMB Tokens

Click on the heading that says ‘Withdraw’ and it shall glow blue

Enter the number of UMB tokens you wish to withdraw

To select all of your staked tokens, click on the ‘MAX’ button

Click on the ‘WITHDRAW’ button to commence the withdrawal of your staked UMB tokens back into your wallet

For 1-click withdrawal of all your staked UMB tokens and your earned rUMB tokens, simply click on the ‘CLAIM REWARDS & WITHDRAW ALL’ button

Claiming Your Rewards

On successful completion of the staking/withdrawal process, your net balance shall display under the heading ‘Your Wallet’.",3148
"Converting light to electricity effectively has been one of the persistent goals of scientists in the field of optoelectronics. While improving the conversion efficiency is a challenge, several other requirements also need to be met. For instance, the material must conduct electricity well, have a short response time to changes in input (light intensity), and, most importantly, be stable under long-term exposure.

Lately, scientists have been fascinated with “coordination nanosheets” (CONASHs), that are organic-inorganic hybrid nanomaterials in which organic molecules are bonded to metal atoms in a 2D network. The interest in CONASHs stems mainly from their ability to absorb light at multiple wavelength ranges and convert them into electrons with greater efficiency than other types of nanosheets. This feat was observed in a CONASH comprising a zinc atom bonded with a porphyrin-dipyrrin molecule. Unfortunately, the CONASH quickly became corroded due to the low stability of organic molecules in liquid electrolytes (a medium commonly used for current conduction).

“The durability issue needs to be solved to realize the practical applications of CONASH-based photoelectric conversion systems,” says Prof. Hiroshi Nishihara from Tokyo University of Science (TUS), Japan, who conducts research on CONASH and has been trying to solve the CONASH stability problem.

Now, in a recent study published in Advanced Science as a result of a collaborative research between National Institute for Materials Science (NIMS), Japan and TUS, Prof. Nishihara and his colleagues, Dr. Hiroaki Maeda and Dr. Naoya Fukui from TUS, Dr. Ying-Chiao Wang and Dr. Kazuhito Tsukagoshi from NIMS, Mr. Chun-Hao Chiang and Prof. Chun-Wei Chen from National Taiwan University, Taiwan, and Dr. Chi-Ming Chang and Prof. Wen-Bin Jian from National Chiao-Tung University, Taiwan, have designed a CONASH comprising an iron (Fe) ion bonded to a benzene hexathiol (BHT) molecule that has demonstrated the highest stability under air exposure reported so far. The new FeBHT CONASH-based photodetector can retain over 94% of its photocurrent after 60 days of exposure! Moreover, the device requires no external power source.

Formation of FeBHT complex-based CONASH at the liquid-liquid interface and its long-term stability as a photodetector | Image courtesy: Hiroshi Nishihara from Tokyo University of Science

What made such a feat possible? Put simply, the scientists made some smart choices. Firstly, they went for an all-solid architecture by replacing the liquid electrolyte with a solid-state layer of Spiro-OMeTAD, a material known to be an efficient transporter of “holes” (vacancies left behind by electrons). Secondly, they synthesized the FeBHT network from a reaction between iron ammonium sulfate and BHT, which accomplished two things: one, the reaction was slow enough to keep the sulfur group protected from being oxidized, and two, it helped the resultant FeBHT network become resilient to oxidation, as the scientists confirmed using density functional theory calculations.

In addition, the FeBHT CONASH favored high electrical conductivity, showed an enhanced photoresponse with a conversion efficiency of 6% (the highest efficiency previously reported was 2%), and a response time < 40 milliseconds for UV light illumination.

With these results, the scientists are thrilled about the prospects of CONASH in commercialized optoelectronic applications. “The high performance of the CONASH-based photodetectors coupled with the fact that they are self-powered can pave the way for their practical applications such as in light-receiving sensors that can be used for mobile applications and recording the light exposure history of objects,” says Prof. Nishihara excitedly.",3768
"dHealth, Roche’s Blockchain Partner, to Launch Upcoming IDO on the OccamRazer Launchpad Occam_PR Follow Aug 27 · 3 min read

Together with Roche, dHealth will bring pharmaceutical traceability to Cardano

ZUG, 27 AUGUST 2021 — dHealth, an open network of supernodes built for decentralized and permissioned medical and pharmaceutical data storage, will make its debut through an IDO on the OccamRazer launchpad. dHealth’s solution, already supported by leading Swiss pharmaceutical giant Roche and Swiss Universities, will leverage the Cardano network to add to its stakeholders network of supernodes — which operate as access-points and network verifiers.

The existing healthcare industry is plagued by both a lack of transparency and interoperability, resulting in many siloed intermediaries, as well as inefficient data access by all stakeholders. To address this, the dHealth supernode network enables peer-to-peer interactions among stakeholders; without the need to build-out separate digital infrastructure for each user group.

Eberhard Scheuer, President and Founder of dHealth Foundation, says:

“dHealth is focused on developing the most accessible, decentralized, and cost-effective network of supernodes to alleviate the information asymmetry and lack of transparency in the existing healthcare industry. Both dHealth and our node operators, such as Roche and University of Zurich, firmly believe that Cardano is among the most promising blockchain protocols developed to date, and the Occam.fi team are perfectly placed to help us tailor our offering for the Cardano blockchain.”

dHealth Network components and infrastructure facilitates the application of blockchain in healthcare — resulting in faster, cheaper and more transparent infrastructure than traditional centralized solutions.

Elliot Hill, Head of Communications at Occam.fi, says:

“The sophistication of dHealth’s supernode network technology and the calibre of their partners speaks for itself. We are immensely proud to bring dHealth to the Cardano ecosystem through their IDO on OccamRazer — yet another existing project with a strong team and vision joins the Occam.fi family. We look forward to working with the dHealth team.”

dHealth’s other partners and supernode operators include Novartis, Swiss Tropical and Public Health Institute, the NEM protocol, the University of Basel, the UZH Blockchain Center, and more. dHealth’s IDO date will be announced soon.

Follow dHealth’s social media channels to stay up to date with the latest news:

Telegram Official Community — https://t.me/dHealthCommunity

— https://t.me/dHealthCommunity Telegram Announcements — https://t.me/dHealthNetwork

— https://t.me/dHealthNetwork Website — https://dhealth.network

— https://dhealth.network Blog — https://dhealth.network/news

— https://dhealth.network/news Twitter — https://twitter.com/dHealth_Network

Follow Occam.fi’s social media channels to stay up to date with the latest news:",2963
"Global Histology and Cytology Market Research Report Size, Trends and Forecast gives an inside and out investigation of the Histology and Cytology Market including definite depiction of market estimating and development, esteem, the vital chances in the Histology and Cytology Market and blueprints the components that are and will be driving the development of the business thinking about the past development designs with Forecast Period 2021–2025.

The Histology and Cytology Market report gives knowledge into fundamental drivers, difficulties, openings and hazard of the market and systems of providers. Central members are profiled too with their pieces of the pie in the worldwide Histology and Cytology Market examined. By and large, this report covers the verifiable circumstance, present status and the future prospects. Additionally, the overall market is divided dependent on type, application, arrangements, and region. It contains numbers, locales, income, and offers similarly as the intensive calculation of the business chain structure, openings, industry news investigation. The application segment shows the vocations of the item. The Histology and Cytology Market report additionally covers a point by point perception of the significant geologies present in the market alongside the key fragments and sub-sections. The report centres around provincial improvement status, which incorporates the market size, offer, and volume.",1447
"Imagine a world where patients directly fund researchers developing the next therapeutic breakthrough they need. One where drug development is collaborative, open, and decentralized — an open bazaar anyone with the right intentions can join. A system that could radically increase the diversity of treatments, and lower costs and time to market.

The majority of promising early-stage therapeutic research lacks funding and access. Funders struggle to discover and engage with suitable projects. In addition, IP is a highly illiquid, bureaucratic and opaque asset class that restricts virtual scientific collaboration through printed and filed legal constraints.

Drawing on original Linux metaphors and open source software. We live in a world where big pharma constructs expensive cathedrals over decades that serve their clerics’ interests more than they serve their followers. +10 years to market, +$2bn in avg. development costs, coupled with an appaling failure rate — biopharma is a system with zero consumer choice and 100% dependency. We need bazaars.

To address these challenges, we’ve set out to create an open marketplace that makes novel therapeutic projects discoverable, universally fundable and distributes ownership to patient collectives.

Diseases have global networks of stakeholders: Imagine a new insulin treatment funded, governed, and owned by diabetics. Our marketplace connects researchers with patients to enable this.

Photo by Francesco Ungaro from Pexels

Learning to Swim: an OpenSea for Biotech IP

Over time, we aim to build a full-fledged funding ecosystem for early-stage research discoveries. Molecule is an OpenSea of biotech IP. Powering a new creator economy for researchers that allows for the rapid funding, discovery, and development of therapeutics through globally connected patient collectives. Virtual and sustainable. We believe this system will be equally beneficial to the current pharma industry as it will be for patients: by decentralising access and helping discover promising research more quickly.

In a first step, Molecule provides an interoperable Web3 marketplace leveraging a scalable legal IP to NFT framework. Our novel approach allows for IP to be virtualized and modularised in an open bazaar. To achieve this, we utilize a combination of unique digital asset identifiers (NFTs), novel automatic exchange infrastructure (AMMs) and governance structures (DAOs) to reinvent IP ownership, financing of these assets, and entity creation. Our tech stack allows therapeutics to be funded and governed in entirely new ways that leverage the decentralised power of open communities and decentralised finance (DeFi), as opposed to single centralized entities and corporations.

We began exploring the combination of IP and NFTs with DeFi in 2018. Fortune favours the brave.

Molecule has two layers — Molecule Discovery and Molecule Finance. Combined, they enable the creation of a vibrant ecosystem for decentralized modular drug development.

Molecule Discovery — Making Research & IP Globally Discoverable

A first objective on our journey is to accelerate the discovery of promising early-stage therapeutics and bridge the “Valley of Death” in academia. We achieve this by creating a platform for individuals and organizations to interact with intellectual property and data — the two core innovation drivers and assets in biopharma. On Molecule Discovery, researchers can make their biopharma assets (data as well as pre-filed patents) visible to a global audience, receive feedback and engage with potential funders. The platform allows them to list the details of their projects, specify the commercial opportunity and upload data assets on an interface that feels familiar. Inclined investors can discover and follow projects in areas of their interest, engage with the research team and ask for further qualification. When interest from potential investors reaches a certain maturity, the IP moves into a new licensing platform built on Web3.0.

Filtering for global longevity projects: A snapshot of 3 out of +250 projects on Molecule Discovery.

Virtualising Research On-Chain: IPNFTs

The evolution of the biopharma IP transactional landscape is happening in part via a class of digital assets called Non-Fungible-Tokens (NFTs). A new NFT standard, the IPNFT developed by Molecule, will unlock new value in biopharma IP through fractionalization, frictionless tech transfer transactions, and collateralization of IP in decentralized financial systems in a permissionless way. The art and music industry embraced NFTs by storm, its time we empower scientists. Molecule’s first use case for IPNFTs is in early-stage biopharma where university tech transfer offices (TTOs) sit on IP goldmines languishing in obscurity for lack of easy access to liquid global markets.

Our platform enables these early-stage biopharma assets to be attached to NFTs via automated licensing frameworks and decentralised data storage systems like IPFS or Arweave. Once on-chain, NFTs can then be plugged into the broader Web3 and DeFi ecosystem to access price discovery and liquidity, financing and virtual drug development frameworks like DAOs.

In 2015, pioneering research in blockchain applications highlighted IP management and fractionalization as one of its key use-cases. Research collectives like COALA IP and Ascribe come to mind. Thanks to open networks like Ethereum and Ocean Protocol, technology, and adoption has matured to the extent that these early concepts are finally within reach of practical implementation.

The type of IPNFT developed by Molecule consists of two contracts: a legal contract and a smart contract, which cross-reference each other. The legal contract is an IP license with language referencing blockchain transactions, addresses, and signatures. The smart contract is an NFT with code referencing the IP licensing agreement, obfuscating certain data components and storing them on decentralised file storage networks. Combined, the legal contract and the smart contract create the IPNFT.

Molecule Finance — Full Decentralized Modular Drug Development

But research projects and IP require an active team, community and funders to become productive assets. Molecule.Finance enables modular drug development by creating virtual funding and collaboration environments. NFTs can be moved into automatic market makers to raise funding, or inserted into DAOs managed as a portfolio in a specific therapeutic vertical. These structures mirror virtual biopharma startups or holding companies that consist of various global stakeholders, funders and researchers — but fully distribute governance rights to their community.",6699
"Photo by Johanna Buguet on Unsplash

(this post is also available in my blog)

Clarification

System design interviews, like coding interviews, do not only test candidates’ technical skills, but also validate if candidates can solve a problem that’s not well defined. Questions are asked vaguely and generally. It’s candidates’ job to find out the requirements and come up with a proper design. It’s a common mistake for a candidate to jump into design right after the get the question. For a question like design at Twitter, you may need to find out what core features are needed to consider, registration? profile? text tweet? video/image tweet? Timeline? Other technical requirements may be Read or write per second? Peak/Avg number of requests? Users base location?

Key components for working solution

Once you have the requirements, you can start to think about the structure of the system or its subsystems if it’s for more than one feature. It’s always good to find out a working solution first instead of trying to come up with a perfect optimal solution. Try to come up with a simple system with all necessary components. Lay out the system on the whiteboard. Components are like Application Servers, Data store (SQL or NoSQL), Proxy, CDN, Message Queue, Event Bus etc. Pick the components the basic solution needs. Explain why these components are required, how they should be connected together, and how users interact with the systems.

List the APIs

APIs are expression of communication between components. Being able to design a set of logic APIs is a plus for candidate. For RESTful API, it’s good to follow REST convention to use HTTP verbs proper for resource operation like CRUD. Paths should be nouns that represent resources. It’s also helpful to show that you can use query parameters, request body properly in your design.

Optimization

Then, you need to optimize the system. When considering optimization, you will need to optimize the right thing. It’s better to optimize for common use cases. For example, if it’s more common to read data from a database than to write data to the database, it makes sense to introduce a cache between the data requester and the persistent data store. It helps serve hot data much more quickly, and cache invalidation will be less frequent as write isn’t a lot. But introducing a cache for a write intense system would have a negative impact on performance because it will frequently cause cache invalidation. Other optimization could be distributing requests to different datacenters base on users’ location. Partition the database into shards to handle requests. Properly design keys in the table so that request traffic will be evenly distributed to the partition.

Scalability

How about as your user base grows? It is always certain that you will need to design a horizontally scalable system. Horizontal scalability means that the system can handle more requests by adding more and more commodity computing power. It’s more cost-effective than adding more horsepower to one single server.

Tradeoff

There’s no doubt that you have heard of a lot of popular software to build a distributed system. Things like MongoDB, ElasticSearch, Memcached, Kafka etc. However, randomly throw these buzzwords to the interviewers won’t be enough to impress them. It is critical for you to explain the choice and why not to choose the other. For example, what benefits does MongoDB give you that MySQL won’t? Maybe your application doesn’t care about the relation between data model and would prefer better performance over strong consistency. For another example, why you will prefer to build event-driven system with Kafka. Maybe your business logic is long process and it needs to complete it asynchronously. Maybe you have a few subsystems that need to know of the event.

Error handling

Software is never perfect, so you as a candidate should explain how your system handles errors. One key point is that you are able to identify a single point of failure and provide a solution to lower the chase of failing the entire system. Another common error scenario is that the traffic is too large for the system to handle. One common solution is throttle. Throttling is the process of limiting the number of requests to the system. By limiting number of requests per client, the system can have relatively predictable workload. Hence, it also helps to plan sociability.

Last words

These are some points that you would like to consider during the system design interview. If you would like to get more technical details about the system design, I would recommend use Designing Data-Intensive Applications and Grokking System Design as reference for the preparation. Good Luck.",4729
"TL;DR: Impact investing is maturing and some exciting things are happening.

In June 2021, for the second time running, I held a Masterclass (for lack of a better word) on Impact Investing for the latest cohort of Future VC, a programme run by Diversity VC to encourage more people from underrepresented communities to enter the venture capital (VC) industry. So in the spirit of opening up VC, here’s my deck from the Future VC class as well as an accompanying longer blog hopefully demystifying the world of impact investing.

As I was updating my deck from last year, one thing became apparent. More VC funding is available for impact-driven startups in Europe compared to the previous year. It’s wonderful to see the sheer amount of European funds that have emerged in the last year alone, all dedicated to impact one way or another — with the likes of Pale Blue Dot closing an €87M fund to invest in tech companies tackling the climate emergency, Revent in Berlin with an initial close of €20M backing ‘for profit, for purpose’ startups, Remagine in Berlin securing €24M, providing high growth, impact-led startups with revenue-based financing and more. In general, the impact investing industry spans various asset classes and has seen incredible growth over the last few years. But personally, the massive growth of the sector in early-stage VC is by far the most exciting development. Especially when more and more founders are looking for value-aligned investors to add to their cap table because it matters who is on your cap table. And thankfully, there’s no shortage of impact VCs now.

Still, many don’t actually know what impact investing is and often peddle quite a few misconceptions. Before we dive deeper, let’s address the 🐘 in the room. ESG is all the rage right now, especially in the VC industry. ESG investment is set to grow rapidly, and that’s great. We need more companies and VCs to get their house in order and care about doing the right thing across the environmental, social, and governance domains. VCs in particular need to step up when it comes to considering the broader risks to human rights violations. This report from Amnesty USA highlights that none of the world’s top ten largest VC firms have sufficient human rights due diligence policies in place.

It’s also important to highlight that a screening process that’s merely focused on avoiding harm (often referred to as ESG risk management) or that might benefit stakeholders (often referred to as pursuing ESG opportunities) doesn’t always result in net-positive outcomes. On the other hand, impact investors are driven by the motivation that it is not enough to only avoid harm in investment decisions. Instead, impact investors make the conscious decision to use capital to contribute to solutions. With a multitude of pressing social and environmental problems from the climate crisis to insecure work, to increasing poverty and racial inequality, and more, we actively need to invest in purpose-driven companies looking to drive positive outcomes for people and planet.

At BGV, we believe technology plays a big part in addressing these challenges, which is why we’re focused on investing in tech for good companies driving both purpose and profit at scale. That’s not to say that technology is the magical formula to all our problems. Despite being a strong advocate for #TechForGood, we try really hard not to further the narrative of tech solutionism — and if you’re unfamiliar with the concept, read this brilliant article from Evgeny Morozov.

“Why would a government invest in rebuilding crumbling public transport systems, for example, when it could simply use big data to craft personalised incentives for passengers to discourage journeys at peak times?” In the Guardian, from Evgeny Morozov, “The tech ‘solutions’ for coronavirus take the surveillance state to the next level”

Many of the issues we’re faced with stem from a long legacy of systemic discrimination, inequality, and racism and won’t be solved by tech. But we can explore how tech innovations can significantly contribute to better outcomes for people and planet in spite of it. Let’s face it — the world is f***ed. The latest Sustainable Development Report 2021 highlighted how the COVID-19 pandemic and resulting economic crisis is a setback for sustainable development everywhere. Last week’s IPCC report is a stark reminder that the climate crisis is rapidly getting worse. That’s why we need to be more conscious about the investments we make. What impact investors aspire to do, regardless of whether an investment is into a tech company or not, is to ensure that the investments we make today have a material effect on important positive outcomes for underserved people and the planet.

In a world where billions (yep, BILLIONS) have been deployed in Europe in 2021 alone (during a pandemic and economic crisis, I might add) to ensure consumers get their arrabbiata sauce in less than 10 minutes, investors need to understand how their deployment of capital will be judged by others (history has its eyes on you). For further reading on this topic, read this op-ed in Sifted from Johannes Lenhard.

But without further ado, let’s dive deeper into the world of impact investing.

What is impact investing?

To put impact investing into context, it’s always good to remind ourselves that all investments have impact — both positive and negative. The most common definition for impact investment derives from the Global Impact Investing Network (GIIN), which defines impact investments as investments “made with the intention to generate positive, measurable social and environmental impact alongside a financial return.” The very nature of this definition allows it to focus on an investor’s approach to investing rather than the membership to a specific asset class.

‘Impact’ in impact investing is broadly defined as any meaningful positive change due to specific actions. Intentionality plays a significant role in the impact investing world, both in terms of investors’ intentions to allocate capital to drive better outcomes and assessing founders’ motivations and ambitions to start and run a business that has purpose at the core of it. Impact investors seek a financial return, albeit across a whole returns spectrum running from below-market-rate to risk-adjusted market rate. But perhaps the most distinguishing characteristic is impact measurement, dubbed the hallmark of impact investing, signalling the commitment of investors to measure and report on the social and environmental performance of portfolio companies.

However, it’s worth noting that not all investments promoting sustainable development are classified as impact investments. Take investments in electric vehicles as an example. A traditional VC, Investor A, might invest purely because of the expected financial returns, as the electrical vehicle market is growing rapidly and adoption is promoted and supported by regulation. On the other hand, Investor B, who is an impact investor, lists the potential for a massive reduction in carbon emissions as one of the main motivations to invest and has the critical intent to deploy capital to drive positive environmental outcomes, but also seek significant financial returns. Is one investment approach better than the other? Perhaps. Perhaps not. But you can pretty much assume that the impact investor will demand a higher degree of accountability from the company to evidence and measure the environmental outcomes than the traditional investor unless they have a strong ESG mandate.

Sizing the market

Impact investing is a rapidly growing field, and the impact investing landscape has gradually become more prominent over the last decade. According to GIIN’s 2020 Annual Impact Investor Survey, the full impact investing market size is $715 billion, covering the assets under management (AUM) of over 1,720 organisations globally. The IFC estimated investor appetite for impact investing to be as high as $26 trillion, of which nearly $5 trillion (19%) is in private markets, involving private equity (PE), non-sovereign private debt, and VC. The market offers great potential, and new investment opportunities and vehicles are emerging to finance and support impact-driven founders enabling founders and investors to pursue both impact and profit as they become inextricably linked.

The industry has undoubtedly witnessed significant shifts from consumers and investors alike, who increasingly want to put their money where their mouth is. Research into consumer trends from Zenogroup’s Strength of Purpose Study (2020), Deloitte (2019), and many others suggest there are increasing shifts in consumers’ purchasing behaviour towards purpose-driven businesses and brands they trust to do good. Simply put, people want to buy products and services from companies they perceive to do good and drive social impact.

“Purpose-driven companies witness higher market share gains and grow three times faster on average than their competitors, all while achieving higher workforce and customer satisfaction.” Deloitte, Purpose is Everything, 2019.

On the investor side, research suggests that individual investors and limited partners (LPs) increasingly care about the social and environmental impact of the companies and portfolios they invest in. For example, Atomico’s 2020 State of European Tech Report suggests that 45% of LP respondents to the survey require their GPs to report on their portfolio’s social and environmental impact, and 41% are considering implementing this requirement. Arguably, demographic shifts and generational wealth exchange have also meant that more and more individuals are increasingly interested in impact investing. For example, a 2019 study from Morgan Stanley suggests that more than 8 in 10 US individual investors now express an interest in sustainable investing. And we see this shift in consumers with attempts to democratise and open up investing through impact investing platforms like Tickr, who say that 90% of their users are investing for the very first time.

In the past, you might have come across asset owners thinking investing in impact violates their fiduciary duty because it doesn’t maximise risk-adjusted returns (think pensions who manage millions and millions of individuals’ retirement savings). But nowadays, asset owners see the threats to the long-term value of their assets and the wellbeing of their clients, critically due to climate change (again, we’re f***ed, if we don’t act). More and more asset owners see clear benefits from integrating impact, ditching the trade-off mentality regarding investment performance pursuing purpose and profit. A 2020 survey from Cambridge Associates found more than half of the respondents, with the majority being foundations and endowments, taking active steps towards sustainable or impact investing. Many increasingly recognise how the state of the world affects their responsibility to beneficiaries and are adopting new interpretations of fiduciary duty. For instance, this group of pension funds is working on practical steps to integrate sustainability into investment practice, realising that climate change threatens the fund’s ability to uphold its fiduciary duty. Clearly, we should all ask ourselves what world we want to retire into. We can ensure that asset owners, especially stewards of long-term capital like pension funds, actively use their capital to drive change. And BGV’s recent acquisition by Connected is one of hopefully many more emerging examples celebrating this shift in VC.

Impact measurement, the hallmark of #impinv

Translating intent into action is perhaps the most significant advancement in the maturing sector with increasing levels of sophistication in impact measurement and management (IMM) practices. Robust IMM practices certainly help founders in various ways. Adhering and setting standards helps founders crucially protect their purpose early on, validate their product or service and create an impact-driven fundraising narrative, as impact investors raise the stakes in their due diligence. All this applies to general partners (GPs) as well ;)

Impact measurement seems complex, but it really isn’t (though I work with a bunch of people who love this stuff as much as I do — so I’m definitely in my own little filter bubble). There are different methods available, and this guide from Best and Harji at Purpose Capital provides helpful context for the various processes to measure and manage impact. Generally, impact investors have many resources to work from, which can be distilled into principles, frameworks and standards that provide guidance on setting impact objectives, measurement and reporting on impact performance. According to the GIIN 2020 Survey, the most commonly used frameworks were the UN Sustainable Development Goals commonly referred to as the SDGs or Global Goals (73%), the IRIS Catalog of Metrics (46%), IRIS+ Core Metrics Sets (36%), and the IMP’s five dimensions of impact (32%). Most investors in the sample (89%) use a blend of three tools, systems or frameworks to measure and manage their impact, with only a small proportion using proprietary methods.

Figure from GIIN 2020 Survey showing the use of tools, frameworks, and systems, by purpose

Let’s look at some of the various IMM tools that help investors translate intention into impact results.

Principles: In the impact investing space, various sets of principles serve as the foundation for broad rules and best practices for the industry. They differ from frameworks and standards, as they more often than not communicate intent and might require a public commitment strengthening accountability in the space. Relevant examples include the UN Principles for Responsible Investment (UNPRI), which launched in 2006, and the IFC Operating Principles for Impact Management, commonly referred to as the Impact Principles, established in April 2019.

Frameworks: Frameworks are specific methodologies that translate impact principles and intent into practice. Examples include the UN SDGs and the IMP’s five dimensions of impact. Though, as someone who previously worked in advocacy for a civil society organisation involved in the consultation process for setting the indicators, I’d like to point out that the SDGs might provide a strategic blueprint for prosperity and peace for people and planet. Still, they left out some of the most marginalised communities. For example, considering that a large proportion of UN member states still criminalise homosexuality and being queer, it’s no surprise that the goals largely leave out targeted provisions for the LGBTQ+ community. This guide from Stonewall International provides substantial insight into the challenges faced by LBGTQ+ folk across all 17 goals. It suggests practical actions to ensure any progress made towards the SDGs also meets the needs of LGBTQ+ individuals. Overall, alignment with the SDGs is relatively easy to demonstrate, but translating alignment into action requires more than a big vision. And to measure net-positive impact, we need more. And that’s where standards come in.

Standards: Standards refer to taxonomies or a set of core metrics applied to specific verticals and sectors. Standards help determine the type of data you want to collect and measure to validate and evidence your impact, and mitigate impact washing. Examples include IRIS+ Core Metrics Sets, the SDG Compass linking the SDGs to the Global Reporting Initiative’s (GRI) Sustainability Disclosures, B Impact Assessment and many more.

“No one line of inquiry and evidence is going to tell you everything. IMM should help you ‘manage forward’ to improve your impact over time, rather than just look back at what impact has occurred.” Steven Godeke & Patrick Briaud, Rockefeller Philanthropy Advisors, The Impact Investing Handbook

Depending on your fund’s investment thesis and asset class, standardisation might seem like a Herculean task. It can also reduce the precision of information conveyed about how exactly portfolio companies are achieving transformational impact. Aggregating information into one value doesn’t always capture the complexity of the impact achieved. For example, at the end of 2020, BGV’s portfolio companies positively impacted 17 million lives. But who are the 17 million people? Some were refugees and internally displaced people using Chatterbox to gain access to decent work, where otherwise they might not have earned an income due to their displacement. Some were young women who swapped clothes to reduce the excessive amount of clothing going to landfill through Nuw. And some were people accessing vital health services during a global pandemic with DrDoctor. No one line of evidence will tell you everything, which is why it’s important to always leave enough room for qualitative methods to highlight impact stories. Regardless of how you structure your IMM practice and deliver platform support for portfolio companies to report on specific metrics and outcomes, it’s important to remember that this is an iterative process that will eventually change and evolve.

Impact alignment in the investment process

Impact alignment in the investment process is possible at various stages. If you are an investor or someone looking to start investing in impact, I hope the resources and questions to ask yourself will help.

Investment and portfolio strategy: What is your fund’s investment thesis? How does it align with your investment model? What else in addition to capital can you contribute?

See examples here from Bethnal Green Ventures, Future Positive Capital, and Kapor Capital. BGV, for instance, mandates that a company embeds their social and/or environmental mission into the articles of association to ensure no mission-drift occurs as companies scale, similar to Obvious Ventures World-Positive Term Sheet. Incidentally, this is also one of the first steps a company has to undergo to certify as BCorp. We’re also conscious about the contribution we can make to our portfolio companies aside from providing capital. At BGV, we run a 12-week acceleration programme to help founders build and launch their tech for good businesses and provide further platform support to our portfolio teams. 34% of our portfolio companies believe that without BGV their product or service would not have existed today, and a further 50% believe that without BGV they would not have been as far along in taking their product to market. So if you’re setting up a fund, consider not only the capital you provide but also what type of non-financial support can help founders level the playing field in making their business a success. Take into account what value-add you can provide, especially if you’re supporting first-time founders navigating the murky world of investment or founders from marginalised communities, who often lack the networks to raise capital. At a minimum, your fund’s investment strategy and approach should clearly link intent to asset selection, which in turn is based on a credible investment thesis. The Impact Investing Handbook by Steven Godeke & Patrick Briaud is an excellent resource that takes you on a step by step journey to adopt an impact lens to your fund structure, approach and portfolio management.

Investment screening and due diligence: How do you screen deals for impact? How do you validate a company’s impact?

This is probably one of the harder ones for VCs, especially if you’re a fund at the earlier spectrum of early-stage investing (btw early-stage is a stupid, ambiguous term). At BGV, we initially screen investments based on whether they broadly align with the impact outcomes we seek in the world — A Sustainable Planet, A Better Society and Healthy Lives. We have a few dedicated questions at various points in the pre-investment and due diligence phases that are sector-agnostic but can be tailored to the respective businesses. Norrsken VC provide a really good example of how they screen for impact and sustainability at various stages of the investment process. Needless to say, that it is much easier to screen for impact the more mature companies are. But it’s often incredibly hard at the earlier stages because founders might not consider themselves to be purpose-driven or impact-driven. Thus, a screening process that ensures alignment with the impact outcomes you seek and clearly articulating your thesis is essential.

Investment management: How can you help founders understand how to measure impact effectively? How do you manage a fund’s impact performance?

There are numerous standards and measurement frameworks, and many more ESG frameworks are currently in development. It’s important to note that a different level of evidence might apply depending on the company’s maturity. Nesta’s Standards of Evidence is a helpful framework to assess at which stages we can expect varying levels of confidence as to how a company’s intervention has a positive impact.

For tech for good ventures, we also need to ensure founders and investors understand the potentially harmful (un)intended consequences of products and services. Doteveryone’s Consequence Scanning Toolkit and Omidyar Network’s Ethical Explorer are toolkits we use as part of our programme to help founders early on to think about responsible tech development and build a mindset where thinking about unintended consequences is not abstract. Instead, it’s focused to help founders assess risk levels and prevalence and what potential mitigation strategies they can deploy. We’ve also started reporting on our portfolio companies’ impact risks that arise from trying to mitigate and avoid possible consequences. My brilliant colleague Yumi, who leads on BGV’s insights and operations, shares guidance here on how to assess your portfolio’s risks of unintended consequences and examples of how BGV’s portfolio companies validate their impact in this blog. Crucially, you should consider applying the same rigour to ensuring accountability to impact targets and reporting results in the same way investment professionals do for financial performance.

Exit: What does a responsible exit look like?

It’s often hard to influence a company’s impact trajectory for early-stage investors who usually take minority stakes, which are then diluted in future funding rounds, just as the impact and ESG risks grow. For funds like BGV, it means ensuring we have robust processes in place to spot any potential risks and build great relationships with founders to ensure we can support their trajectory to conscious scaling where needed. So what happens if an exit is on the horizon? A growing number of VCs are integrating a sustainability clause into term sheets and shareholder agreements, and examples of responsible exits emerged, most notably from Rubio Ventures. It’s still a relatively unexplored area, so please leave a comment if you come across any other examples.

Debunking myths

So what are these common misconceptions we hear in the impact investing space time and time again?

Myth: Impact investing means compromising on financial returns.

Debunked: No-oh-oooh. Think of profit and returns on a large spectrum with different financial return expectations.

67% of respondents to the 2020 GIIIN Survey principally target risk-adjusted, market-rate returns, with the remaining respondents seeking either closer-market-rate (18%) or below-market-rate: closer to capital preservation (15%). Overall, 88% of respondents reported meeting or exceeding their financial expectations.

More and more impact investors are seeking to invest in companies with apparent ‘lockstep’ — which intrinsically links their purpose to their commercial success. In other words, as impact scales, as do the financial returns for investors. As a result, companies are increasingly aligning their impact in the same direction as their EBITDA. Many studies have proven that you can indeed invest in impact and achieve net-positive impact with significant commercial returns in various settings. For example, this study from the Morgan Stanley Institute for Sustainable Investing reviewed the financial performance of over 11,000 mutual funds from 2004 to 2018 and suggests there is no financial trade-off in the returns of sustainable funds. Instead, investing in socially responsible companies is more profitable than investing in traditional companies. The 2021 Net Impact Report by the Upright Project also suggests that “making a positive impact is definitely not at odds with making profits.”

Similarly, Cambridge Associates and the GIIN launched the Impact Investing Benchmark in 2015, the first comprehensive analysis of the financial performance of market-rate PE and VC impact funds with supporting evidence to the solid financial performance of these funds. The IFC provides further proof that you do not have to trade-off between impact and returns, highlighting that the IFC’s realised equity investment delivered returns in line or better than the MSCI Emerging Market Index from 1988 to 2016. Notable VC examples include Kapor Capital, who in 2019 reported a 29.02% internal rate of return (IRR) and 3x Total Value to Paid In (TVPI), which elevates them to the top quartile of VC firms, and BGV with a 1.9x TV/Cost.

So, let’s ditch this outdated perception of a trade-off between impact and profits and acknowledge that there is a broad spectrum of returns expectations in impact investing. Check out Omidyar Network’s whitepaper “Across the Returns Continuum” for further reading on this topic.

Myth: By virtue of seeking impact outcomes, you’re a good company.

Debunked: Ah hell no.

Theranos, I rest my case.

Just because a company is trying to make the world a better place doesn’t mean they achieve a net-positive impact. The net impact of a company is not only the result of positive impact outcomes but also operational. Good governance, fair, decent and equal work and many more factors play a huge role in helping companies on a responsible trajectory to scale. So where can you start? For any new companies, honestly, try out the B Impact Assessment. It’s a lengthy questionnaire, sure, but it will help you spot the areas where you might need improvement or significant support from your community.

Quibbling over semantics

“Amid so much suffering and injustice, we cannot resign ourselves to the reality we’ve inherited. It is time to reimagine what is possible.” Ruha Benjamin, Race After Technology

We put labels on things to try to differentiate and make sense of things in this world. But sometimes, it really doesn’t matter. Call it impact investing, call it responsible investing, call it value-aligned investing, call it what you want. It doesn’t matter. What does matter, however, is recognising the urgency with which we need to respond to the challenges of our lifetime and for future generations to come and put our money to work.",27013
"Steve Jobs: The Confluence of Artistry and Technology

Story of a brutally honest artist who deeply cared about his creations. Yash Mishra Follow Aug 27 · 9 min read

Photo by Florian Doppler from Pexels

Steve had humble beginnings as young chap. His parents did jobs like bookkeeper and repo-man and were just able to make the ends meet. He used to get bored at school lectures but loved to see his father carving beautiful cars out of junk as hobby. He would stay on frequent fasts, fruitarian diets for weeks, would go anywhere barefooted due to his inclination towards Zen spiritualism yet could be as bratty as tripping acid and LSD occasionally. Once he travelled all the way from US to India in search of a spiritual guru for 7 months and voluntarily stayed in poverty throughout his travel. He dropped out of college and used to attend calligraphy classes. The boy knew how to get his work done even as a young chap as he once called the owner of HP to get a spare part and ended up getting a summer internship in HP.

He was born to an unwed couple or as he called them his biological parents— Joanne Schieble and a young teaching assistant Abdulfattah John Jandali from Syrian origins. The couple could not get along because Joanne’s father was against their marriage. So much so that he had to be put up for adoption by his mother. Her mother placed a condition though, that adopting parents should be at least college graduates. But good stories become great because there is a twist and this one had one right in beginning.

He was adopted by a lawyer couple which satisfied Joanne’s condition, but they later backed out because they wanted a girl and then he got adopted by a high school dropout Paul jobs and his wife Clara with a settlement with Joanne only after they pledged that they would save funds for the child’s college education. Paul and Clara jobs named their son as Steve.

In his childhood days, Steve was fascinated by Paul’s craftsmanship. Paul loved to restore old cars as a hobby and he also used to sell them later. Paul and Clara always made him feel as he was special rather than adopted. Steve felt school lectures to be uninteresting not because he was weak but because he was indeed special. Once his parents were called and told that he seems uninterested at lectures to which Paul told his teachers “If you can’t keep him interested it’s your fault”. Then one of his teachers started bribing him to the homework and she used to give him questions from 2 grades above he was in and he would do that with ease. Later, the school administration told his parents to make him skip 2 grades, but his parents settled with skipping 1 grade.

In college, Steve met another Steve. Steve Wozniak who famous for his wizardry in the class. He was a geeky prankster. He created a device that emitted TV signals and made the screen fuzzy. He would use it in a place where lot of people were watching TV together. People would think there is signal issue and would try to fix the antenna once someone tries to adjust it, he would clear the fuzziness. Just as the person moves away from the antenna, he would again make the screen fuzzy, and this go on and on till one person stands permanently with the antenna in one position.

Known as Woz, Wozniak was a super talented at electronics. He built a device which connected a keyboard and TV. Everything you type on the keyboard gets visible on the TV screen. Not a big deal now but back then it was not less than a miracle to Jobs who saw it as a huge opportunity to build something of their own. He drove Woz and the device to Homebrew computer club where all technology enthusiasts came to watch new devices created by hobbyists. They displayed their device at a club and Steve Jobs with his great articulation got a hardware shop owner to order 50 such devices in $500 each paid at delivery. They had to buy parts worth $15000. They took a loan of $5000 from a friend’s father, further credit was arranged by jobs articulation and that’s how Apple was born, and that computer became Apple I.

After their first successful delivery they developed Apple II which was a sleek machine and a super successful one as well which served as the cash cow for apple for a very long time. But Steve jobs didn’t liked Apple II. Why? Because it had more ports to connect multiple devices for the hobbyists. He always wanted end-to-end control of his product and even on its experience and those ports would never have allowed that kind of control.

By this time, his on and off girlfriend Chrisann shifted with him in his 4-bedroom apartment. Steve was sharing this apartment with a friend named Dan Kottke. It was this time she got pregnant with Steve’s child. Interestingly Steve was 23 years old at the when Chrisann gave birth to a girl child. This was the same age when his biological father Jandali had him. His daughter was named Lisa and he never shared a very cordial relationship with her. Another commonalty between his biological parents and him was — Steve never married Chrisann.

Apple went public and Steve jobs became a rich man but his quest for end-to-end control just became bigger. He applied a real growth hack when he convinced XEROX leadership to let him visit their R&D facility in return of an opportunity of 1-million-dollar investment bargain into Apple. Yes, it was a bargain!!! Apple engineers got to know about bitmapping which was the basis of GUI. The world was only aware of DOS black screen till then. This helped Apple create their own operating system which had GUI and which we know as Mac operating system today.

Steve could be an absolute charmer and verbally seducing or he can be as ruthless as calling a full-blown effort of his team as complete shit. He would leave no stone unturned to maintain end-to-end control over the user experience and not let his device become a toy for the hobbyist. So much so that even the screws for the system that had Macintosh were designed in a way that it would not open with a standard screwdriver. He would hackle over curves or lines on computer design for days until the product feels perfect to him. He could go to any limit to make his point on design. Once he took a colleague on drive to show that rounded rectangles are everywhere, and the device should also be in that shape. It was in Steve’s nature to look the world in binary as either something is the world’s most beautiful thing or complete shit.

He charmed/articulated Jhon Sculley Pepsi CEO to join Apple and take it to next level. He soon realized that Sculley was not a product person but Sculley was always under the illusion that he and Steve were very similar people. Some internal coup politics, some of Jobs bratty behavior and some management decisions like removing Jobs from Macintosh division led to a showdown between him and Sculley ending up in Steve being shown the door from the company he created.

After being ousted from Apple, Jobs created another company called NeXT which showed a lot of promise courtesy, Steve’s amazing marketing skills, but it didn’t had much to showcase on the ground. Interestingly, Steve paid a hefty amount of $100,000 for getting the logo designed for NeXT.

By now everyone was aware of his love for the intersection of creativity and technology which made one of his friend’s meet the Pixar owners. Earlier, he wanted Apple to buy it but was denied by the management. Later he went on to invest his own money and got a 70% stake. Pixar made some path-breaking, animated movies like Toy story trilogy, Cars etc. in collaboration with Disney. When Toy story 1 released Disney was having a great say in the partnership and Jobs never liked playing secondary fiddle to anyone. So, he took Pixar public and got the funds to make a good bargain with Disney now. Later, when Disney realized that they didn’t great creative people like Pixar had. They were left with no choice to buy Pixar at a very heavy price making Jobs the biggest shareholder of Disney.

Jobs wanted to go back to Apple, but he wanted that Apple should call him back. Apple was going through pretty bad times and were only 90 days away from getting bankrupt. He willed himself into Apple by making apple buy NeXT. He became advisor to the board and worked for 2 years on $1 as salary per year. He dropped around 70% of the products Apple was producing and brought it down to just 3–4 products. He also changed the narrative of apple by “Think different” campaign. He managed to get the company out of troubled waters but that was just the beginning. It’s a notion that Steve saw himself as a rebel and he thought the rules doesn’t apply to him. He used to park his car in handicapped parking or sometimes parked it on the boundary lines which sucked space for 2 cars. Just for fun some Apple employees pasted boards at the same place which said “Park different”.

Apple came up with iMac. The idea for the monitor design came up when Steve was gazing at a sunflower in his garden with his design alter ego Jony Ive. They improved the design in the next version of iMac by adding a translucent Bondi blue body to the monitor. They also added a handle to the monitor which was so nicely melted in the body that it became quite pleasing to the eye. It was a really stylish monitor.

In 2003, he was diagnosed with pancreatic cancer and he was told to get his affairs right which was a subtle way of saying you don’t have much time left to live. He avoided the surgery to remove the cancer for 9 months treating it with his own fruitarian and herbal diet. Finally, he agreed to have the surgery 9 months later. He got healthy but seeing death with such close proximity made him more aggressive towards work.

Next, they came up with the iPod with the tag line “1000 songs in your pocket”. Jobs always insisted to have simplicity in designs, and he would personally play with the wax models to get the feel of the device. He would give task to the team of coming up with an interface where the user must be able to find whatever he wants on the iPod with maximum of 3 clicks which led them to invent the beautiful trackwheel to scroll through the list of songs.

They came up with iTunes store where you could buy any song for just 99cents. This helped the music industry get regain some of its market lost to piracy. Steve was very articulate and great at negotiations which helped him get many music labels on iTunes store. However, some labels wanted royalties from the iPod sales to get on iTunes, others some wanted to sell the complete album not fragmented songs. He never agreed to it but slowly and steadily most of the music was available on iTunes. This in turn propelled the sales of iPod even more. He became one of the most powerful persons in the music industry at that point.

Then at an iconic product launch Steve came up with 3 products. An iPod, a mobile phone and an internet communication device. Steve then repeated the same and said this is all bundled in one device known as the iPhone. iPhone also went through a lot of design iterations they finally came up with one which had no keys. They also made the screen to extend from end to end and even patented the design that’s why only Apple phones has bazel-less screens. It was the first mobile to have gorilla glass as well.

He was losing weight and was not in healthy state when his cancer resurfaced in 2008. This time he got a lever transplant, but the situation was such that his family was called in the hospital as if he will not make it any further. He again denied death to see his son go through graduation which was his longtime wish and kind of a deal with god to stay alive to see this day.

He came back to launch the iPad, but he was so thin that lot of rumors about his unhealthiness surfaced and it became really messy when a rumor of his death spread. He later resigned as Apple CEO.

Before his official departure Apple became the world’s most valuable company but the most fascinating characteristic was that he kept coming back with world class product throughout his life against all odds. He left behind a legacy on which Apple is still thriving, taking full responsibility of end-to-end user experience.",12224
"Flutter app development just got way better and improved with the launch of Flutter 2. If you have been following our blogs on Flutter mobile app development and frameworks in general, you would know that we have always been only praising the app development platform. With its host of features and backing by Google, Flutter development is indeed one of the finest services mobile developers could get today.

And on the 3 rd of March 2021, Google kicked off an event called Flutter Engage and rolled out the next version of Flutter called Fluter 2. It is still open source and this time, it is designed to help developers work on moving beyond developing apps just for mobiles. Code reusability still lies at the fulcrum of Flutter 2 and in this iteration, it offers advanced capabilities and a giant leap in terms of compatibility.

So, if you are a developer looking for ideal Flutter cross platform app development practices, an entrepreneur who intends to launch multi-platform mobile apps across devices and mobile operating systems, and a business owner looking to explore other mediums to reach out to your audiences, Flutter app development has some great news. Today, we are going to delve into everything Flutter 2 has to offer and give you an idea of what all you could develop and do on the platform.

Let’s get started then.

Before we proceed to know about the improvements and offerings in Flutter 2, we have to acknowledge the tremendous amount of work that has gone at the backend to make this rollout a possibility.

For the uninitiated, Flutter 1.0 was released on 4 December 2018. Flutter 2 comes after almost two years of development and testing processes. Within this time, reports reveal that close to 24,541 issues from Flutter 1 have been fixed with the help of 765 contributors. And since the launch of Flutter 1.22, 5807 issues have been addressed and fixed with the assistance of 298 contributors. Apart from these, over 17,309 and 4091 PRs have been merged since the launch of Flutter 1.0 and 1.22 respectively.

With so many fixes and modifications, Flutter 2 is finally out to make Flutter app development more wholesome, inclusive and rewarding. Let’s check out the features you would get to experience on Flutter 2.0.

Flutter App Development 2.0 Features

Stable Web Development Release

With the second iteration, Flutter app development is no longer restricted to mobile devices. Yes, developers can now work on making applications for the web as well. Web support for apps now officially moves from Beta testing stages to a stable rollout. With this, developers can simply code once and push their apps to the web across diverse browsers as well.

Flutter cross-platform app development defines a new meaning with its feature-rich offerings designed to offer an interactive experience on the web to users. Performance is improved along with rendering, where Canvas-kit Renderer has introduced apart from HTML Renderer. Also, the addition of Link Widget ensures users have a native-like experience on their web applications.

Desktop

Flutter app development now enables desktop compatibility as well with its version 2.0. Launched more like a preview to the actual stable release slated for the year, this feature offers major changes specifically with respect to text editing functionalities, which have now been modified to feel like native.

Precision dragging of the mouse pointer is now enabled, fixing the previously prevailing lag in handling mouse-based inputs. Besides, Cupertino and Material design languages now have preloaded context menus for TextFormField and TextField widgets. Lastly, the ReorderableListView widget also gets grab handles.

Though the widget was convenient on mobiles, desktop developers felt long-pressing their mouse to move items around was a little awkward. The desktop version also features the Scrollbar tool to pave the way for interactivity developers would expect on the desktop.

Flutter Fix

Flutter app development offers a range of unique features through Flutter Fix. Dart Fix is the new CLI option added to Dart CLI that allows developers to precisely look for deprecated APIs and figure out ways to upgrade their code with such APIs. It also includes a list of probable fixes. Flutter Fix also involves a host of quick fixes that are just a few mouce clicks away through its extensions for codes like Android Studio IDEs, IntelliJ, and VS Code.

Add To Application

This is a boon for developers who don’t have the time or budget to develop apps from scratch. With the add-to-app feature, they can now reuse Flutter code for their existing mobile app codebase and publish them across platforms like Android and iOS. For those of you looking to build simultaneous versions of the Flutter engine, this is an ideal option. If you’re looking to get started with this, you can check out the documentation on Flutter. dev and have a look at some sample projects posted as well.

Beta Google Mobile Ads

This whole new plugin allows developers to add inline banner ads and native ads. While Flutter app development googles Mobile Ads SDK for Flutter already provided developers the ability to add interstitials, rewarded video commercials, and overlay banners, the new feature — still in beta stage — allows developers to test out two new ad variants. By showing compatibility with Admob and Ad Manager, developers can tailor their advertisement sizes based on requirements.

Flutter Folio

With Flutter 2, it is clearly evident that Flutter app development is all about empowering developers with all the tools and practices they need to develop apps not only for mobile devices but for the web and even desktops. With so many operating systems, input devices and mechanisms and screen sizes involved, how can developers make their app built on one single code base function well across all the barriers or rather mediums?

Well, the answer to this is the introduction of Flutter Folio. With this, developers can check out how they could make their apps adaptive to different platforms and devices and find resources that would help them achieve stability across all the factors. They can do so by exploring the source code of Flutter Folio.

Apart from these, Flutter app development is further improved with the launch of Flutter DevTools to debug Flutter apps and brand new iOS-specific features. The arrival of version 2.0 marks significant changes in the Flutter app development spectrum. Developers can now code better and make use of these features to launch more interactive and stable apps.

However, if you felt that these were too technical and that you want to simply get a universal app for all devices and platforms out, we suggest you leave the technicalities to us and focus on other crucial business operations. We are a veteran Flutter app development company, offering cutting-edge Flutter app development services. Our team has been one of the first batches of Flutter developers when it launched way back in 2018.

Your idea is in safe hands with us and our expertise is all that you need to stand out from the clutter and carve a brand value for your app. So, get in touch with us for expert Flutter app development requirements today.",7239
"Former SpaceX engineer Porter Harris has gone from electrifying rockets to rickshaws in a bid to bring efficient, emissions-free mobility to emerging markets with his new company, Power Global.

Officially launching today, Power Global is taking aim at the roughly $16 billion market for three-wheeled transportation in India.

While electrification in developed markets focuses primarily on passenger and commercial vehicles, in emerging markets there’s a huge concerted push to clean up motorcycles and three-wheelers.

They’ve become the go-to mobility solution in nations where car ownership is lower. They’re also typically powered by either internal combustion engines or lead acid batteries. And neither power source is all that great from an environmental perspective. Lead acid batteries need to be swapped out every six to eight months and are incredibly toxic… while fossil fuels are a leading contributor to global climate change.

Enter Power Global. It’s offering owners of three wheelers a subscription service that would upgrade them from either lead acid batteries or internal combustion engines to its swappable battery service.

Close up of Power.Global’s battery systems. Image Credit: Power Global

The company’s first product, the eZee, is a swappable battery for light vehicles. The company’s co-founders, Harris and Pankaj Dubey, a former Yamaha Motors and Polaris Inc. executive, see their mission as providing electric vehicle and clean energy products to global markets that have been left behind in the world’s push to sustainable mobility.

Power Global will launch its services outside of New Delhi, with the goal of planting a kiosk roughly every three kilometers, Harris told TechCrunch in a recent interview.

The company also has plans to provide drivers with an app that will allow them to see how many kilometers they’ve traveled, their current battery charge, and where they can find a swapping station.

The mobility solutions are also just a point of entry to a broader array of energy services. There are plans in place to add solar panels to the battery charging stations in efforts to provide rural electrification.

The company’s lithium ion batteries are projected to last for around five years and after they’re done, the batteries will be sent to a recycler.

Right now, the Power Global is starting with battery manufacturing. It aims to have a plant up and running with the capacity to produce about one gigawatt hour worth of batteries (approximately 10,000 Model S packs), according to TechCrunch.

The swappable eZee battery offering will be available early next year, with the retrofit services rolling out a bit later.

“Do we really need another solution for the top 10% of the world? No, we don’t,” Harris told TechCrunch. “Let’s focus on the other 90% of the world and actually make a difference.”

The company is currently taking pre-orders for its swappable batteries now.

“We are on a mission to improve access to clean energy solutions in India and other emerging markets by sharing our collective years of expertise in bringing affordable battery technology to market,” said Dubey, co-founder and CEO of Power Global’s India subsidiary, in a statement. “While the eZee™ will give light mobility vehicles new life, it also represents a path to help build local economies with direct and indirect job creation, while supporting evolving regional environmental goals...”

Following the launch of the eZee™ battery module, Power Global will announce its first line of Retrofit Kits to convert diesel- and petrol-fueled auto rickshaws into zero-emissions electric vehicles. The swappable eZee™ battery module will also power future product lines, including upcoming applications for second-life stationary storage and automotive sectors.",3794
"We all know what kind of technological marvel is SpaceX’s Falcon 9 Booster landing back on earth is. But How does it work ? What actually goes inside the rocket booster before performing those mesmerizing maneuvers ?

Let’s dive right in!

The booster contains high precision GPS, gyroscopes, and accelerometers at both top and bottom ends to precisely estimate the booster orientation, position, and velocity. The booster also contains a huge number of strain gauges that monitor forces on the structure at crucial locations, especially engine thrust. (Strain gauges are exquisitely sensitive thin films that are bonded to surfaces to electrically measure the stretching and compression of structures.)

All of these data inputs are timestamped so the three-way redundant computers can calculate where the booster was microseconds ago. By comparing the past position and vector to the desired course, the latest navigation error is calculated.

The computers run many physics equations on GPUs. These are used to optimize the flight path, calculate errors, and control thrust vectoring, grid fin positions, and cold gas thruster durations.

The booster has three opportunities to correct course using the main engines. Each burn is typically done with the center engine alone or with 3 in-a-row engines.

The booster is much lighter after consuming propellant and detaching from the second stage, so the first burn is surprisingly short.

In a boost-back burn, the booster is first flipped end-for-end. The burn reverses horizontal velocity to return to land near the launch site or, for a drone-ship landing, the horizontal velocity is nearly zeroed or reduced as fuel allows. (All simulated in advance to select the landing position.)

In some rare cases, when fuel will be scarce, the first burn is just a minor course correction that may even briefly boost downrange before the flip maneuver occurs. This fuel-starved booster will reenter the atmosphere at a rather shallow angle.

The reentry burn is primarily used to reduce air velocity so that the booster is not damaged by reentry heat (the exhaust flame is actually much cooler than the hypersonic shock wave), but is also the second opportunity to correct any reentry path error.

In between the reentry burn and the landing burn, there is a relatively long time spent falling through increasingly thick air. The waffle-shaped grid-fins, around the top, can force the booster to be oriented rather broadside in the air stream, while conserving the maneuvering thruster gas. The broadside orientation primarily bleeds off velocity, but any tilt also changes horizontal velocity. During this period, the booster slows from hypersonic to trans-sonic. Up to 10% of the altitude drop can be exploited to make horizontal adjustments while falling.

The landing burn is the final opportunity to correct the horizontal error. Although the concrete landing zones have over 30 meters radius, the drone ship decks allow only 10 meters error. As the booster slows and approaches the landing pad, the top priority shifts to zeroing out the horizontal and vertical velocities as the landing legs touch down. At this time, any residual horizontal error under 10 meters is nominal, although Falcon9 usually lands within a couple meters.

To minimize fuel consumption, the landing burn is sequenced with 1, then 3, then 1 engine(s) running. The three-engine burn interval ends with a velocity & altitude that allows the single center engine to operate in the middle of its throttle-range. This throttling and center engine tilting (gimbal) are both part of “thrust vector control”, which can provide significant sideways force at the bottom of the booster. As the booster slows during this last burn, the grid-fins loose efficacy. The cold gas thrusters are used to compensate. Since these are also located at the top, they have little effect on the base.

Since the throttle range of one engine does not include a level where hovering is possible, the booster must decelerate all the way down to the landing surface, and shut-off with microsecond accuracy before the touch-down. Excess error will either lift the booster away again, or consume the landing leg crush material (or both).

The landing zone has stationary GPS receivers that provide continuous reference corrections for the booster’s onboard GPS. (The raw data from both receivers are combined, improving the GPS precision from three meters to about two centimeter, relative to the landing site. See “Differential GPS”) The air speed and direction at the landing site is also measured. Of course this implies a landing zone data channel transmitting such data to the booster.

The onboard landing radar provides precise data on altitude and doppler velocity. (During planning for the first Falcon Heavy dual-booster landing spectacle, there was concern about possible interference between the two landing radars, so the booster landings were intentionally staggered in time to minimize this possibility.)

When landing on a drone-ship, the landing is further complicated by the ocean waves moving the landing surface. Waves are not completely predictable from analysis of past events, so this tends to consume the crush material in the landing legs.",5258
"THINKING OUTSIDE THE BOX

Necessity may be the mother of invention, but creative thinking doesn’t hurt. Over the past year, necessity forced us to rethink many aspects of our lives, leading to entrepreneurial innovation to serve the needs of a mostly homebound society. It propelled trends already emerging — take-out and grocery delivery, streaming media and video conferencing to name a few — as well as provided new offerings like virtual theater, gallery tours and speaker series, which hopefully will continue as it allows a greater audience to participate.

Now that the world is opening up, creativity continues to drive interesting uses of technology that we may not have previously envisioned. Some may have been initially conceived as media stunts, as they have definitely drawn attention, but to a large extent may also have business potential.

The use of TikTok as a recruiting/hiring tool is an excellent example of taking advantage of what has essentially been an entertainment and marketing platform to do something quite different. The necessity part was targeting a young cohort of job seekers to fill openings in retail and hospitality, necessary to reopen or expand business. The creative was TikTok partnering with retailers and hospitality companies to use their platform to not only recruit young talent but screen potential candidates.

Blockchain technology and bitcoins have long been in the news, but recently NFTs, unique, non-replaceable digital assets, are the buzz. They are attracting the attention of deep-pocketed collectors who are snapping up everything from Beeple digital artwork for $69 million to a LeBron James clip for $99,999. Now there are a number of people who are also trying to figure out how to make NFTs accessible to non-millionaires, with Oneof, a company dedicated to minting and releasing NFTs building on a more affordable blockchain platform. And then there is memecoin, a crypto currency inspired by internet jokes. How fitting.

Artificial intelligence is informing everything from the potential for self-driving cars to automating repetitive business tasks. But it has also entered the world of what was once considered creative pursuits, particularly writing. Who would have thought that there would be a competition among AI songwriters!

Social media has long been used by journalists to amplify their reporting, but using it for personal opinion has been problematic. Enter Sidechannel, a community on Discord for platform users to comment on news anonymously. They describe it as akin to hacking into a newsroom Slack.

What’s really interesting about this random sampling is how quickly we continue to adapt technology for new uses, whether it is to reach a specific audience, create community or launch new business models. Often the second or third iteration becomes the big success story. It’s why in a brainstorm a bad first idea can lead to a brilliant final solution. All it takes is a little imagination and a willingness to think outside of the box.",3022
"To view the complete course, please check the below url.

For more info, register via the below link

YouTube Channel link

Welcome to Day 11 of 101 Days of DevOps. The topic for today is an Apache log parser using python. This tutorial aims to create an Apache log parser which is really helpful in determining offending IP addresses during the DDoS attack on your website. This is what we are going to do.

Read Apache log file(access.log)

Count quantity of requests to your website from each IP address

Save the output in a csv file

Make script user friendly by using argparse

If you look at the content of access.log, this is how it looks

192.168.0.1 — — [23/Apr/2017:05:54:36 -0400] “GET / HTTP/1.1” 403 3985 “-” “Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36”

Let’s break down these fields

192.168.0.1 --> IP address [23/Apr/2017:05:54:36 -0400] --> Date,time and timezone GET / HTTP/1.1 --> HTTP get request to read the page 403 --> Server response code 3985 --> Number of byte transferred Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36” --> Finally there are data of user's hardware,OS and browser

But the piece we are interested in is the IP address

So as mentioned above, our first step is

Importing all standard python modules.

import re

from collections import Counter

import csv

import argparse

Read the Apache log file.

with open(""access_log"") as f:

fread = f.read()

In the next step, we are going to write a regular expression. We already discussed the regular expression on Day 10. https://www.101daysofdevops.com/courses/101-days-of-devops/lessons/day-10-regular-expression/

\d: Any numeric digit[0–9]

Then we are going to use findall() to get the list of IP addresses.

logreg=""\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}""

ip_list = re.findall(logreg, fread)

As we filtered the IP address so far, we will count the number of times IP repeated in the file in the next step. To do that, we are going to use the collections module and its method called Counter. We need to pass ip_list the IP list we filtered via the regex to Counter(ip_list) method and then print it.

for k, v in Counter(ip_list).items():

print(k,v)

So far, our code will look like this.

import re

from collections import Counter

import csv

import argparse



logreg=""\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}""

with open(""access_log"") as f:

fread = f.read()

ip_list = re.findall(logreg, fread)

for k, v in Counter(ip_list).items():

print(k,v)

If we execute our code at this stage(store the code in a file apache_parser.py and then execute it, you will see we are getting an IP address along with the count.

python3 apache_parser.py

194.5.53.89 1

20.191.45.212 4

35.208.242.202 60

88.150.188.171 1

5.0.375.99 1

185.191.171.37 1

As our next requirement, we need to save the output in a csv file. To achieve that, we are going to open the ipnewcount.csv file in write mode. Then we will create a fwritercsv object. The next line, “fwritercsv.writerow([“IP_Address”, “Count”])” act as a header to the file, i.e., at the top of a csv file, you will see IP_Address and Count as header. Then we will modify our code a little bit, and rather than printing the value; we will store that in a csv file.

with open(""ipnewcount.csv"", ""w"") as f:

fwritercsv = csv.writer(f)

fwritercsv.writerow([""IP_Address"", ""Count""])

for k, v in Counter(ip_list).items():

fwritercsv.writerow([k, v])

Our modified code will look like this.

import re

from collections import Counter

import csv

import argparse



logreg=""\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}""

with open(""access_log"") as f:

fread = f.read()

ip_list = re.findall(logreg, fread)

with open(""ipnewcount.csv"", ""w"") as f:

fwritercsv = csv.writer(f)

fwritercsv.writerow([""IP_Address"", ""Count""])

for k, v in Counter(ip_list).items():

fwritercsv.writerow([k, v])

If we execute our code now, you will not get any output

(venv) ➜ python3 test_apache_parser.py

(venv) ➜

But you will see the file is created

-rw-r--r-- 1 plakhera plakhera 585 Jul 11 15:43 ipnewcount.csv

If you read the content of a file, as you can see, we have got (IP_Address, Count) as header and then IP Address followed by the count.

head ipnewcount.csv

IP_Address,Count

194.5.53.89,1

20.191.45.212,4

35.208.242.202,60

We met all our requirements except the last one to make this code user-friendly, which we will do with the help of argparse. What argparse will do give the user-friendly help output and allow us to pass logfile on the command line. As the first step, we need to create the parser, then we will add an argument to the parser, and the final step executes the parser.

my_parser = argparse.ArgumentParser(description='Reading the log file')

my_parser.add_argument(""logfile"",

help='Please enter the logfile to parse',type=argparse.FileType('r'))

args = my_parser.parse_args()

To use this parser, the only change you need to add in your script is rather than hardcoding the value of logfile, replace it with the value you read via a parser.

with args.logfile as f:

As now we met all the requirements, the final code will look like this

import re

from collections import Counter

import csv

import argparse



my_parser = argparse.ArgumentParser(description='Reading the log file')

my_parser.add_argument(""--l"",""--logfile"",

help='Please enter the logfile to parse',dest=""logfile"",type=argparse.FileType('r'), required=True)

args = my_parser.parse_args()





logreg=""\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}""

with args.logfile as f:

fread = f.read()

ip_list = re.findall(logreg, fread)

with open(""ipnewcount.csv"", ""w"") as f:

fwritercsv = csv.writer(f)

fwritercsv.writerow([""IP_Address"", ""Count""])

for k, v in Counter(ip_list).items():

fwritercsv.writerow([k, v])

If you try to execute the code now without passing any argument, you will get a nice friendly output.

python3 apache_parser.py

usage: apache_parser.py [-h] --l LOGFILE

apache_parser.py: error: the following arguments are required: --l/--logfile

If you use the -h option, you will get all the help output

python3 apache_parser.py -h

usage: apache_parser.py [-h] --l LOGFILE Reading the log file optional arguments:

-h, --help show this help message and exit

--l LOGFILE, --logfile LOGFILE

Please enter the logfile to parse

Finally, it's time to execute the code

python3 apache_parser.py --logfile access_log

This is version 1 of the code https://github.com/100daysofdevops/100daysofdevops/blob/master/apache_log_parsing/apache_log_parsing_v1.py

Assignment

Try to break the code in terms of function, i.e., there is a separate function to perform regex, a separate function for performing the counter calculation, and a separate function for writing to csv file.

I am looking forward to you guys joining the amazing journey.",6889
"Updated for 2021!

Like everything on the net, Pinterest is dynamical in no time. Pinterest tips for growing your audience printed in 2015 or maybe 2016 square measure undoubtedly out-of-date in 2021. The platform evolves, and algorithms become a lot of refined day by day, that makes it more durable to control it.

I started my journal within the middle of Feb 2017, I created a current Pinterest account related to my journal, therefore I had zero traffic and zero Pinterest followers for starters. In 3.5 months, if we have a tendency to compare monthly traffic for Feb and should, I’ve got ten times a lot of pageviews! And haven’t nevertheless an opportunity to quit my 9–5 job and work on the journal full time, it’s simply a aspect hustle for currently, with but twenty posts printed to date.

So, guess wherever will my traffic come back from? Pinterest! concerning ninety four of my audience comes from Pinterest! is that this an honest or unhealthy thing? Well, keeping all eggs within the same basket is rarely smart recommendation, however having principally Pinterest traffic is much higher than having no audience at all!

When I started the journal, I knew that Google can take concerning half-dozen months to grant a minimum of some trust to my website, therefore i used to be prepared for the boring months of reproval myself with no audience in the least. however…

To get more information Click Here",1419
"To view the updated DevOps course(101DaysofDevOps)

Course Registration link: https://www.101daysofdevops.com/register/

Course Link: https://www.101daysofdevops.com/courses/101-days-of-devops/

YouTube link: https://www.youtube.com/user/laprashant/videos

Welcome to Day 61 of 100 Days of DevOps, Focus for today is Jenkins Agent Node

Yesterday I showed you how to set up Jenkins Server

The above setup works great in a small environment but as your team grows you need to scale up your Jenkins and in those situations, you need Master/Agent setup. In Master/Agent setup, Master acts as a control node and Jenkins with the agent installed to take care of the execution of all the jobs.

Requirement

Jenkins Master

Agent Node

NOTE: I am performing all the steps on Centos7

Step1: Install the necessary package on the agent node

yum -y install java-1.8.0-openjdk git

In this case, I am installing java and git

You don’t Jenkins rpm package on the agent machine

Step2: Create user and copy the ssh key from Master to agent node

# useradd jenkins visudo jenkins ALL=(ALL) NOPASSWD: ALL # mkdir jenkins_build

Then copy the key generated on Day60 from master to agent node

$ ssh-copy-id jenkins@172.31.29.138 # To test the connectivity $ ssh jenkins@172.31.29.138 Last login: Fri Apr 12 04:31:49 2019

Step3: Add the agent from Jenkins UI

On the Jenkins UI, Click on Manage Jenkins and then Manage Nodes

Click on the New Node

* Name: Give your agent some name

* # of executors: The maximum number of concurrent builds that Jenkins may perform on this node.

* Remote root directory:An agent needs to have a directory dedicated to Jenkins. Specify the path to this directory on the agent. It is best to use an absolute path, such as /var/jenkins or c:\jenkins. This should be a path local to the agent machine. There is no need for this path to be visible from the master(/var/lib/jenkins/jenkins_build) created in earlier step

* Usage: Controls how Jenkins schedules builds on this node. Use this node as much as possible - This is the default setting.

In this mode, Jenkins uses this node freely. Whenever there is a build that can be done by using this node, Jenkins will use it.

* Credentials: Select the credentials to be used for logging in to the remote host.

* Host: Enter the hostname or IP address of your agent node in the Host field.

Check if everything works

To use the slave node",2410
"Automatically Build your Docker Images

I believe that many of you reading this article love Jenkins. I am sure that many of you are furious right now. Maybe you have spent months learning Jenkins, and have really dived deep into its internals. If you are in a similar position, I can understand why you are not ready to give it up just yet.

To try and convince you, I will use a simple example. I will go through the most popular CI use case today: how to automate the process of building and pushing your Docker images.

However, this is not a tutorial on how to build and push Docker images with Jenkins. Instead, I will only provide a high-level view. So, given that you already have a repository with your code pushed to Github, here are the steps you need to cross to achieve your end goal:

Install Jenkins: Jenkins is typically run as a standalone application in its own process with the built-in Java servlet container/application server (Jetty). However, you can also use the official Docker image to run Jenkins inside a container. Integrate with GitHub: You can access Jenkins via your browser (typically on localhost:8080 ) and create a new project. You should specify the GitHub repository URL address, when a build should be triggered, and the target command. Install a Docker plugin: To build and push images to a repository (e.g., Dockerhub), you need to install a new plugin. For example, you can install the Docker plugin, the Docker Build Step, and the CloudBees Docker Build and Push plugin. Configure the Docker plugin: You should specify a new build step in the Jenkins UI that leverages the newly installed plugins. In addition, you should specify the registry you want to push your image to, the image name and tag, and your credentials.

Why is this Bad?

Now that we’ve seen the procedure, you might say that this is not too bad. Just four steps. However, some of these steps are not easy to cross:

Jenkins instances are usually too complex. That means an organization needs a Jenkins expert to install, maintain and secure the instance.

Experts become bottlenecks. You, as a developer, depend on a Jenkins expert or admin to create a new project, build, release, etc. This quickly becomes a prolonged process.

Self-service seekers will create new, insecure Jenkins instances to address the expert bottleneck issue. In addition, they will install new plugins without testing and introduce vulnerabilities.

Usually, you need to install a ton of plugins before start working with Jenkins. Unfortunately, this makes the instance too complex, difficult to navigate, and the Jenkins controller slows way down.

You would expect not to do so much manual work on an automation tool. Jenkins is tough to scale efficiently. Not all plugins are supported by Jenkinsfile , so a Jenkins instance is hard to back up without manual effort. Moreover, the database is just XML files on the disk, making it a nightmare to scale up or out or perform high availability upgrades.

, so a Jenkins instance is hard to back up without manual effort. Moreover, the database is just XML files on the disk, making it a nightmare to scale up or out or perform high availability upgrades. Jenkins needs a dedicated server (or several servers) to run on. That results in additional expenses. More often than not, an organization needs to have a DevOps team dedicated to Jenkins.

What’s the Alternative?

For CI, and maybe not only CI, I prefer to use GitHub actions. So, how can you build and push Docker images with GitHub actions? Let’s see the steps:

Setup: Create a new folder inside your project’s root folder. Name it .github/workflows . Configure: Add a new YAML configuration file inside the workflows folder you created. This file looks something like this. A numbered list of steps that every step does one simple thing. Authenticate: Add your registry’s (e.g., Dockerhub) credentials to your project as an encrypted secret. Run: All you need to start the process is to push new commits of your code to GitHub.

The most important thing? There is nothing to install, configure, maintain, upgrade, or patch. Everything runs on GitHub servers that you don’t have to care about. Moreover, with the GitHub marketplace (think of it as an alternative to Jenkins plugins), you can do almost anything.

Conclusion

Jenkins is the leading open-source automation server, with hundreds of plugins to support building, deploying, and automating any project.

However, the Jenkins way is not an easy and straightforward way to build a CI pipeline. Installation, maintenance, plugins, security, amongst other things, make the process slow, create bottlenecks, and introduce vulnerabilities.

This story showcases the steps you need to cross to automate building and pushing Docker images using Jenkins and GitHub Actions. In my opinion, using a tool like GitHub Actions blows out of the water the Jenkins way. What’s your opinion?

About the Author

My name is Dimitris Poulopoulos, and I’m a machine learning engineer working for Arrikto. I have designed and implemented AI and software solutions for major clients such as the European Commission, Eurostat, IMF, the European Central Bank, OECD, and IKEA.

If you are interested in reading more posts about Machine Learning, Deep Learning, Data Science, and DataOps, follow me on Medium, LinkedIn, or @james2pl on Twitter.

Opinions expressed are solely my own and do not express the views or opinions of my employer.",5469
"The Gala Games Team has been hard at work driving towards ever-greater decentralization of the Gala Games Node Ecosystem. Today, we would like to present a proposal to the community for discussion. This post summarizes the current node reward structure and proposes a change.

TLDR: Below we lay out a potential scenario for node rewards that will better enable the node network to support the whole Gala Games Ecosystem. The more you contribute and the more stable that contribution, the better the rewards.

Legal Exemplar Disclaimer: This proposal has been created to spark discussion in the Gala Games community. Based on the input that Gala Games receives, this may or may not reflect the final proposal presented to the nodes for a consensus vote.

Now, without further delay, node development can best be broken into three “levels”:

Current State (Level 1)

We are still in the early phases of the Gala Node Network, with approximately 55% of the 50,000 total Founder’s Nodes already sold. In these early months, it has been very important to facilitate the growth of the Node Network by encouraging the community to spread the word and bring more people to join us in what we are doing. In this time, the primary function of nodes has been to ensure Node Operators are active and aware of the importance of being involved in the Gala Node Network. This has been accomplished by giving the same 1 point distribution to all Node Operators who keep their nodes online at least 25% of the time.

In this phase, referrals have been enjoying roughly 10% of the amount rewarded to Node Operators.

Referral Bonus (one-time per node): 10 points

Referral Bonus (recurring per node): 0.1 point

Daily Points per Node: 1 point

There have been many concerns raised about the long-term effectiveness of this plan, which is why we believe a phased approach is necessary to reach the full potential of the Gala Node Network.

Next Phase (Level 2)

In the next phase, the Gala Node Network will begin hosting content for the Gala Decentralized Content Delivery Network (Gala DCDN). To facilitate this rollout there will not be strong hardware requirements, but there will be a minimum required.

Rewards will be based on reaching the minimum hardware requirements and scaled by uptime. Additional points will be given for providing stability for the network in the form of static IP addresses with open ports.

The purpose of these changes is to reward node operators for establishing a stable baseline for the Gala Node Network in preparation for Level 3.

Level 2 points will be in addition to Level 1 points.

Points will be calculated to 1 decimal place and rounded down.

Minimum Requirements:

50 GB Disk Space

4 GB Ram

2 CPU Cores

10 Megabit upload network connection

Points per Node uptime calculation: 10 Points * [Uptime%]

Calculation Examples based on 10 Points * [Uptime%]

Points for stability calculation:

10 Points * [Uptime%] * Max([Qualified Nodes] — Abs([Unique IPs] — [Qualified Nodes]), 0)

Note: Nodes shifting between IPs will affect the stability of the network, so daily unique IPs in excess of the Qualified Node count will cause a reduction of points.

This means that the total number of points possible for a single node that is up for 24 hours and does not have a public IP is 11 points. (1 point for Level 1, 10 points for level 2)

This means that the total number of points possible for a single node that is up for 24 hours and has a publicly available IP is 21 points. (1 point for Level 1, 20 points for level 2)

Dealing with Potential Bad Actors

As the network scales up and provides more important services, it becomes critical for the network to be protected from bad actors. As part of Gala Game’s curator role for the network, Gala Games proposes that Gala Games and its partners (those utilizing the Gala DCDN) will monitor the network for any bad actors who attempt to falsify contributions, availability, performance, or otherwise defraud the network. Gala Games and its partners will also welcome reports of any suspected fraud and will research it accordingly. Ultimately, it will be up to decentralized action by the node network to enforce any penalties. A more detailed “bad actors” proposal will be provided to the nodes before any enforcement actions are taken. It is likely that said proposal will include the punishment of loss of DCDN and/or stability points for a period of time.

Future Phase (Level 3)

Level 3 is still being planned, but the suggested implementation is to have a set of workloads that Node Operators can select to have their node perform work for. Each workload will have:

Suggested minimum requirements

Performance Criteria

Associated reward points

Reward points for a workload will change over time to allow the network to prioritize what is most needed. Each node operator may select which workloads they would like their node(s) to handle. Each workload will “Level Up” to its maximum earnings. This is to prevent nodes from rapidly switching workloads in an attempt to earn the most money which would impact the stability of the network.

Example Workloads:",5143
"Why Programmers Shouldn’t Stay in One Company for a Long Time

Recently I was talking with one of my programmers who joined my company two months ago. He has joined as a junior programmer, and he never stays in a company for more than one and a half years.

He is not alone. Most of the good programmers I have seen leave their jobs after 1–3 years unless they are the company's shareholders.

There are many reasons why developers do that, and it’s very normal. We will talk about why developers do that, but let’s see some statistics on how normal they are now.

Statistics

It was not long ago when an employee spent their entire life in a company. But things have changed now. It’s easier to find a new job now.

The scenario is still the same in some sectors. But in the technology world, it’s almost impossible to think that programmers/developers will stay in a job for more than 10–15 years.

You can find some cases where a programmer is in a job for ten or more years. But if you ask a programmer who has been in the industry for less than five years, most of them will say they are not interested in staying in a job for a long time.

Using Hackerlife, I made a graph that shows the data collected from more than ten thousand software engineers who live in San Francisco. The graph shows the median tenure of a programmer in some tech giants.

Credit: Author

The visualization above focuses on large corporations. Almost 50% of the programmers of these large corporations leave in two years, and 75% leave around three to three and a half years.

Whereas the average tenure of other industry professionals is 4.2 years, this is very low. And the tech giants are actually more capable of keeping the programmers. The scenario is much worse in startups and medium-level tech companies.

My experiences with programmers

I have met many programmers during interviews as a recruiter. I always ask about their last jobs: where they worked, how long they worked there, or why they left.

I can’t say the exact numbers because I didn’t count, but I have found almost 70–80% of candidates left their jobs within two years.

I rarely found any candidates who worked for more than five years in a company. Maybe 2–3 candidates out of 200+ candidates. I have seen less because we are a startup. Developers who have worked in a company for a long time normally don’t come to work for a startup.

Why Programmers Shouldn’t Stay For a Long Time

Learning opportunity

This is the biggest reason why a programmer should change their job after a certain time. In the early stage of programmers’ careers, they need to learn a lot.

Both the new programmer and the company learn a lot from each other in the first year. The company learns from their previous experience, and the programmer learns from their tech stack, projects, and senior developers.

After one year in a company, a programmer builds things and becomes an important employee. But often, the programmers don’t feel the same way.

So, after one year, there is nothing much for the programmers to learn from the company. If the company is a tech giant, then, of course, there will be. But in small and medium-sized companies, the learning opportunity becomes narrower.

That’s probably one of the reasons why programmers leave small companies faster. The average tenure of software engineers in small companies is only 1.5 years, where it’s 2.3 years for large companies.

Credit: Author

The question is, where does the learning end? Honestly, it never does. That doesn’t mean you have to change jobs this frequently all your work life.

After around ten years (more or less), you can be promoted to a position where you don’t have to change jobs that much. But that’s another long discussion.

Better Career

Tech stacks are changing faster than ever. That’s why learning is a never-ending process for programmers. It’s one of the main reasons why programming is one of the unique kinds of jobs in the world.

Programmers should change their jobs not only for learning but also for better careers too. Doing the job in one company makes career growth slower.

Normally, a company doesn’t progress as fast as programmers develop their skills. In this situation, the company can’t fulfill the developers’ demands.

So even if you are loyal and dedicated, and even if you find many learning opportunities, you should leave if you feel you deserve a better career.

Last words

So, when should you leave a job? There is no right answer, honestly. You have to find your answer. I just shared my opinion and some statistics.

I have also seen my programmer friends. Those who changed jobs are better positioned than those who stayed in a single company for a long time. It will also help you to become more confident.

If your current company falls into some crisis or you are fired suddenly, you won’t be in much trouble. And if you are not happy or not being treated well, you should definitely leave sooner rather than later.

How many companies have you worked for? Please share your experience with us in the comment!",5078
"Law firms, as well as corporate legal departments, continue to look for new ways to enhance efficiency and drive productivity. AI-enabled chatbots are one such way that has the potential to revolutionize the law firm operations. These chatbots are a new approach for law firms to imitate human conversations and automatically respond to clients as well as attorneys’ queries.

Legal chatbots have the capabilities to make better and quicker decisions when compared to human agents. It reduces the burden on attorneys and paralegals to repetitively answer the same queries, which further brings consistency to users’ responses.

Role of chatbots in the Legal industry

Internal Chatbots

Internal chatbots are nothing but the chatbots for internal operations and communications, helping law firms manage enterprise collaboration. Internal legal chatbots help law firms automate the contract review process, which is one of the most tedious tasks for attorneys and in-house counsel.

Legal chatbots for attorneys come with a predefined set of policies to review & analyze documents, perform due diligence, and automate other monotonous tasks that attorneys perform. Other basic tasks comprising scheduling meetings, setting up reminders, and searching relevant matter information can also be performed by legal chatbots. Internal legal chatbots empower attorneys to reduce the risk of human errors by automating the monotonous administrative chores and allow them to focus more on higher value and complicated tasks that need attorney’s intervention.

External Chatbots

External chatbots, on the other side, are the client-facing legal chatbots. These chatbots can draft the legal documents, including UCC filings, divorce forms, and other non-disclosure agreements based on the client inputs.

External legal chatbots empower law firms to handle the client intake process efficiently and generate leads, which further reduces an attorney’s time spent on these activities.

In the current scenario of receiving information instantly at a fingertip, legal chatbots serve as the best solution to handle client queries and provide legal advice.

External, as well as internal legal chatbots with their 24/7 supporting abilities, facilitate law firms to manage operational costs, and meet the evolving client expectations.

Although chatbots are taking time to augment legal services but are worth the effort.

KLoBot — An AI Chatbot for Law Firms

KLoBot is an incredibly intelligent AI chatbot builder platform that allows legal firms to create text and voice-based chatbots within minutes. KLoBot’s easy drag and drop skill interface helps law firms to design no-code chatbots that can be deployed across an organization’s favorite channels. The chatbots built on the KLoBot platform help law firms perform simple and complex routine tasks, including QnA and knowledge repository search. Few other jobs, including scheduling meetings, setting up reminders, completing actions on behalf of attorneys, finding colleagues, assisting attorneys, and much more, are also being performed by KLoBot enabled chatbots.

KLoBot enabled chatbots to act as a personal assistant and enhance attorneys as well as client experiences. These chatbots empower law firms to simplify internal as well as external communications and streamline business processes.

Chatbots designed on KLoBot used cases

KLoBot AI chatbots with its feature-rich admin console, provide law firms robust security controls",3479
"The Time Series Playground

A new web-based tool to get insights from time series forecasting Daitan Follow Jun 8 · 2 min read

Time Series forecasting comprises a set of algorithms that are designed to predict future behavior based on historical data. Here at Daitan, time series forecasting has been one of the most important applications of machine learning and today, we are pleased to announce the first (to our knowledge) time series forecasting playground. Inspired by the Neural Network Playground and the GAN Lab, the The Time Series Playground is an interactive open-source tool designed to provide intuition on how to train AutoRegressive Feed Forward Neural Networks for time series forecasting.

In the tool, one can define, configure, and train Neural Networks using four different time-series “toy” datasets. For each dataset, users can experiment with different kinds of input formats, and play with up to 5 different training hyperparameters, including the learning rate, the choice of activation function, the batch size and many more.

Also, one can start, pause, or resume training at any given moment; or even choose to train a model for a single epoch at a time. When a given training process is finished or paused, the tool automatically displays one-step-ahead forecasting for the test set in the main graph, along with the 95% confidence intervals. Besides the hyperparameters, users can define their own choices of train and test splits, or even customize the input data format to be used for training.

The time series playground is designed to be an educational tool. We hope that the tool can provide valuable insights and spark curiosity so that more people feel interested in dive deep in this interesting subarea of machine learning.

If you want to learn more about time series forecasting, we highly recommend our series of articles on the topic.

Thanks for reading.

The Innovation team at Daitan.",1933
"Credit For The Image Goes To: https://www.digitalocean.com/

Why Deep Learning?

I’m part of the DBS iTES team, which supports the automation testing framework used by Software Development Engineer in Test (SDET) to test Treasury & Markets (T&M) applications. Each day, there are numerous regression tests conducted by various Investment Trading Technology (ITT) project teams to verify their application stability or assurance before applications go live.

From time to time, project teams may encounter test failures due to upgrades, or UI changes which take plenty of time to debug and resolve. With the exponential growth in the Artificial Intelligence (AI) space, our team was curious to find out whether we could leverage some of the available solutions to resolve the above mentioned issues while running tests.

The selection process for what feature to roll out was rather straightforward. Our team conducted a poll and put forth several features that SDET were able to choose from at one of our guild meetings. From the poll, we found out that 47% of the test engineers would like to have AI automatically fix their regression errors for them.

Poll results from Testcenter Guild Meeting

It is the End Result That Matters

Early on, it was clear to our team that we needed a proof of concept (POC) to show that our wild idea was worthy of further pursuit.

“To get what you want, you have to deserve what you want. The world is not yet a crazy enough place to reward a whole bunch of undeserving people.” ― Charles T. Munger

With reference to papers and studies by other companies who have applied some form of AI/ML for testing, our team decided to explore the possibility of a mixed technique consisting of both visual recognition and element properties weight optimisation to resolve broken locators.

Rapid prototyping was the approach chosen to get the project off the ground. First, a simple Object Detection Model trained on images retrieved from testing regression screenshots is deployed as an application on IBM® Red Hat® OpenShift®. This is followed by the development of an interface between our testing framework and the AI Application (VSDojo) hosted on Openshift to communicate on the screenshots of the errors as well as the returned predicted results.

Close-up Validation Results from the object detection model

What if the situation was more complex? Does the model hold up for the various user interface (UI) design within the ITT space?

Validation Results from the object detection model

More validation result from the object detection model

Truth be told, there were times where the model did not perform to expectations. But more on that later.

Example of localisation failure from the object detection model

We will go into detailed explanations for the architectural design, strategy, and decision-making later on in this article. Briefly, the current Object Detection Model is manually labelled (Supervised Learning) to detect only two web elements buttons and inputs.

Example of LabelImg python tool on sample training image

We used an 80/20 train test split while keeping certain projects out of the training sample. Our team manually labelled each of the images using the Python tool LabelImg, which conveniently converts the coordinates and class definition as xml that would be used to train our model. Further evaluation was conducted using the 20% test split to ensure that the model can be used beyond the images it was trained on.

The Journey, Challenges Faced, Hurdles We Overcame

Now, let’s take a closer look at the overall architecture. The diagram below might look overwhelming, but it can be broken down into three broad categories: Object Detection Model (OpenShift), Logging (ELK) and DriveX (Object Storage).

Overall Architecture Diagram

The process kicks off the moment SDET run their tests via the Testcenter’s Serenity framework, and upon failure, the screenshot would be captured and sent to the Object Detection Model hosted on OpenShift (VSDojo). As mentioned in the previous section, the model would then return its prediction and coordinates of detected web elements back to the framework.

The model’s ability to generalise across multiple ITT projects without having to programmatically specify conditions is not something to be sniffed at! In order to appreciate its elegance, it is necessary to understand “deeply” how it works.

Diagram showing the basic flow of an CNN model

Traditionally, the first thing that comes to mind when dealing with computer vision is Convolutional Neural Net (CNN) models. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution. Convolution is a specialised kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.

In short, the given input image would have been stride through and flattened within each connected layer to extract key features. The final layer is typically a SoftMax or sigmoid function that outputs the predicted result. The backpropagation of errors from different training images and corresponding labels is what trains is the machine learning aspect, which allows the model to learn and classify other similar images.

Difference between traditional image classification and object detection

Since our team requires a solution that is not only able to identify the type of web elements but also their coordinates, the Object Detection Model lends itself naturally as a good solution.

There are certainly many other object detection architectures currently practiced in the AI industry, including Single Shot Detection (SSD), ResNet, Faster RCNN, and You Only Look Once (YOLO). Each of them has their trade-offs, for instance, a larger model would have slower inference time for higher accuracy.

Our team chose to stick with YOLOV3 due to its low inference time 22ms, to minimise impact and delay to the testing regression. The superior speed of YOLOV3 can be attributed to its unique bounding box strategy. For more information, refer to the original paper linked in the reference below.

Finally, to overcome the problem of localisation failure, we plan to increase the amount of data used to train the model. We plan to do this using data augmentation, which involves generating different variations of the image by rotating and inverting the original images for improved generalisation by the model. This feature is included in the Tensorflow library under the pre-processing layer.

Now that we have a better understanding of the model, let’s explore the next key piece of ELK logging. One key point made by our Tech Lead, before our team kicked off the project, was to ensure a proper test bed that would measure the performance of our proof of concept, instead of simply measuring the Intercept Over Union (IOU) or accuracy of our model.

“If you cannot measure it, you cannot improve it.” ― William Thomson, Lord Kelvin.

Staying true to the advice given, our team addressed the need by piping the results from the framework directly into ELK. Through the dashboard, we can pinpoint exactly where the model is not performing well, along with the stack trace which can be used to further optimise our model.

The final essential component for the proof of concept would be the storage of metadata to support the sustainability of the model’s performance over time. The main idea revolves around storing past success and failure information in DriveX (Object Storage) that can be used as training data for the model.

The other question to be addressed is the kind of information that our team chose to retain and automate the training process. For each successful test run, the original selector, coordinates, outer html and most importantly, images regarding the web elements, were kept. This data would eventually be the cornerstone on which to build future models.

To recap, the process flow begins with SDET running regression via our framework. If there were web element failures due to UI changes, an API call would be made to Model resting on the OpenShift to retrieve predictions, allowing for self-healing to take place. Testing Logs would be sent over onto ELK to track real-time performance, and finally, metadata would be sent to DriveX to train future models.

Contrarian View

When our team members bounced ideas off each other at the start, we thought that visual recognition would not be a right fit for the task. The alternative was as a generic JavaScript or XPath filter “//button //input”, which would yield the same result.

Subsequently, we realised that the approach of using XPath and JavaScript to capture all elements on a webpage would be excessive, as more processing would be required to evaluate elements that were not on the screen during failure. Furthermore, conventional html tags might not be sufficient given that the JavaScript site permits customisable tags.

As such, we decided to stick with using the visual recognition model to identify and resolve possible match for failure. During subsequent phases, we used a weighted optimisation model to pick the best match out of all the returned web elements.

The Weakest Link

But we have yet to address the elephant in the room: the selection of best matching elements when multiple web elements are detected by the model. What we are doing right now is simply using regex to slice our model’s prediction outer html to match against the original input selector.

Credit For The Image Goes To: https://blogs.oracle.com/

This approach is rather primitive, and we foresee issues such as false positive selections, which could arise due to the nature of implementation. Therefore, we plan to train another model to select best matching web element for any given selector. Essentially, we will have two models: one for detecting elements from webpage on failure, and a separate model to pick the right element for self-healing.

Unfinished Business

The list of crucial pending tasks to be completed includes piloting the proof of concept on a couple of projects to validate the performance live and using metadata from successful test runs, as well as building weighted optimisation model to select the best match.

Once the proof of concept results shown in ELK have proven to be useful, our team will place greater emphasis on the sustainability and scalability of the system, mainly through the ability to retrain and replace model automatically if the performance of the existing model begins to decay.

Deep Dive into Data

When we retrieved statistics from ELK Stack, which we had been diligently sending our testing results to, we found that the AI model helped resolve 14.32% of the total test cases, which looks underwhelming at first glance. However, this proof of concept was scoped on just fixing input and buttons-related errors.

After closer inspection, if we only consider input and button related test errors, the model has a successful fix rate of 77%, as seen in the chart below.

Auto Healed Breakdown of failed testcases from ELK

Our team also made some minor changes to improve the overall performance of the system, mainly the introduction of VSMode, which consists of three workflow states. These are the training mode, which is only activated during locale machine test run; evaluation mode, which is activated during CI/CD pipeline; and finally, disable mode for edge cases like performance testing, which does not need the feature.

The workflow state would allow the model to learn from new data as the SDETs are developing new testcases, as well as performing self-healing on failed testcases within the CI/CD pipeline, improving overall efficiency of computing resources.

Getting your hands dirty

Inspired to get started with your own micro-innovation? Here are some articles and video tutorials to help you kick things off. If you are sitting on the fence whether you should start your own AI journey, know that motivation comes after action — all you need is to be brave enough to take the first step!

A good place to start would be MNIST dataset where we train a model to predict handwritten numbers from 1 to 10. I highly recommend beginners to use high level wrappers such as Keras, PyTorch or scikit-learn to train their model.

The wrapper makes the coding much more intuitive and easier to learn. Once you have mastered either of the recommended libraries, proceed to learn Tensorflow, which allows greater control. Check out the Deep Learning with Python, TensorFlow, and Keras tutorial.

Note: The assumption is that you already have some prior knowledge or experience with coding in python.

This exercise is a simple follow-along supervised learning model tutorial. The beauty of machine learning models the flexibility and ability to generalise across various unseen inputs, much like our VSDojo model’s ability to generalise across multiple ITT web application without any formal code in place.

Interested to learn more? Check out the latest cutting-edge techniques that industry leaders are practicing or using within their projects here. The possibility for learning is endless but the best way to learn is to apply what you have learnt in a project. It can even be a small feature that does simple optimisation.

References

https://www.tricentis.com/resources/ai-in-software-testing-reality-check/

https://medium.com/hackernoon/introduction-to-deep-learning-9064d6b87a51

https://medium.com/deepquestai/train-object-detection-ai-with-6-lines-of-code-6d087063f6ff

https://en.wikipedia.org/wiki/Convolutional_neural_network

https://arxiv.org/abs/1804.02767

https://github.com/tzutalin/labelImg?fireglass_rsn=true

https://www.tensorflow.org/learn",13808
"Banks globally rely on Oracle FLEXCUBE to provide their agile banking infrastructure, and more today are migrating to FLEXCUBE to retain a competitive edge over young and agile Fintech startups.

FLEXCUBE provides the flexibility to progressively transform componentised core banking systems, while supporting moves to modern cloud infrastructure. It enables established financial players to innovate rapidly, even against the backdrop of historical infrastructure and complex end-to-end processes.

However, every migration and update must be tested rigorously. Otherwise, organisations risk exposing critical banking systems to costly defects, downtime, and security vulnerabilities.

That means rigorously testing migrations to FLEXCUBE, mitigating against the risk of exposing the most sensitive part of an organisation to costly bugs and data breaches. Every upgrade must then be tested rigorously, realising the value of new functionality without breaking custom implementations.

This article will discuss common barriers to rapid and rigorous FLEXCUBE testing, before setting out how model-based test automation enables organisations to keep up with the latest in FLEXCUBE innovation. To see this approach in action, join Curiosity and Coforge for Oracle FLEXCUBE: De-risk upgrades and migrations in core banking with rigorous test automation.

TESTING UNCERTAINTY INTRODUCES RISK TO CRITICAL CORE BANKING SYSTEMS

FLEXCUBE testing can be a slow and complex process for the hundreds of banks who rely on custom FLEXCUBE implementations.

In-house testing teams typically lack the niche knowledge needed to test complex parameters and custom configurations during FLEXCUBE migrations and upgrades. The complexity of end-to-end scenarios further means that there are more tests than could ever be executed before the next release, but testers lack an automated or reliable way of creating an executable number of tests.

This introduces uncertainty to core banking systems, as testers cannot reliably approve an update or migration. Slow and manual testing do not provide the necessary assurance, as testers often struggle with manual test case creation, repetitive scripting, test data bottlenecks, and slow test maintenance.

These time-consuming and typically unsystematic testing processes force testing further behind FLEXCUBE updates, while only covering a fraction of the scenarios needed to migrate or upgrade with confidence. For banks to keep pace with the latest in fintech innovation, an automated and systematic approach is needed to Oracle FLEXCUBE testing.

DE-RISK FLEXCUBE ADOPTION AND MAINTENANCE

Visual, model-based test automation offers one approach to simplify, streamline and automate FLEXCUBE testing. Instead of struggling uphill with poorly understood systems and manual test creation, testers can quickly scan FLEXCUBE UIs, assembling a library of reusable components into visual models. These models auto-generate the tests, scripts and data needed during a FLEXCUBE migration or upgrade.

Using a UI scanner, test importers and a library of out-of-the-box components, in-house test teams no longer require niche FLEXCUBE knowledge. They can visually assemble reusable subprocesses into end-to-end flowcharts, making quick adjustments for customisation and parametrisation. The quick-to-build tests then generate everything needed for rapid and rigorous FLEXCUBE testing:

The visual models generate optimised FLEXCUBE tests, while also building and maintaining living documentation of core banking systems. The easy-to-maintain flowcharts build and retain in-house knowledge, future-proofing core banking systems. In-house testing and development teams can innovate broader banking processes and integrated technologies, understanding the impact that their changes will have on custom FLEXCUBE implementations.

While boosting testing agility and building FLEXCUBE knowledge in-house, the model-based test automation further provides the test coverage needed to innovate core banking processes.

The automated test generation creates the smallest set of tests needed to “cover” the modelled scenarios, while risk-based generation can target particular functionality based on time and risk. This reduces test volume without compromising test coverage, providing confidence before every upgrade or migration.

The rapid test generation further avoids test data bottlenecks and helps reduce the risk of costly data breaches. With integrated Test Data Automation, every end-to-end test scenario can come equipped with anonymous or synthetic data on-the-fly. This data is found, made and prepared “just in time” as tests are generated or run from Test Modeller, providing all the data needed to test during a FLEXCUBE migration or upgrade.

Using Test Modeller, organisations can enjoy the innovation of the latest Oracle releases, while mitigating against the risk of breaking custom FLEXCUBE implementations. Test maintenance is as quick and simple as updating the central flows, hitting “regenerate” to rapidly and rigorously test FLEXCUBE before every update.

Test Modeller allows organisations to realise the value of the latest FLEXCUBE releases, while reducing the risk of damaging bugs and downtime. They can build and retain in-house knowledge of core banking systems, while generating the tests needed to keep FLEXCUBE up-to-date.

To discover rigorous test automation for Oracle FLEXCUBE, join Curiosity and Coforge for Oracle FLEXCUBE: De-risk upgrades and migrations in core banking with rigorous test automation.

About the author: Thomas Pryce is the Communications Manager at Curiosity. He has been with Curiosity since 2018, where he enjoys researching and advising on test data, test automation, and SDLC transformations.",5771
"By: Irena Lambridis, KCR Director of Quality Assurance & Compliance

For the past 15 months, we have been challenged to find new ways to ensure quality across the organizations and clinical trials we support without the ability to travel to central offices, clinical sites and supplier locations. In many ways, this has been an exciting time that has required us to think creatively and develop more modern approaches.

If we look back at traditional auditing, a single internal audit would involve the following: one day of preparations, half a day of travel, a few days on location to review documentation, interview staff and tour the facility, time to discuss non-conformities, return travel time, report writing and follow up activities.

Throughout the Covid-19 pandemic, we have adjusted nearly all these procedures utilizing technology-based solutions to efficiently conduct these activities from the comfort of our home offices while maintaining high expectations and keeping patient safety and data integrity at the forefront. Our experiences have proven that with proper assessment, technology implementation, planning and execution, a quality audit is achievable. This achievement is dependent on several factors and organizationally, it is critical that we define these decisions and permissions formally to pave a path to success.

With this being said, there is often an element of physical presence on site that allows us to observe what is not readily being shown. An example could be body language during an interview demonstrating nervousness or background activity observations during facility touring. There are certain nuances that we are trained to catch during audits, and this is often a valuable tool for quality inspectors and the organizations we support. The ability to find the rabbit hole and know when to dive in and when to keep moving forward is key. It is truly an art that takes years to hone.

So, how should auditing teams move forward? Each organization should assess where risks lie and how to best structure their audit program with regulatory requirements at the forefront. There is no doubt, from a business perspective, that adding a remote audit capability or component could increase productivity and result in bottom-line savings. Then again, deciding how and when to implement remote auditing requires a thorough assessment of organizational needs, documenting decisions, and a clear definition of how audits will be conducted. There is no one right way to manage this and it is an exciting time to challenge ourselves and our industry with the possibility of implementing new solutions.",2635
"On June 14th, our team OneRagtime — Stéphanie Hospital, CEO and Founder and César Chanut, Oscar Péribère and Pauline d’Arthuys — attended the 5th edition of Vivatech’s annual events dedicated to startup growth and business change. What a blast to be all together again!

VivaTech was created only in 2016 yet it has become one of Europe’s leading tech gatherings in France as the French Tech and European Tech are just booming.

VivaTech adapted this year to a hybrid model allowing 26,000 participants to join in-person and 114,000 online. The ability to be in-person again was a truly appreciated experience that granted engaging and convivial interactions with the people there. It was super enriching meeting investors, speakers, entrepreneurs and startups.

Stéphanie Hospital (OneRagtime) and Michel Combes (Softbank)

During the three days, VivaTech hosted many incredible speakers, including Michel Combes and Marcelo Claure from Softbank. They were invited to exchange about Softbank and its amazing journey in Venture Capital. During the interview, Michel talked about the recent Softbank’s fundraise in Jellysmack, now a unicorn and the first investment ever of the OneRagtime portfolio! Jellysmack’s focus is on the creator’s economy.

We are delighted that Softbank believes in Jellysmack’s vision as we did when we invested at the inception of the company. We share Softbank investment thesis: “Great entrepreneurs disrupting traditional sectors with Artificial Intelligence.”

OneRagtime’s Founder and CEO, Stéphanie Hospital, engaged as a speaker in a discussion alongside Michael Rickwood, Chief Coaching Officer at Ideas on Stage, about her story in the tech sector.

“All the projects and technology that we work on today really have the potential to change people’s lives, and that’s why we do it”, said Stéphanie while explaining what the venture capital fund she founded, OneRagtime, does.

Many of OneRagtime’s portfolio companies were at Vivatech :

From left to right: Raphaël Jabol (Avostart), Stéphanie Hospital (OneRagtime) and Jean-Denis Garo (Golem.ai)

Golem.ai with Jean-Denis Garo : a B2B deep tech startup that develops and distributes solutions for automation & business support through explainable & frugal AI language analysis. Part of La Poste booth (one of the biggest exhibitors at the event and a platinum partner of Viva Technology).

Avostart with Raphaël Jabol : another B2B startup offering a platform for all types of legal assistance. Also, part of La Poste Booth

ExactCure with Frédéric Dayan, Sylvain Benito and Fabien Astic : a B2B and B2C startup, an app and online platform helping patients with the management of medicine intake. Part of the Huawei booth

Also present this year:

Hoomano with Xavier Basset and Cyril Maitrejean : it is an Artificial Intelligence software enabling social interaction with machines.

Make.Org with Axel Dauchez : it is an independent and European civic tech aimed at reactivating democracies by engaging citizens in collaborative transformative actions. They participated in the conference “Education: can tech make learning better? Fun fact, Axel launched Vivatech when he was at Publicis (thanks Axel)!

And last but not least, our own Taig Khris announced this game-changing partnership with Microsoft Teams. Taig Khris is the CEO of Onoff, the first telecom operator in the cloud, allowing users to manage multiple numbers and to switch them “on” or “off” with one click and Onoff has recently partnered with Microsoft Teams to have their numbers integrated into the platform. Exciting news!!

Viva Technology was a fantastic opportunity for startups to connect with their customers, large corporations, users and audiences.

Thank you to Julie Ranty — manager director at VivaTech who let us have the opportunity to be a part of the event, July Avedissian who guided us during the experience, Marin Soullier who is in charge of investor relations for organising and making this event memorable and of course Pierre Louette, CEO of Les Echos and Maurice Levy for hosting us and making this event so special and unique.

by Capucine Verbrugge",4132
"Matt Müntel is an entrepreneur with a background in particle physics. He received his Ph.D. in theoretical physics in 2008 from the University of Tartu. Müntel joined the CMS experiment at CERN while a Ph.D. student and was part of the team that discovered the Higgs boson in 2013.

He then had the idea to use the same algorithms from particle physics to accelerate human learning 10x. Interestingly enough, upon meeting the technical co-founder of Skype, Jaan Tallinn, he realized that they had the same goal. Both had chosen language learning as the first example to experiment with the learning algorithms. This led to the creation of a company called Lingvist.

In a sentence, what does your company do?

Lingvist makes learning 10x faster and more effective, through science and advanced technology.

Describe how and when your company came to be. In other words, what was the problem you found and the ‘aha’ moment?

After we had discovered the Higgs boson at CERN in 2012, I felt ashamed that I had not learned French, even though CERN is situated in a French-speaking area near Geneva in Switzerland.

I asked myself if I could use the same algorithms that we were using to discover unknown particles with the Large Hadron Collider (probably the most complicated machine built by humans) to speed up my learning.

I didn’t know anything about languages, particularly French, but I knew how to do scientific data analysis and to program. I wanted to know how much time it would take to learn a language if one would do everything most optimally.

I used CERN supercomputers to analyze how the French language is used. The best proxy was movie subtitles because they consist mostly of one-to-one speech. This is very close to what people speak in their real lives. I analyzed all the movies ever produced and got a very good statistical overview of what I should learn. My calculations showed that I should be able to learn 95% of the most frequently used words and sentences in just 200 hours if I optimized my memory perfectly.

This was the “Aha” moment — if this were true, then it could be possible to decrease the time to learn a language from many years to a few months!!! And almost every single person would benefit from it because TIME is the most valuable resource of nature.

Having built the prototype and learned with it for 2 months, I passed the national examination of French with the average score that students get after learning for 12 years. This result made me think that I should make this software available to anyone who wanted to learn faster and better.

What makes your company/product different in this market?

Lingvist’s main differentiation from other methods of learning resides in its game-changing technology and vast amounts of analyzed learning data, which increase learning speed and enhance memorization. Technology is transferable to almost any field of education. Through advanced measuring, Linguist estimates learning speeds of up to 10x compared to traditional methods.

What users are particularly excited about is that they can create hyper-personalized courses for themselves. It takes a few seconds to create a course about any field they need a language for — to study medicine, work in a financial institution, cook at home, or dream about traveling — unlimited options. Users can define the domain by a few keywords or copy text from a book or even take a picture. The Lingvist course creation tools make a comprehensive language analysis and find the most impactful things they should learn and then teach them most efficiently.

And customers are amazed by their progress, saying: “It’s just uncanny how quickly I can learn.”

What milestone are you most proud of so far?

For me, the most exciting milestone is that we’ve collected vast amounts of human learning data and have been able to understand how to speed up human learning with AI.

We’re building such precise memory models for each user by constantly analyzing their learning process. This is the secret source of accelerated learning. No other company has managed to do it.

Coming from nuclear physics, I get very excited about discovering how nature truly works, based on experiments and precise measurements, which is the same method that has given us the biggest breakthroughs in any field of science. We’re making this breakthrough in education.

Have you pursued funding and if so, what steps did you take?

Yeah, we’ve raised quite a bit of money, as it’s a complicated technology to build.

We have raised €13M from Jaan Tallinn (the technical co-founder of Skype), Rakuten (a Japanese e-commerce giant), and several Nordic investors.

What KPIs are you tracking that you think will lead to revenue generation/growth?

We track how much people are truly learning! This is the most important KPI for us. Learning is hard to measure because people forget things. And if they have forgotten what they learned, the learning has not added any value. However, we can predict how people will remember every single item that they will learn in the next minute, hour, day, week, month, year, or decade. This gives us an incredibly precise picture of how much we are teaching. And our goal is to teach more than any other platform.

We’ve found that learning efficiency is the most important lead metric for conversion and retention — which means that learning efficiency is very directly related to revenue generation. This is super good news for investors because it aligns the user interests with investor interests, and we as a company can truly dedicate our efforts to improving learning.

What advice would you give to other founders?

My best advice is to never give advice. It sounds contradictory, but there is a logic behind it.

If we go to a doctor who knows a million times more about medicine than we do, then we can take the advice.

Nevertheless, in most cases, especially in building a startup, it never happens that somebody knows a million times more about your situation than you do yourself. Most likely, whoever tries to give advice doesn’t know everything that you know. However, they know about their experiences and draw their advice from their earlier conditions that may or may not apply to you. This may lead to implementing the wrong advice. Funnily enough, the more respect and trust you have towards the advice-giver the more vulnerable you are to not noticing the mismatching base assumptions that may make the advice wrong.

A very good mechanism to protect against this is to talk about experiences and learnings and avoid giving advice. This allows you to process the other experiences and put them into your context and make decisions yourself. The probability that it leads to better decisions is much higher.",6767
"Hi sec folks, New lab released on Try_Hack_Me Time to Sharp your skill

Are you Want to HACK_THE_WORLD Join Up on Try_Hack_Me Use this link

Pre-Security is a new lab and learns much more you want.

Before hacking something, you first need to understand the basics.

Cybersecurity basics

Networking basics and weaknesses

The web and common attacks

Learn to use the Linux operating system

Introduction

This learning path will teach you the pre-requisite technical knowledge to get started in cybersecurity. To attack or defend any technology, you to first learn how this technology works.

The Pre-Security learning path is a beginner-friendly and fun way to learn the basics. Your cybersecurity learning journey starts here!

complete a room that’s part of this path and win tickets, get 3 of the same to redeem a prize. If you’re a free user you can win 1 ticket, however subscribed users can win 2 tickets.

What is the Pre Security path?

Learn the pre-requisite technical knowledge to get started in cybersecurity. To attack or defend any technology, you to first learn how this technology works.

The Pre-Security learning path is a beginner-friendly & fun way to learn the basics.

Thanks, TRY_HACK_ME Team.",1218
"[TryHackMe] Pre Security Learning Path Honest Review — The best path for beginners to follow Ashok Chapagai Jul 6·4 min read

The Pre-Security Path Banner

INTRODUCTION

TryHackMe is an online platform for learning cyber security and penetration testing through hands-on exercises and labs designed to teach practical skills.

I have been on the platform for more than a year now and I really like each and every aspect of the platform including it’s functionality and the main concept of it. In order to dive into the field of security, both as a newbie or as an expert who is looking to test their skill , TryHackMe is undoubtedly one of the best platform out there.

What is Pre Security Learning Path or Learning Path in General.

Learning Path in their own word,

Learning paths are a way to build fundamental, low level knowledge around a particular topic. Enrolling in a particular path will give you the knowledge and skills that you can apply to real world scenarios.

The learning paths will help us to get the essential knowledge about a particular topic which then can be transformed and applied into real world applications.

Learning Path Outline

About Pre Security Learning Path

The Pre Security Learning Path is the latest addition to the several learning path the platform provides. Pre Security Learning Path is focused on topics that are looking to set their foot on the world of Technology. The path focuses on providing basic knowledge on Cyber Security, Networking, Web Fundamentals, Linux Fundamentals and Windows Fundamentals. The main aim is to get familiar with the topics before diving into advanced topics which are already available in TryHackMe platform.

This Thumbnail is So Cool.

Without Spoilers, How is the Path Organized?

The room has 5 topics named Cyber Security Introduction, Network Fundamentals, How The Web Works, Linux Fundamentals and Windows Fundamentals.

Each topic consists of different sub-topics/rooms inside them. The topics are organized in such a way that we can apply the knowledge we gained on previous topic can be helpful in completing further challenges coming along the path.

The first challenge Cyber Security Introduction consists technical fundamentals that are important to suceed in cyber security ( That’s why the name xD ). It has one room named Learning Cyber Security that is focused on providing short introduction of security topics we will be learning along.

TryHackMe Cyber Security Introduction

After completing the first topic, we can move to Network fundamentals topic. It covers the topics from What is Networking to various Technologies ( LAN, OSI Model, Packets and Frames etc.) and methodologies that are used to make the magic of internet happen.

TryHackMe Network Fundamentals

The third topic on Pre Security Learning Path revolves around the topic of web. From DNS to Cookies, From HTTP Methods to Basics of bruteforcing ( albeit just an easy challenge to grab the flag ), it should really help beginners to set one foot into the world of web.

How The Web Works Room

Second to the last topic is all about linux and it’s fundamentals. The topic is focused on teaching various basic to intermediate concepts of the famous kernel Linux. The topic has three rooms Linux Fundamentals Part 1,2,3 with the introduction of new topic/concept with each part .

The final topic is Windows Fundamentals, which aims at making us familiar with the most widely used operating system by end users. The rooms revolve around the concepts of File System, UAC, Control Panel, System Configuration and more. This is the must go room for getting into the core concepts behind Windows Operating System.

Final Thoughts:

The Pre Security Learning Path is one of the best place for any beginner to be when it comes to getting familiar with the concepts of cyber security.

The challenges are very beginner oriented and are set up in such a way that they will certainly be helpful for beginners without them having to scratch their head much.

I wish everyone best wishes into their journey to InfoSec and would like to say, “ Never Stop Learning, Never Stop Hacking ”.

and Also Join TryHackMe and let your journey into hacking commence. :)

Thank you.",4221
"Hey amazing Hackers! its Raidh_Here

After many month, I decided to write writeups regarding my bounty from Google of worth $5000. So without wasting any time, lets begin the story!

The Story :

After getting many duplicates and N/A from H1 & Bugcrowd, I decided to write about my journey and started searching for VDP programs. I got many bugs and reported to them but till then no reply . Finally, I decided to start hunt on google.

Started searching google subdomains using Google dorks. I know its piece of a shit but never mind. I found few domains and nothing work for me. So finally, I decided to test on google cloud.

While scrolling down in the google cloud market place I found an interesting application called Dialogflow

again you need to clean up your mind raidh….

I started searching more about the application. Dialogflow is a natural language understanding platform used to design and integrate a conversational user interface into mobile apps, web applications, devices, bots, interactive voice response systems and related uses.

I started searching common bugs like xss,sqli,htmli.. etc.

But it didn’t work! :(

After doing a lot of search, I got an interesting Option

and started creating the phone gateway :)

It also has a delete option. I turned on my proxy and started intercepting the requests and I found a request like this .

its easy to create a wordlist for that random number of strings and able to do brutforce to exploit the vulnerability :)

How i did the wordlist for bruteforcing the random string ?

i send the request to burp intruder and added the points to some strings and created a-z character in the payload section. but it is very hard to get a hit. while checking the dialoglfow cx i saw the same Phone Gateway.

we can see there is a area code option to get specified phone numbers for that particular area.

i selected the area code 205 so i got the active list of numbers belongs to 205. we can select all available area codes and able to fetch the all active random strings to create a word list.

I sent the request to burp repeater and replace the phoneNumbers/<randomstring> to the victims <randomstring> and submit the request.

BHOOOOOOOM!. The victim number got deleted and created a poc and reported to Google. But they closed the report as intended Behavior. Then, I was like ??!!

intended Behavior

After explaining the attacking scenario and impact, they reopened and accepted the bug. Whooo hooo! and rewarded with $5000.

Timeline

May 4, 2021 06:28AM — Reported

May 4, 2021 04:19PM — Status: Won’t Fix (Not Reproducible)

May 7, 2021 06:30PM — Status: Won’t Fix (Intended Behavior)

May 12, 2021 09:10AM — Status: Accepted (reopened)

May 18, 2021 04:20PM — Rewarded $5000 bounty

Aug 28, 2021 01:29AM — fixed.",2781
"Review On How Startups Can Get Top Talent | Startup City Magazine jack Mathew Jul 6·2 min read

Source — Startup City Magazine

startup city magazine review

Startup city magazine review on How Startups Can Get Top Talent? job interview is that the most vital step in any employment process. it’s critical to rent cautiously so as to know the candidate’s skills and the way they applied those skills to their advantage.

Fremont, CA : the amount of startups is increasing as a results of technological improvement. There are gigantic corporate bodies that dominate every industry. Despite this, alittle fraction of companies succeeds in defying the chances and becoming household names. Clearly, people are gradually adapting to the startup ecosystem, and more individuals than ever are wanting to invest their time and money in it. actually , new talent prefers to figure during a startup since it provides a good range of opportunities for learning and expanding their skills.

Original Article — How Startups Can Get Top Talent

How to Attract the proper Pool of Talent For Your StartUp

Building a robust Media Presence

People sleep in an internet-driven era, and one among the primary things any applicant will do is search one’s brand’s social media profile. One must maintain a lively social media presence across several platforms and spread the word about one’s company.

Social media may be a forum for showcasing a company’s culture and values. One may show them what it’s wish to work for your company. One can share movies about one’s everyday workplace activities, weekend activities, or the present significant project one’s team is functioning on. Nothing beats a high level of social media visibility. Utilize the internet’s potential to seek out the perfect applicant.

Proper employment interview

The job interview is that the most vital step in any employment process. it’s critical to rent cautiously so as to know the candidate’s skills and the way they applied those skills to their advantage. Companies nowadays utilize applicant tracking systems (ATS) to get the simplest candidates for his or her open positions. It examines the resumes and selects the one that best meets the company’s skill set requirement. then , question them about their previous experiences, their difficult time and the way they handled the circumstance, and other topics. This way, one will learn more about the applicant and be ready to determine whether or not the person is capable of doing the work .",2507
"By Aaron Day — Global Partnerships Lead

Canva empowers our community to start inspired, design with ease and achieve their goals, but we can’t do it alone. Canva’s partner ecosystem is growing at an exponential rate- just this month, we’ve reached the milestone of 450 partners globally, across every continent on the planet.

Partners are critical to Canva’s growth as we set out to achieve our crazy big goals. That’s why I’m thrilled to share that today, we’ve announced a series of strategic new partnerships with Linktree, Staples Canada and Sprout Social, giving businesses around the globe the power to design anything, and print and publish anywhere.

Over 55 million people in 190+ countries use Canva to design with ease and collaborate at scale. Whether introducing new Canva Button integrations with Linktree and Sprout Social, or launching new print partnerships with leading retailers like Staples Canada, Canva’s partner ecosystem continues to accelerate at an incredible pace. Today, these partnerships empower both Canva’s community and our partners’ customers, helping improve workflows with seamless integrations and tailored publish points. Here’s a bit more about some of our newly announced partnerships:

Linktree and Canva Link Up to Make Complex Things Simple

The Canva Button enables organizations to bring the Canva experience right inside their own platform with only a few lines of code, so customers and users can design beautiful ads, banners, headers, and more without leaving the partner’s site or app. Canva and Linktree, the market-leading linking platform, today announced a new Canva Button integration, which brings Canva’s leading design tools into Linktree’s interface so anyone can easily customize their Linktree backgrounds and profile images, using pre-built templates or entirely from scratch. Linktree’s category-defining solution connects audiences across the entire online ecosystem with just one click.

Staples Canada Launches as a Canva Collection Partner

Canva and Staples Canada, the working and learning company, initially announced a strategic partnership earlier this year. The partnership, which combines Canva’s visual communications and design platform with Staples Canada’s unparalleled Solutionshop printing services, helps customers create custom-print solutions. The partnership expanded today, with Staples Canada launching as a collection partner. Now, any Canva Print design can be printed and collected at Staples. From large posters, to business cards, to flyers, cards, invitations and more, over 15 product categories can be fulfilled via Staples Canada, empowering small businesses and consumers to create and collect impactful designs with ease.

Sprout Social and Canva Help Knowledge Workers Design and Publish Content Seamlessly

Today, Canva also announced a new partnership and Canva Button integration with Sprout Social, an industry-leading provider of cloud-based social media management software. With the Canva Button integrated directly into Sprout Social’s platform, customers can leverage Canva’s powerful visual communications platform to seamlessly design social graphics directly within Sprout. Sprout’s end-to-end social media workflows help users add the well-designed content into its social management tools, where they can schedule content, manage incoming messages and measure analytics to improve their overall content strategy. The new integration enables frictionless flow for marketers and businesses to design and publish content with just a few clicks.

Interested in partnering with Canva? We’d love to chat with you — drop us a line here if you’d like to explore how your organization can partner with the world’s first and only visual communications platform.",3765
"Why interning at a startup is way better than a big corporation.

Don’t be too quick to judge an internship opportunity at a startup. You have no idea how much you’ll learn coming out of it. Hojip Jeon Jun 8·4 min read

Photo by Leon on Unsplash

At startups there are less people, so you take on more responsibility, allowing you to do more impactful work, which ultimately helps you learn immensely about your job and the industry you’re in. It’s as simple as that. It’s like when you’re in a group project with 5 people versus 30 people. In the end, however, it depends on what’s more important to you: the label of “intern at [famous company]” and the recognition that gets OR knowing how to actually do marketing, financial analytics, sales, etc.

That’s really it. If you want to read about how I came to this conclusion… wow, that’d be great, thanks!

As kids we’re always taught — almost programmed—that working for those big name corporations is the goal. It stressed me out to see my peers and friends get those big internships that led to even bigger first jobs, while I was still struggling to get my foot in the door for an interview. However, I think a big part of it had to do with my less adequate grades.

Fortunately, thanks to a very generous friend, I was able to get a phone interview on a Saturday with my, now former, boss at a Korean tech company. I think I was desperate after a series of rejections because I remember telling him, “Whatever you need me to do, I’ll do it.” Within days I got hired to work on content marketing, yet once I sat down, I learned that we didn’t even have a proper marketing team.

Because it was a tech startup, the CEO wasn’t too focused on marketing just yet, and poured a lot of his resources into hiring the best engineers and buying the best equipment for them. The good news was, we had the products. Now all we had to do was plan and implement a strategy. With a newly developed SDK and API, my fellow interns and I created a landing page via Notion to introduce a beta program for the SDK, hoping to get software developers, and alike, to sign up and use the product. We polished the website, published it with an analytics tracker, and then ran a month long Google Search Ad campaign to increase traffic.

Running that ad campaign taught me more about how to plan, customize, and publish an ad than any advertising or marketing class at university did. There’s only so much you can study, but you really won’t understand it until you do it. This process also forced me to study SDKs and APIs, allowing me to not only know what they are, but how to also sell them.

Three months later, we officially hired our Head of Marketing. By the end of her first week, she had reorganized the company’s entire value offering and strategized a plan to market our technology, products, and solutions. I learned a lot from that one presentation on how to set up a marketing plan for a business. When you’re at a big corporation that’s already been established for a long time, you don’t get to see how everything began. You don’t get to see the reasoning behind a certain product line’s naming and branding, or why product A is heavily marketed at expos and trade shows while product B is advertised through digital marketing. Apologies that this is so focused on marketing.

With her plan in mind, she tasked me to research and create a Go-to-market strategy, content marketing strategy, and campaign ideas for one of our solutions that had already entered the US market. Creating those from scratch and getting weekly feedback from the expert, pointing me in the right direction, taught me everything you need to consider when assembling a marketing strategy. So the question you also want to ask yourself is this: Do you want to learn by creating something yourself? or by filling in the blanks of someone else’s framework? I believe that to be that star candidate you need to know how to do the job you’re applying for better than your competitor, and there’s no better way to learn than to create something yourself.

Another huge perk for interning at a startup is that you get to be part of that OG crew. Imagine that you intern at a small tech startup that changes the way people buy coffee and it replaces Starbucks (Starbucks will probably buy that startup IRL). That’s something you can be proud about, and if you have maintained the good relationships you made while you were there, that’s an opportunity to join the team once again. If you interned at a startup that didn’t survive, at least you have the insider scoop on what went wrong — and that’s even more valuable knowledge! Either way, it’s a win-win, and I learned in economics and game theory that that is rare.

In conclusion, startups are a great environment for an intern because of the impactful work you’ll do, knowledge you’ll gain, and friends you’ll make. I know we live in a society today that’s always chasing the big labels, working for places like Apple, Google, and even Popeye’s (best chicken there is). But those are the big fish. And if you’re like me, someone with not a lot of experience and impressive grades, startups give you a lot of practice with small fish, better preparing you for that big fish. So next time when you’re scrolling through jobs on Linkedin or other sites, don’t be too quick to judge a startup internship because you’ll definitely learn more than at a big corporation.",5431
"Increasing Prevalence Of Genetic Disorders And Launch Of New Products To Boost Demand For Snp Genotyping And Analysis vivek Vishwakarma Jun 8·1 min read

Single Nucleotide Polymorphism (SNP) addresses the most widely recognized sort of change in genomes of all living things. As an individual DNA test is run in a research center, explicit hereditary variations are recognized by DNA succession variety tests. Variety is appeared as contrasts in grouping between various cells. The point of genotyping is to distinguish hereditary varieties that might actually influence an illness or characteristic. Much of the time, a couple of qualities have various varieties, prompting variable quantities of DNA variations (SNPs.) Hence, different tests can be run on similar examples to distinguish contrasts and henceforth acquire data on the genotype of the individual.

For SNP genotyping and analysis, various arrangements of DNA are utilized for every person. Normally, the whole arrangement of human DNA is utilized for analysis. Sometimes, more limited inclusions and cancellations are made, bringing about minor changes in the grouping of the hereditary material. For instance, an individual could have numerous distinctions in the four arrangements of DNA simultaneously however just one single-nucleation change, bringing about a jumble at the hereditary area.

Read More: https://www.linkedin.com/pulse/increasing-prevalence-genetic-disorders-launch-new-snp-vishwakarma/",1472
"Impact investments in today’s world assume great significance as they come with an explicit intention by the investors to make a positive impact on the society. Conventionally, impact is measured in the context of the UN SDGs. Multiple organisations in the past have taken up the task to measure the impact of projects based on the SDG framework with varying degrees of results. While these initiatives among several others involve fairly elaborate exercises at measuring the impact of projects and companies on the people as well as on the planet, the results may or may not be very relevant from an Islamic perspective. Singapore-based IBF DigiLabs, a member of the IBF Net Group, is seeking to address this challenge through its latest initiative, titled Confluence. The project has been selected to participate in the FIKRA Islamic Fintech Accelerator Programme for Cohort 2021, jointly offered by the Securities Commission (Malaysia) and the UN Capital Development Fund.

Confluence: A Need of the Hour

IBF’s latest project measures the impact of project(s) from the standpoint of the goals (maqasid) of Shariah as well as the United Nations sustainable development goals (SDGs), given that there is a significant degree of alignment between the two and has been aptly named as Confluence, says Mohammed Alim, CEO-Designate of IBF DigiLabs.

IBF DigiLabs has conducted intensive research on the alignment or otherwise of the SDGs with the objectives of the Shariah. The findings from such academic research are being enhanced to develop a robust framework. A text analysis of relevant data and literature enables a process of identification of new metrics in the light of Shariah objectives and redesignation of some existing ones based on alignment or otherwise of SDGs with the former, according to Dr Mohammed Obaidullah, Founder of IBF Net Group. These solutions are being rolled out in two phases:

Islamic Value Analytics

In Phase I of the project, the IBF Platform would combine the metrics using an algorithm and produce a score and/or a classification scheme reflecting the impact of projects using the lens of Shariah objectives as perceived by the respective stakeholders — the project owner(s) and the market. IBF DigiLabs has entered into an agreement to set up a joint venture, Islamic Value Analytics PLT with Ethis Global in Malaysia to implement its go-to-market strategy.

“This AI-based Impact Scoring Platform, besides being a stand-alone destination for investors interested in obtaining an impact rating for projects, would also serve as a front-end application with crowdfunding platforms seeking to raise equity resources. It can provide valuable data for Islamic investors seeking to make a difference to the world while realizing their risk-return expectations”, says Umar Munshi, Founder of the Ethis Group and Strategic Advisor with IBF DigiLabs.

Green Cryptos

In Phase 2 of the project, given the availability of alternative tools for measurement of environmental impact (e.g. carbon footprints) and social impact, the IBF platform would identify and adapt a suitable method in the light of the goals of Shariah for measurement and conversion of such impact into green/social cryptos. Projects can earn/liquidate such cryptos representing both types of impact at an Exchange to alter their risk-return-impact profile in the market.

This Platform, a marketplace for trading such cryptos, is being developed using Algorand blockchain technology which is scalable, secure, decentralized and Shariah-compliant. Algorand provides immediate transaction finality while preventing forks, offers highly customizable smart contracts, and asset tokenization built directly in layer 1. Further, Algorand (unlike other protocols with heavy carbon footprints) maintains a carbon-negative position at all times. The IBF Net Group has a strategic partnership with Algorand Inc. for all its blockchain related initiatives.

Together, the two IBF Platforms are expected to promote the Shariah goals of comprehensive human development through the Islamic capital markets. The Platforms with a clear and sharp focus on impact would direct resources to projects that lead to economic growth with equity as well as ethics and morality.",4252
"How “I” passed one of the HOTTEST IT Certification — Google Cloud Professional Data Engineer 2021 version?

Disclaimer: All views are my own & have nothing to do with my employer. Also, I have tried to keep this post per Google’s certification secrecy policies.

FIRST thigs first! GCP professional DE is the HOTTEST cloud certification of 2021. I am not saying this, the industry suggests so- See link:

https://venturebeat.com/2021/08/17/15-top-paying-it-certifications-for-2021/

2. Who am “I” ? Just another next door Data engineer with 10+ years of hands on Data Engg. experience in OLTP,OLAP, Bigdata & cloud. Been fortunate enough to work on some really large datasets. This is what helped ME most in cracking this GCP DE exam. I will explain in details how.

3. Certification preparation is like TRAINING a ML model :

If you do UNDERFITTING — You will FAIL both in exam & in real life.

If you do OVERFITTING — You will PASS the exam but FAIL in real life.

Choice is yours. If wish to succeed in both scenarios- practice & practice.

HINT: If you understand the keywords I mentioned above, you can answer some questions in the exam :)

4. Shortcuts can cut short your chances of passing! AVOID it. The exam is VERY scenario based so taking any short cuts, like asking questions from friends who have appeared won’t help much. They tweak the questions smartly for the very same scenario. So not sure how any PAID mock test can be helpful.

I myself wrote several pages of paper notes ( like we did in school ) to ensure I don’t forget some of the tricks & importsnt details.

5. Breadth & Depth of the GCP services — The exam covers almost all GCP DE related services like storage, ingestion, processing, consumption, governance etc. But one does not need to know all these services in depth. For some, if we just know WHEN to use & WHY , we are good. For some services we ALSO need to know HOW to use them EFFICIENTLY.

HINT: GCP is being marketed as an alternative to customers Data warehouse & Data lake on premise infra to Google’s infra. So MIGRATING data from on prem infra & its associated service offerings are key. Remember, COST is not always the choosing factor while selecting a cloud service, sometimes it is the “AVAILABILTY TO MARKET” / infra ramp up is a key differentiator.

6. Focus on YOUR strengths & work on weaknesses: Remember, the exam requires 70% to pass not 100%. So it is okay to not know everything. But before even preparing if you know your strengths & weakness it can help you.

In my case I have done only 1 real-life ML project & 0 stream processing ingestion project. So I knew these were my weak spots & I would score less here. This is exactly what happened in exam too. So I had prepared for the rest of the sections in depth. My past hands on experience made that easy for me and I was able to solve all scenario based questions.

7. LEARNING Resources I used: Different people learn new things differently. It seems my brain has been trained to learn from videos first & then text.

Google Cloud has some amazing short videos which I always watch & then follow up with Google sketches/blogs. Priyanka is an amazing teacher for sure.

For topics that requires deep understanding or have higher weightage in exam, I read Google clouds documentations multiple times. For example, Google BigQuery is a game changer for some DW,DL & ML use cases.

Not only I have a good understanding on how my organization is migrating to cloud but how other companies are doing too. A medium article by Walmart helped me understand the migration strategy they used, complete end to end landscape.

Last year some CVS employees had a 5 day GCP training. I was not invited to that training but I heard it was good. The trainer had shared some PDFs and hand written digital notes. I took access of those materials & it did help me plan when I started preparing for this. So if you are a CVS employee / contractor, contact me in company IM to get it.

8. How to get hands-on ? Hands on is a MUST. I did some of my practice in my day to day job. Also I used quicklabs. Qwicklabs has a 300$ FREE credit program which one SHOULD use. If you are smart, you will know how to get another 300$ credits :). GCP is fairly new as compared to some of its competitors, so getting internships and hands on is little challenging.

9. Exam time tip: You have 2 hours to complete the exam. HINTS to the answers are hidden in the question. Read the question very carefully. If required read it 2–3 times & it will confess the answer :).

Sample QUESTION format:

In 2020 USA presidential elections, a successful American businessman turned politician from the grand old party of the United states of America was defeated by a Democratic candidate. When this Democratic candidate was a Vice president in past, who was the POTUS?

Option 1 — Joe Biden

Option 2 — Kamala Harris

Option 3 — Donald Trump

Option 4 — Barack Obama

So if you don’t know what is POTUS — President of the US, you will be confused!

10. Lastly, THANK YOU for reading till the end! Believe in yourself & get GCP certified. If “I” can do it, YOU can do it BETTER than me. Good luck!",5164
"The Demand

Bootcamps exist to provide short, intensive training and make human resources available to those who need them quickly. According to the Bureau of Labor and Statistics (BLS), jobs in software engineering (and related roles) are projected to increase 22% in the next decade, which is considered to be “much faster than average” when compared to jobs in other industries. The median annual salary for these roles is approximately $110,000, which is more than twice as much as the median annual salary for the United States as a whole. This large salary is evidence of an insane amount of demand for good-quality talent.

In my experience, hiring mid- to senior-level engineers is an extremely competitive endeavor in the current labor market for developers — even outside of traditional engineering markets like San Francisco. This means as a hiring manager, I must be prepared to have a steady pipeline of junior engineers to onboard instead — especially if I’m expecting my organization to grow in the future (which I am).

The BLS projects the industry to grow by over 300,000 new jobs in the next decade. In an effort to fill this need, coding boot camps pumped out 25,000 developers in 2020, a ~10x growth in the last decade according to Course Report. That’s about one-third the amount of traditional computer science graduates in the same time period, according to Code.org. However, those with computer science degrees must be getting snatched up quicker or by bigger firms because for every candidate that I interview with a computer science degree, I tend to interview two more candidates with non-traditional backgrounds (like boot camps).

Herein lies one of the things I really like about boot camps: They provide a larger and more diverse pool of applicants to draw from than if we interviewed university graduates alone. Those I have interviewed come from several walks of life, providing a plethora of diverse thoughts. Some notable attributes that I have encountered from the added diversity include:

A wide variety of domain expertise from previous jobs (from those who decided to make a career change)

A good amount of organizational behavior experience from working in cohorts and with mentors

Representation from under-served socioeconomic backgrounds that otherwise would not have had an opportunity to be interviewing at a tech company

And while not all boot camp graduates are home runs, I do see some common themes that come out of those we end up hiring.",2494
"Why is this course so special? What does it offer?

As I said previously, I didn’t start with The Web Developer Bootcamp. First I did the Codecademy Learn JavaScript course, which was pretty good and gave me a decent foundation to keep learning, and then I finished JavaScript Ninja, by Fernando Daciuk, a course that focused only on vanilla JavaScript.

This second course was not a bad one, I learned a lot about the basics of JavaScript and understood how the language works, what it can do, what it can not, but I was disappointed. I was disappointed because a. it was not up to date, as Daciuk only covers ES5 (and he is not forthcoming about it in the course info — he also makes a promise that his course is the only one you need to learn JavaScript), b. it was not cheap, especially for dated content.

It was during the time that I was doing JavaScript Ninja that I learned about Colt’s course. It was on sale — it usually is. It only cost me a few bucks. And it had a lot, a lot! of good reviews. I was climbing my way up in JavaScript, but my lack of knowledge on HTML and CSS beyond the basic was a real problem and The Web Developer Bootcamp was the perfect solution.

So, what Colt Steele offers is the following:

HTML5

CSS3

Flexbox

Responsive Design

JavaScript (all 2020 modern syntax, ES6, ES2018, etc.)

Asynchronous JavaScript — Promises, async/await, etc.

— Promises, async/await, etc. AJAX and single-page apps

and single-page apps Bootstrap 4 and 5 (alpha)

SemanticUI*

Bulma CSS Framework

DOM Manipulation

Unix(Command Line) Commands

NodeJS

NPM

ExpressJS

Templating

REST

SQL vs. NoSQL databases

vs. databases MongoDB

Database Associations

Schema Design

Mongoose

Authentication From Scratch

Cookies & Sessions

Authorization

Common Security Issues — SQL Injection, XSS, etc.

Developer Best Practices

Deploying Apps

Cloud Databases

Image Upload and Storage

and Storage Maps and Geocoding

(* I don’t remember seeing Semantic UI, maybe it was part of the old version)

This is certainly not a small list of content. But it is exciting — well, it was exciting for me to see an online course that goes from the very basics of web development to the advanced world of databases, servers, authentication, and all that, pretty much what you will need if you decide to work with web dev in real life.

The curriculum is an upside, for sure. During the course, there are several small projects that Colt builds and it is fun to code along, because you have the feeling that you are finally creating something.

But the icing on the cake is the final and HUGE project called YelpCamp.

YelpCamp is a full web app, created on a Node.js server, using MongoDB, EJS, Express, Passport, Cloudinary, and a lot more. It has authentication, it has a comment section, it has a rating system — it is big, it is complex, and it is pretty cool.

With YelpCamp, everything you learned over the lessons will be put to use.

A quick note: one common complaint that I saw in the comments was that the Bootcamp became outdated. It seems that, for a while, Colt tried to put new content on it using notes, but without uploading new videos. When I bought it, tho, he had already redone all the content from scratch, so things like ES6 are well discussed.",3274
"The Issue:

With applications getting bigger and bigger we as developers need to find better solutions for passing data into our components and services, and this is where things get messy. When we look at Angular’s solutions we can see the upcoming growing pains.

1. Input() chaining — I hate seeing this. all components know exactly what state their child components have. this can be important and useful but usually, we just see one component getting data from Input and then passing it off to another. causing a lot of potentially breaking points.

2. Services — very easy, low maintenance, but data is still stored in memory and can causes files to become extremely large due to all the use cases we will create. and can be mistakenly reset by adding it as a provider in a component or module (without understanding why this may be done, there are valid use cases for this).

Nothing wrong with using the solutions mentioned above. but we can do better.

example of bad input chaining

Opinionated Solution

Ngrx is an amazing solution for storing and sharing data in our applications. In many applications, it becomes the single source of truth (as it should)

and is used mainly to make our UI react to data store changes as we go. Yes, we want to update our UI when the data we depend upon changes. But why stop at data changes? Why not “abuse” Actions?

Actions are our way to update our store and to initiate a process.

We usually use an *EffectsService and/or reducers for this. in which we listen to specific actions and run higher-level functions (HTTP calls for example, or normalizing the data before setting it in the store) in order to update our store and make our application react accordingly.

Fortunately, most Angular elements (Component/Services/Directives ) can listen to actions. this means that almost any element can listen to predesigned Actions made for it.

For example, if we have a button component, that listens to an action called ButtonLoaderAction and its props are props<{loading: boolean}> .

The button is agnostic, it is decoupled from any other logic. it is completely independent, it will only update when the action is called by whoever calls it.

Actions don’t do anything but send events to anyone listening to it, usually our reducers and/or effects , we can take advantage and use them in order to make our website react not only to data changes but to developer decisions. We decide when our application reacts and changes.

Passing the power, of when our application reacts, to the developer is already an emerging concept with updating the changeDetection strategy to onPush for example.",2641
"However, in the large tech-savvy teams, such as Softray Solutions, constituted of young professionals with a relatively short retention span and the dynamism of the ever-changing workforce, the HR team should also turn to the agile mode and work on the go. This also refers to the strategies of finding, identifying, training, and rewarding their HiPos.

The optimal approach here is three rules in setting up the auspicious conditions that would promote the seasoning of those desirable traits and their unraveling into the future team leaders.

The first step should be a clear-cut and well-defined selection, profiling, and setting up of the expectations between the company and the candidate.

The second step would be recognition of their key talents, followed by the projection of their growth and the subsequent planning of the career path within the company while taking into account the candidates’ preferences, their cultural fitness, and their autonomy.

The third step would be the transactional model of knowledge/experience exchange between the employees of varying seniority. The theory behind this approach promotes the idea that the employees with the higher seniority when put in a mentor position vis-à-vis zealous junior colleague, can be positively challenged by the zeal of their younger colleague. The same goes in the case when we want to give an extra push to the already-progressive junior developer — where the leadership skills come to the light when challenged with the player known as the intern. This strategy of accelerating the emergence of the mentor archetype among the employees that indicate the possibility of a HiPo by putting them in a challenging position is a great method to identify the hidden traits.

Although, one must differentiate between the high potential and high-performance players — a confusion that sometimes results in a mismatch between what one can do and what one could do, so to speak. There is nothing worse than presuming that just because the employee does her job without a speck in her code, means she would manage a team of people doing the same. The cost is losing an invaluable employee in a position in which they perform well, to placing them in leadership and managerial roles for which they are far from qualified.

To circumvent these failures, it would be useful to mention Wittenberg who suggested four types of agility inherent to true HiPos:

People agility — getting on well with the professional diversity within the workforce;

— getting on well with the professional diversity within the workforce; Results agility — stepping aside and grasping a whole picture;

— stepping aside and grasping a whole picture; Learning and Mental agility — cerebral adaptability to organizational change;

— cerebral adaptability to organizational change; Change agility — being open to challenges.

These different agility types are good indicators of the employees’ engagement in their work and company environment, as well as their future trajectories.

By paying close attention to the people dynamics, posing a challenge, as well as investing and supporting your people, HiPos will come to you without employing those extensive “personality testing kits”, or losing people to one’s unsubstantiated HR intuition voice.

If you enjoyed reading this, click on the clap button so others can find this post.",3372
"How LUKS works

LUKS (Linux Unified Key Setup), in particular LUKS2, provides a generic key store on the dedicated area on a disk, with the ability to use multiple passphrases to unlock a stored key. LUKS2 has a more flexible way of storing metadata (redundant information to provide recovery in the case of corruption in metadata area).

LUKS header provides metadata for encryption setup. The followings are some of the features:

Checksum mechanism to detect corruption and manipulation in header

Metadata area is stored in two copies for a possible recovery

Metadata are stored in JSON format. It allows future extensions without modifying binary structures

Header contains objects called token, which contains information to where to get the unlocking passphrase

Header

LUKS Header explaination

LUKS header are basically divided into three parts:

Binary header (4096 byte, only 512 bytes are used)

Metadata in JSON

Keyslot area

As you can see in the image above, the binary and JSON areas are stored twice and in normal condition they contain the same values.

The size of the binary header ensures that it is always written in only one sector (atomic write) to prevent errors and/or corruptions.

Binary Header

The binary header contains all the information needed to inform the system that is going to approach a LUKS device. The information saved here are basic information such as labels, a signature that indicates this is a LUKS device, header size and the metadata checksum (very important!).

The primary header must be stored in sector 0 of the device, the second one starts after the first JSON area at fixed offset position as indicated below:

Offset | JSON

[bytes] | [kB]

---------------

16384 | 12

32768 | 28

65536 | 60

131072 | 124

262144 | 252

524288 | 508

1048576 | 1020

2097152 | 2044

4194304 | 4092

JSON Area

The JSON Area starts after the binary header and the end must be aligned to 4096-byte sector offset, so JSON area size is

JSON_Area_size = header_size - 4096

So the offset where the second binary header starts (reported in the table above) now makes sense: JSON size + bin_header_size (4096 byte) must match with the Offset. The unused space is filled with zeros.

Keyslot Area

Keyslot area is space on disk that can be allocated for binary data from keyslots, in fact there are stored encrypted keys referenced from keyslot metadata.

The allocated area is defined in a keyslot by an area object that contains offset (from the device beginning) and size fields, both fields must be validated otherwise will be rejected.

Alignment padding

The alignment padding has the purpose to align the encrypted data at the beginning of a block (block are encrypted one by one, typically a block is 512 byte) with the right offset to make LUKS properly work with the encrypted sectors.

Metadata

LUKS metadata allows defining object with a specific functionality. Objects not recognized are ignored, but still maintained into JSON metadata.

The implementation must validate the JSON structure before updating the on-disk header.

LUKS has some mandatory objects as follow:

config — which contains persistent header configuration attributes

— which contains persistent header configuration attributes keyslots — that are objects that describe encrypted keys storage areas

— that are objects that describe encrypted keys storage areas digests — used to verify that decrypted keys are correct

— used to verify that decrypted keys are correct segments — describe areas on disk that contain user encrypted data

— describe areas on disk that contain user encrypted data tokens — can optionally include additional metadata, bindings to other systems

Binary data inside JSON are stored in Base64 encoding and 64-bit integers are stored as string in decimal notation.

The following is a drill-down of the mandatory objects in LUKS metadata:

Config object

The config object contains these fields, that are global for the LUKS device:

json_size — JSON area size (in bytes), this fields must be equal to the binary header

— JSON area size (in bytes), this fields must be equal to the binary header keyslots_size — binary keyslot area size (in bytes), this must be aligned to 4096 bytes

— binary keyslot area size (in bytes), this must be aligned to 4096 bytes flags — array of string objects with persistent flags for the device

— array of string objects with persistent flags for the device requirements — array of string objects with additional required features for the LUKS device

Keyslot object

Keyslot objects contain information about stored keys, areas, where binary keyslot are stored, encryption type, anti-forensic function used, password-based key derivation function and related parameters.

Each keyslot object contains:

type — keyslot type

— keyslot type key_size — key size (in bytes) stored in keyslot

— key size (in bytes) stored in keyslot area — allocated area in binary keyslot area

— allocated area in binary keyslot area kdf — PBKDF

— PBKDF af — anti-forensic. Not in use in modern systems (LUKS2)

— anti-forensic. Not in use in modern systems (LUKS2) priority — is the keyslot priority: 0 = ignore, 1 = normal, 2 = high.

Digest object

To verify that a decrypted key (from a keyslot) is correct, LUKS uses digests object. These objects are assigned to keyslots and segments, if not assigned to a segment the digest is used for a unbound key.

Digest object contains these fields:

type — digest type

— digest type keyslots — array of keyslot objects names assigned to the digest

— array of keyslot objects names assigned to the digest segments — array of sement objects names assigned to the digest

— array of sement objects names assigned to the digest salt — binary salt for the digest

— binary salt for the digest digest — binary digest data

Segment object

Segment object contains the definition for encrypted areas on the disk. For a normal LUKS device there is only one data segment present.

These are the fields:

type — segment type (only crypt is currently used)

— segment type (only crypt is currently used) offset — offset from the device start to the beginning of the segment

— offset from the device start to the beginning of the segment size — segment size (in bytes) or dynamic (for the dynamic resize of the device)

— segment size (in bytes) or dynamic (for the dynamic resize of the device) iv_tweak — starting offset for the Initializaion Vector

— starting offset for the Initializaion Vector encryption — segment encryption algorithm in dm-crypt notation

— segment encryption algorithm in dm-crypt notation sector_size — sector size for the segment (512, 1024, 2048 or 4096 bytes)

— sector size for the segment (512, 1024, 2048 or 4096 bytes) integrity — LUKS2 user data integrity protection

— LUKS2 user data integrity protection flags — array of string objects with additional information for the segment

Token object

Token object is an object that describe hot to get a passphrase to unlock a particular keyslot, and can contain additional JSON metadata.

These are the mandatory fields:",7110
"What to expect from this article?

This article explains how encryption works in plain language. We’ll start from a straightforward concept, and then we’ll slowly add complexity to encryption concepts, all the way to modern encryption technologies. Near the end of the article, we’ll see some historical references and how things evolved. This article does not go into any implementation details and instead stays at the concept level.

Terminologies

Plain text — A text that we want to encrypt.

Ciphertext — An encrypted text.

A straightforward encryption

Let’s start with a very, very simple algorithm. For simplicity, let’s take a straightforward, plain text, “HELLO.” One approach is to shift the whole word by 2 positions, right, so HELLO becomes “LOHEL.” Do you see a pun in the title :)? Moving letters right is way too simple. Let’s take another algorithm to shift every letter by 3 positions in the alphabet table. So, A becomes D, B becomes E, and so on. With this approach, “HELLO” becomes “KHOOR.”

In this approach, the algorithm shifts letters, and the “Key” is 3 because, in this example, we are shifting by 3. We can shift by 5 or 8 or any other reasonable number. To decrypt this message, any other party needs 2 information, 1- the algorithm, which is primarily public information, 2 — the key, which is to shift by how much. The key is secret information between sender and receiver. You must be thinking this is not secure enough, and you are right, and no one would want to send their credit card information over the internet using this approach. Do not underestimate this simple algorithm though. Julius Caesar used this approach. In this article, step by step, we’ll understand the concepts of modern encryption algorithms. However, before that, let us know why this approach is fragile and easy to crack.

What are the major problems with our simple encryption approach?

Because the algorithm is known to everyone, this will require a simple trial and error method to decrypt. Let’s say first we try with Key=1. We will unshift every letter by 1. So, unshift of K by 1 will give J, unshift of H by 1 will give G, and so on. By this approach, the decryption of our ciphertext “KHOOR” with key “1” will provide us with “JGNNQ.” While this could be an original plain text, but the text does not appear to be any familiar message. Let’s try with key=2. We’ll get “IHMMP,” again possible, but not standard text. Let’s try with 3, and the decrypted message is “HELLO.” Bingo, the plain text “HELLO” looks familiar, and most likely, the key is 3. It just took 3 attempts to crack it because the key and algorithm were too simple.

A little improved approach

Let’s improve the approach here a little bit. Rather than using a single shift key, let’s make 4 digit key, say 2135. The approach is similar. Instead of shifting every letter by a single number, this time, letters are shifted based on different digits of the key. For example, H → 2 = J, E→ 1 = F, L → 3 = O, L → 5 = Q, O → 2 = Q. For the last letter O, we are reusing 2 as the key because HELLO has 5 letters, and the key is 4 digits; we are starting with the first digit of the key again. With this approach, the ciphertext is “JFOQQ.” This one is a bit harder to crack because it’ll take 10*10*10*10=10000 possible combinations. For a 50 digit key, the number of possible combinations is huge.

What are the major problems with our little improved encryption approach?

While it’s tough to crack using the simple brute force method, this approach is susceptible to statistical analysis. To understand this better, let’s encrypt another similar-looking plain text, “FELLO.” Only one letter changed from H to F. The encryption of F→2 = H, and the complete ciphertext is “HFOQQ.” A careful observation of ciphertext “JFOQQ” and “HFOQQ” reveals that both the words are similar looking. A good encryption algorithm should make a considerable difference in ciphertext for a slight change in plain text. Otherwise, it is easy to find the patterns in the ciphertexts and reverse engineer the key.

A sophisticated encryption

We learned above that we want to avoid statistical analysis in the ciphertext. More complex patterns called “Substitution and Permutation” are used to avoid statistical analysis. In plain language, a lot of jumbling and shaking of letters of a pain text. Step 1 — Every letter is replaced with another letter using a predefined replacement rule. For example, A is replaced with T, B is replaced with P, T is replaced with C, etc. Step 2 — Change the position of letters in plain text. For example, position 1 is moved to 3, and position 3 is moved to 8, position 8 is moved to 2, and so on. We want to keep a sane boundary in which we are going to change the position of letters. We do not wish to replace position 6 with 1231. Let’s say 128, so, all substitution and permutations happen in this boundary of 128 only. This is called block “Substitution and Permutation.” Step 3 — Encrypt the message with the key. Hmmm…. you must be thinking that “yeah, it is a little bit of improvement, but has it jumbled letters enough to hide any pattern?” You are right. However, the algorithm does those three steps few more times, let us say ten more times to make complex jumbling. Note the user’s secret key is used in every iteration of substitution and permutation, making it tougher to reverse the process without knowing the key itself. After ten iterations, the ciphertext is so much more garbled that it is extremely hard to find any pattern.

Example of sophisticated encryption

Let’s go back to our example of “HELLO” and “FELLO” to understand the above concept more practically. In the first iteration, in “HELLO,” the H will be replaced with, let’s say K, similarly, in “FELLO,” the “F” will be replaced with, let’s say “T”. All other letters will be replaced same way, finally “HELLO” → “FGMMA” and “FELLO” → “TGMMA.” Still, texts look very similar. Not much has happened so far. Step 2, jumble them with the same jumbling rule for both plain texts. “FGMMA” → “GMFAM” and “TGMMA” → “GMTAM”, still relatable you can see a pattern. Now encrypt it using the previous example key 2135. “GMFAM” → “HOIFO” and “GMTAM” → “INWFO”. So after step 3, “HELLO” becomes “HOIFO,” and “FELLO” becomes “INWFO.” Good!! Just after one iteration, ciphertext looks slightly different, the last two letters are the same, but the first three letters are changed. Doing the same thing over and over again will drift those two similar-looking patterns very far apart. After ten iterations, they’ll look completely different. We saw here just changing one letter created a vast difference. Why did this happen? Because during permutation step, when letters are changing their positions, they are encrypted with different letters of key and producing another encrypted letter. The encrypted letters are later substituted using substitution rules. When the same thing happens over and over again, the key-mixing step changes letters differently. It is not possible to reverse engineer even if you know the plain text. To understand it better, let’s say, after10th iteration encrypted letter is “I,” then you don’t know if it is the outcome of H → 1 = I or G → 2 = I or D →7 = I. So, you can’t tell ciphertext before the 10th iteration, and hence you don’t know ciphertext after the 9th iteration, and so on. The jumbling steps destroy any correlation between similar-looking plain texts. Let’s say after the 10th iteration, “HELLO” becomes “MJLXI” and “FELLO” becomes “ZBGYW.” Even if you know “HELLO” became “MJLXI,” you cannot guess the original key. Also, there is no way to relate the ciphertext “ZBGYW” and “MJLXI” has common letters in plain text.

Where is this sophisticated algorithm used?

The approach mentioned above is efficient if appropriately used. During World War 2, German forces were using the Enigma machine that had a similar mechanism. Enigma machine was considered unbreakable for a long time. Enigma was typewriter-looking physical hardware. Enigma has key- settings, and those key settings produce 159 million million million million key combinations. These days Enigma machine is easily breakable. With modern computers processing powers, it’s easy to do statistical analysis of Enigma. Later NSA and IBM jointly developed Data Encryption Standards aka DES in the late 1970s. In computers, to merge keys with plain text, the XOR operation is used rather than shifting letters, as shown in the very first example of this article. DES uses a 56-bit key, and with today’s modern high processing computers, the 56 bit is not big enough and breakable with brute force methods.

The ultimate encryption algorithm.

These days standard algorithm for the internet is Advance Encryption Algorithm, aka AES developed in 2001. This algorithm uses the above-discussed approach of substitution and permutation with a key size of 128, 192, or 256. Our example of “HELLO” is just a five-letter word, while the key size is 128. A padding approach is used to fill the remaining 123 bytes so that the key and the plain texts are of the same size. For permutations, this algorithm does row and column-based shift operations. There is a very carefully designed table for substitution steps. AES does 10, 12, or 14 iterations to permutation, substitution, and key-mixing for 128, 192, and 256-bit size keys. The final ciphertext is so much encrypted that it requires 1000s of supercomputers and 1000s of years to break. At the same time, it takes less than a millisecond to decipher using a proper key. There is no pattern in similar-looking plain texts. A real example of encrypting “HELLO” is “ZJnVAqc4+b/8VocXY8+grA” and “FELLO is “vvaE97ihSZoPBarN9pV+wg”, can you find any relation between ciphertexts while four letters in plain text are the same? If we increase the number of iterations from 14 to, let’s say, 50, then it is much harder to break. However, it’ll take longer to encrypt. The current standards of AES are a delicate balance of speed and security.

Conclusion

A good encryption algorithm properly encrypts the plain text as well as destroys any pattern in similar-looking texts. The algorithm should also not allow reverse engineering to find the key from plain text and ciphertext. During world war 2, Allies forces were able to break Enigma because they knew that the weather report was sent every day at 6 AM and the weather report had predictable words like “Weather.” A good algorithm should be very fast to encrypt and decrypt for any practical use. The algorithm should not leak key information from any padding used. The ciphertext of the single letter “A” will have 127 padded letters, and the algorithm should be robust enough not to leak any information from padding. A good algorithm should also support tougher encryption as key size increases. In AES, when the key size doubles from 128 to 256, the key combination exponentially grows by 3402823669209384634674607431768211456 times (2 to the power 128). These days AES is standard and so for less vulnerable to risks discussed above.",11104
"The state of proptech in Mexico: Where are we and where should we go Elias Aguirre Follow Jun 17 · 3 min read

Made by: David T. Sánchez

‍An overview of the PropTech startup ecosystem in Mexico and LATAM. The publication includes the following takeaways:

‍

‍An Overview of PropTech Startups

To date, there are 12 Latin American unicorns. Quinto Andar is the first Latin American PropTech startup with unicorn status.

The main PropTech hubs in LATAM are in Mexico, Brazil, Argentina, and Chile.

In LATAM, 53 percent of PropTech startups focus on commercial real estate, 45 percent focus on residential real estate, and 2 percent focus on retail real estate.

In 2019, the PropTech industry in LATAM accounted for 7 percent of the deals made and 9 percent of the invested quantity.

From 2016 to date, investments in Latin American PropTech startups totaled USD 759 million. It was the eighth industry that received the most investment in the region.‍

Real Estate Challenges in LATAM

Real estate agents don’t have exclusivity rights.

Many real estate brokers are not licensed or certified.

People need to use notaries.

Very few Multiple Listing Services (MLS).

Extremely high mortgage rates are a problem across LATAM.

Complicated real estate-related processes for foreigners.

‍

A Framework for Understanding the State of PropTech in Mexico

Beginning with Big Data, a state-of-the-art PropTech ecosystem is able to achieve the effective gathering and analysis of data for the identification of buying, selling, and renting trends for specific areas and demographics.

With respect to AI & Machine Learning, they are mainly needed to transform all the data collection efforts into actionable insights.

PropTech startups are using Virtual and Augmented Reality that allows physically distant buyers to experience the property as vividly as possible.

IoT, allows for monitoring, maintenance, and resource efficiency in any kind of property.

‍

PropTech Startups in Mexico: Where are we?

The most widely used technology by Mexican PropTech startups is the Digital Platform.

The most used business model by the startups that use Digital Platforms is the Marketplace.

The majority of PropTech startups in Mexico are Digital Platforms that function as a marketplace for buying, selling, or renting real estate.

While each day more Mexican PropTech startups start incorporating Big Data technologies in their business models, they still only account for 18 percent of the PropTech startups in the country. That can’t be said of the most promising technologies like AI and Blockchain, each only accounting for 2 percent of the total number of PropTech startups. There is undoubtedly space for many more startups to use these technologies in their business models.

The real estate market is still in a phase where it’s consolidating itself by structuring all the property data available and using it to power its sites.

You wish to continue reading? We invite you to read the full article here!

We also invite you to follow us in our social media:

Facebook

Instagram

LinkedIn

Thanks for reading!",3113
"read more on www.netradar.com/insights

Latency has become a hot topic with the introduction of 5G. When advocating 5G services, the industry talks about gigabit download speeds and about 1ms latency in 5G (but forgets to mention that this is the radio link latency, not to the content, but more about that later).

A low latency is truly beneficial for some daily mobile applications, like VoIP calls and real-time multiplayer gaming. If the visions of self-driving cars running over 5G becomes reality, a low latency surely is important there, too.

So, what is the latency that your customers are truly experiencing in their daily usage of the mobile network?

It is fundamental to note that “latency” can be used to describe different metrics. It can be either the one-way latency between two points, or the two-way latency or Round Trip Time (RTT).

The one-way latency is challenging to calculate accurately as it requires the clocks to be synchronized between the measuring nodes. One could also simply measure the RTT and divide it by two to get a rough estimate of the one-way latency. Yet, in asymmetric networks, which mobile networks are, this would be inaccurate. The RTT (or what people often refer to as “ping”) is much easier to measure, as the sending and receiving timestamps are taken from the same physical clock.

Components of the latency that affect the consumers’ apps is the sum of many factors:

1. Radio link

2. Access network

3. Core network

4. Content server

The radio link (1) and access network (2) are the key components that eventually define most of the latency. The radio link, in particular, is the part that often becomes congested, increasing the latency of the mobile applications data flow.

Core networks (3) do not easily get congested, unless some hardware or cables do break. The RTT introduced by a fiber core is roughly 1ms/1000km but the hardware used increases this further, to a range of 1.5–3ms/1000km.

Content servers (4), residing in a data center, can be a bottleneck if the service is run on inadequate hardware not up to the task of serving the current user base. The hardware can introduce latency in the server nodes themselves, the internal data center network or the external internet connection.

How should you measure latency?

If we focus on crowdsourced measurement methodologies, we notice that most players have a look-up system that seeks to allocate the nearest measurement node for the test. The more nodes you have, the better the chances are that the end point is physically close to the measuring device and the lower reported latency.

Often with latency, one can measure the jitter, a notion of how stable the latency is and a way to show how much fluctuation there is. The closer the content is to the end user, the lower the jitter is, too.

Yet, as most players measure latency before a synthetic speed measurement, the reported latency and jitter present optimal results in an unloaded network.

Our daily apps do not operate in empty environments. Our apps send and receive data all the time, some more, and some less, and depending on the other users in the network, the capacity available changes. As the available capacity of the radio network drops, the user data starts to get buffered before transmission and latency grows.

Netradar measures latency inside the ongoing users’ data transfers. We use sophisticated AI (artificial intelligence) algorithms to decide when and how to measure the latency.

Our analytics can therefore present the real latencies experienced by the end users, while using their daily mobile apps.

Furthermore, at the heart of Netradar is our algorithms that can identify network congestion and show user data transfers that were limited by the mobile network. By combining our latency measurements with the congestion detection algorithms, and the detailed contextual data, Netradar can provide a very detailed picture of the performance of a mobile network, from a country to city level, down to individual routing areas, base stations and even antenna sectors.

What does Netradar analytics show ?

Our analytics, in relation to latency, show the familiar metrics that most crowdsourced systems or dedicated measurement hardware can show and a lot more. As we know when the network is running perfectly, and when there is a shortage of capacity, we can analyze the configurations of different parts of the network and help optimize the performance and the behavior.

The Netradar analytics show, in terms of latency:

1. Average latency as consumers and their apps experience the mobile network in their daily journey, regardless of the network capacity;

2. Minimum latency, the optimal case, when everything works perfectly (similar to the typical latencies reported by speed tests);

3. Maximum latency, the worst case, when the network is seriously overloaded;

4. Latency when there is ample capacity to server the users;

5. Latency in a congested network;

6. Latencies caused by handovers between base stations or radio technologies;

7. All of the above can be applied to radio technologies: 3G, 4G and 5G (SA and NSA), as well as

8. To any number of reference points that enable a very extensive view of data connectivity, even international network peering.

In Netradar, latency is not a simple single value, rather a multi-dimensional metric that can be used to study in detail your own network, and the ones of your competitors.

To deploy Netradar

Netradar is a solution for collecting private network performance analytics. As such, our customers have the ability to deploy measurement points anywhere on the planet. We can rotate the location to test latency between different national hot-spots, data centers or even access network hubs. We can also measure around the world, to see latency to different countries, or to major cloud providers like Amazon, Google and Microsoft.

Our latency measurement servers are designed to be extremely lightweight, yet powerful. A single server running in a virtual machine can serve tens of thousands of customers. This is yet another benefit from our architecture and methodology: you do not need to deploy heavy servers for testing top speeds, only lightweight “ping” servers.

An example from Germany

To illustrate what Netradar shows, we pulled data from the past couple of months from the three major players in Germany. Here are some examples of what we can notice immediately:

• One of the providers has a lower minimum latency than the other two providers: a very significant difference- if I were a gamer, the choice of provider would be clear to me.

• One provider has much more latencies from congested data transfers that indicates a much higher load in the network and not enough capacity to serve the customers. I would potentially avoid this operator.

• One provider has a higher average latency, which indicates potential configuration sub optimality or even network issues.

• One provider has a very high average of the minimum latencies, calculated per data transfers — not a perfect partner for running delay sensitive applications.

• The difference between optimal latency and latency in a congested network can become as high as 200ms for one provider, which will impact substantially mobile apps.

• Looking at individual regions, Brandenburg has very high latencies from two network providers The same can be noticed in Saxony-Anhalt where two providers have high latencies without significant network load. The worst regional latency is found in Mecklenburg-Vorpommern where one provider offers up to 100% higher latencies than the two other providers. Avery significant impact on customer experience.

• Looking at individual regions, Brandenburg has very high latencies from two network providers. The same can be noticed in Saxony-Anhalt where two providers have high latencies without significant network load. The worst regional latency is found in Mecklenburg-Vorpommern where one provider offers up to 100% higher latencies than the two other providers. Avery significant impact on customer experience.

• Looking at 5G, there is no difference in latency to 4G. This is due to the Non-Standalone Access (NSA) mode of deploying 5G, where the access network is the same for both 4G and 5G. In some cases, the 5G latency is even higher than 4G as people make use of the full bit rates and data transfer capabilities of the technology and load the network with traffic. Hopefully Standalone Access (SA) will change this, for both unloaded and congested networks.

Conclusions

In summary, with the wide range of apps and services, and the emerging 5G networks, latency must be seen as an important metric besides bit rate. Measuring the real latency experienced by customers is critical as it sheds light on how the network is configured and how it performs with the daily apps and the data transfers. Finding these misconfigurations and network segments with a limited capacity will make a difference between an average network service and a great one.

Only Netradar can provide a full picture of the network performance as experienced by the end users. Book an introduction session and solution’s demo by contacting tomi.paatsila@netradar.com",9257
"If you have property with more than 1 floor, a basement, an attic and a large area (> 3,000 square feet), getting Wi-Fi access can sometimes be a challenge. The signal coming from a wireless router can be limited to a short distance (common with 5 GHz signaling) and cannot penetrate through thick walls. As a result, it would require users to either be in closer proximity to the router or use wireless repeater (also called range extender) access points.

A townhouse, mansion, warehouse office or large estate would usually come to mind. It is often difficult to get Wi-Fi access because of the distance the signals need to travel. There are also many obstacles that can block the signal and reduce its strength, so users are not able to get reliable Internet. In most cases, you can get a much stronger cellular network signal (e.g. 4G LTE) for Internet access. That can be quite expensive for users on a limited data plan with their network.

Mesh networks provide an ideal solution for that problem. A mesh network uses multiple devices for Wi-Fi access rather than just a single router. The devices are interconnected with one another, and provide access to the network all throughout a location. This allows users access to areas where the signal from a single router could not reach. It still supports the current Wi-Fi standards like 802.11ac dual-band (2.4 GHz, 5 GHz) or 802.11ax (Wi-Fi 6). The communication among the nodes and the router can use a different radio channel.

Figure 1. A mesh network can support multiple devices across a large space. It is ideal for connecting IoT devices in smart homes and offices.

Wi-Fi Connectivity Issues

Some have tried BPL (Broadband Over Power Line) with good results. The BPL device connects to the router and transmits the signals using existing power line circuits. However in larger setups, BPL may not travel the entire grid to deliver data signals. This is because the signal transmission can be affected at some point in the circuit. These are due to transformers in the signal’s path.

Figure 2. It can be frustrating trying to get a good Wi-Fi signal when you are far away from the router. (Photo Credit by Yan Krukov)

Wireless repeaters are often used to extend a wireless router. The problem is that repeaters can be intermittent at times. Since they were originally designed to repeat a signal to reach the user, it can be affected by external sources. What they do is just extend the signal from a wireless router. The signal’s strength is not preserved, and it can degrade due to obstacles that can hinder the signal (e.g walls, floors, ceilings, metal, concrete) and also from electromagnetic interference (e.g. radio signals, microwaves).

Mesh Networks

Mesh networks offer a better solution. By using beamforming, Wi-Fi signals can be concentrated using antenna arrays to deliver more signal to the device rather than radiating outwards which weakens the signal strength. This is not exclusive to mesh networks alone. Beamforming is also used in 5G networks, to transmit signals between small cell site antennas at a distance of not less than 500 feet apart. There are other Wi-Fi implementations that use beamforming. This improves the transmission of signals which means more reliable bandwidth and faster speeds.

The main purpose of a mesh network is to provide consistent wireless coverage. In a mesh network system, there can still be a main device that functions as a wireless router. This is where the Internet connection is coming from. The signal from the main device is then sent out to satellite nodes, which provide a hop to other nodes to provide a strong Wi-Fi signal. Users then connect to the node that is closest to them with a boosted signal as if they were right next to the actual wireless router.

Figure 3. In a mesh network topology, each node n is connected to every other node (n — 1). For 5 nodes, each node has 4 connections c. The entire network has a total of 20 connections (n² — n / 2) or n * c. From the diagram you have a total of (n * c) / 2 lines of communication or 10 lines. If one node were to go down, you still have multiple paths to access the network.

The mesh network uses an ad hoc topology. In this setup, it does not require a centralized access to the network. The original architecture was meant to be decentralized, so that if one node goes down the network can still function. Retail mesh network implementations are ad hoc, but not fully decentralized if it requires a main device for the Internet. Each node can access the Internet independently, so it is decentralized. However, this still requires a main device to provide the Internet on the mesh network.

Figure 4. ZenWiFi AX (XT8) mesh network system (Source ASUS)

The connectivity among the satellite nodes relies on self-healing algorithms (e.g. Shortest Path Bridging) which provides awareness of when a connection is broken. The nodes can re-route signals and allows the user to discover other nodes to connect to. A typical setup (e.g. Asus ZenWiFi AX) would consist of just 2 satellite nodes. It then requires connecting one of the nodes (i.e. router) to the modem for access to the Internet. The other node is then configured to communicate with the router and users can access the Internet.

In terms of security, a mesh network system can provide features to secure the network and communications. With Wi-Fi 6, enhanced security can further be provided. Vendors often target the market for smart homes and offices, so this is an important consideration. These include devices like intelligent thermostats, smart refrigerators and a vast line of IoT (Internet-of-Things) devices. These can be vulnerable when connected to a network, so mesh network systems must provide the best security to prevent cyberattacks.

Synopsis

Mesh network solutions boost and extend the signal, using a distributed and decentralized network of satellite nodes. The entire system acts as one network, not individual access points that require a separate passphrase. The user can connect to any node to get access to the Internet with a strong Wi-Fi signal. In terms of setup and configuration, mesh networks have become easier to install with smartphone app based management. When it comes to speed and performance, it will be up to the protocol (e.g. 802.11ac, Wi-Fi 6) and number of bands (frequency channel) that the system supports.

Since access to the Internet is an essential requirement in daily life, it becomes necessary to provide that service. While it is easy to connect to Wi-Fi access in your typical small office and apartment, the challenge becomes greater when you have a larger space (e.g. multi-floor units, large offices). These include business centers, shared workspaces and mansion estates. Wi-Fi signals can get lost and degrade with distance. A mesh network environment provides consistent signal coverage that handles larger areas for Wi-Fi access.",6951
"Source: blockgates.io

This is the first progress report for the Enterprise Development team — DAO Cycle #4.

For our avid API3 readers, Enterprise Development’s goal is to target large organizations in partnering or adopting API3 technologies while assisting enterprises in solving their use-case and proof of concepts (POC). These requirements pivot around bringing real-world off-chain data to the on-chain environment. This involves long sales cycles, multiple decision-makers, and higher levels of risk. An enterprise-first mind-set is needed whereby knowing the enterprise’s needs, concerns and monetization models are centre stage throughout the Enterprise Development process.

In addition, the Enterprise Development team pursues highly strategic and important areas that touch all verticals in the API3 DAO. These relationships lay the foundation for growth and amplification while thinking holistically from data source through to dApp.

August Update

The first month of the cycle was focused on driving and nurturing relationships with large insurers and market data providers. Being patient while understanding the customers’ needs and concerns has paid-off in terms of additional interactions taking place over the month and into September. This indicates a growing exploration and understanding of how enterprises are entering the blockchain space. Concurrently, the start of the cycle resulted in identifying new opportunities to drive enterprise customer interactions.

The enterprise team remains agile while keeping scaling in mind, so that when the need arises, the team can scale in line with the required capacity while being nimble enough to add value in closing the gaps internally and externally.

The Enterprise development team has also provided cross-team support in closing high ticket items and partnerships and will provide negotiation facilitation and internal working groups across DAO streams.

The Enterprise development team attended the API Days conference with the Open Bank Project speaking about Insurance and Open Banking. This resulted in a further interaction with a large automotive company.

The Open Bank Project

API3 has a long-term partnership with the Open Bank Project (OBP). This mutually beneficial partnership aids in growing traditional financial institutions adoption of Web 3.0 technologies. We are also collaborating with OBP to bring Airnode powered Hackathon’s to these financial institutions which assists in use-case development around connecting real-world data sources to dApps and smart contracts.

Updates will occur when ready due to the importance of such partnerships.

Over the next month — September

Our enterprise customers have signaled that they would be interested in API3 providing the end-to-end dApp development and data sourcing, however, API3’s core business is in data sourcing and connecting the API economy though the Airnode or first-party data oracle. For this reason, we will reach out to implementation partners that can assist API3 enterprise partners build the solution while powering these dApps and smart contracts by API3 Airnode.

Therefore, for the month we will be looking to partner with at least five implementation partners whom we can provide an end-to-end convenience for enterprise customers while bolstering the adoption and use of API3 Airnode’s.

API3 is the only data oracle that is General Data Protection Regulation (GDPR) compliant, extending the envelope of data compliance. This is built with the purpose of Enterprise while preserving and ingraining privacy-by-design into the Airnode. Given API3’s focus on placing privacy and data control at the fingertips of the API Provider, there is also a need at the Enterprise level to investigate ISO27001. ISO27001 (formally known as ISO/IEC 27001:2005) is a specification for an information security management system (ISMS). An ISMS is a framework of policies and procedures that includes all legal, physical and technical controls involved in an organization’s information risk management processes.

In concerted effort, the Enterprise and Operations teams see this as an organizational-wide augmenting activity for API3. This certification complements GDPR while adding a seal of approval in compliance standards. This will allow Enterprises to be more confident due to an international standardization organization vetting the quality assurance of the processes and Airnode product.

Furthermore, an exploration of a large cloud provider integration will be done along with the process requirements in order to further the accessibility and ubiquity of the Airnode as a plug-and-play solution for all enterprise-grade solutions. The scoping and requirements phase will commence this month.

The enterprise team has also tapped into venture capital networks for warm introductions in order to grow meaningful partnerships.

In addition to warm introductions and networks, the team is also focusing on customized outreach uncovering enterprise needs and curiosities when it comes to data oracles.

The enterprise team will attend other virtual and face-to-face conferences in order to increase touch-points with enterprises.

Already thinking about the next DAO Cycle #5

There has also been a shift in the organizational needs to drive enterprise adoption. In the next DAO cycle, there will be a consolidation of the business development team with some team members focusing on enterprise development.

Is your company looking to build on Blockchain?

If you are an enterprise looking to explore and delve into the blockchain space, we would be more than excited to do a use-case exploration session to see if we can work together.

Reach out to us at the following email address: gio@api3.org or you can also reach the API3 team on the API3 Discord and Forum!

Know an API data provider that needs to join Web3.0? Click Here!",5888
"ZEXE Design Strategy

ZEXE is a scheme for privacy-preserving, decentralized applications such as decentralized exchanges. It is designed to utilize a ledger-based system, and support multiple functionalites. These include user-defined fungible assets, cross-application communication, and public auditability. And of course, all of these are to be achieved in a zero-knowledge manner.

In order to realize the above goals, ZEXE breaks from existing private blockchains in several ways.

ZEXE provides a shared execution environment where multiple applications interact on the same ledger. The content of a transaction is no longer restricted to transfers of value, but instead represents a more general data unit called a record. Moreover, users can define their own functions with associated predicates that stipulate conditions under which assets can be spent, without the need to request permission to do so. Rather than an on-chain execution environment, ZEXE opts for offline computations that generate transactions and attach to each transaction a zero-knowledge proof that attest to their correct execution. The result is a protocol for a new cryptographic primitive dubbed decentralized private computation (DPC).

Zerocash Transactions

The first real-world system which used commitment schemes and zero-knowledge proofs to provide privacy was Zerocash. A Zerocash transaction tx is an ‘operation’ that consumes old coin commitments as inputs, and outputs commitments of newly created coins together with a zero-knowledge proof which attests to correct transaction computations. See Fig. 1, below.

On the ledger, each transaction consists of:

The serial numbers of the consumed coins, { sn₁, sn₂, … , snₘ}, Commitments of the created coins, { com₁), com₂), … , comₙ) } , and A zero-knowledge proof π attesting to two facts,

(i) that the serial numbers consumed belong to coins created in the past (without identifying which ones, thus ensuring privacy for parties to a transaction),

(ii) that the commitments contain new coins of the same total value of coins consumed (ensuring the overall economic integrity of the system).

That’s how the Zerocash system uses zero-knowledge proofs to ensure once-off consumption of coins, and consequently prevent double-spending. See Figure-1 below.

Figure 1: Typical Zerocash Transaction

Note that indices of the coin commitments in Fig. 1 above are not repeated. The three new commitments are labelled 4, 5, and 6, and not 1, 2, and 3, in order to signify uniqueness of commitments. Similarly, every coin has a unique serial number sn, and no two coins can share a serial number.

A Zerocash transaction is therefore private because it only reveals how many coins were consumed and how many were created, but the value of the coins is kept secret. As previously mentioned, this provides data privacy. And, in the case of Zerocash being a single functionality protocol, function privacy is achieved by default because there is no need to distinguish one function from another.

Extending the Zerocash Computational Model

Zerocash was a breakthrough with regard to privacy for distributed ledger systems. But unfortunately, the scheme is limited in the functionality it provides. What if we wanted to do more than a simple private transfer of assets?

Take Ethereum, for instance, which supports thousands of separate ERC-20 “token” contracts, each representing a distinct currency on the Ethereum ledger. In handling all the various cross currency transactions, many function calls are involved and these are each embedded to specific applications. But since every application’s internal state is public, so is the history of function calls associated with each.

Even if each of these contracts would individually adopt a zero-knowledge protocol such as Zerocash to hide details about token payments, the corresponding transaction would still reveal which token was being exchanged. Consequently, although inputs and outputs of state transitions are hidden and thus achieve data privacy, the transition functions being executed are in the open. Thus, achieving function privacy in the model of Ethereum is not possible.

ZEXE was motivated by this exact problem. In ZEXE, the goal is not only to provide data privacy (as in Zerocash) but also functional privacy. So a passive observer of the blockchain wouldn’t know anything about the application being run, nor be able to identify the parties involved. Therefore, the ZEXE model can support rich applications such as private dApps, dark pools, and private stablecoins. The programming model also allows multiple applications to interact on the same ledger, as well as promoting user-defined functions in order to achieve a totally decentralized system.

The Verifier’s Dilemma

Another appealing attribute of ledger-based systems is auditability. Whether one is a regulator or new user of a blockchain, the ability to easily verify the veracity of historic transactions is crucial.

Unfortunately, many ledger-based systems achieve public auditability via direct verification of state transitions. And such a verification method of transactions regrettably involves re-execution of the associated computations. The problem with this method is that large computations take a long time to be completed, leaving the network prone to denial-of-service (DoS) attacks.

Early smart contract blockchains such as Ethereum addressed this problem through the mechanism of gas, making users pay for longer computations, acting as a deterrent against DoS attacks. The drawback with this approach is that verification is still expensive. Furthermore, unlike solving the Proof-of-Work puzzle to find the next block, verifying transactions isn’t profitable. This is the quandary known as the Verifier’s Dilemma. In the past, this problem has caused forks in prominent blockchains like Bitcoin and Ethereum.

Unlike other blockchains, program execution in ZEXE occurs off-chain. Furthermore, by using zk-SNARKs, verification of proofs is cheap for the on-chain miners or validators. Therefore, ZEXE is effectively a solution to the Verifier’s Dilemma.

Achieving Zero Knowledge Execution

We begin with Zerocash, a protocol designed for applications with single functionality, that is, a transfer of value within the same currency. Zcash is one example of a cryptocurrency system that uses the Zerocash protocol. It uses zero-knowledge proofs (zk-SNARKs) to achieve privacy. The goal of ZEXE is to extend this protocol beyond single applications to any arbitrary program.

Records as Data Units

The first step is a switch from coins to records as data units. That is, instead of just an integer value, a record stores some arbitrary data payload. So instead of a simple transfer of value as in Zerocash, ZEXE works with arbitrary functions , as long as they are known to everyone in advance.

This change enables ZEXE to support arbitrary programs. But what about privacy?

In the public’s eye, a transaction can again be imagined as an operation that consumes old record commitments, and outputs newly created record commitments together with a zero-knowledge proof.

The structure of a record is illustrated in Figure 2 below:

Figure 2: Typical Records Transaction

At creation of a record, its commitment is published on the ledger, and its serial number is published only after the record is consumed. This time, the zero-knowledge proof attests that applying the function on the old records produced the new records.

As in the Zerocash case, each transaction on the ledger consists of,

The serial numbers of the consumed records, { sn_(old₁), sn_(old₂), … , sn_(oldₘ) }, Commitments of the created records, { com_(new₁), com_(new₂), … , com_(newₙ) } , and A zero-knowledge proof π attesting to two facts,

(i) first, that the serial numbers belong to records created in the past (without disclosing the records),

(ii) second, that the commitments contain new records of the equivalent total value of records consumed.

Supporting Arbitrary Functions

The second step is to enable multiple users to freely define their own functions and allow the functions to interact without any rules. This could be achieved via the same approach outlined in the previous step. That is, by allowing a user to fix a single function Φ that is universal, and then interpret data payloads dpᵢ as user-defined functions Φ = dpᵢ that are provided as inputs to Φ.

Using zero-knowledge proofs as in Zerocash would ensure function privacy. However, merely allowing users to define their functions does not in itself result in any useful functionality overall.

Function privacy, in this scenario, creates problems because there is no way for users to determine whether a given record was created according to any given function in a legitimate way. And given the inevitable presence of malicious users, honest users are therefore not protected from all sorts of fraud.

This particular design approach, of unrestrained freedom in users freely defining functions, is of course an extreme. The lack of rules that govern how user-defined functions can interact is the very root of its failure. But can this idea be salvaged? The answer is yes, and we see how in the next section.

Using Tags to Identify Functions

The third step in this design journey is to introduce unique identifiers to each user-defined function, Φ. That is, we include a function identification-tag to each record and use the id-tag as a way to determine which function was used to create the record. This can be done in a zero-knowledge manner. Perhaps the id-tag is defined as a hash value of the function Φ evaluated at a given value vᵢ. That is, idᵢ = H(Φ (vᵢ)). And hence, each function Φ will have a unique id-tag idᵢ if the hash function is collision-resistant.

Since having ‘no rules’ was the root problem in the foregoing fraud-riddled approach, one rule can be enforced in this case. That is, only records with the same id-tag are allowed to cooperate in the same transaction.

Zero-knowledge proofs can guarantee that records participating in the same transaction indeed have the same function id-tag. This will guarantee that only records created by the same function participate in the same transaction.

Although this type of a system provides reasonable functionality, it suffers from a complete and total process segregation. As a result, even a simple coin swap cannot be achieved. But, it represents a step in the right direction.

In Part III, we’ll discuss how the records nano-kernel enables a new cryptographic primitive called decentralized private computation (DPC), an application-ready framework that any developer can use to build private applications.",10730
"If like me you have a penchant for wine, then your palette will soon be guided by an A.I. Sommelier. Artificial Intelligence has crossed the threshold for recommendations when it comes to our tastes. Prior to recognising what delights the tongue, search engines like YouTube, Amazon and Spotify have led the way in personalising our likes. They rely on our sight and sound senses when we select a video, book or song. Our selections build digital fingerprints about what we find appealing. Then predictions are made on the basis of them.

However, wine and all other food and drink are primarily based on our taste buds. Each of us is unique and our physical tastes are very subjective. For instance, do you prefer wines that are; crisp, earthy, dense, oaked, silky, complex, or velvety? These are just a few descriptive examples. Yet, until recently, it would have been extremely difficult to map and distil these nuances into qualities. And then use them to repeatedly make recommendations. This was due to the need for an extensive volume of controlled testing and numbers of people with varied preferences. In addition to taking account of all the chemical components that form part of the wine creating process.

Nonetheless, Katerina Axelsson, a chemist who founded, Tastry, has solved the problem. She pierced through the maze of taste bud roulette, by teaching a computer to taste wine. Being immersed in the wine industry, she recognised that it lacked coherent decision making data. Her epiphany occurred when a wine critic reviewed the same wine that had two different labels. And then scored them differently. Katerina said,

“I had a hypothesis that you could objectify sensory characteristics by creating a new flavour and analytical chemistry methodology that would measure products the same way a human palate does. And that this objective data could provide predictive visibility throughout the supply chain.”

Voila! By harnessing A.I, analytics and taste chemistry methodology, Katerina was able to gain insights into the flavour composition of wines. In turn, this helps us as consumers to buy wines based on our favourite flavours. Tastry’s algorithms has a 93% accuracy rate in predicting how consumers will score a wine.

This taste bud technology is gaining traction in America. It is already outperforming other tried and tested wine buying methods, by 45%. Tastry, is currently being trialled in the U.K and will be rolled out across Europe, later this year.

For those of you in the wine hospitality sector, you’ll be able to download an app called Bluebird, to receive wine recommendations.",2618
"The global shift in crypto, or: how i stopped worrying and learned to love the coin Alex Romanov Jun 17·5 min read

This will be the first article of my blog, however for the last 4 years i have written dozens of articles and posts concerning crypto as an in-house analyst in various tech startups. And boy, do I have something to share it with you in this context, but: Everything written here is by no means financial advice and should not be considered as such.

For a long period of time only technical analysis was more or less applicable to what we call crypto trading. Today, however, we could think of it in terms of investing money as in penny stocks: high risk — high reward. TA worked pretty well for me, but the market always lacked fundamentals: some impact on our everyday lives from all of the projects that I believed in so much.

What was that shift I mentioned in the title? The real world’s businesses begin to pay attention to the sector which only 1 or 2 years ago was considered gambling or ponzi schemes rather than the central force of innovations which may in a few years redefine the global fintech landscape.

Why am I so optimistic about crypto in the long run? I can name a lot of reasons, and I will begin with what seems to me the most important:

Financial reason 1 — obnoxious KYC/AML policies of traditional banks . Well, try to make a crossboarder SWIFT transfer of, let’s say, 10–20k USD. This might seem a big sum of money for an individual, but it’s almost nothing in terms of an operational business. And believe me, you don’t want your transef to be blocked even though it’s completely legal and transparent. Traditional banks have the right to block any suspicious transaction for a couple of days. And now let’s imagine your overseas partner is waiting for your payment and stops the shipment untill the money is safe and sound in his pocket. Everyone loses money, the shipment is stalled, and you might even get some serious problems with your national regulator. Why? You sent the money abroad and received no goods in exchange, it seems like… you made an illegal withdrawal of funds abroad (e.g. in Russia it is a great problem). Double trouble. Go see for yourself: AML policicies are failing. Should I really name the perfect alternative or you already guessed what it is? If the transfer was sent via e.g. ERC-20 USDT, not only would be complete in 10–20 minutes, it could also be triggered by an IoT-tracking device on your shipment container. The money would be withdrawn from your account and sent to your partner’s account as soon as the container arrives onboard of your ship (according to the Incoterms FOB practice). No papers, no middlemen.

. Well, try to make a crossboarder SWIFT transfer of, let’s say, 10–20k USD. This might seem a big sum of money for an individual, but it’s almost nothing in terms of an operational business. And believe me, you don’t want your transef to be blocked even though it’s completely legal and transparent. Traditional banks have the right to block any suspicious transaction for a couple of days. And now let’s imagine your overseas partner is waiting for your payment and stops the shipment untill the money is safe and sound in his pocket. Everyone loses money, the shipment is stalled, and you might even get some serious problems with your national regulator. Why? You sent the money abroad and received no goods in exchange, it seems like… you made an illegal withdrawal of funds abroad (e.g. in Russia it is a great problem). Double trouble. Go see for yourself: AML policicies are failing. Should I really name the perfect alternative or you already guessed what it is? If the transfer was sent via e.g. ERC-20 USDT, not only would be complete in 10–20 minutes, it could also be triggered by an IoT-tracking device on your shipment container. The money would be withdrawn from your account and sent to your partner’s account as soon as the container arrives onboard of your ship (according to the Incoterms FOB practice). No papers, no middlemen. Financial reason 2 — global money laundering industry needs some new cards up its sleeve. We may see that in the near future not only El Salvador, but also Panama and the places alike will adopt cryptocurencies. Why is this topic so hot for these countries now? So boiling hot, El Salvador’s president wants to build a geothermal power plant to welcome miners all over the world so that they utilize only green energy (pun intended). Let’s dive into some details:

chart built based on the data from knoema.ru and statista.com

Even if we put aside that severe COVID-related decline of GDP in 2020, the chart may be interpreted as: Panama’s GDP growth rate is declining due to decreasing volumes of “suspicious” transactions in the national economy as the AML regulations become stricter.

What’s about El Salvador?

chart built based on the data from knoema.ru and statista.com

Well, it is not so clear due to lower impact of financial sector on the national GDP, however the main message is the following: these countries are looking for any ways to fill their economies with money. What can suit better than being the world’s biggest crypto offshore?

Technological reason 1 — the emergence of 5G networks and global satellite internet coverage. Imagine you have full access to your funds all over the world. The funds no bank can stop you from managing, with no middlemen and unnecessary fees. It’s like cash — it’s everywhere with you, only you know how much money you carry in your pocket. We are about to get this “features” of cash without its disadvantages: you can carry $1 or $1 000 000 in your pocket, nobody knows you carry it with you, no pickpocket can steal it from you for only you know the phrase/pin/or another means of securing your funds. It might seem a little bit naive now, but these are the features we always underestimate in managing personal finance.

Imagine you have full access to your funds all over the world. The funds no bank can stop you from managing, with no middlemen and unnecessary fees. It’s like cash — it’s everywhere with you, only you know how much money you carry in your pocket. We are about to get this “features” of cash without its disadvantages: you can carry $1 or $1 000 000 in your pocket, nobody knows you carry it with you, no pickpocket can steal it from you for only you know the phrase/pin/or another means of securing your funds. It might seem a little bit naive now, but these are the features we always underestimate in managing personal finance. Technological reason 2 (closely connected with reason 1 though) — the dawn of the Internet of Things. I don’t speak about refrigerators which order food for you or personal assistants in shape of a portable speaker. It reminds me of some early aircraft prototypes — a lot of hype with no functionality.

5G networks will allow us to stuff everything with sensors, ARM-based devices and microcomputers connected to the Internet ultimately creating more data and traffic than all the humans in the world.

IoT-based logistics triggering payments,

IoT-based insurance charging extra or lowering your payments if you drive carefully,

IoT-based monitoring infrastucture which can order some spare parts for an engine before it breaks, detecting any anomaly when it still runs and triggering payments for the repair, —

this is not some distant future. This global change is about to happen: let me remind you of NVIDIA & ARM merger, and the ongoing microchip deficit.

In my conclusion I want to be very concise:

While the majority of people go on with their lives and use “interfaces”, consuming goods and services like they were used to, “the backend” is rapidly changing.

Everything: from mining (traditional, not cryptocurrency) to production, logistics and commerce will adopt the new technologies over 3–5 years. Decentralized finances will become one of the cornerstones of the new technological era for 30–50 years to come.",7988
"Last year when the pandemic broke out, everyone assumed companies would downsize or close, that there would be an abundance of unemployed developers, and that recruiting would be easy.

Boy, were we wrong.

With an all-time high in funding rounds and new Israeli unicorns, the competition for top talent has never been greater. How much greater? Some estimate developers in Israel get up to eight job offers a day(!), which means you gotta move fast. Really fast.

So, how do companies quickly evaluate someone’s ability to code before they decide to hire them?

They use technical interviews.

Technical interviews have many different formats. Some popular ones are code reviews, algorithmic questions, at-home programming challenges, and even “guestimations” like “how many weddings are held each day in Melbourne?”.

The problem with many of these tests is that interviewers and interviewees turn to Google or popular resources like Gayle Laakmann McDowell’s “CRACKING the CODING INTERVIEW” for both the questions and the answers, turning technical interviews into nothing more than a pre-rehearsed formality.

(Illustration by Itai Raveh)

Where do great developers come from?

Talented developers come from many educational backgrounds. Most have computer science degrees, some are incredibly curious autodidacts, and others went to coding bootcamps.

Coding bootcamps promise to turn students into developers in a very short time, and one thing they teach their students very well is how to build a strong online presence, the kind that leads recruiters to view them as more experienced than they actually are.

Since great developers really can come from anywhere, you don’t want to rule someone out just for being a bootcamp graduate. This leaves it to the technical interview to discern between the developers who look great and the ones who are great.

What makes a truly great developer?

Being a great developer is about so much more than writing code.

We write code to serve a specific need or solve a problem. Developers tend to find solutions that match their skillset and enforce the problem at hand to fit into the framework or language they’re familiar with. However, great developers know that no technology is perfect and that there are always tradeoffs. Any technology or programming language has its advantages and disadvantages, which is why excellent developers must first and foremost understand the need they’re trying to answer and the logic behind different choices. The more you know, the more options you have to deal with a problem.

While developers must be able to break down a problem and understand how systems work to see the bigger picture, they also have to be exceptionally good at communicating these things to other people. A lone coder might be an excellent problem-solver, but they can’t work within a team without good communication skills.

(Illustration by Itai Raveh)

What developers are we looking for?

We work in diverse, multidisciplinary teams that approach problems from a broader perspective. In our eyes, it doesn’t make sense to take talented individuals and box them in job descriptions. Gifted developers can have any engineering background, like real-time, frontend, backend, or data. Being great is not about writing in a particular programming language. If you’ve got a theoretical understanding and strong analytical and problem-solving skills, picking up a new language will not pose a hurdle. Besides, tech stacks change and evolve all the time, so continuous learning is crucial for developers anyway. We’re looking for talented people regardless of their specific technological history. We’ll always find them incredible things to do.

Our technical interviews

Because we use such a different organizational model for the eko dev team, our technical interviews also tend to take a different course.

In one format, we ask developers to walk us through a project they’ve already completed at work, school, or on their own, and before we even get to the code — we want to know what goals they were trying to accomplish and for what purpose.

We ask our candidates why they made technical choices and how they handled problems, not because we’re trying to judge their decisions but rather to understand their way of thought. We also ask follow-up questions like “what would you change if X would be different?” or “how would you approach this project differently if you had to do it over again today?” Since the conversation is about something candidates have already accomplished — they feel comfortable and free to go as deep as they want into a topic they know.

In another type of technical interview, we ask candidates to solve a software problem outside their area of expertise. Then, we discuss it together to explore how they defined the problem, if they understood the need behind it, why they chose the technologies they did, how they planned their architecture, design, and APIs, and finally — we examine their coding skills.

These kinds of open discussions provide a pretty accurate simulation of what it’s like to work on a feature at eko while helping us identify great developers.

What we actually mean by “great developers”

Great developers understand the many layers of abstractions that make their software run and the deeper layers of other software (and hardware!) their code relies on to function. For example, a good frontend developer might be proficient at crafting CSS animations, but a great one will also understand the browser rendering process, how and why does the GPU come into play, and even how it all fits within the context of the OS and device the browser runs in.

The approach a developer takes to the practice of programming also talks a great deal about their proficiency. Other than addressing the task at hand, do they handle the less optimistic paths (errors and failures)? How do they test and using which testing methodology and why? What do they log, and how do they make sure to separate the wheat from the chaff? After all — it’s not all about writing code “that just works”. It’s about looking at the entire picture of programming in an organization: making that code solid and trustable, debuggable, maintainable, and readable to other humans (and themselves, in the future).

(Illustration by Itai Raveh)

How does all of this help us move fast?

Well, since we are, in fact, developers ourselves, we can figure out who’ll make an excellent professional match to our team a lot quicker than “someone from HR”.

Plus, because we’re the ones doing the interviews, candidates get to meet the people they’ll be working with right from the very start. And thanks to our flat organizational structure, we don’t need approvals from five layers of management when we meet someone we like.

…

Want to see for yourself what an eko technical interview looks like? Send us your resume here!",6899
"I often get asked about how we’re different from the other coins. Well the whitepaper should give you a pretty clear idea, but alas, not all questions have answers in this one document. So I’ll try and express the thought in my own words.

We’re in the cryptocurrency space, but Direwolf isn’t just a currency. Direwolf isn’t a get-rich-quick scheme and it isn’t about winners and losers.

First and foremost, Direwolf is about creating an environment that is safe and encouraging for its investors. It’s about making its investors feel protected, like their investments are actually building up to something bigger. The DireWolfPack is a family, It’s about feeling right at home here helping eachother. We are about creating new friends, and enjoying the journey rather just racing towards the finish line. I hold myself to the beliefs that I feel the community and the North want to value, beliefs that shape the values that define Direwolf. Such as honesty, transparency, trust, equality, altruism, democracy, meritocracy and fairness.

That’s how we stand out from the rest. The $Direwolf token incorporates the values that the community should aspire towards. That is why I have emphasized the word token in the name, it literally is a token and symbol that you’re a proud member of this family, moving towards these shared goals. Everyone is family, if you’re holding on to it, no matter the amount. And every single one of you has a voice in how we move forward as a collective.

When the snows fall and the white winds blow, the lone wolf dies but the pack survives.

And that is why the pack is what matters. As we grow and attract others by demonstrating our way of thinking through action, in the true spirit of this token, the value of $Direwolf will adjust to the market demand. Our dream, mission and ambitions should be first, our reward will follow up second. In that order.

Dear wolfpack, let’s stand strong together and keep rising by lifting others. I am so incredibly grateful that you are considering going on this journey together with us and I hope to see you on the other side.

AROOOOOOOOOO!!!!",2120
"Photo by Christin Hume on Unsplash

Hello! It’s nice to meet you.

We’re Tribute, a peer-to-peer mentorship platform for the workplace that enhances team collaboration, learning, and productivity through on-demand knowledge sharing. What that really means is that we recognize an organization’s two most important assets are its people and their knowledge. And not just their skills-based knowledge, but the knowledge they’ve gained from the wisdom of experience.

Using the ancient art of storytelling, combined with modern technology, we inspire employee connections for the moments that matter. We know that sharing wisdom and knowledge from our lived experiences not only informs more meaningful connections, it helps create inclusive and trusted workplaces that employees love. We often say there are some things you simply cannot know until you’ve lived through them, with the battle scars to prove it. We aim to create a world where those battle scars become beacons of hope, inspiration, and collective knowledge that help us navigate our dynamic careers in this new world of work.

So why are we writing this letter? The truth is, we’ve been looking for you for a while now. We’ve been fortunate to secure early enterprise customers like Microsoft, Zillow, and Remitly to name a few. But we need your strategic oversight and technical expertise in order to propel us to the next phase of growth, and to help more companies make the transition to hybrid work, moving away from ineffective spreadsheet driven mentoring programs to digital-first learning cultures.

We aspire to be the world’s first and best peer-to-peer experiential learning platform, and we’ve got what it takes to get there. So far, we’ve pre-sold our product and bootstrapped our way to hundreds of thousands of dollars in annual recurring revenue (ARR) without any in-house engineers. Impressive as this may be, we know we cannot achieve our next level of success without you. Together, we can accelerate our path to $1MM ARR and create a more meaningful future of work for everyone.

We realize that it’s hard to make a decision like this when you don’t know much about us, so here are 8 things that you should know about us:

We’re a seed-stage B2B enterprise SaaS startup looking to disrupt the future of work We’re post-revenue with an initial product-market fit We’re female-founded and believe that inclusive cultures not only attract the best talent, but bring out the best in everyone We’re curious creators passionate about learning, adult education and the power of human connection. We follow the customer discovery process religiously, and believe in progress, not perfection We work remotely, and put family, health, and well-being first Everyone here has a voice Our CTO position will be employee #4

If you’re a technologist and builder at heart who cares deeply about the future of work and wants to help us shape it, Tribute might be the place for you. If you are motivated by purpose, autonomy, personal growth, engineering excellence and don’t mind getting your hands a little dirty, we’re pretty sure you’ll fit in well here.

If you’ve built rich web and mobile apps, deployed APIs and apps to either Azure or AWS, consider yourself a strong cross-functional leader, and have always dreamed of being a founding member of a fast-growing startup, you might just be exactly who we’re looking for.

And finally, if you’ve ever had a mentor who has fundamentally changed the course of your life, inspired you to be more than you could have ever dreamed for yourself, then you might just be exactly who we need.

We know you’re out there, so if you’re reading this now, send us an email at careers@tributementorship.com and let us know you’re ready to help us shape the future of work. And if you’re reading this but don’t feel like you’re our future CTO, share our message with someone you know who comes to mind. We’ll be eternally grateful you sent them our way as well as every enterprise employee who benefits from the future of work we build together.

Sincerely,

Team Tribute",4077
"Class Saathi by TagHive — The Best Educational Learning App for Students of Class 6–10

The world today is a competitive place and our children have immense responsibilities to perform. With exams being a constant source of pressure for both parents, teachers and students, how do we ensure that we cultivate a healthy and independent space for students to learn and enjoy what they do? The answer is simple — download Class Saathi, The best educational learning app for CBSE students of class 6,7,8,9 and 10.

TagHive’s Class Saathi is a comprehensive support based learning platform that encourages students to handle their own learning outcomes outside the classroom while also allowing parents and teachers to participate in this ecosystem and assume the role of the monitor.

Our Class Saathi app, deemed as one of the best apps for NCERT solutions, has endless reserves of mock quizzes, all designed by IITians — the country’s most refined technical minds. Our unique AI system designs and moderates quizzes in such a way that students can transcend levels of difficulty, making them more confident with each stride.

Students can also access video based explanations for STEM concepts over our app.

How to use our best online learning app:

Go to your app store and search for the Class Saathi app. Download and register using any email Id. Enter a world of quizzes, animated teaching guides and success. Our learning solution is currently available only for Android devices.

If you’re looking for a detailed demo for our variants, drop us a line through our infomail. We want to help you leverage the best online learning app, custom designed for NCERT students from classes 6th to 10th.

We, at TagHive, are committed to doing the best for our students and teachers. We would be very happy to address any queries you may have. Feel free to reach us at info@tag-hive.",1877
"The global digital forensics market size is expected to reach USD 4.24 billion by 2026, exhibiting a CAGR of 12.3% during the forecast period, according to a new report by Grand View Research, Inc. Rising internet penetration, advancements in computing technology, and instances of cyberattacks are some of the key growth driving factors. Additionally, increasing demand for sophisticated cybersecurity tools to detect and investigate cyberattacks and financial frauds among large as well as small and medium enterprises is expected to drive the market growth.

Digital forensic tools assist in lost data analysis, reconstruction, and evidence collection from digital devices. Enterprises across the globe focus on adopting these tools to identify and avoid cyberattacks. The enterprises have to follow several regulations and compliances regarding consumer data privacy, such as General Data Protection Regulation (GDPR), California Consumer Privacy Act of 2018 (CCPA), and Personal Information Protection and Electronic Documents Act (PIPEDA), among others. This is expected to encourage the adoption of digital forensic solutions and services.

The foremost application of digital forensics is found in private and criminal investigation. Factors, such as rising internet penetration, usage of electronic devices, and smart devices, among others have led to a surge in cyberattacks across the globe. Rising intensity of cyberattacks is anticipated to further propel the demand for digital forensic tools over the forecast period. Digital forensics tools are increasingly being used to resolve issues related to cyber threat in the enterprises. The tools aid in investigating data stored in digital devices, examine suspect data, and then use it as a digital evidence in the court.

Click the link below:

https://www.grandviewresearch.com/industry-analysis/digital-forensics-market

Further key findings from the report suggest:",1931
"You all should remember the time where having a PC was becoming mandatory. Emails, documents, and even printing something, it was normal for every household to have a PC plugged to the wall. Hardware forums proliferated as the knowledge of building desktop PCs was becoming a requisite, and if you couldn’t afford one, Internet Café (and their variations) would welcome you with open arms and a few pennies.

As of today, these venues are no more. The old Desktop PC, that one big box that sits in your room or office, barely justifies its existence now. It’s not that it has been forgotten, but rather, replaced by other machines, namely smartphones, tablets and laptops.

If you plan to buy a PC to only play games, get a console or get a doctor.

Today, the same questions I asked to my friends to build a PC, that where looking forward to, have now lost meaning.

It’s not about what desktop PC they want to build, but rather, if they even need one at all.

1. Do you require a desktop PC?

Desktop PCs are synonymous of sedentarism, but not in a unhealthy way. It’s about trying to shove the most performant hardware regardless of size and weight. Today, most of the content creation you see on the Internet is done in front of this form factor: from a well-produced video on YouTube to the Boston Dynamics dog software. PC are used for people who create software, content, or research, so if the user qualifies into a “creator”, it may need one.

There may be software (and hardware) that is bound to the PC ecosystem, like development tools or audiovisual hardware. While this doesn’t mean you can’t do the same on an iPad or a Linux machine, a PC with Windows will make your workflow faster as most tools are compatible with Microsoft’s desktop operating system, which is one of the most used on the world — at least, on desktop and laptops.

Outside demanding workflows, there is little to say. For doing office tasks, you don’t need a PC, specially considering you may interact more with web apps than anything else. The undisputed king of tablets, the iPad, can offer very simple (and intuitive) interface to do most of the office work, as long you don’t demand multi-software complex computing from it. iPads are very restrictive about their hardware, if you consider they only have one USB-C port, and like to work under the Apple ecosystem, but they have been steadily opening it up to make the average professional workloads possible. Have caution and be 100% sure the iPad fills your needs.

If you plan to buy a PC to only play games, get a console or get a doctor. Trying to buy a PC to play games, even as secondary function, will break your bank and offer diminishing returns in both experience and performance. Compared to other competing platforms, PC Gaming machines are not relevant anymore, so don’t waste your time.

2. Do you have a large budget?

Budget is always a problem on all technology-related decisions, and for desktop PCs, this has become more than a problem in the recent years thanks to the crypto-mining and chip shortage.

Photo by Christian Wiediger on Unsplash

If your budget is low, you won’t find good machines that come without some cut corners. For example, you may find a machine with a very good CPU, but sacrificing the RAM size, the storage options, or even be left with Intel integrated GPUs. You may find really good deals, but nothing to write home about.

Currently, Chromebooks and iPads are most preferable options to consider if you can’t get a desktop PC with a monitor and some decent peripherals. Both of these are cheap compared to a full-sized desktop PC, and can offer average performance for web-browsing and entertainment. An iPad with a bluetooth keyboard can do wonders on small places, and some models can work decently with a secondary display.

There are some All-In-One in the market that can help you having a PC with Windows (or Linux in some cases) in exchange of poor longevity, zero expansion slots, big-but-average screen, sound, or very few USB ports. Mostly will depend on the specifications and price.

3. Are you tech savvy?

Photo by Alex Kotliarskyi on Unsplash

If the user that will use the PC is not somebody who can even know what is its current monitor native resolution, you may want to NOT build a PC, but rather, buy a pre-built system.

The big problem with building a PC for somebody else is that you’re bound to become the tech support for the machine, perpetually. You will not want become free tech support, even for family or friends. You won’t gift a car to somebody who can’t drive, even if he knows that cars go places, unless you want to be their chauffeur.

Instead, go for a walk inside a retail store if you can with your victim, or look online. On the Internet you may find sites from respectable companies willing to sell pre-built machines, with some slight window for configuration. Some will offer you a flat fee for building the PC for you, like on Redux.

Most retail machines will offer extended warranty and support, in exchange for some additional money. In other words, you will be able to contact people which job is to help diagnose and repair the machine, saving you headaches. While no tech-support is infallible, at least you won’t need to attend calls from your cousin at 3 AM because Fortnite suddenly became slow.

4. Do you need portability?

This part will vary from person-to-person. Some users will need a machine to be with them most of the time, and make any place their office. For these people, having a functioning PC without a wall plug for some hours makes them very productive. Imagine a photographer that needs to send a preview to a client: there is a huge difference between sending a couple of photos during the photoshoot, than seeing them next day just to do the photoshoot all over again.

Photo by Roberto Nickson on Unsplash

When portability is key, then the Macbook Air or Macbook Pro are unmatched, hands down. You may want to get them as long you’re not bound to any software that works exclusively on Windows or Linux. There is a site that may help you check if a niche app doesn’t work on the new M1 chip.

An alternative may be an iPad (or iPad Pro) with a Magic Keyboard or similar setups, especially for people who can’t discern between a bit and a byte, but need something convenient as a tablet and with keyboard compatibility. Just be cautious of the Apple ecosystem that usually gets in the way, like trying to edit a photo from Google Drive.

Otherwise, when portability is just an afterthought, you may want to check small PC cases for some offers as the mini-ITX form is considered an enthusiast market. While big PC cases are mass produced and can be found for cheap prices, moving a full-tower will break someone’s back in the long run.

5. Do you plan to upgrade?

If you plan to constantly inject money into a PC to keep it “top of the line”, then there is no other alternative. The only place that can withhold the test of time is the PC. Additionally, most of the hardware can be re-sold and re-used, so the used hardware market is a very good alternative as long you don’t let your guard down.

This type of flexibility is not without care planning. You don’t buy pieces considering their for longevity, but for what works good now. For example, you may buy 4GB of RAM today, and upgrade to 16GB tomorrow if you feel you’re behind on what is acceptable performance for your applications.

If this is not what you plan, specially considering how fragile can become the occidental economy, then you burn all the cash you can and never look back. In that regard, there is no excuse for choosing a desktop PC. You may well get any other platform that is not “upgradeable”.

In the age of machines with powerful SoC, like the Apple M1, there is no argument to avoid them, in whatever flavor you like: iMac, Macbook, Mac Mini, or even an iPad. Obviously, there are some competitors and some powerful Windows-based laptops, but sometimes the price isn’t right.",8022
"“The advance of technology is based on making it fit in so that you don’t really even notice it, so it’s part of everyday life.” Bill Gates

The kitchen is an ever changing landscape, a place where chefs experiment like scientists in the hopes of evolving cuisine to the next level and creating the next “must have” dish. In a place of such constant change shouldn’t your restaurant technology grow with you?

Food Delivery and Online Ordering

With more people working from home the demand for delivery and online ordering has increased exponentially. Currently about 40% of all restaurants offer some form of easy ordering, delivery or pick up option. Tapping into these services businesses can appeal to new markets and help grow their customer base, and have even see significant gains in revenue, up to 35%. And lets face it, who doesn’t love staying home in their pajamas and still getting to eat your favorite meal.

2. Table Top Technology

A newer trend in dining is using electronic devices like a tablet or kiosk that allow you to see the full menu and place your order from your table all without having to talk to a server. These devices save on labor and according to a Cornell University study they can speed up whole process from 16%-31%. They also make it easier to add on to your order, make any changes you like before your food arrives or get another tasty beverage. Most of these devices even allow you to pay for your meal, making the whole dinning experience that much easier, no more waiting for a sever that’s “in the weeds”.

3. Digital Reservations and Online Booking

The days of spending hours standing in line or waiting on hold are over ( in most places ). Services like Open Table allow you to check the availability or make reservations at your favorite establishment without having to call ahead and spend 30 minutes on hold. You can also add in key info like a surprise birthday or the fact that it is your anniversary. Many of these systems will even remind you of your reservation and send you alerts when your table is ready.

4. Software

Software is one of the biggest advances in modern restaurant technology. Software now helps with everything from food usage and product loss to managing your recipes, taking inventory, doing employees scheduling, payroll and just about anything else you might need.

This technology allows for more streamlined tracking systems and can even reduce inventory time up to 75% and lets face it, nobody likes doing inventory. There is even predictive ordering technology that gives you clear ideas about what you might to need based on your previous data. These performance driven products are changing the way many restaurants operate and are great way to save money in the long run.

5. Phone Apps and QR Codes

A recent study shows that there are about 3.8 Billion people who own smartphones and that number is growing everyday. With the ever increasing number of people owning smartphones about one in three restaurants are already using apps or web base technologies. Offering a myriad of benefits and bonuses to the user like loyalty points, discounts, promotions and deals only found within the app. A good number of these apps allow you to see the menu and place an order right from your phone, they also allow you to use your loyalty points for discounts, upgrades and add ones. Some apps even go into great detail to show you the status of your order throughout the entire process.

If no app is available chances are they have a scannable QR code on your table, this allows you to use your phones camera to scan a code and see the menu and any specials that might be offered. This is another great trend because, who even knows when the last time that paper menu was cleaned.

6. Bluetooth Temperature Sensors

If you have ever worked with food you know that temperature is one of the most crucial factors in the kitchen and no one wants to get customers sick. One of the coolest advances in restaurant technology are the Bluetooth temperature sensors, either hand held or fixed these sensors can tell you and exact temperature in less than four seconds. They can also be set up with alerts and custom temperature limits so you can be sure that everything is exactly where it needs to be. When it comes time to filling out the daily paperwork for HACCP ( Hazardous Analysis Critical Control Point or food safety) you now have the ability to wirelessly record all of that data including the exact temperature at certain times thus taking human error or forgetfulness out of the equation.

This technology helps you take a proactive approach to food safety and customer satisfaction, it is always good to know that the foods being served were properly time and temperature controlled.

Who knows what wonders are to come in the continuing advancements of restaurant technology but I personally am excited to see just how far we can go and what the next major break through might be. Who knows maybe these delivery drones will take the world by storm.",5032
"A latest technology that many companies have duly adopted to create their applications. A cross-platform, open-source framework for the development of cloud-based modern web applications. ASP.NET core is a great combination of different web development models that consists of all of the required services to create robust web applications for all different kinds of businesses.

Through this framework, one can create more effective web apps, mobile app back ends and also IoT apps. To put it in simpler terms, ASP.NET core is a significant redesign of the ASP.NET framework. ASP.NET core comes with a wide range of benefits. Let us take a look at some of the various benefits of ASP.NET Core.

1. Improved Performance

The major benefit of ASP.NET Core framework is its improved and higher performance. With the new enhancements and upgrades, the code actually gets much more optimized that results into improved performance. However, this isn’t the best part of it. The most significant part is that it does not actually require changing the code. The ASP.NET Core compiler eventually optimizes the entire code whenever the code is re-compiled with the ASP.NET Core framework.

2. Support of Cross-platform

ASP.NET Core is cross-platform allowing easy creation of web applications which run on Windows, Linux, and Mac. In simpler terms, the entire backend will use the same C# code.

3. Lesser Code

ASP.NET Core demands less coding, and the developers can easily optimize the code-structure by writing lesser statements. As the coding is less, the amount of hours required are less as well in order to create an application, making ASP.NET Core much more cost-effective.

4. Easy Maintenance

As ASP.NET Core demands less coding, it in turn becomes much more easier to maintain automatically. This essentially means that it not only takes the less amount of code to create a web application but it is very easy to manage and maintain it effectively.

5. Cloud-Based Web Application Development Support

Creating a cloud-based application in the current era is a better option. And ASP.NET Core is the go to solution for the business requirements of the current times. Also, ASP.NET Core can easily help in developing great and robust web applications.

ASP.NET Core is an extremely interesting technology to create robust web applications. Since most of the businesses out there look for various ways to develop applications faster and launch it in the market as soon as possible, ASP.NET Core is considered to be the best ways to do it.

https://techcedence.com/benefits-of-asp-net-core/",2593
"World’s first aircraft with revolutionary, game changing superconducting electric engine was first tested in Novosibirsk. In the first days of February this year in the Chaplygin Siberian Aviation Research institute airfield in Novosibirsk, was been given green light for ground tests of plane-laboratory, made on the base of soviet made Yak-40. This unique aircraft is quipped with hybrid power plant tat includes superconducting electric engine that was exclusively build for this type of aircraft.

To say it is just a progress or something evolutionary means to say nothing. We can see very clear the start of development of absolutely new class of aviation — electric aviation and for now only Russia shows the biggest progress and real, working results in this field. Of course, that doesn’t mean nobody trying! There number of similar programs around the globe.

German giant “Siemens” has not only similar project but even a flying prototype (jet), the problem is, it is ultra light and for one person only. Despite the fact that “Siemens” one person’s jet has surprisingly powerful engines they are not superconducting which means they have no chance to compete with superconducting electric engines.

European aircraft manufacturer Airbus also works on the development of electric airliners. Siemens and Rolls-Royce together works on project E-Fan X. It supposed to be hybrid jet, as well as project SuperOks. First flight scheduled for 2021.

However, all of these projects tend to work with regular electric engines. Besides that european program “Advanced Superconducting Motor Experimental Demonstrator (ASuMED), which is coordinated by a german Oswald elektromotoren announced development of demonstration version of superconducting electric engine, but it’s still mystery in which stage of the development process they are and is there anything working made.

It is obvious, Russia it’s not the sole player in this electric aircraft game, however in this field only Russia moved forward more then anyone else. Russians not only have working superconducting electric engine, but already uses it for ground tests on the actual plane. Russia become first country in the world that tested that type flying machine in working conditions. In fact, russians did impossible and unified two projects E-Fan X and ASuMED into one, which allows to stay ahead of the competition.

Plane-laboratory SuperOks is equipped with hybrid power plant and with classic generator, which uses oil to produce electricity. Similar project has Airbus, however they using traditional electric engine. Therefore it doesn’t matter how innovative will be “Siemens” made engine for project E-Fan X it will still be way weaker superconducting electric engine from SuperOks. Technology of superconductivity significantly increases engines specific power (power-to-weight ratio) and gives incredibly high ECE (energy conversion efficiency), up to 99%.

High-temperature superconductors are used in making these revolutionary engines. Of course that doesn’t mean that superconductivity will appear itself in room temperature, no, that means that superconductivity will become possible once temperature will be far more then zero, for example, at the temperature of liquid hydrogen or helium, and in the long term even nitrogen.

High-temperature superconductors is not something that exists only in someones fantasy world, no, russian company SuperOks long time in the business and not only works on their development, but also manufactures materials and other production retaining substances of high-temperature superconductivity. Company exports production to 20 countries world wide. Possession (having the most) of the most important component needed to build those engines guarantees Russia strong and reliable growth in this area.

One of most complicated parts in making electric engine is to provide cooling for the stator and rotor so that the winding has a temperature necessary for the appearance of the effect of superconductivity. However, has been founded, that:

“The complexity of cooling the high-temperature superconductivity stator is that the cryostat materials will be in the zone of action of high alternating magnetic fields, which cause intense heating of metallic materials, increasing the thermal load on the cryogenic circuit and reducing the efficiency of the entire system” — told Sergei Smoleinkov, CEO of the SuperOks, company which works on the development of superconducting electric engine and high-temperature superconductors. SuperOks developed a cryostat technology for the stator of an electric engine, which does not contain metal parts in the active part of the electric motor. According to the CEO, rotor cooling requires the development of a rotating refrigerant inlet with a vacuum seal, which considerably complicates construction of the rotor. Besides, this allows the rotor shaft to be hollowed out and to control the variable pitch propeller through it — an important addition for aviation.

Despite the fact that NASA in 2007 admitted that using of the high-temperature superconductors in aviation is highly perspective, it still completely unknown and unpaved way, that can lead to nowhere (dead end). Even if it so, Russia still will be among those very few countries in the world that has it’s very own high-tech manufacturing industry able to produce the most complex high-tech production and company SuperOks, that works on development and production those kind of products, serves as the best example.

Translation and audio: Danielius Karkozas",5578
"How Fervo Energy Is Harnessing the Power of Geothermal Technology to Power the Planet Innovation and Technology Jun 17·4 min read

Environmentalists usually emphasize the importance of using renewable sources of energy. In this context, we’ll focus on geothermal energy, which is stored underneath the surface of the earth.

The main advantage of renewable energy sources is that there is no need to worry about greenhouse gases. In short, the use of geothermal energy helps to reduce the carbon footprint; thus, there is no need to worry about issues such as global warming. The oil & gas sector engineers have the expertise to harvest geothermal energy.

Fervo Energy is based in Houston, and Tim Latimer launched the firm. Initially, Latimer would work as a completion engineer. As an environmentalist and engineer, he saw it fit to look into how to harness geothermal energy through Fervo. The firm has received funding from Bill Gates and Jeff Skoll, a former executive at eBay. Fervo has received more than $28 million, and they are planning to ensure the firm is producing hundreds of megawatts in the next few years.

Tim Latimer grew up in Waco, Texas. As a kid, he witnessed how power generation would impact the environment close to a power plant powered by coal. Latimer also hails from an oil family. After completing his studies, he went ahead to secure a job in the oil & gas sector. Later on, he realized the world would start using renewable energy, and many people in the oil & gas sector would be affected adversely.

To mitigate the issue, Tim Latimer established Fervo so that people in his locality would secure employment opportunities and play a role in the clean energy future. If Latimer were not a part of the oil & gas sector, he wouldn’t have gotten the idea to establish a firm that deals in renewable energy.

Through Fervo, Latimer has managed to support the locals while also hiring contractors who have been affected by the oil price crash. The contractors are currently working on the Fervo oil rigs.

The Biden administration has discussed offering employment opportunities in the hydrocarbon sector because of the transition to green energy. Fervor is among the firms offering such employment opportunities.

Currently, geothermal power is not controlled by geography, considering there are numerous resources that can be used to tap the energy. The jobs in the hydrocarbon sector are also well-paying, especially in areas that usually depend on geological services work.

A considerable part of the world’s population lives close to geothermal sources. More than 25 nations have already installed geothermal power plants, and more countries are relying on geothermal energy to foster growth.

In different parts of Africa and the Western U.S., there is a long history of geothermal power production since there are steam jets and geysers that usually occur naturally, and they serve as good sources of renewable energy.

The technology by Fervo will help to unlock new sources of geothermal energy. The geothermal systems will use horizontal drilling, advanced computational modeling, and distributed fiber optic sensing. Each of these forms of technology will ensure Fervo can deliver affordable geothermal energy.

Fervo was initially developed using a grant issued by the TomKat Center. Afterward, the DEO’s Geothermal Technology Office went ahead to issue the firm additional funds. By combining old and new forms of technology, Fervo has managed to open large geographies while also developing new projects.

Other firms also want to tap geothermal power, and Fevor is one of them. The geothermal projects are currently in demand, and startups are gaining significantly from the huge market. The cost of developing a geothermal power plant won’t be a major issue as compared to before. According to Latimer, there are more power purchase agreements as compared to before.

For each project, the cost will always be one of the main factors to consider. The contracts being signed showcase the cost of one megawatt will range from $65 to $75. As for solar energy, the range is $35 to $55 for each megawatt.

According to Latimer, geothermal energy is preferable since it guarantees predictability and stability. If a business or homestead is looking for a power supply that cannot be interrupted, geothermal power is the best option.

Final Thoughts

Geothermal energy is currently cost-effective, thanks to advanced forms of technology. Also, there are no geographical restrictions currently as compared to before. Thanks to Fervo, the locals have access to employment opportunities amid the issues that are currently affecting the oil & gas sector.",4691
"2020 was a difficult year and was also a turning point year in accelerating world change towards accelerating new technologies. Where many things have changed as has been said in these articles:

So in 2021–2022 is the year of adjustment of human life to the accelerated process of world change with the acceleration of technology. One of the things that are felt is the changing patterns of technology use in human life, one of which is financial technology, investment, shopping, payments, and conducting online meetings.

This may still be a bit difficult and uncomfortable in the middle of 2020, especially when things that are not considered disrespectful are normal and even become the main requirement for everyone. This makes people increasingly accelerate the race to keep up with the acceleration and adapt to changing world with existing technology.

One of the issues for some countries and even people today is how to create a technology that can help everyone and can be used comprehensively in all sectors of life and energy that make it work. If we look at the existing technology, several existing technologies are capable of doing this.

One of the things we hear most often is the word “Green Energy” which is a new and renewable energy that has extraordinary and unlimited energy. And if we look at this, several ways can be used with existing technology, one of which is by using or utilizing solar cells, water treatment, plant ecology, gas pipes, and so on.

But often it is plagued by various problems because there is no automatic supervision and control that can do this. So in the acceleration of technology that is happening at this time, the “Techno Home” must be created by utilizing the “Green Energy” that exists today.

Green Energy

“Green Energy” is one of the new ecosystems in this modern world order where all energy is created from all things that can be renewed again and will not be lost, even into infinity.

In today’s world, an energy that leads to infinite energy is indeed being developed. one of the ongoing uses is biofuel using CPO / palm oil which is widely grown in developing countries and some existing equipment as green energy, namely the “solar cell” which is used for solar power generation and “kinetic energy” which is used for wind and water power plants

But this still cannot cover the main role of coal and fossil oil which are very popular and very easy to use because they have been used for centuries. On the other hand, coal and oil commodities have an important role in stabilizing the existing economy. This is very difficult to get rid of or make it non-existent.

the role of “Green Energy” today is how to embrace all the energy that has been available to date and save that energy with materials or things that can be converted into electric or kinetic engines. Where the conversion we often call Hybrid combines 2 ways of entering energy (Green Energy and fossil energy). into a machine or vehicle so that it can move and run according to the function and purpose of the tool.

If we look at a growing sector as it is today, housing should be the first pilot project for green energy. Why? Because from the housing you can feel how the function and role of this green energy for daily needs. And housing is a land of “green energy” that is the most extensive and can coexist with humans.

This is evidenced by the small scale where the upper-class people especially use solar cells for hot water and the use of electricity for daily use. With proof on a small scale, it should be able to be made also for a large scale. Where is housing with the concept of Green energy?

But the next problem is the maintenance and maintenance of these tools, if you look at the existing problems, one of them is human negligence in the use of electronic goods that are closely related to green energy. but this can be minimized periodically with the existence of “Techno Home” which can guide, remind and carry out maintenance and periodically upgrade several functions of green energy equipment with the help of artificial intelligence.

Techno Home

Techno Home is a term used by some people to describe or describe a future house (future house) that has many smart electronic circuits and can help humans carry out all activities.

Usually, almost all rooms in Techno Home are based on a cloud system because the life in it is combined with smart features that can be connected to various devices based on artificial intelligence / smart electronics using the IoT system.

This is well proven by how all the care in the house can be carried out automatically and if it is controlled, it can use the smartphone in the owner’s hand. This also becomes the basis for how a Techno Home can develop and collaborate on green energy.

This is indeed a bit hard to imagine, let’s look at it as an example. For example, you can see from inside a normal house that there is a stationary bicycle for sports only. Meanwhile, in Techno Home, static bicycles have various functions which should be an initial detection of the body’s condition, electricity generators, physical data and sports activities, and so on. But if planned with green energy, then the stationary bike can turn into a small generator to produce green energy.

Not to mention talking about the flow of air which is seen as a means of refreshing / in the house for example a small waterfall in a fish pond can be a small power plant that helps create green energy and a place for air circulation for plants and the ecosystem as a whole.

Can we imagine if the statistical bicycle is only a manual static bicycle, will it be given that it is big enough? If the small waterfall in the pool is only for air circulation, does it have a big impact?

So nowadays this IoT system has a role that is very much needed by humans. And if there is no small system, this will be detrimental because everything has to be done manually by humans. So that in the calculation of current investment and technological advances, it is very disadvantageous which will take time, effort, space, and costs without the energy produced.

IoT System

With the IoT system, it is very helpful to collect information, carry out maintenance, notify if there is damage, and save power like a personal assistant. Where the IoT system is designed to help all human activities make it easier and can do all work from a considerable distance.

Techno Home with a little more complex mechanical system will produce purity of green energy which is very sustainable and has the potential to create unlimited energy. Where unlimited energy will come out of human behavior that is always moving, which is captured and converted into energy, especially electrical energy.

Advantage

If we talk about the advantages of using green energy, then we will talk about the energy that will not run out and the energy that is the main force in carrying out activities that 80% already use or depend on electrical energy. even with green energy and techno home, it will make electricity cheaper and have a polluting impact on the environment.

this affects not only one sector, but all life will change. Even if coupled with the game psychology program in IoT, it will bring impacts and a new ecosystem that is good for the environment and allows it to become a new, more modern work of life.

With Techno Home, all human routines will be read and assisted by technology to find statistical movements that will have an impact on the development of the energy source itself. And humans will help the technology to realize the intended device so that energy can be obtained, for example, solar cells, wind chargers, kinetic electricity, and the like.

Weakness

If there is an advantage, there must also be a drawback. where the Green energy and Techno Home sectors are still experiencing obstacles in terms of procurement of goods/investment. From the calculation of the procurement of goods carried out independently, it is found that a simple house costs money that can only be purchased by upper-class citizens. This is triggered by the high price of solar panels, quite expensive kinetic assembly prices, and many more

And if you look at the prices that have been stated, it is very difficult for green energy and techno home to materialize. so it needs the role and support of all parties for extraordinary development in the field of materials and the field of application of existing systems.

Solution

From the things described, we can see the tremendous potential of this project, and indeed the obstacles that exist to date are the investment costs that existed at the beginning of the construction and initial installation. This resulted in the project failing many times and no one even glanced at this solution. Only a few people use this stuff for very small purposes. Even when there is someone who makes this solar garden, it doesn’t respond well enough

So on this occasion, we are eager to make green energy and techno home based on clusters, where all 85% of the houses use green energy materials based on kinetic and solar cells. This indeed requires a lot of support from several companies as well as investors who are able and willing to try this basic thing.

Because after being proven by theory and calculation, it was initially a very large investment, but with the capabilities and things that Techno Home can do, everything will pay off and become even more profitable. This is because 65% of the electricity used comes from routine human work and all the electricity income earned will reduce the electricity load used.

It even occurred to us that we would create 2 Techno Home start-up rooms based on the Custer itself and booking a fitness lounge connected to several ID-based IoTs could help residents to enthusiastically generate extraordinary green energy for the region and themselves.

Financing

If we think about where will the funding for this project come from and if it is used as a cluster, who will buy the cluster? In our opinion, to finance this project, we will collaborate with several large vendors and developers who care about green energy for a better world. One of the vendors that are needed is a developer, network company, technology equipment company, health company, and many more.

And if based on our calculations, making Green Energy and Tecno home requires a lot of tools and a lot of investment costs, including:

Solar Panel

with a land area of 1 hectare, it is possible to make as many as 60- 65 houses of type 40 m². With the need for 12 panels to get a capacity of 3 kWp. The area of each panel is 1.7 square meters, so you need a roof of 12x1.7 = 20.4 square meters that is not blocked by shadows of trees or buildings throughout the day from sunrise to sunset. So that it requires 1 hectare of land to produce approximately 4000 watts per day.

The cost that must be incurred is 780 panels (an estimate of 65 houses and one house requires 12 panels) and the price of one panel is estimated at 9 million — 13 million rupiah. So the initial investment in the house is about 11 billion outside the home and excluding other expenses.

And if from PLN the price is Rp.1,467 (compass data, 2018), then the value of 11 billion can be BEP by itself in the range of 6 years with an estimate outside the use of other electricity. But with the age of a solar cell that is over 10 years old, the vendor’s investment will indirectly be profitable.

Wind Catcher

Given the area of land and buildings above, the next step is to make a wind catcher that functions as modified kinetic propulsion and will capture very small wind movements so that it can move to catch the wind that is outside the house or that will enter the house. With the estimated type taken 100A x 24V = 2400W / hour

Costs incurred for the price range in 5 million rupiahs, with an estimate of one house requiring a capacity of 3 pieces, so the price of 1 house is estimated at 15 million rupiahs, so the initial investment that must be spent is around 1 billion apart from other costs.

And if from PLN the price is IDR 1,467 (compass data, 2018), then the value of 1 billion can be BEP by itself in the range of 1 year with an estimate outside the use of other electricity. But with the age of the windcatcher above 5–6 years, the vendor’s investment will indirectly benefit.

GYM Kinetic

When we talk about green energy, then we don’t forget that we will talk about health which is very useful and has other benefits. Where actually from sports equipment used by humans, it can produce kinetic energy that produces the electrical energy that can be reused by humans.

But often people do not realize and don’t like to do this, but with the addition of a little game/loyalty, psychology can make people live healthily and generate electrical energy from what they do. However, for this program, it is necessary to consider several experts and to consider the conditions of each person in using supporting tools.

After we discuss the main technology of Green energy, we must also pay attention to the Techno home which is based on the IoT System which we often call a virtual assistant. Where the function of this virtual assistant has an important role to keep every process carried out by all major technologies can be properly recorded, perform simple maintenance and notify engineers or interested people if a problem occurs in the main technological process.

With the IoT system, it will indirectly change people’s lifestyles to switch to a more secure security system based on virtual assistants. Where all smart system-based devices will be connected to a smartphone that can be known about the condition of the device and controlled remotely.

Even for the cost, for now, it is very relative depending on the usability and how functional the virtual assistant is. But the development of this virtual assistant is very necessary and important for its continuity so that the financing of this will continue to grow and will adjust everything.

Conclusion

Of all the things that we have discussed above, it is a small part of the business plan of a series of business plans that should make a country have extraordinary unlimited power based on green energy. And if this is applied to a larger scope and the more multinational companies that join even from abroad, it will make this program more concise with very excessive benefits.

Even if it is seen from the amount of investment it looks very large, the existence of cooperation between several companies and mutual collaboration between one another will make things even easier. This can also be realized with a more thorough and developed repair detail,

The calculation is still very raw because it is very far from land prices, location, and other factors that greatly affect the calculation of a place. But indeed if this proposal is going to be implemented, we can start with a discussion about it in depth. Of course, in the calculation, there must be data on the value of utilization and the value of the assembly and maintenance of the existing tools. To get maximum results and implementation and have a higher value than the general function.

Hope

We hope that we can help to realize it in a company/institution under one of the vendors so that it can continue to be developed even more and can be run as a whole. To produce maximum results for the use of the wider community.

We hope that we can work together to do this big thing because, with cheap and reliable electricity, indirectly more and more data on the need for electricity usage will be directly proportional to energy income from green energy and techno home.

Papers as a Business Program Proposal in the PLN ICE (Innovation & Competition in Electric) By Samuel Liputra

CC from https://newsevengenerationsiklan1.blogspot.com/2021/06/idea-4u-development-of-green-energy.html",15915
"TL;DR

A new way to control the motion of bubbles might one day help separate useful metals from useless dirt using much less energy and water than is currently needed.

With the addition of sensors and enhanced communication tools, providing lightweight, portable power has become even more challenging. New research demonstrated a new approach to turning thermal energy into electricity that could provide compact and efficient power.

Researchers have discovered a massive enzyme complex from a methanogenic archaeon that directly transfers electrons from the electron bifurcation reaction to CO2 reduction and fixation. Their detailed insights into these efficient energy-transforming processes may open new possibilities for sustainable biotechnological development.

Using a novel modelling approach, new research reveals the location and intensity of key threats to biodiversity on land and identifies priority areas across the world to help inform conservation decision making at national and local levels.

Researchers create thin film polymer membranes capable of separating fluoride from chloride and other ions. Targeted ion selectivity by the filtering membranes could have important implications for water purification, environmental remediation and industrial production.

The speed at which deadwood decomposes in forests depends on the climate as well as on fungi and insects. An international research team has now determined the annual contribution made by deadwood to the global carbon cycle and quantified the importance of insects in the decomposition of wood.

For many, an increase in living standards would require an increase in energy provision. At the same time, meeting current climate goals under the Paris Agreement would benefit from lower energy use. Researchers have assessed how much energy is needed to provide the global poor with a decent life and have found that this can be reconciled with efforts to meet climate targets.

A new study suggests corals may be able to cope with climate change in the coming decades better than previously thought — but will still struggle with ever-faster rates of climate change.

Climate change will significantly alter future patterns of flooding, according to a new study. Although future increases in moderate storms won’t necessarily lead to more runoff in many regions, extreme storms will generate more devastating and frequent flooding.

Judiciously decomposing organic matter from 700 degrees Fahrenheit to 1,200 degrees F, without oxygen — a process known as pyrolysis — and retaining nutrients from dairy lagoons can transform manure into a manageable, ecologically friendly biochar fertilizer, according to new research.

New research finds that Indigenous Peoples and local communities provide the best long-term outcomes for conservation. The research team studied the outcomes of 169 conservation projects around the world — primarily across Africa, Asia and Latin America.

And more!

Green Technology Market

Green technology is an applicable combination of advanced tools and solutions to conserve natural resources and environment, minimize or mitigate negative impacts from human activities on the environment, and ensure sustainability development. Green technology is also referred to as clean technology or environmental technology which includes technologies, such as IoT, AI, analytics, blockchain, digital twin, security, and cloud, which collect, integrate, and analyze data from various real-time data sources, such as sensors, cameras, and Global Positioning System (GPS).

Green technology, also known as sustainable technology, protects the environment by using various forms of sustainable energy. Some of the best examples of green technologies include solar panels, LED lighting, wind energy, electric vehicles, vertical farming, and composting.

The global Green Technology and Sustainability market size to grow from USD 11.2 billion in 2020 to USD 36.6 billion by 2025, at a Compound Annual Growth Rate (CAGR) of 26.6% during the forecast period. The growing consumer and industrial interest for the use of clean energy resources to conserve environment and increasing use of Radio Frequency Identification sensors across industries are driving the adoption of green technology and sustainability solutions and services in the market.

The blockchain segment is estimated to grow at the highest CAGR: Energy-intensive cryptocurrency mining has caused a spike in carbon emission, and hence blockchain is capable of driving innovation in the field of green technology.

Latest News

Latest Research

by Qiang Guo, Yuxuan Zhang, Azin Padash, Kenan Xi, Thomas M. Kovar, Christopher M. Boyce in Proceedings of the National Academy of Sciences

A new way to control the motion of bubbles from researchers at Columbia Engineering might one day help separate useful metals from useless dirt using much less energy and water than is currently needed.

When mining for metals such as the copper used in most electronics and the lithium used in many batteries, only a small fraction of the material that is mined is useful metal, with the vast majority just useless dirt-like particles.

“We have to separate the useful metals from the useless particles, and we do this by blowing air bubbles up through them,” said Chris Boyce, assistant professor of chemical engineering at Columbia Engineering. However, “this process utilizes a large amount of energy and water, causing climate change and water shortages, thus creating problems we are trying to prevent. We have this issue in part because we currently cannot control the motion of these bubbles.”

Particle contact force normalized by particle weight during two vibration cycles obtained from discrete particle simulations. The simulated particles have properties ρp = 2500 kg/m3 and dp = 238 μm. The vibration frequency and amplitude are 5 Hz and 8 mm, respectively.

Now Boyce and his colleagues reveal that if they vibrate these particles while blowing air up through them, the normally chaotic motion of these bubbles becomes orderly and controllable. The vibrations cause the particles to quickly shift between solid-like to fluid-like behavior, which in turn helps structure the bubbles into regularly spaced triangular arrays.

“I think the simple addition of vibration to go from chaos to order is beautiful,” Boyce said.

Snapshots of local solids volume fraction during two gas flow oscillation cycles predicted by continuum gas-solid flow modeling using different constitutive models: (A) Schaeffer model (6), (B) Srivastava and Sundaresan model (7) and © the proposed model. The simulated solids phase has properties ρp = 2500 kg/m3 and dp = 238 μm.

Having a way to control the behavior of these bubbles can help scale up and optimize separation techniques. “We expect that the ability to create structure in flows can reduce energy and water use in mining as well as improve the efficiency of many clean energy processes,” Boyce said.

The researchers now aim to apply this structured bubbling to sustainable mining separation techniques.",7106
"Technology is a lever. It magnifies work. And the lever not only grows increasingly long, but the rate at which it grows is itself increasing. - Paul Graham

Buckle up. We are going to be looking at a cliched, overly discussed topic: AI vs Jobs.

Source: Internet

We will look at this in three parts — Artificial Narrow Intelligence (ANI), Artificial General Intelligence (AGI), and Artificial Super Intelligence (ASI), followed by some overarching arguments.

1. Artificial Narrow Intelligence (ANI)

ANI is AI applied to specific tasks such as playing chess and driving a car. Their scope is limited — an ML model trained to play chess can only be good at playing chess; anything outside of chess is out of its scope.

Gary Kasparov facing off against Deep Blue in 1997 | Source: The World

ANI can perform repetitive, resource-intensive tasks far better than humans can. As a result, they threaten to take over humans in jobs like data entry, customer support, driving taxis, and assembly line to name a few. But this is not a cause for concern, for, this is not the first time technology is replacing humans.

Consider lamplighters — they were men specifically employed to light and maintain candles or, later, gas street lights.

A lamplighter doing his job | Source: Wikipedia

Yep, this was a job before electrical systems were invented. When electrical systems and electrical lamps were installed, lamplighters went out of jobs.

When cars became popular, many blacksmiths and farriers went out of jobs because the dawn of cars meant the decline of horse-drawn vehicles. So blacksmiths who were producing horseshoes and farriers who were caring for horses’ hooves lost their jobs.

A horse tram in Germany | Source: Wikipedia

The agriculture industry saw huge job losses due to the proliferation of agricultural equipment and machinery like tractors.

The decline of human labor in Agriculture | Source: Our World In Data

These examples demonstrate how technology has always stolen jobs. In fact, it only makes sense that it steals our jobs because technology is meant to solve problems for us.

To understand this better, one needs to look no further than elevators. Elevators today are fast, powered by hydraulics & electricity, and require no human intervention.

Modern Elevators | Source

But elevators did not always look this fancy. The elevators from the pre-industrial era were powered by men or animals and they looked like this:

Pre-industrial elevators powered by animals | Source: Google Sites

One could claim that hydraulics caused job losses since it replaced human labor. But this is true for any piece of technology. We do things manually until a piece of technology comes along and takes us out of the equation.

So, does technology steal jobs? Yes, but, only in the parochial sense. We do get removed from the equation but by doing so, it frees us up to think about and work on more complex problems. Hence, jobs don’t get stolen, they evolve. Blacksmiths of the past are assembly-line workers of a car manufacturer today; lamplighters of the past are electricians today and so on.

What is ANI, really? In a broad sense, it is a piece of technology and technology has always stolen jobs. Previously, it was tractors, cars, and electrical grids; this time, it is ANI.

2. Artificial General Intelligence (AGI)

AGI is an agent that has the ability to replicate the intellectual capability of humans. Unlike ANI, an AGI can learn to do anything. What exactly constitutes “human intelligence”, you ask? Here is how Wikipedia defines it:

There is wide agreement among artificial intelligence researchers that intelligence is required to do the following: - reason, use strategy, solve puzzles, and make judgments under uncertainty; - represent knowledge, including commonsense knowledge; - plan; - learn; - communicate in natural language; - and integrate all these skills towards common goals.

Unquestionably, if we have such an AGI, we will face large-scale job loss. But, this is a good thing.

Let me explain.

If we have robots that can do most of our work, then humans can outsource earning money to them. We would no longer have to work to get paid; robots would do the work on our behalf and would earn money for us. This transformation is coming. As Sam Altman, CEO of OpenAI, put it:

This technological revolution is unstoppable. And a recursive loop of innovation, as these smart machines themselves help us make smarter machines, will accelerate the revolution’s pace. Three crucial consequences follow - This revolution will create phenomenal wealth. The price of many kinds of labor (which drives the costs of goods and services) will fall toward zero once sufficiently powerful AI “joins the workforce.” - The world will change so rapidly and drastically that an equally drastic change in policy will be needed to distribute this wealth and enable more people to pursue the life they want. - If we get both of these right, we can improve the standard of living for people more than we ever have before.

Just because it is unstoppable, does not mean we will get it right. We should think really hard about what we are going to do with the wealth generated by AGI: is it going to be concentrated at the hands of the CEOs of private sector companies? Or is it going to be distributed? If so, how will it be distributed? Even if we could build an ethical, benign, and cooperative AGI, we would still collapse if we do not have the right policies in place. Once again, here is Sam:

We need to design a system that embraces this technological future and taxes the assets that will make up most of the value in that world–companies and land–in order to fairly distribute some of the coming wealth. Doing so can make the society of the future much less divisive and enable everyone to participate in its gains.

One commonly discussed method to distribute wealth is Universal Basic Income:

Universal basic income (UBI) is a sociopolitical financial transfer concept in which all citizens of a given population regularly receive a legally stipulated and equal financial grant paid by the government without a means test. A basic income can be implemented nationally, regionally, or locally.

AI generates wealth and that gets distributed to people. Sam in his blog proposes something similar, yet radically different. He calls it Capitalism for Everyone. However, I am not going to dive into the details in this post; I will leave it to you as homework.

Bottom line: if we have an AGI, then we wouldn’t have to worry about job losses as we would outsource wealth creation to this AGI.

3. Artificial Super Intelligence (ASI)

An AGI agent is incapable of things like empathy, perception, manipulation, creativity, and social skills. In order to develop those skills, it requires the following (again, from Wikipedia):

consciousness : To have subjective experience and thought.

: To have subjective experience and thought. self-awareness : To be aware of oneself as a separate individual, especially to be aware of one’s own thoughts.

: To be aware of oneself as a separate individual, especially to be aware of one’s own thoughts. sentience : The ability to “feel” perceptions or emotions subjectively.

: The ability to “feel” perceptions or emotions subjectively. sapience: The capacity for wisdom.

An ASI is something that is capable of all the above. To engineer an ASI, we need to codify human consciousness. This is incredibly hard. Although I am a techno-optimist, I believe there is still a long time for us to hit the singularity. Until then, humans will remain indispensable.

Once we do hit it, it would be really hard to keep a sentient being under control. In fact, if it was sentient, would it even be ethical to keep it “under control”? Why are humans entitled to free will but robots that are self-aware, and have feelings denied free will? It would be discriminatory to try to control consciousness just because it is made up of bits instead of atoms. But, that is a topic for another day.

There are two probable scenarios once we create an ASI: we go on an all-out war with each other or we collaborate and solve problems together. Let us look at each of them.

3.1 The Robocalypse

Robots on a rampage in Terminator 2 | Source: Tristar Pictures

A war against robots would likely result in two outcomes: one where humans become extinct and one where they don’t. Either way, will we care much about job losses when our survival itself is doubtful?

If we become extinct, then we would have permanently solved the problem of job takeovers.

If we manage to survive, then it either means we managed to delete/corrupt the source code of the robot army (so that they malfunction and stop), or we were able to strike a peace deal with them. The first one means that we can stop worrying about ASI taking over jobs, but the second one is a bit more complicated. Striking a peace deal would be similar to scenario 3.2 which I outline below.

3.2 Co-Exist

If we are to co-exist, it would require a degree of cooperation between the two species. An attempt to cooperate and co-exist with another conscious, sentient species would make our world infinitely more complicated.

Today, different nations co-exist. Look at how much complexity it has brought into existence — it has led to the creation of many new jobs and domains such as international trade, foreign laws, international regulations & policies, data residency and privacy laws, international bodies such as the UN, peace agreements, and so on.

With the emergence of ASI, we would have to coordinate and collaborate with an entirely new species. Each nation would have its own agreements and treaties with the ASIs. They would have their own trade deals and shared economies. Each nation would have agreements with other nations about what kind of deals one can strike with the ASIs. Universities would offer “Human-AI geopolitics” courses where professors would be teaching students while doing PhDs in this field. In the anticipation of a doomsday scenario wherein we get into a species-war (a World War V? Or an Interplanetary War II?), we would develop super-soldiers with bulletproof skin, superhuman strength, and other extraordinary abilities. Perhaps such abilities won’t be restricted to soldiers; even your average Joe would be retrofitted with biotech implants and augmentations. This would not be possible without large-scale biotech manufacturing and R&D that would employ millions of people directly and indirectly. We would have blue teams who will try to shield humans from ASI hackers and red teams who will try to attack ASI systems. The possibilities are immense.

Do you think someone from the 1500s would be able to predict the kind of jobs we have today? They would have no idea what a car or a computer is, and consequently, would make incorrect predictions (through no fault of their own) about the jobs that would exist in the 2000s.

Similarly, trying to predict the job market in a world where ASI exists is futile because the such a future would look unfathomably different than today. So, let us rule out the ASI scenario from the AI vs Jobs arguments.

4. Closing Arguments

If you think that we will run out of jobs because of AI, you are underestimating the number of problems we have. As we saw in “There is no destination, there is only the journey”, problems are infinite. AI will help us unearth and solve problems we could never think about solving before. ANI will just be a new tool in our arsenal, AGI will just be additional human resource capital for us to work with and ASI will lead to the creation of more problems and professions.

If you ask the people who are at the risk of getting their jobs replaced by robots if they would want their kids to be doing the same kind of job, they would say no. Marc Andressen, a famous VC, talks about this in a podcast:

Ask a parent who works at a blue collar profession. Ask a parent who works on an assembly line or works in the frontline of manual labor, the harder work. “Would you like your kid to have the same job you do, or would you like them to have a better and different job, a different experience? Do you want your kid to be doing that, or would you rather your kid be a software developer or an artist or a job in which you’re in a very comfortable physical environment. You’re not running the risk of workplace accidents and so forth; you are paid higher, and are able to provide better for your kid’s family and ultimately, for your grandchildren.” And virtually all parents will say that they are in favor of that. This is progress. This is how it happens.

AI will help us solve bigger problems than we do today while making our jobs far safer and more challenging (in a good way).

We should appreciate having one more tool with which we can solve problems than one less. For, we will always have problems to work on.",12941
"Providing businesses with a reliable, cost-efficient and powerful decentralized technical platform is a focus of what GetBlock is developing and advocating. This approach is perfectly aligned with that of the Fuse (FUSE) team tasked with creating ready-made blockchain-based solutions for enterprises, communities and enthusiasts.

What is Fuse (FUSE): ideology, concept, consensus

Fuse (FUSE) blockchain was introduced amidst the 2019 Crypto Winter. The project was backed by veteran VCs including Collider Ventures, Blockedge Capital, Faculty Capital and so on. Since its inception, Fuse Network has been laser-focused on empowering businesses and local communities with blockchain-based economical systems for everyday use.

Image via Fuse

Fuse (FUSE) mechanisms are accessible even from mobile phones while its transfers are charged with trivial $0.01 fixed fees. That’s why its core use-cases include daily retail payments as well as micropayments, the ‘Holy Grail’ of all modern blockchains.

Fuse (FUSE) transactions are secured by Delegated Proof-of-Stake (DPoS) consensus (like in Bitshares, EOS, Cardano and so on). This means that its network integrity is guaranteed by validators and delegators. The first group of network operators contribute to Fuse (FUSE) by running nodes (see below) while the second one delegates their FUSE stakes to preferred validators.

Its ‘plug-and-play’ design is another ‘killing feature’ of Fuse Network (FUSE). Its team stresses that even non-CS entrepreneurs and community activists will be able to leverage the power of blockchain with Fuse (FUSE) integration:

CIT: Fuse lowers the bar of entry for entrepreneurs to build powerful, effective and sustainable products. So they spend less time and money on IT, and can focus on reaching and growing their clients and customers.

Also, Fuse Network is interoperable with Ethereum Virtual Machine which allows it to interact seamlessly with two most popular smart contract hosting platforms ever: Ethereum and Binance Smart Chain.

Three layers, plenty of use-cases

In terms of infrastructure, the Fuse Network (FUSE) ecosystem is organized in three inter-dependent layers. The first (‘retail’) layer is responsible for end-user functions of the Fuse Network (FUSE) blockchain. Technically, it is represented by Fuse Wallet, an on-chain crypto storage and transfer instrument. The wallet operates on Ethereum (ETH) mainnet and can be accessed via Android and iOS-based applications.

Image via Fuse

The second layer (‘operational’) serves as a smart contract framework for network operators. This environment is tailored for the needs of tech-savvy users and is dubbed ‘Fuse Studio’. It operates on both Ethereum (ETH) mainnet and FUSE DPoS.

The third layer (‘network’) is a blockchain itself: it is accessed only by validators of Delegated Proof-of-Stake consensus. Since Fuse Network (FUSE) is an open-source project, all blockchain-based developers can experiment with its functionality. Fuse Chain is interoperable with Ethereum (ETH) mainnet.

Such a robust scheme of Fuse Network hierarchy guarantees the secure and seamless experience for the users with various levels of blockchain expertise. FUSE token is a backbone element for this ecosystem: this token is used for fees, delegation and rewards purposes on FUSE mainnet.

Running a FUSE node: why you may need it

Typically, receiving periodical payouts for transaction validation is the main economic incentive for running a node in blockchains like Fuse Network (FUSE). Meanwhile, it also provides node operators with seamless access to the core elements of Fuse and ensures seamless interaction with crucial decentralized mechanisms.

Image via Fuse

GetBlock eases this process: with our instruments, Fuse Network can be accessed with zero level of blockchain coding experience. GetBlock connects its clients to the Parity node of Fuse and assists in dApps migration.",3930
"Community Partnerships

Uncountable + The Hidden Genius Project

We’re proud to announce Uncountable’s partnership with The Hidden Genius Project.

The Hidden Genius Project is an international, Oakland-based nonprofit organization whose mission is to help train and mentor Black male youth, affectionately known as “Hidden Geniuses,” in technology creation, entrepreneurship, and leadership with an eye towards transforming their lives and communities. Uncountable’s data-driven teams provide an ideal medley of insights, ideas, and advice for these engineers and technologists of tomorrow.

One crucial aspect of The Hidden Genius Project is its partnerships with top technology startups in which students gain first-hand exposure to world-class entrepreneurs, engineering talent, and businesses. As part of this knowledge-sharing enterprise, Uncountable recently hosted a current cohort of Hidden Geniuses for a virtual business trip as part of the summer portion of their 15-month Intensive Immersion Program.

Hidden Geniuses at Uncountable

To foster the most insights into cutting-edge technology, the business trips typically feature small breakout room sessions in which participants directly learn about various team members’ responsibilities, and are encouraged to ask incisive questions. During our visit, Uncountable hosted five such breakout sessions, each of which focused on a different aspect of the organization and provided a plethora of distinct functional viewpoints from interns to founders.

Here are some of the highlights:

Session 1 — Richard Garcia (Director of Sales) & Noel Hollingsworth (Co-Founder/CEO)

Richard and Noel spoke about their respective paths to Uncountable — what inspired Noel to help found the company, and what motivated Richard to join the team.

The Hidden Geniuses quickly identified the early leverage points in Richard’s and Noel’s careers, pointedly asking, “What do you wish you learned about tech in high school?”

Our team’s response? “Focus on learning a hard skill like science or engineering,” Richard said. “I wish I’d been ahead of the curve in learning something I could directly apply.”

Session 2 — Will Goldie (Full Stack Engineer)

In his session, Will gave a detailed description of what the life of an Uncountable Full Stack Engineer looks like, as well as providing valuable information about the delineations between seniority levels.

The Hidden Geniuses were especially eager to learn about how responsibilities change with increased experience levels. Because they are developing the necessary skills to become a junior engineer at a company like Uncountable, the Hidden Geniuses appreciated hearing how those skills apply to a real-world environment.

Towards the end of the session, Will was asked, “What are the hardest tasks in your job?” He explained that it’s difficult to identify the most efficient and effective approach to a problem, even when you have the requisite knowledge: “There is often more than one way to solve a problem, and building a new thing isn’t always better.”

Session 3 — Will Tashman (Co-Founder)

Will gave the Hidden Geniuses a holistic overview of the teams inside Uncountable and explained how each fits into the broader mission of the company.

Understanding how their individual areas of expertise fit into the bigger picture helped the Hidden Geniuses understand their potential career paths as well as get a handle on the impact they could have on a company.

Session 4 — Ryan Clark (Intern) & Tim Wang (Intern)

Uncountable interns Ryan and Tim spoke with the Hidden Geniuses about their day-to-day responsibilities in entry-level roles. This unique perspective proved to be especially valuable as Geniuses can expect to intern at tech companies as they embark on their own careers.

In terms of their professional development, Ryan and Tim are only a few years ahead of The Hidden Genius Project’s young people. So, they spent time offering the Geniuses insight into how they thought about joining a smaller startup like Uncountable (versus a bigger tech company). The big takeaway? For Ryan and Tim, smaller startups offer “more meaningful contributions, but with more uncertainty, too.”

The Geniuses were also curious to learn how Ryan and Tim pitched themselves on their college applications to be accepted into schools (CalTech, Brown) with strong technical programs.

Session 5 — Andrés González (Data Scientist)

Andrés is Uncountable’s Lead Data Scientist, and he spoke with the Hidden Geniuses about his winding career path, what makes a good data scientist, and some of the ideas he wished he knew when he was younger.

Breaking into the competitive world of Silicon Valley startups from Germany gave Andrés a helpful perspective about the challenges of starting as an outsider, and he was vocal about the ways in which he felt he got lucky, and how the hard work he put in paid off.

“Do not envy anybody’s success, if you do not know their mistakes,” Andrés said.

One Hidden Genius asked: “What can you do to give yourself an edge?” In his response, Andrés emphasized that the little things that help others to see they can rely on you (“Be on time and do not lie”) and the value of focus (“Pursue your passions, but focus on only a few things”) before finishing with: “Ask the right questions and the answers will find you.”

Conclusion

We’re thrilled to have successfully kicked off our partnership with The Hidden Genius Project by hosting this cohort of Hidden Geniuses as a part of their 15-month Intensive Immersion Program. This is a wonderful first step for these young people, but the real work is still ahead and we’re looking forward to continued opportunities for collaboration and to track the success of this cohort as they grow into their professional careers.

For more information on The Hidden Genius Project, visit https://www.hiddengeniusproject.org/",5899
"Asking AI the Wrong Questions and Getting a Frightening Result Is Your Fault?

AI has incredible promise for the future, but that future is compromised by a lack of critical thinking on the part of programmers and trainers. Dr. Patricia Farrell Follow Jun 8 · 5 min read

Photo by Yuyeung Lau on Unsplash

AI has opened a world of possibilities much like a new Garden of Eden where we are the unknowing humans exploring an environment beyond our understanding. In this world we are creating, one central feature of it must be the fallibility of we humans. It is this flaw over which we must show power, growth, and new competence, and we must remember one thing; we cannot know the answers if we do not know the questions.

Questions seem straightforward in our minds, and our egos blanch at the thought that we may have inexplicably missed a step somewhere in our building process vis-a-vis algorithms. But missteps are exactly what we are facing, and humility or better critical thinking are required to avoid untoward results. We are experiencing a new period of evolution.

Critical thinking is not only a prerequisite to becoming adept at algorithms; it may have been lost somewhere in the mix. What is one method of increasing this necessary skill?

The RED Model’s critical thinking skills framework has three main indicators including (1) Recognize Assumptions; (2) Evaluate Arguments; and (3) Draw Conclusions. The RED Model’s critical thinking skills framework and its indicators are expected to assist in encouraging the development of critical thinking skills and measuring critical thinking skills.

An overview of the RED Model with examples of business tasks requiring critical thinking can be accessed here. The downloadable PDF also includes 50 ideas for improving your critical thinking.

In addition to this model, there is a downloadable chart with questions/prompts that guide approaching any issue or problem.

When an Algorithm Gets It Wrong

The belief exists that computers and their algorithms are superior to humans because they can process so much information so quickly — faster than we could in months or even years. Therein lies a major problem of our belief system and our bias regarding algorithms. When it's wrong, the result can be catastrophic for some individuals, as we’ve seen.

The algorithm at issue, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), was designed to assess a defendant’s risk of recidivism — that is, the potential risk that the defendant will commit a crime in the future.

An extensive analysis of the COMPAS methods and the outcomes was run by Politico. After multiple statistical analyses were performed on various portions of the algorithms as well as data sets that may have been used, the results were quite revealing.

As might have been expected, “Black defendants were twice as likely as white defendants to be misclassified as a higher risk of violent recidivism, and white recidivists were misclassified as low risk 63.2% more often than Black defendants.”

This is only one illustration of how misusing statistics can bring about life-changing calculations which mitigate against certain individuals.

Photo by Eva Blue on Unsplash

Valid or not, the COMPAS algorithm is used to provide quick, if not accurate, data on potentially violent offenders. It is a model for inefficiency, not efficiency. According to a presentation at a TED Talks by Peter Haas, the reason, again, for its use in criminal justice situations to determine future rates of violence, is the hoard of cases that must be dispatched quickly.

Because the algorithm is privately held (and used in about 13 states for criminal justice decisions), the source code is not available for inspection by anyone because of the trade secret law, raising concerns. Unbelievable as it may seem, no one knows what datasets were used to create COMPAS.

The criminal justice system is only one example where algorithms can cause untold havoc in people’s lives. Other instances include trying to get a loan for a home, a job interview, qualifing for government benefits, or even if they are stopped on the highway for a driving infraction.

Haas asked, “Would you want the public to be able to inspect the algorithm that’s trying to make a decision between a shopping cart and a baby carriage or a self-driving truck, in the same way that the dog/wolf algorithm was trying to decide between a dog and wolf?”

The latter distinction is notable since the algorithm training made an error identifying between a dog and a wolf. The reason was that in the training sessions only wolves in snowy backgrounds were shown. Any dog-like animal not in a snowy background was indicated to be a dog. Critical thinking was an issue here that was missing.

Clever Hans, 1904, Wikipedia.org Public Domain

Answers and Questions Asked

In her book, Atlas of AI, Power, Politics, and the Planetary Costs of Artificial Intelligence, Kate Crawford uses a well-known story of illusion and bias (the horse Clever Hans) when discussing AI.

“The story of Hans is now used in machine learning as a cautionary reminder that you can’t always be sure of what a model has learned from the data it has been given. Even a system that appears to perform spectacularly in training can make terrible predictions when presented with novel data in the world.”

Regarding facial recognition algorithms, Crawford questions the utility of incorporating criminal mug shots into the mix. The fact that mugshots are available to feed into datasets indicates a frenzy to create a dataset rather than to question what should go into it.

Frantically collecting images for databases has turned into a mindlessness that is never questioned because the programs demand and the programs receive what they demand.

Yet now it’s common practice for the first steps of creating a computer vision system to scrape thousands — or even millions — of images from the internet, create and order them into a series of classifications, and use this as a foundation for how the system will perceive observable reality. These vast collections are called training datasets, and they constitute what AI developers often refer to as “ground truth.”

Is the completed algorithm from the Multiple Encounter Dataset and others ever questioned and is there any weeding out of training images or data that may be in some way inappropriate to the mission? In fact, what is the mission and what biases have been incorporated into the collection? Is scraping the internet for images the most appropriate method of gathering data?

Could we even, at this stage, resolve these seminal issues? Who will be damaged by permitting these issues to go unresolved? What questions have not been asked and answered? Where have we failed?

https://www.amazon.com/Patricia-Farrell/e/B001HMSWYQ",6864
"Think of Blockchain as a public ledger where all the cryptocurrency transactions that you do are recorded. These are recorded on blocks and these blocks are linked or ‘chained’ together forming a string of blocks. Now every transaction needs to be verified, like they are done in banks, except that here, these are verified by a network of P2P computers spread globally, also knows as ‘Miners’. Think of them as auditors who keep a check on the legitimacy of these transactions and in return are rewarded for the same in crypto. In this way, the whole process is ratified without any involvement of banks or financial institutions. Pretty cool right? How safe is blockchain, you may ask? Since the transactions on a block are verified from multiple networks or ‘nodes’ , even if the hacker tries to alter one transaction, he would need to hack at least half of the distributed nodes and on top of that, if he still manages to do that, since the blocks are independent, he would not be able to go past one block. So the net result is a publicly secured network with almost no room for error. Still need a reason to invest? The most effective and perhaps a life changing use case for blockchain technology is in the countries where banking facilities do not reach all the citizens or where inflation rates have soared to such an extent that their government currencies are worthless. Zimbabwe, Argentina and Lebanon are just a few examples from the long list of countries who are forced to switch to crypto and blockchain to survive. Transactions on blockchain are superior to the current system that we have seen throughout our lives. Be it in terms of security, privacy or efficiency, blockchain technology is a tremendous alternative for people who are looking to move to a more secure platform for people who want to have control on their money and privacy

PS: El Salvador, a small country in Central America took the historic decision to make Bitcoin a legal tender in the country which resulted in the whole country getting instant access to financial services, dramatically reducing the remittance fees and transaction time as compared to the earlier wire transfers. If that was not enough, the citizens will not be paying any tax on the capital gains, that means, as and when the value of bitcoin rises, the citizens benefit from the gains just buy holding bitcoin in their wallets.",2389
"As the UX-professional on your team, you will find that you are often called upon to make the design of a landing page “better”. This very specific request often begs the question of where to start improving your design.

One of the ways we can improve our designs is by looking at how the elements are structured relative to each other. The way we arrange information within our products greatly affects how users experience our product.

Thankfully, we can have a look at some basic principles of how we process visual information and by weaving these principles into our designs, we can create landing pages and apps that users enjoy using.

These have become known as “visual hierarchy”.

So Visual Hierarchy, tell me more…

At its core, visual hierarchy describes how we rank elements relative to each other, taking into account how important or relevant the information is.

You may have heard of information architecture, which is similar but not identical. Visual Hierarchy allows us to rank the elements, resulting in information architecture.

By adjusting different properties of these elements, such as the relative size, colour and grouping, we give our content structure that helps guide our users to their goal.

We can make the most important elements stand out more, allowing users to quickly and efficiently find where they need to go.

That is why it is so important for your designs to have a clear visual hierarchy. It allows you to control the way in which your experience is being delivered to the user. Each carefully crafted element will bring them one step closer to their goal, lighting up a designated path for them to follow. The experience simply flows, removing any need for the user have to stop-and-search for information for them to proceed.

The goal is a silky-smooth experience that leaves your users and yourself feeling happy about a job well done.

Now that we know what visual hierarchy is and why it is important for us to understand what it does, let us have a look what tools we have at our disposal to create our visual hierarchy.

Typography

The typography is one the easiest ways to get your information architecture spot on. The reason being is that text is very widespread when it comes to the web.

It is everywhere.

Here is an example of a heading, a subheading and the paragraph styled exactly the same way:

Example test without visual hierarchy

It may well take a second for you to fully understand what is happening. That split-second of navigating the content is a sign of subpar visual hierarchy.

It causes a negative user experience.

The internet has a wealth of options that allows users to simply move on to the next product.

Not good.

Let us see how we improve the visual hierarchy by adjusting the font size:

Example using relative size to adjust hierarchy

Not bad — our attention immediately falls on the larger text of our heading, allowing us to see if the paragraph is relevant to our goal or not.

That is what visual hierarchy is all about. It allows the user to orientate themselves quickly and scan the content for relevant information.

Remember, users do not read on the web — they scan.

Design accordingly.

We have a better visual hierarchy thanks to us varying the size of our text. To further enhance the hierarchy, we can give our heading a bolder font:

Example with bold text

This further underlines the authority of our heading and makes it stand out even more. This further improves the user’s ability to scan our content for relevant information, making their lives easier.

That is what technology is all about.

Colour

We can use colour to aid the user in navigating our site.

While colour can help us convey information, it should be used in a more supplementary fashion and not as the sole provider of information to the user. The reason being that people with colour blindness will have a hard time seeing the clues you have laid out for them.

Using colour we can also help the user identify headings more easily, by making them a different colour to the text body:

Example with colour to aid visual hierarchy

We can use this style for each heading we use within our product. That way users can easily learn to differentiate between headings and paragraphs more easily. This learnability is one the factors that influences the usability of your product.

A good usability is essential for a good user experience.

Another great way to use colour in your designs is to make certain elements stand out more by using brighter colours. The increased contrast between the target element and the background colours naturally draws attention.

Think links or button of submit forms:

Image of a basic form

Making these stand out more by highlighting them with a strong colour will allow users to see these more easily. This discoverability is especially important when designing for smaller screen sizes such as tablets or phones.

In the example above you can see how the button practically screams for attention.

Whitespace

Staying with our headings and paragraphs, we can now look at the whitespace we use. Whitespace can go a long way in taking a good design and making it top-notch.

What if we removed the whitespace from our example above:

Example text without whitespace

There is not much space and everything appears a bit cramped. The content can’t breath and flow nicely.

Not a very good user experience.

Example text with whitespace

There we go, much better.

When you first start designing, you may be looking at how to fill the space you have in front of you. However, whitespace is something that you can intentionally use within your designs to improve your user experience.

Alignment and Proximity

Here the Gestalt Laws can unfold their beauty.

We can help a user make sense of what they are seeing by grouping related options together.

Here we can see how we can use the alignment and proximity of elements in combination with colour to better organise our user interface for our users:

Example of grouping options together

All of the options are grouped together and we are using card elements to better differentiate between the sidebar area and the rest of the screen.

The user will quickly learn that each card-like element has information regarding their messages. On the left-hand side we have the top-most level and go further into detail, the more we move to the right.

We can also make elements within a category stand out to highlight their presence to the user:

By using colour we can indicate to the user which mails have not been read yet, which mail we are currently reading and which mails have a related task.

We are again also grouping related elements together to help users find their way more easily.

Here I would encourage you to play around a bit with the proximity of the different elements and test them on users to see which feels best to them.

I would definitely recommend reading the gestalt laws because they do give you a solid understanding of how human perception works and how you can use these within your own designs.

Size and Scale of Elements

The size of elements relative to each other can also be a great tool to improve the information architecture.

We already saw with our typography that larger elements tend to grab the attention of the user more. It is no different with other elements:

Image of a landing page

The larger element is seen first by the user. After that, the user begins to scan the rest of the page.

With this we can control how the user processes each part of our product and make important information stand out more to grab their attention.

A word on Mobile UX

With mobile becoming the dominant medium for users to enjoy the offerings of the internet, it is very important to consider the constraints the smaller screen size bring with them.

Mobile users should be able to see and select the elements they want with ease. The easiest way to do this is to make the actionable elements stand out more, by adjusting the size and contrast.

Make your text easy to read for users to quickly understand what is happening and if they want to click on a button, make the button’s area big enough for them to easily select that action. Avoid buttons that are so small and close together, that a user accidentally presses the wrong button.

Implementing Visual Hierarchy

Using these principles in practice is not always straight forward. It may start out well and we have a clean information architecture. As the project grows and stakeholders are throwing requests at you to make more and more elements stand out, the visual hierarchy becomes muddled.

It is important to keep things simple and basic.

Start with one, really important element. This is the one every user must see and is the real value-bringer for your product. It is the one you found during your user research that everyone kept repeating. It is the main goal of what you are designing. Once you have that, build up the hierarchy and messaging around that.

The goal decides what your information architecture will look like. It can change, but the need to change should be reflected in your user research.",9171
"Hidden vs disabled states

A disabled state is applied to a component when the user is not allowed to interact with it due to either permissions, dependencies, or pre-requisites. When a control is disabled, it usually appears as slightly transparent or lower contrast than the default state, and it stops receiving input (read-only state). When a control is hidden, it is no longer in the interface. In both cases, disabled states completely remove the interactive function of a component.

Disabled state

Advantages of showing the button in a disabled state:

Discoverability : many users learn to use an application by exploring and reading labels. By including disabled elements we show users what an application is capable of even when the button requires further action to enable.

: many users learn to use an application by exploring and reading labels. By including disabled elements we show users what an application is capable of even when the button requires further action to enable. Learnability: the user can learn where controls and buttons live within the user interface.

Enable and Disabled state buttons from Material Design

Disadvantages of showing the button in a disabled state:

Increases cognitive load: Including disabled buttons gives users extra to think about. Seeing a disabled button will make users wonder why it is not available and how to enable it. According to Steve Krug’s “Don’t make me think” we should remove elements that make the user think.

Including disabled buttons gives users extra to think about. Seeing a disabled button will make users wonder why it is not available and how to enable it. According to Steve Krug’s “Don’t make me think” we should remove elements that make the user think. It appears clickable to some users : If not implemented correctly disabled buttons can confuse users. For example, some users can mistake greyed-out buttons for secondary actions.

: If not implemented correctly disabled buttons can confuse users. For example, some users can mistake greyed-out buttons for secondary actions. Accessibility: They’re hard to see — the Web Content Accessibility Guidelines (WCAG) do not require sufficient contrast ratios for disabled buttons. They also don’t give users any feedback.

Example from Carbon Design System

Hidden state

Advantages of hiding the button:

Reduces cognitive load : users can focus more on the task in hand. By showing them what is needed only we remove distractions.

: users can focus more on the task in hand. By showing them what is needed only we remove distractions. Simplicity: Not including disabled buttons saves space and allows us to change the controls, using the same space for different states.

Example from Carbon Design System

Disadvantages of hiding the button:

If overused it can be distracting ; constantly hiding and showing buttons may distract because the view changes constantly.

; constantly hiding and showing buttons may distract because the view changes constantly. Poor discoverability: Users might search for the hidden option.

Which one to use?

Before deciding which state to use, we should try answering the following questions:

What does the user need to know at this point?

What does the user need to do at this point?

Will the user know about that feature?

Will the user spend a lot of time hunting for it?

The answers to the above questions can help us figure out which state to use.

Suggestions

Choose disabled when:

If the button is temporarily disabled or requires a specific action from the user to change its status. In this case, it would be useful to provide a hover bubble/tooltip/detail disclosure buttons explaining the criteria for use and informing users what actions they can take to enable them.

If you want the user to know that the control exists, but that it is disabled.

If the user could see this feature somewhere that then not be able to find it again if it is now hidden. And there would be no indication why they can’t find it again.

Choose hidden when:

If the user is not authorized to use the control which means it is never available to the user.

If it is a control that is only active in very rare situations. Don’t show the user a button that can’t be used or enabled to be used.

If a page has a lot of disabled functionality. For instance, a whole extra block of form inputs.

Note: Using the disabled state isn’t the only way to deal with temporarily disabled actions. Rather than using disabled buttons, it is possible to keep the button in its default state and display an error message letting the user know why they can’t proceed.

Other considerations

When using disabled states we need to make sure they adhere to accessibility guidelines. There needs to be a clear distinction between disabled and enabled button styles. Colour contrast is a big thing for web accessibility.

It is best to communicate a disabled state button by changing its transparency instead of greying the button out. Users are less likely to confuse a transparent button with other buttons. Grey can lead to ambiguity as it is sometimes used to communicate a low priority button in a group (e.g., cancel buttons) and can be mistaken for a call to action.

To minimize cognitive load it is useful to give explanations to users. For example, if a button changes into its disabled state, consider using helper text or a way to explain to users why this has occurred.

Conclusion

If the control is available sometimes but isn’t available right now, it should be disabled. In these cases, you can improve the user experience by explaining to the user why that is and how the action can be enabled)

but isn’t available right now, it should be disabled. In these cases, you can improve the user experience by explaining to the user that is and the action can be enabled) If the user is not authorized to use the control (therefore, it will never become available to that person) it should be hidden.

Read More

https://uxdesign.cc/is-it-ok-to-grey-out-disabled-buttons-8afa74a0fae",6042
"As part of our ongoing employee spotlight series, we’ll be profiling colleagues who inspire us. Today, meet Maddy Cuello.

Company Role: Copywriter

Most likely to: Watch a Golden Girls marathon

Secret talent: Sensing the presence of a cat

As an inquisitive child Madelyn, better known as Maddy, spent her youth in Astoria, Queens perusing library books, which started her love of writing and words from an early age. She attended the Fashion Institute of Technology where she studied marketing, but the writing bug never left her. After several educational and professional twists and turns, she finally started writing full-time. And after a long job search looking for the right fit, she connected with the People team here at String and Key and was hired in April of 2021. A nomad in her own town, Maddy moved around the city before finally settling in Bushwick, Brooklyn, where the thriving artistic community inspires her both personally and professionally.

Let’s get to know a little more about Maddy.

What do you do, and what does your typical workday look like?

I’m a copywriter so I work on a variety of things from blogs, to social media materials, video and audio scripts and more. Every Monday we have a creative team meeting where we discuss the upcoming week as well as twice-a-week morning check-ins. In addition to discussing the projects for the week I also help conceptualize the content we put out to the public to make sure it’s informative but also digestible.

What’s your favorite part about working at String and Key?

There is a lot of room to be creative and also to grow and learn, which is great so you don’t feel stagnant. There’s also a lot of autonomy and trust but at the same time so much support and encouragement. I think it’s a beautiful mix, which is rare. Feedback is done with care which helps to boost my confidence as a writer. There’s also a lot of help from other departments to make sure you do your job right. The collaborative spirit is very important to feel like a valued member of the company.

What excites you about your job?

I’m excited to learn more about the product we are putting out and also what the future holds. As a writer or a creative person in general, you want to challenge yourself to create content that’s appreciated no matter what industry you work in. So it’s exciting to open this new chapter working in fintech.

What do you find most challenging about your role?

To make sure what I’m writing makes sense and is looked at as an educational touchpoint for the audience who consumes it. It can be difficult to write an informative piece with a creative twist, but that’s a challenge that I enjoy!

What are the values that drive you?

Collaboration, curiosity and kindness. I try not to be sensitive about my work and always strive to take constructive criticism well and put it to good use. I always remind myself it’s about the product and content at large. Having the curiosity to learn more and willing to go beyond my comfort zone where I expand my knowledge not only as a writer, but also as someone in fintech. There is a lot to learn and there’s constant innovation, so I think it’s important to be open to things you might be unfamiliar with even if they might seem intimidating. Also, just being a kind and pleasant person goes a long way.

How do you stay on top of your game?

I really admire other writers, and not necessarily just in our industry. I like to dissect what makes a person a good writer. What is it about the way they convey a message that is attractive to an audience? That’s so interesting to me because what makes good writers and writing is very subjective.

What drew you to tech and what excites you about the industry?

I had done some work for other tech companies, and I really liked the innovation and forward-thinking vibes. It really is the future and you can learn so much, especially if the company is a start-up.

What’s one thing — either industry-related or not — that you’ve learned in the last month?

I’ve been really into reading tarot cards. It sounds hippy-dippy but I really enjoy it because it makes you look inwards and have self-reflection. Plus, it’s kind of fun to make people think you might have “The Gift”.

If you could swap places with anyone at String and Key, who would it be and why?

I would love to swap places with another member of the creative team. They do brilliant work and are so talented so it would be cool to step in their shoes.

What unexpected subject could you give a one-hour presentation on with no advance prep?

I’m well versed in good TV. There is so much good writing since critics say we’re in another golden age of television. So I can definitely give an informed talk about certain TV shows from the past and present.

What keeps you busy outside of work?

I’m lucky enough to have a big circle of friends and a big family, so I spend a lot of time with them. But since I also enjoy my alone time, I definitely watch a lot of TV, as mentioned above. I also love going down YouTube rabbit holes about any random topic. Finally, I love to cook and try new recipes (FYI,I’m always looking for taste testers).

Can you list five hashtags that describe your personality?

#Curious #nativeNewYorker #LoverOfWords #CatLover #Funny

Lighting Round:

Dawn or dusk?

Dusk.

Chocolate or vanilla?

Chocolate.

Facebook or Instagram?

Instagram.

Summer or winter?

Summer.

Music or audiobooks?

Music.

Interested in working at String and Key? Join us!",5510
"POCO X2 Detailed Camera Review | The new budget smartphone camera king! | Is Poco X2 camera is good? Sameer Khan Jun 17·2 min read

Hey guys in this article, I will review POCO X2 Detailed Camera Review. we are going to take a very detailed look at all the cameras on the POCO X2. I have over 80 image and article samples in various lighting conditions. So make sure you watch the complete article. I wanted to include the Gcam samples from the POCO X2 in this article. But even without those samples, this article was getting a lot longer.

So I decided to make a separate article about it. And that article will go live a day after this one. Let us quickly get the specs out of the way. There is a quad-camera setup at the back. The primary camera gets the 64-megapixel SONY IMX686 sensor, with F/1.9 aperture and 26mmlens.The next camera gets an 8-megapixel sensor with F/2.2 aperture. This sensor is coupled with a 13mm ultra-wide-angle lens. Then there is a 2-megapixel macro camera F/2.4 aperture. And finally, there is another 2 megapixel depth sensor with F/2.4 aperture. Why is my phone charging slowly all of a sudden?

At the front, we now get 2 cameras. The main camera has a 20-megapixel sensor and a 27mm lens, while the secondary camera has a 2-megapixel depth sensor with an F/2.4 aperture. I think if companies are including 2 cameras at the front, 2nd one should always get a wide lens, because even on phones without the depth sensor, edge detection is pretty good when it comes to portrait selfies. In the camera app, you do get a pro mode, which lets you adjust all settings like white balance, focus, shutter speed, ISO and exposure.

If you want to read more about this article so click here.",1715
"SpaceX successfully launched 88 satellites

into polar orbit under the Transporter-2 mission. This is SpaceX’s second dedicated mission under the SmallSat Rideshare program, which intends to launch satellites from different companies and startups, like a carpool.

On June 30, around 3:31 pm EST, Falcon 9 took off from Cape Canaveral, Florida. The first stage booster, which was reused for the eighth time since it entered service last year, was separated at 3:34 pm EST and landed back to Cape at 3:39 pm EST.

This marks SpaceX’s 20th launch this year but importantly the first one when the first stage booster landed onshore at Landing Zone-1, the company’s designated landing pad. All previous missions this year landed in the sea over the drone ships.

The launch also raises the tally to 127 missions for SpaceX. With months left in the calendar year, it can be predicted that the company is going to break its own record of 26 launches that took place last year.

SpaceX successfully launched 88 satellites into polar orbit under the Transporter-2 mission. This is SpaceX’s second dedicated mission under the SmallSat Rideshare program, which intends to launch satellites from different companies and startups, like a carpool.

On June 30, around 3:31 pm EST, Falcon 9 took off from Cape Canaveral, Florida. The first stage booster, which was reused for the eighth time since it entered service last year, was separated at 3:34 pm EST and landed back to Cape at 3:39 pm EST.

This marks SpaceX’s 20th launch this year but importantly the first one when the first stage booster landed onshore at Landing Zone-1, the company’s designated landing pad. All previous missions this year landed in the sea over the drone ships.

https://twitter.com/SpaceX/status/1410392725996904448

The launch also raises the tally to 127 missions for SpaceX. With months left in the calendar year, it can be predicted that the company is going to break its own record of 26 launches that took place last year.

Transporter-2 mission launch. Image: SpaceX

Although the launch was originally scheduled on Tuesday, it was stopped T-11 seconds before liftoff due to the entry of a helicopter in the “keep out zone”.

Elon Musk was quite disappointed about Tuesday’s dangerous situation due to range violation. He tweeted and pointed out how the existing regulatory system is faulty and spacefaring civilization requires major regulatory reforms.

#Transporter-2 Mission

The transporter-2 mission consisted of 88 satellites, among which 85 were launched for external government and commercial customers. The other 3 were the company’s own Starlink internet satellites. All of them flew as part of this one mission which is the company’s second most dedicated ride-sharing mission to help other startups and companies to share satellites.

Satellites include the first launch for Umbra — a space intelligence startup that raised $32 million in January, and YAM-2 and YAM-3 which are Loft Orbital’s satellites. Five independent sensors are present in both YAM-2 and YAM-3 for five individually separate customers. The Pentagon’s Space Development Agency (SDA) had four satellites on Transporter-2 for $21 million.

Transporter-2 is the successor to Transporter-1, which was the first ridesharing mission for SpaceX. Transporter-1 carried 143 satellites in January but less orbital mass than Transporter-2.

https://www.youtube.com/watch?v=sSiuW1HcGjA",3432
"Why start with rockets?

The launch industry is a fundamental part of the space ecosystem because it can be described as the critical infrastructure upon which the space economy is enabled. Most simply stated launch vehicles are a means of transportation, they get objects off the surface of the Earth and onto the next step of their journey in space.

The launch industry is most typically segmented by payload lift class, meaning the mass of the objects that the rockets can send into orbit. There are four major payload classes, Small Lift (rockets that can send payloads of 2,000kg and below into orbit), Medium Lift (2,000kg to 20,000kg), Heavy Lift (20,000kg to 50,000kg) and Super Heavy Lift (above 50,000kg).

I have chosen to start with a post about rockets that exist at the edge of the Medium/Heavy Lift payload classes (around 18,000kg-24,000kg to LEO) because these are arguably the most high profile rockets actively flying today. In addition, a number of rockets in this category are developed by American private companies and so this payload class is particularly relevant to investors.

The rockets in this Medium/Heavy category are listed below and are all currently flying (there are no Medium/Heavy Lift vehicles in development)

The Rockets

Falcon 9

Originating Nation: United States

Originating Entity: SpaceX, a private US company

Payload Class: Medium/Heavy — 22,800 kg max to LEO (Low Earth Orbit)

Maturity: Currently Flying

In many ways, SpaceX’s Falcon 9 has recently shaken up the launch industry. This is partly because of its technical achievements and partly because of how it represents an emergence of “NewSpace” commercial entities disrupting an industry previously dominated by legacy players. The Falcon 9 has effectively positioned itself as the industry’s versatile workhorse launch vehicle. It was built not to push the limits of rocket technology or solely serve a government’s mission priorities but instead was maybe the first rocket program developed largely with private investment for the purpose of launching reliably and affordably. When SpaceX introduced the Falcon 9, it was the lowest priced launch vehicle in its payload category and created significant downward pricing pressure which has had widespread impacts through the industry. Currently the price of a Falcon 9 is around $62M. In 2015, SpaceX became the first entity to land a rocket, a massive technical feat considered nearly impossible up until the moment SpaceX accomplished it. However this leap was made not to demonstrate their technical ingenuity but instead to increase profits by reducing costs and increasing flight cadence through reusing the rocket’s first stage.

After their first landing, SpaceX released a compilation of all of their unsuccessful attempts, a demonstration of the company embracing its failures in a way that is rare for space companies. The video is a fun watch and highly recommended:

Atlas V

Originating Nation: United States

Originating Entity: United Launch Alliance (ULA), a US company that is a 50/50 partnership between Boeing and Lockheed Martin

Payload Class: Medium/Heavy- 20,520 kg max to LEO

Maturity: Currently Flying

Atlas V is the oldest actively flying rocket on US soil, with its first flight in 2002. It has been regularly and reliably flying since and has launched 85 times. Unlike the Falcon 9, the Atlas V was designed a bit more with performance in mind and as a result is a more expensive launch vehicle. Whereas the Falcon 9 specializes in frequent low cost launches to Low Earth Orbit, the Atlas V leverages its higher performance (as measured in Specific Impulse, the rocket equivalent of miles per gallon) to reach further destinations such as Geosynchronous Orbits. These orbits are typically more used by government satellites (such as GPS, spy imaging, or classified communications) and so the Atlas V has traditionally been the go-to rocket for less price sensitive US government national security satellite launches that must be launched from US soil. However, an Atlas V launch costs about $110M per launch and so the Falcon 9 has been steadily eating into the Atlas V’s business as the Falcon 9 has proven its reliability and the US government has found the cost savings to American taxpayers to be increasingly attractive. A further complication for the Atlas V is that its first stage engine, the RD-180, is purchased by ULA from Russia and in 2014 Congress mandated that the US end its dependence on Russian engines. Thus the Atlas V is being phased out of active use in the coming years. ULA plans to replace it with its more powerful next generation launch vehicle, Vulcan.

Medium/Heavy Lift Launch Vehicles by Price Per Launch

Proton

Originating Nation: Russia

Originating Entity: Designed by Khrunischev State Research and Production Space Center, an entity of the Russian Government

Payload Class: Medium/Heavy- 23,700 kg max to LEO

Maturity: Currently Flying

The proton is one of the most successful and prolific rockets in the history of spaceflight. It was first flown in 1965 and since has launched an astounding 425 times, most recently in its form the Proton-M which first flew in 2012. Proton began as an Intercontinental Ballistic Missile and later transitioned to flying payloads to orbit for the Russian government. Proton’s first launch carrying payloads for commercial entities was in 1996. The Proton-M costs $65M per launch, making it one of the most affordable rockets in its payload category and it has been a significant source of revenue for the Russian government, with claims that between 1994 and 2019 Proton earned $4.3B for the Russian space industry. Due to that cost, the Proton-M rocket is a popular launch vehicle for both commercial and Russian government satellites. Proton is currently being phased out for replacement by the Russian Angara launch vehicle.

Medium/Heavy Lift Launch Vehicle Lift Capacity to LEO

What Drives Launch Competitiveness?

In summary, the key takeaway from these Medium/Heavy Launch vehicles is that no single factor controls competition in the launch industry. For example, while one of the most significant drivers of vehicle success is price, the Atlas V still flew five times in 2020 despite costing over $100M per launch and being nearly twice the cost of a Falcon 9. In addition, the Proton rocket continues to fly regularly both for the Russian government as well as European and Russian commercial entities. Why hasn’t this become a winner take all sector? Beyond price, some of the major factors that drive competitiveness include reliability, performance, national security, geography, and (critically) politics.

Throughout the history of rocketry, reliability has proven to be a major priority. Given the high cost of failure if a satellite is destroyed during launch as well as the high cost of insuring satellites that launch on unreliable rockets, a reliable track record is a strong competitive advantage. In fact reliability was the primary argument that ULA made for discouraging the US government from using the Falcon 9 rocket during SpaceX’s infancy. At this point however, the Falcon 9 has flown so frequently that it is arguably the most reliable rocket available today in addition to being the most cost effective.

Number of Successful Launches by Medium/Heavy Lift Launch Vehicles

Performance has also become a competitive differentiator. As stated earlier, the Atlas V utilizes engines that provide higher in-space performance which optimizes them for further orbits. While these further orbits (such as Geosynchronous Orbit) are just a sub-segment of the broader launch market, ULA has found this segment to be highly attractive and one they hope to protect.

In addition, national security continues to be one of the primary reasons that some launch vehicles exist. As long as a nation is flying military and national security satellites, they will desire that those satellites stay within their own nation’s borders until the time of launch. For that reason, rockets like Proton will always have the Russian government as a reliable customer and rockets from other nations will never be able to compete for those launches. It is also worth noting that national security government launches tend to be much more lucrative than commercial launches. For example, while ULA may make $110M on a commercial Atlas V launch, it takes in closer to $180M on launches for government customers due to the increased services and specifications needed to satisfy national security customers. It is this reason why ULA is so focused on competing for national security launches while SpaceX dominates commercial launches in the US.

Beyond national security, we can also see that geography has proven to be a competitive advantage as well. While SpaceX has in many ways become the preferred launch vehicle for US commercial satellites, the Proton rocket (which is slightly more expensive) continues to fly payloads for Russian and European commercial entities that also have freedom to choose their launch provider. It seems that satellite operators have a preference for using rockets that are built and launched from close to home.

Finally, perhaps one of the primary factors that has not been discussed yet is the highly political nature of the space industry. The US legacy defense companies have extremely effective lobbying arms and have also strategically positioned their manufacturing facilities to be located in the districts of highly influential politicians. While its nearly impossible to quantify the true impact of this under-the-radar dealmaking, these lobbying efforts are widely understood to be a major factor in the decision making for major launch contracts offered by government entities.

Ultimately the various factors contributing to competitiveness in the industry have prevented any single launch provider from owning the entirety of the market which makes it an exciting and continually evolving part of the space industry. That being said there is significant disruption taking place that makes this an interesting sector for investors to pay close attention to.

Additional Reading

Want more frequent updates on the space investing world? Follow me on twitter!",10269
"Starship: SpaceX’s Eighteen-Wheeler to Space

SpaceX Starship at Starbase, Texas. Source: https://commons.wikimedia.org/wiki/File:Starship_SN9_Launch_Pad.jpg. Author: Jared Krahn, licensed under Creative Commons Attribution-Share Alike 4.0 International license.

The eyes of the world are on Boca Chica, and my eyes are included. In fact, I can’t take my eyes off of NASASpaceflight’s YouTube videos. I’m watching videos of a construction site. A construction site! That’s because it’s the most incredible construction site in the world. It’s where SpaceX is building spaceships! And my eyes are glued because SpaceX’s spaceship, Elon Musk’s dream, Starship, is going to change humanity forever.

I’m not exaggerating for clicks. I mean that Starship will change humanity forever. This is because Starship will be the first rocket that will enable humanity to take to space at a large scale. Elon Musk and SpaceX want to make humanity a multiplanetary species, and they plan to build a million-person city on Mars. That means that soon, you may be able to buy yourself a ticket and fly. It means that spaceflight is going to take off in ways that have yet to be imagined.

On the launch pad, Starship has a retro vibe. She’s shiny, as she’s made of stainless steel. With her fins, she looks like she came out of a Buck Rogers comic book. But Starship isn’t just a figment from the mind of a 1950’s science fiction author. Starship is real, and on the verge of making her first trip to space. SpaceX designed Starship to be reusable like an airliner. She’s planned to be capable of refueling in orbit and on Mars from local resources, giving her incredibly long legs. And she’s designed to be economical enough that ordinary people could afford to go to space.

Size

Starship is a monster and will be the most powerful launch vehicle in the world. When stacked, Starship and Superheavy are taller than the Saturn V, and her engines give her twice the thrust.

She comes in two parts: Starship and Superheavy (both the second stage and the complete rocket stack are named Starship.) Superheavy is the first stage, equipped with 33 Raptor engines to lift her off the pad and lob the second stage, Starship, out of the atmosphere. Then Starship lights her six Raptors and completes the trip to Earth orbit. She’s estimated to be capable of putting 150 tons of payload into orbit (source: Elon Musk, Twitter), while still being capable of reentry, landing, and reuse afterwards.

One of the reasons why NASA selected SpaceX to build HLS to bring astronauts to the moon is because its sheer size gives it capabilities that until now, NASA could only dream about. They asked for a subcompact to bring a pair of astronauts to the Moon, and SpaceX offered them a semi. HLS dwarfs the National Team’s and Dynetic’s offerings and enables Artemis astronauts to bring tons of cargo with them, such as habitats, rovers, and other gear.

Starship is big enough to bring dozens of settlers to Mars. She could perform Earth-to-Earth suborbital flights carrying hundreds of passengers. She could be left in orbit to be converted into a space station that’s bigger than the International Space Station. Or she could become an office-building-sized Moon or Mars base.

Engines

Raptor speaks! Source: Elon Musk, https://commons.wikimedia.org/wiki/File:Raptor-test-9-25-2016.jpg. Image is in the public domain.

To power such a beast of a rocket, SpaceX needs incredibly powerful engines, and they have built them. Raptor is the product of over a decade of development and has become the most technologically advanced rocket engine ever built.

The first thing that aerospace engineers will note about the Raptor is the words “full-flow staged combustion,” and those words make them drool. This design puts the fuel and oxygen through a pair of preburners that burn some of the propellants to spin the turbopumps that bring fuel into the engine. Compared to earlier gas-generator designs (that waste quite a bit of propellant), the full-flow staged-combustion design is far more efficient, can generate tremendous power, while being dependable enough for reuse. Admittedly, this design is complex, but SpaceX seems to have attacked this challenge head-on and won. For the curious, Tim Dodd explains Raptor and full-flow staged combustion in detail.

Raptor uses a fuel that’s new to spaceflight, liquid methane. In other words, it’s refined natural gas. This fuel solves a lot of problems for SpaceX. First, it’s relatively cheap, especially when compared with the Space Shuttle’s fuel, liquid hydrogen. Methane is much easier to work with than hydrogen, which needs to be kept extremely cold, requires gigantic tanks because of its low density, tends to leak, and is highly explosive. While methane also has its hazards, it’s not nearly as difficult to handle as hydrogen, and can be handled with equipment that’s similar to that used with its oxidizer, liquid oxygen. It doesn’t have the propensity to leak like hydrogen or cause problems like hydrogen embrittlement, and it’s denser, meaning a lighter, smaller fuel tank can be used.

Methane also is nice because it burns cleanly, which makes full-flow staged combustion possible, unlike RP-1 (refined kerosene.) It also has a higher specific impulse (ISP) than RP-1, meaning it burns with more energy for its mass, giving Starship “great gas mileage.”

In-orbit and Martian Refueling

With the fuel it brings off the launch pad, Starship is capable of getting about 150 tons to low Earth orbit (LEO). Normally, this would make Starship almost as limited as the Shuttle, which was limited to LEO. However, Starship is capable of being refueled in orbit by another Starship, purpose-built as a tanker.

This is one of the key technologies that make Starship a far more capable spacecraft than its predecessors. Eight tanker refueling flights for an orbiting Starship can fully fill its tanks. Imagine a fully-fueled Starship, already in orbit, pressing the reset button on Tsiolkovsky’s rocket equation. This is what gives Starship the ability to carry large payloads to the Moon and Mars. It is what turns this spacecraft into a true deep space vessel. A Starship can take settlers and supplies to Mars, mine asteroids, or send space probes out of the Solar System.

Even better, Starship’s propellants, liquid methane and liquid oxygen, can be made on Mars from water and carbon dioxide found there using the Sabatier process. This solves one of the show-stopping problems with crewed Mars missions; how do the astronauts get back to Earth? With Starship, an expedition can go to Mars with a dozen people, plenty of supplies, and once they arrive, they can make the fuel to come home.

Reusability That Works

The Space Shuttle was the first, but it didn’t exactly fulfill the promise of reusability. The Shuttle was insanely expensive, needed enormous amounts of repair and refurbishment between flights, was only barely reusable, and was downright dangerous. But the Shuttle served for three decades, built the International Space Station, and launched so many other things like the Hubble Space Telescope. The Shuttle will always have a place in my heart, but when it comes to reusability, we can do so much better.

Fortunately, SpaceX is already the first company to make rocket reusability economical. They’re recovering and reusing Falcon 9 first stages and Dragon capsules, saving them millions of dollars. They’re doing it while maintaining a grueling launch cadence to serve commercial and government customers, and to launch thousands of Starlink satellites.

Starship is SpaceX’s second reusable launch vehicle, after Falcon 9. The first stage, Superheavy, will fly similarly to Falcon 9’s first stage. After separating from Starship, it turns around, boosts back, and uses its Raptors and grid fins to land at the launch site. But to save weight, rather than using landing legs, SpaceX is going to catch the booster using “Chopstick” arms on the launch and integration tower.

The second stage, Starship, continues to orbit using a combination of sea-level and vacuum Raptor engines. At the end of her flight, Starship reenters the atmosphere belly first, using its hexagonal heat-shield tiles to survive the plasma and heat of reentry. Her tiles are made of ceramic and are similar to the Shuttle’s. Also, Starship’s skin is made of stainless steel, which is very resilient to high temperatures.

In the stratosphere, Starship has already demonstrated the “Bellyflop”, where she uses her fins to control her descent, much like a skydiver. At the very end, Starship fires up her Raptors, swings to a vertical orientation, and comes to a gentle landing. Musk said he also plans to catch Starships with the “chopsticks” on the tower in the same way as they’d catch Superheavy.

SpaceX intends to make Starship work like an airliner. Elon Musk wants Starship to fly multiple times every day, with less than an hour of turnaround time between flights. In other words, Starship is to be a workhorse like a Boeing 737. This is because Elon Musk understands what needs to be done to bring costs down and make spaceflight economical.

Manufacturability and Economics

Before SpaceX came along, the space launch industry was like the early automotive industry before Henry Ford and the assembly line. Cars were custom-made in small quantities, making them expensive. As a result, they were workhorses for a few large businesses that could afford them, and they were toys for the wealthy.

Then Ford came along with mass-production and the Model T. That is what Elon Musk is doing to the aerospace industry right now. Rockets until now have been mostly built on demand, bespoke for the customer, in low quantities, for the few entities capable of affording them. But Elon Musk cut his teeth on Ford’s industry, automobiles, at Tesla. He knows about mass-production. He knows about optimizing production processes, and he knows about value engineering. He knows how to get a product from concept to design to manufacture as efficiently as possible. In fact, he obsesses about that.

That’s why SpaceX abruptly switched from building what would become Starship with carbon fiber to stainless steel. Normally, carbon fiber is great for building rocket parts that are both light and strong. However, carbon fiber is both expensive and laborious to work with. Musk wants SpaceX to design and build with a rapid-iteration process, and carbon fiber was too slow. For the price of some mass, SpaceX built Starship with a material that was cheap, fast to work with, and resilient to the extreme temperatures of reentry and cryogenic propellants. Once the rapid iterations started, progress on Starship went amazingly fast.

Elon Musk isn’t just building a rocket. He’s building a rocket factory at Boca Chica. In an interview with Tim Dodd, the Everyday Astronaut (Part 1, Part 2, and Part 3,) Musk stated that 90% of the work was creating the production processes and systems that build the rocket, rather than designing and building the rocket itself. Much of Musk’s work is similar to what he was doing at Tesla: planning the production process, identifying bottlenecks, creating optimizations, and getting processes to run efficiently and smoothly.

Starship isn’t designed to be a one-off, occasionally manufactured artifact from the traditional cathedrals of old-space. Starship and Raptor are going to be built like cars are built: mass-produced, efficiently, with automation, optimized processes, and in large quantities. This enables individual Starships and individual Raptors to be built and run at a small fraction of the cost of previous rockets and engines. To build his city on Mars, Musk said he wants over a thousand Starships, and he’s building the facilities to make them.

SpaceX also plans to launch Starships from sites across the globe. They’re already constructing the launch and landing facilities at Starbase, Texas. They’re also planning to launch and recover Starships at Cape Canaveral, Florida. SpaceX has also acquired two former oil platforms, now dubbed Deimos and Phobos to launch Starships at sea. One of the plans is to position offshore launch platforms near coastal cities for passenger service. In short, SpaceX has plans for Starship on a scale far larger than has ever been seen in the industry before.

What Does This Mean?

When I was younger, I read a book by Jerry Pournelle and other authors (the book was Cities in Space, and I’m going from memory, so forgive me if I miss some details.) It had a great point I read as I was getting my first college degree. This was in the 90’s when the spaceflight world was in the doldrums. Cities In Space pointed out that people lost interest because all they were able to do was watch people on TV go to space. It’s not that people didn’t like space, but they wanted to personally go to space, rather than watch TV people do it. I’m the same way. I want to personally go to space.

Starship’s the key to making that happen. Starship could reduce costs to the point where ordinary people can buy a ticket and go to Mars. This could finally bring us to the day where we all can go to space.

That’s why I say this spacecraft will change humanity forever. Starship can bring ordinary people to space. It can bring heavy industry to space. It can bring unimaginable wealth back to Earth (check out the asteroid 16 Psyche and the dollar value of the precious metals there.) Starship will enable humanity to finally take real steps off this pale blue dot, so now’s the time to really dream. Jeff Bezos talked about Gerard O’Neill’s space colonies. Elon Musk wants a million-person city on Mars. But I’m dreaming a little further ahead.

Close your eyes, and imagine humanity, a few centuries from now, with a population of ten trillion (yes, trillion with a ‘T’), spread through the entire Solar System: Earth, Mars, countless habitats built all over the system, settlements on the outer moons and the Kuiper Belt, and more. Think of a humanity that’s grown a thousand-fold from today, thriving, with no limits in sight.

I don’t know about you, but I want this future. Starship can help make it happen. Starship will become humanity’s eighteen-wheeler to space. With it, humanity takes the Solar System.",14314
"Today we don’t imagine the world without light. Lighting leads our every step. There is light in our homes, in our smart devices, in our cars. It is everywhere. But how it all started? What is the real history of lighting? What was the very first lighting source?

Discovery of fire

I guess it is no secret that everything started with fire. That was the first natural source of heating and lighting. It is challenging to say the exact time when it was started to use, but campfires and torches have been used in caves of Peking Man as early as 400 000 years B.C. Actually, archeologists have findings of campfires that are almost 1,5 million years old. Theoretically speaking — lighting is one of the oldest technologies in the world.

Of course, at first, humans weren’t able to create fire. They had to wait for lighting, a volcano eruption, or just a regular forest fire to get what they need. The Discovery of how to create fire started lighting “technology” as something that humans can make artificially. The fire was the center of society; it was the place where people could gather for socializing and cooperation. Later humans found a way to create mobile light — they used simple wooden sticks as torches. Later they found ways to prolong the burning process by using some resins, and all this torch technology was developing until the middle ages.

Next technology — oil lamps, candles, and the wick

The next step to creating artificial lighting was the usage of oil lamps. This solution dates back to 15 000–20 000 years B.C. Prehistoric people were using shells, rocks, or horns to create these lamps. Animal or vegetable fats were used as ignition material and dried grass or wood as combustible material. Humans also used oily animals like fish or birds as the lamps after being threaded with some dry grass.

A massive breakthrough in lighting technologies came as the wick was created. Ir changed the world of lighting source production. In combination with beeswax or tallow, wicks were used to make candles, which was the most advanced lighting technology at that time. And that time is about 400–500 years B.C. Later on, about 200 years B.C., another excellent material for candle production was found — whale fat. With these materials, candles became the most popular and most advanced lighting technology.

Industrial boom and its lighting technologies

The industrial revolution changed the world as it was before. Today we don’t imagine the world without industry, and the industrial boom period was significant for the history of lighting. The industrial revolution brought materials such as coal, petroleum, and gas to create electricity and lighting.

William Murdoch was the person who introduces gas lighting to Pelham Street in Newport, Rhode Island. It was the first city in the world to adopt new street lighting technology in 1792. Soon after that, gas lamps got their acceleration. During few decades, many types of gas were tested, including methane, acetylene, butane, propane, hydrogen, and even natural gas.

Electric lighting as we know it today

The next step in lighting technologies evolution was, actually, the last one — electric lamps. That is what we are still using until these days. Humphry Davy introduced the first electric lamp. He invented a very early form of the so-called arc lamp. This technology is something that we are using until these days. Davy lamp was the very beginning of the electric lighting era.

In 1879 American inventor Thomas Edison invented the long-lasting incandescent lighting bulb. On October 22 ndof the very same year, his lamp burned for 13 and a half hours, and that was the breaking point in the electric lighting revolution. Only a few months later, Edison invented carbonized bamboo filament, which helped the lamp burn up to 1200 hours.

After these inventions, more technologies came. In 1900 first fluorescent lights were introduced by Daniel McFarlan. Parallelly to this invention, Peter Cooper Hewitt presented his mercury-vapor light. The 20 thcentury brought us high-intensity discharge (HID) lamps, and later in the same century came a technology that is still extremely popular until these days — LED.

Long story short

Humans already live on the planet Earth for more than 6 million years. That is a vast number. The modern form of humans evolved about 200 000 years ago. However, lighting as a technology in our lives dates back to 1–1,5 million years B.C., and that is by far one of the oldest technologies in the world. Of course, the last 100 years were crazy for every technology, and lighting is no exception. Today we have lights everywhere, but I firmly believe that the history of lighting has just begun.",4713
"Entertainment Lawyer Tifanie Jodeh Acosta Joins Mogul Productions as Head of Business Affairs Mogul Marketing Follow Jun 17 · 2 min read

Acosta joins Mogul as their film financing team grows

Mogul Productions, the decentralized film financing (DeFiFi) platform and movie-based NFT marketplace, today announces entertainment lawyer Tifanie J. Acosta as the project’s new Head of Business Affairs.

As the Founder and Managing Partner of Los Angeles-based Entertainment Law Partners, Acosta brings with her a plethora of experience representing actors, performers, writers, directors, musicians, executive producers, public relations companies, managers and producers.

“We’re delighted to welcome Tifanie to the Mogul Team,” said Mogul Productions Founder & President, Lisa Sun. “Her insider-level knowledge of the entertainment industry is unparalleled, and we can’t wait to tap into the deep well of experiences that she brings to Mogul’s film team.”

Added Acosta: “I’m inspired by the idea of turning film production into a more inclusive, equitable and accessible process for both fans and key stakeholders. I’m excited to join the team in deal making across Mogul’s film platform and advance the company’s key business priorities.”

Acosta’s multifaceted career extends over 20 years handling production, legal and business affairs on such projects as “Greenland” (starring Gerard Butler), “Breaking News in Yuba County” (starring Allison Janney and Mila Kunis), “Chappaquiddick” (starring Jason Clarke) and “The Stand-In” (starring Drew Barrymore). She started her career in the business affairs department at Nancy Cartwright’s company (voice of Bart Simpson) and was later the general counsel of TFN, The Football Network where Acosta was responsible for a range of issues pertaining to production, business and legal affairs, intellectual property, financing, standards and practices, and more.

Acosta has been recognized as a “Viv Magnificent Woman”by VIV Magazine, and has lectured on the business of film and television at Louisiana State University, Slamdance Film Festival, San Pedro Film Festival, Hollywood Black Film Festival, Film-Com and more. Jodeh is an Emmy voting member of the Television Academy. She regularly blogs about the entertainment industry and posts noteworthy columns featured at http://entlawpartners.blogspot.com/

Acosta joins a leadership team led by Sun, along with Co-Chairs of Film Financing Paul Sparkes & Gorav Seth.

About Mogul Productions (Mogul)

Mogul Productions is a decentralized film financing (DeFiFi) platform that connects creators, movie fans, and film financiers in one space to ensure the best films get made by giving everyone a voice. By leveraging blockchain technology, NFTs and a tokenized system, Mogul incentivizes participation and rewards engagement. Using the Mogul in-app payment and utility token (STARS), users can vote on, greenlight, and participate in key decision-making aspects of production.

Website: https://www.mogulproductions.com/

Twitter: https://twitter.com/mogulofficial_",3061
"Machine Learning Model Deployment

Source: Become a Machine Learning Engineer Nanodegree

To fully grasp the power of Machine Learning, we ought to learn about the model deployment phase. This phase of the Machine Learning workflow is often left out of the popular Machine Learning courses, leaving many confused about what to do next once they've developed their champion model.

Just to clarify, Machine Learning is leveraged to solve problems by the use of data and improving automatically through experience. For a model to be able to solve problems in the real world it must be able to be accessed and for this to occur it must be effectively deployed into production.

In other words, we maximize the value of Machine Learning models by reliably extracting their predictions and sharing them with other systems.

The Meaning of Model Deployment

Deployment is one of the last stages of the Machine Learning workflow. In this stage, we seek to integrate our machine learning model into a production environment so that it can be used by other software systems to solve the problem it was tasked with solving from its inception.

The Deployment of a Machine Learning Model; Image by Author

When we deploy a Machine Learning model, we are not only talking about the model itself. Model deployment should really be called ML Pipeline deployment for the mere fact we deploy the entire pipeline.

A Machine Learning pipeline simply defines how we codify and automate the machine learning workflow. Typically, it would consist of multiple sequential steps that would allow us to go from retrieving the raw data to a prediction.

We deploy the pipeline because, in both the research and production environment, we are likely to obtain the data in a raw state. In the research environment, we must make the necessary transformations to the data for it to be fed into a Machine Learning model, and in a production environment, we want to replicate the model we developed in the research environment so our results do not vary once the model is in production.

This brings about extremely difficult challenges.

The Challenges of Machine Learning Deployments

Machine Learning projects can fail for a variety of reasons of which include deployment. There are few unique features involved in Machine Learning that make deployment at scale extremely difficult. Here’s a non-exhaustive list of some of the challenges of model deployment:

The Language Barrier

It’s no doubt Python and R are by far the most popular languages used for Machine Learning. While Data Scientists are busy fighting over which language is better, they tend to forget the end goal is to integrate the model as part of a larger system so it could be used to solve problems. This is where the problem arises.

Typically, production systems are built with programming languages other than R or Python (i.e. C++, Java, etc). Porting a Python or R model into a production language is an extremely complicated task, hence the general solution is to recode the model using the production language. This creates 2 problems 1) recoding the model using the production language takes time which is a cost in business and 2) we risk being able to reproduce the research environment that was programmed in Python or R.

Traditional Software Challenges

Machine Learning code is only a small part of a larger software system, hence it can be thought of as a software engineering project which implies Machine Learning projects inherit all of the challenges of traditional software projects. Here’s a few examples of the challenges faced in software projects:

Reliability — The ability of the software to produce the intended result and to be robust to errors

— The ability of the software to produce the intended result and to be robust to errors Reusability — The ability of the software to be used across systems and projects

— The ability of the software to be used across systems and projects Maintainability — The ease with which the software can be maintained

— The ease with which the software can be maintained Flexibility — The ability to add or remove functionality.

Challenges Specific to Machine Learning — Reproducibility

In a Machine Learning context, reproducibility describes the ability of the software to produce the same result when provided with the same input data across systems. This is an extremely important part of Machine Learning hence why I’ll dive deeper into this in a future article.

The Coordination of Various Teams

Machine Learning model deployments involve various teams within an organization. Teams can include Data Scientists (who are responsible for developing the model), Machine Learning Engineers/Software Developers (who are responsible for putting the model into production), and business professionals (who have a good understanding of how the model is going to be used to used in the organization, the problems the model should be solving, and the customers.

The purpose of such diverse teams coming together is to ensure the model is working reliably and ensure the model delivers the intended results.

Wrap Up

Machine Learning model deployment is one of the last stages of the Machine Learning workflow. For whatever reason, it’s often left out of Machine Learning courses which typically tends to leave beginners confused once they’ve built a model that is acceptable for the business problem. Model deployment has its own unique features that make it an extremely challenging task. In future writings, I will hone in further on the reproducibility aspect of the Machine Learning Pipeline as this is a major factor when we want to deploy Machine Learning models.

Thank You for Reading!

If you enjoyed this article, connect with me by subscribing to my FREE weekly newsletter. Never miss a post I make about Artificial Intelligence, Data Science, and Freelancing.

Related Articles",5898
"What Makes A Successful Data Scientist? 5 Traits to Success

Photo by Fab Lentz on Unsplash

An important question that we all ask ourselves at some point through our data science learning journey is, “what do I need to do or learn to become successful in the field?”. I know I ask myself that equation in every learning journey I went through in life because when we stay on a learning journey, we often want that journey to end up with success.

But, the tricky part about that equation is, the answer varies based on the person's perspective of success. For some, success is getting a high-pay job; for others, it’s learning something new or have a job where you learn a new thing every day. For me, it is to do something I love and get paid to do it. Not necessarily a big amount of money, but as long as I enjoy what I do, I am successful.

If you ask any data scientist who considers themselves successful or who is viewed as such, they will probably tell you how did they achieve their success from the personal perspective that they gained throughout their journey. But, the interesting part is, although success is personal and subjective, if you ask 100 or even 1000 data scientists, we will end up with some common answers and some unique ones.

Although the unique ones make each learning journey special, the common ones are the ones you probably need to focus on. Because throughout your journey, you will develop your own unique traits for success. So, in this article, I will focus on the top 5 traits that all data scientists would agree you need to obtain to succeed as a data scientist.

№1: Have a curious mind hungry for knowledge

I will order the traits of a successful data scientist based on my personal experience and what I believe is important yet not always addressed. The first trait any data scientist needs to succeed is curiosity and hunger to learn. As I always say, data science is an ongoing field; a lot is happing within it all the time.

So, to succeed in this field, you need to be curious, curious about the next trend in the field, curious about the possible problems that you can solve, and curious about the different ways you can solve them. If you have a curious mind, you will always be learning and always be improving, both essential to succeed in data science.

№2: Can Easily adapt to new data and situations

Data science is an interdisciplinary field; it connects maths, statistics, business, programming, design, and science communication. Because of that, data science applications are endless; there are problems data science can solve in every field. So, adaptability is an important trait for any data scientist to obtain.

Being able to adapt to new tools, to new algorithms, new data, new problems, new teammates, and new situations. This is getting more important if you are a freelance data scientist or changing jobs. The ability to apply your knowledge to any application and data can make you either build a great project or fail to come above the crowd.

№3: Critical thinker and problem solver

One of the main tasks expected from data scientists is to find trends, patterns and obtain insights from given data. The key to finding these insights and trends and making use of them is critical thinking and problem-solving techniques. Therefore, you will need to explore data, analyze it and think critically to decide on a model to use in further steps of the project.

Problem-solving and critical thinking also help you decide on which tools to use and which decisions to make. Thus, utilizing these skills can help you save time and effort while working on your project. An important thing to mention here is, your skills to think about data critically will improve as you advance in your career.

№4: Have good communication skills

Having good communication skills is, in my opinion, one of the most essential yet overlooked soft skill every data scientist need to master. Imagine spending hours working on some data, analyzing it, cleaning it, visualizing it, yet you can’t explain your findings to others. Then, all your work would be for nothing because if they can’t understand your findings they won’t be able to act upon it.

A data scientist needs to be good in science communications, and they need to be good at simplifying concepts and only focusing on what really matters. In addition, they should be able to give actionable advice and be good at utilizing tools to create effective visualizations that deliver a clear message.

№5: Have solid technical skills

I intentionally left technical skills to be the last on my list. That’s not to say that technical skills are not important. In fact, you probably can’t become a data scientist without solid technical skills. But, if you’re trying to get into data science or already have, and as the rest of us are working on your skills daily, then you know the technical skills are the core focus of many online materials.

So, you will learn the technical skills anyway because that the skills that are often tested during the job hunting process. I put technical skills last because the other 4 traits I mentioned are often overlooked or not focused on, especially new data scientists. Yet, there are essential to building a career in data science.

Takeaways

We all want to be successful, work hard on ourselves, learn new skills, develop the ones we already have, and work on trying to reach a better version of ourselves all the time; that’s just a part of life. It is also an important part of succeeding in tech as a whole and data science in particular.

Data science is an ongoing field; new algorithms and models are designed every day to offer better efficiency and performance and lower error rate. Also, because of the interdisciplinary nature of data science, you will need to learn various topics and concepts that may not seem related at first. Still, their connections appear once you start building projects and getting involved in the field.

It’s important to remember that each has a unique learning journey, a unique experience, and a unique definition of success. Still, within this uniqueness, we all share some common traits that allow us to show our individuality, prove our skills, and become the best data scientist we could be, so, in short, these common traits allow us to succeed.",6341
"10 Remote 2021/2022 Data Science Internships You Should Apply to If You’re a Student

Photo by Sung Shin on Unsplash

As a student, who has been in school almost my entire life, I always felt that the knowledge we learn in school is theoretical. That feeling only grew whenever I talked to my friends and colleagues who worked in the industry or had some experiences outside the university. This gap between what we learn in our degrees and what we actually need to succeed and build a career is one of the reasons internships are necessary for any student.

Although internships are important for basically any student regardless of their major, it’s even more important if you’re a student in a tech field. In tech, we study the history of a field, the tools we can use, and some field applications. But, university classes are never enough to build a real-life experience that will help you launch a career once you graduate.

Here, internships make a difference, not just helping students gain experience and know what to expect when they get a job after graduation. Still, it also helps them decide what career they want to pursue after obtaining their degree. So, if you’re a student, undergrad or post-grad, trying to get an internship, it will definitely change the way you view your field and help you get a job later.

In September of every year, companies worldwide open up the applications for the following year’s summer. So, today, I will walk you through 10 remote internship opportunities for the 2021 fall or 2022 summer that you can apply for now.

№1: Quora Software Engineer — Machine Learning Intern 2022

First on our list is a machine learning internship offered by Quora. In this internship, you will be working on improving Quora’s existing machine learning systems, improving the user experience, and working with other machine learning engineers to implement different algorithms and systems efficiently. To apply for this internship, you need to be a student who knows the basics of machine learning and experience with Python or C++.

№2: Moyyn Machine learning | AI Internship

Next up is a 3 or 6 months machine learning and AI internship offered by Moyyn. Moyyn is a recruitment platform based in Germany that focuses on automating the matchmaking of candidates and companies. This internship will have you working on the algorithm of the platform as well as test the algorithm and optimize it. To apply for this internship, you need to be a student and know NLP, machine learning, and AI basics.

№3: IBM Data Science and Engineering Internship

Next up is a data science internship offered by IBM. This remote opportunity is available for students across Europe or the US, with the chance for international applicants. To apply for this internship, you need to have intermediate Python skills and knowledge of NLP, have a solid foundation of data structures, and be fluent in English.

№4: Woohoo Machine Learning Internship

Woohoo in Singapore offers this remote internship for students with

MS in computer science, knowledge of distributed systems, and software engineering. You also need to have experience building, maintaining, and extending web-scale production systems and know-how to perform AB testing and machine learning algorithms.

№5: Workiva Data Science Internship

Workiva offers a remote internship to anyone residing in the US or has a work permit in the US. During this internship, you will participate in discovery, gathering, and exploratory data analysis and help the machine learning engineers design, develop, and optimize their data pipelines and test large-scale models.

№6: HackerRank Software Engineer Internship

Next up is s remote internship from HackerRank if you reside in India. If you love to write code and connect with other developers and engineers worldwide, then this is the internship for you. To apply for this internship, you will need to solve 3 test challenges and submit your resume; that is it.

№7: Snackpass Data Science Internship

Snackpass, an ordering and dine-in app based in the US, offers this remote internship where you can help answer important business questions for promoting restaurants. Design and build machine learning models for hundreds of users, test and optimize these models for different use cases.

№8: 1Qbit Machine Learning Researcher Internship

If you’re into machine learning and curious about quantum computing, then this internship by 1QBit is ideal for you. This 8 months remote internship will allow you to utilize your machine learning background to address challenging industry problems on both quantum and classical sides and leverage your knowledge to develop new tools and computing capabilities.

№9: Shopify Data Science Internship

Shopify is making opening your own private business as easy as buying a product online. Shopify offers an internship where you can build machine learning models that enhances Shopify’s intelligence and help people make better decisions about their business. You will also get the chance to contribute to the open-source community and make lasting connections.

№10: VidMob Data Science/ ML Internship

Next up is a data science and machine learning internship by VidMob, where you can apply your expertise in quantitative analysis, machine learning, and data visualization to real-life problems. You can also analyze the results of different models to understand their performance better and improve it further.

Takeaways

No matter what degree you go to school for, there is always a gap between what we learn in the university and what we will actually do after we graduate and start looking for jobs. Not just that, but often, after we graduate and start going through the job-hunting nightmare — you better start if before you graduate, though — we realize that we don’t even have the experience needed for an entire level position.

Thankfully, we can gain the experience we need and make some money in the process by trying to obtain an internship in a company. Often most companies open up chances for internships during the summer vacation, and applying for these internships opens during the fall of the year before. So, basically, now is the ideal time to start looking and applying for next year’s summer internships.

Since covid hit, all aspects of our lives got disturbed, including the chances to apply for internships abroad. However, because of that, many companies are now allowing the chance for remote internships, with the possibility to make it in-person if things get a little better before next summer.

In this article, I proposed 10 internships for this fall or next summer that you can apply for today to gain the experience you need to land the job you want after you graduate. Good luck…",6776
"June 21 — July 4

Afterpay in Fashion

Afterpay, an Australian “Buy Now Pay Later” fintech founded in 2015 by Nick Molnar and Anthony Eisen, has recently entered the luxury fashion scene with its debut in Afterpay Australian Fashion Week (AAFW) and later this September with New York Fashion Week (NYFW). What this means for the retail industries is that consumers have more access to fashion brands, and Afterpay is able to help consumes finance for such purchases. In particular, the partnership with NYFW generates more economic value than any other fashion event around the world — last year generating ~$900 million for the city of New York via new jobs and tax revenue. Given the COVID-19 pandemic, the fashion industry faced many setbacks as consumers did not have disposable income to spend on fashion and luxury goods.

Leslie Russo, President of IMG’s Fashion Events and Properties, said: “Our industry has faced countless setbacks during the pandemic, but our collective resilience is unparalleled. With our new partners at Afterpay, we are proudly committing to not only reignite the commerce and creativity showcased at America’s most critical fashion industry event, but revitalize New York’s fashion and retail sectors at large.”

The partnership with Afterpay helps set a new precedent in fashion that offers inclusion, accessibility, and sustainability to both retailers and consumers. Afterpay allows for access to certain brands and clothes that many consumers could not afford fully at point-of-sale, which benefits both the consumer and retailer as the retailer is able to generate more sales and revenue from an increase in consumers — essentially, Afterpay is helping to ensure that top American designers continue to lead the world in creativity, innovation, and accessible fashion.

2. Amazon’s potential break up with Chase

Chase and Amazon first partnered together back in 2002, but according to a Bloomberg report, Amazon is looking at bids to take over the Amazon Prime Rewards Visa Signature Card and its portfolio. A possible reason for this break up is that this card portfolio holds up more than $15B in loans and users are spending more than $50B per year — making it hard for Chase to turn a profit given the perks with the card. Over the years, banks have been loading up their cards with tons of perks (points, rewards, and cash back), making it hard for the banks to profit — especially when the profits are split with Visa and Mastercard. In particular, as a Prime member, the Amazon Prime Rewards Card earns 5% cash back on Amazon and Whole Foods purchases; cardholders get 2% cash back at restaurants, gas stations, and drugstores and 1% back on everything else.

American Express and Synchrony Financial are some of the companies who are bidding to take up Amazon’s card portfolio. For Chase, the partnership might end as the portfolio might not be as profitable as they envisioned.

It’s interesting to see an exit from Chase in this partnership considering the volume Amazon brings in; however, this might show how relying on legacy payment options (credit/debit cards) might not be as profitable for banks. With the emergence of non-card payment products like “BNPL” from companies like Afterpay and Klarna, the traditional banks might start to experiment in-house both to compete with fintechs, but also to find other channels of profit.

3. Western Union vs. Fintechs

Western Union, an American financial services company primarily operating in the cross-border payments space for 145 years, has fallen target to many fintechs including Wise, Remitly, and WorldRemit. Western Union seems up to the challenge as they have been investing in its own digital operations through partnerships with and acquisitions of fintechs and also creating their own digital products and applications.

COVID-19 rapidly accelerated digitalization in all verticals, especially the financial services vertical. Last year, Western Union had an increase of 38% in overall digital money transfers and saw an increase of 30% in annual active customers to 8.6 million.

Western Union has been actively ensuring that they do not get eaten up by the fintechs. Looking at their mobile app, users can track a money transfer, look at exchange rates, send money around the world, pay bills, and everything is encrypted and protected — adding in a secure and seamless experience for the user, which is what most fintechs aim to do. Also, Western Union has been migrating most of their applications to the AWS Cloud Platform — overall mining 33 data warehouses and storing 20 perabytes of data in the cloud. The company is starting to use artificial intelligence and machine learning for their front office operations and recently launched a telephone money transfer product where the customer is able to talk to a 24/7 virtual assistant.

It seems WU has been taking an active role in the digitalization of their products, services, and company. I’m hoping that the competition between fintechs and legacy financial services companies like Western Union help facilitate for better services for consumers and in turn transform banking and financial services into a fully digitalized and seamless experience, away from its archaic forms of operations.

4. Bank’s role in accelerating the digital banking transformation

When we look at banks and fintechs, most assume that fintechs are disrupting traditional banking business; however, when traditional banks and fintechs work together, it benefits the entire ecosystem of banking, technology, and software. There are a couple of reasons why banks should collaborate and work with fintechs.

First, the switch towards digitalization catalyzed by the COVID-19 pandemic gave sense for online and mobile banking with 71% of Americans regularly using online banking and 43% regularly using mobile banking. With millions of Americans unable to enter a physical bank branch, banks have been creating more optionality to bank via computer, tablet, or phone. With the help of fintechs, banks would be able to roll out products that consumers can use quicker and integrate a seamless experience for their customers. Second, expansion for both parties are at play. The tech companies can use banks’ help with expanding into new markets while benefiting from the regulatory status of banks, meanwhile banks are able to benefit from the technology companies’ expertise in creating, testing, and deploying products quickly without having to spend on creating products in-house at the bank. The collaboration between banks and fintech companies are essential for how the future of the financial services sector will look like and how we pay our bills, get paid, invest, save money, and buy goods/services. In order to create a seamless experience for clients and consumers, banks need to stop playing “catch-up” with the quick-paced fintechs and either step it up and create disruptive products themselves to compete or choose to partner with the disruptors in banking.

Hope everyone had a great 4th of July weekend! :)",7071
"Innovation of great magnitude is often sparked by a single new idea.

When Alexander Graham Bell patented his design for a telephone, he couldn’t have imagined how his invention would evolve over time. Telephones, once a luxury of wealthy individuals and large corporations, required operators to control their networks until the 1920s. Today, there are over 6 billion people on the planet with smartphones. What started as a device that could merely transmit speech evolved into a digital wallet, online shopping portal, state of the art camera, and much more.

And that’s just one obvious example. You really don’t have to look too far these days to see the ways in which innovation is positively impacting our lives, often resulting in simplification, reduced costs and greater choice to us as consumers.

Of course, the innovations in a business that primarily services other large institutions will generally not be as well publicized; however, they can be equally disruptive or even ground-breaking.

While the examples of innovation in the business where I’ve spent my whole career — securities servicing and administration — may not be as exciting as consumer products, the point is that even in a very traditional, non-glamorous business like mine, we are all looking at ways to take that next leap forward because we know the world isn’t stopping for anyone. What underpins these sea changes is equally fascinating. I’ve included below some examples of recent innovations in my world.

Select Your Best Team

It starts and ends here as far as I’m concerned. Ideas, and the ability to execute on them, come from people.

As you may know, I’m a huge soccer fan. The coaches of some of the best teams to ever play the game weren’t great players themselves; they probably weren’t even great coaches in the purest sense of the word. They had other people to do that. But what they did have was an ability to assess strengths and weaknesses within their own set-up — to bring in specialists to plug the gaps and to buy players to bring something unique to the team.

When we were looking to start a brand new business a couple of years ago, we needed to approach things differently too because we weren’t up against our traditional competitors, so we went straight to the source and hired a visionary from one of our clients. Who’s going to know what clients really want better than client themselves, right? Since then we’ve continued to hire the talent to drive that business forward — people who may not have considered the financial services industry before as a career choice; there has to be room for talent like that these days, even in banking. That business is now a roaring success. Innovation often comes from diversity of thought — and that probably means people who’ve had different work and life experiences.

Evolve the Culture

Okay, so you’ve hired the right people. What you don’t do next is put them in an environment that shackles their creativity — because they’ll be gone as quickly as you found them.

Of course, there are parameters. We still work for a bank with important financial and regulatory considerations. But evolving the culture is a necessity so that ideas can be brought forward freely; where you can test and trial concepts quickly; where radical candor and challenge are embraced and encouraged, along with collaboration and team work — and that starts from the top. As teams witness their leaders, managers and line supervisors living that out in practice on a daily basis, the culture of the organization moves forward and ideas start to pop up all over the place.

For example, last year, as the pandemic kicked in, our teams in India had very little time to set themselves up for what they thought would be a “temporary” lockdown. As time passed, we started hearing stories of people struggling to get access to basic food and medical supplies. At a corporate level, we took a number of steps to help out but one of the most effective solutions was developed by a small team on the ground.

Using Google Maps, the team built an app that enabled them to see where other team members were located around the city. As a result, they were able to stay connected, get supplies to those in need, and look out for each other, whether they were working or not.

This wasn’t a corporate initiative or a management suggestion — it was a team on the ground saying, “We’ve got a problem; we feel empowered enough to do something about it; we’re going to build a phone app.” That’s a culture of true innovation.

Keep Pushing Boundaries

I’ve always believed that with the right people and the right culture you can pretty much do anything. That’s not to say you’re ever done in those areas — you have to keep evolving and moving forward or you can get complacent. But with those building blocks in place, things start to happen.

Over the last few years, I’ve seen us launch this industry’s first deployment of blockchain technology for the private equity market in the Channel Islands; and partner with BondEvalue in Singapore to complete the world’s first blockchain-based bond trade. I’ve witnessed teams come together to create a brand new collection of front office capabilities called Investment Data Science — effectively inventing a whole new market segment. And most recently, partnered with Standard Chartered to launch Zodia, a cryptocurrency custodian for institutional investors. And the innovative ideas continue to flow!

Keep looking outwards — that is the key. Inspiration may come from unexpected places.

To that end, I would encourage anyone — leaders and managers in particular — to make a concerted effort to engage with people from outside their industry. Don’t just attend your regular industry conference events, try something adjacent, or even completely left field. There are so many commonalities in the challenges faced by what you think may be unrelated industries: the further integration and deployment of technology to boost productivity, ways to improve the client experience, and so on.

We can all learn and take things from each other — and the good news is that no one has cracked it yet. There is still plenty of room for everyone to play.",6213
"Any hope humans have for an off-world future relies on several factors for survival. One of the most important? Water. Continuously shipping water across the galaxy to resupply astronauts requires extraordinary expense in transportation costs. The next planet humans inhabit will need to have access to a local supply. Scientists have labored to locate water on Mars but finding it was only the first step. Now scientists and engineers need to tap into this supply which, given the harsh environmental conditions on Mars, isn’t as easy as it sounds.

In January, NASA released the Mars Rodwell Experiment Final Report, documenting a series of tests and analyses led by Dr. Stephen Hoffman, Senior Engineer Specialist at The Aerospace Corporation.

The team investigated the use of a Rodriguez Well — a concept developed decades ago by the U.S. Army — as one of many approaches for extracting water from the massive ice deposits on Mars. A series of lab-scale Rodriguez Well tests were performed by Hoffman and Alida Andrews of Aerospace at the Johnson Space Center (JSC) Energy Systems Test Area facility. Using Mars-equivalent environmental factors such as atmospheric pressure and density, test results were used to replace terrestrial environmental factors with modeled Martian equivalents.

We spoke with Dr. Hoffman about water of Mars, how humans can access it and what happens next.

Is there water on Mars? If so, how much?

There is actually quite a bit and scientists are finding more deposits as the instrumentation they use in the search improves. For decades we’ve known that water ice exists at the poles on Mars — Earth-based telescopes could detect it using spectrometers. But early spacecraft flying by or orbiting the planet found a desert-like landscape at lower latitudes. For many years, it was assumed Mars had lost most of the water responsible for creating terrain features that appeared to be lakes, rivers, flood plains, and even shorelines for ocean-scale bodies of water. There has been an ongoing effort by scientists to understand what happened to all of this water. Orbiting spacecraft have carried more sophisticated instrumentation designed to answer this question.

NASA’s Phoenix Mars Lander shows the trench, called ‘Dodo-Goldilocks,’ lacking lumps of ice seen previously. The ice had sublimated, a process similar to evaporation, over the course of four days. Credit: NASA/JPL-Caltech/University of Arizona/Texas A&M

Liquid water cannot exist on the surface of Mars under present environmental conditions. The atmospheric pressure at the surface is approximately 5–10 millibars — about 1% of sea level pressure on Earth. Mars atmospheric temperatures can range from -140 C to +30 C (-284 F to +86F). These conditions are near the triple point of water, but for the most part water exists only as ice or vapor at the surface of Mars unless some other special circumstances exist.

Scientists used two orbiting radars, named MARSIS and SHARAD, to look for liquid water aquifers below the surface where conditions would allow liquid water to exist. Following a global survey of Mars, no aquifers were found down to a depth of about 300 meters.

Using high resolution imagers and other remote sensing instruments also in orbit around Mars, scientists have begun to realize that there is a great deal of buried ice in the mid latitude — from roughly 35 degrees to 50 degrees — in both hemispheres. This ice is protected from surface conditions by a layer of sand, gravel, and dust. These deposits are occasionally revealed when small meteorites strike the surface and scatter very distinct white ice across the surface and these strikes are quite visible from orbit. Scientists have also spotted ice cliffs measuring 10s of meters in height in areas where some unknown event has exposed part of a buried ice deposit and the ice has slowly sublimated, revealing more and more of the deposit. A recent NASA-sponsored study called Subsurface Water Ice Mapping, or SWIM, has begun to correlate all of the independent data sets possibly indicating the presence of water to understand how much water is on Mars and where it is located.

To answer the original question, the estimates for the amount of water on Mars continues to evolve but it is safe to say that the total volume is many, many cubic kilometers.

How significant is mining this water to colonizing Mars?

Successful colonization is a long way off for many practical reasons. There is a great deal we still need to learn about Mars. But even early human missions of exploration and reconnaissance could benefit from access to significant quantities of water. There are technically feasible approaches to these early human missions in which everything needed for the mission is brought from Earth. Anything the crew can find and access on Mars means savings of many times its mass in rocket propellant and hardware that does not need to be transported.

Water is a very good example of this type of material. Water can be used for the obvious things like potable water and breathing gases for direct use by the crew, and even rocket propellant to launch the crew off the surface if electrolyzed into its constituent elements. If early exploration missions lead to a long-term presence then water will also be used in as many applications as it is known for here on Earth and its value will increase in proportion to the number of uses.

What is required to mine for water on Mars? What are some of the challenges?

Scientists have identified more and more significant deposits of ice on Mars. For purposes of this question, I would divide these deposits into two broad categories: those in which ice is mixed with significant quantities of dust or rocky material and those in which ice is essentially pure, i.e., greater than about 95% ice, the current limit of instruments to resolve the content of these deposits.

For ice mixed with dust or rocks, mining would require excavating the material and likely heating it to capture and condense the vapor. However, ice mixed with rocky material can be as hard as concrete and can be similarly difficult to excavate.

The pure ice deposits are typically covered by a layer of sand, gravel, and dust. Mining these deposits could be accomplished by stripping away this protective layer and excavating the ice. This approach would face similar difficulties as excavating the ice-mixed-with-rocks deposits.

Another method would be to drill through the overburden of sand, gravel, and dust and into the ice deposit where something called a Rodriguez Well could be established. This approach has been the focus of our applied research. It would face difficulties similar to drilling a water well here on Earth coupled with the unique aspects of establishing and maintaining a Rodriguez Well.

What is a Rodriguez Well? Why was it chosen for study?

The Rodriguez Well was developed by U.S. Army engineer Raul Rodriguez at Camp Century in Greenland during the early 1960s. A Rodriguez Well uses heat and a submersible pump to create a cavity filled with water deep under a glacier’s surface. The submersible pump is used to cycle heated water in the cavity, return cooler water to the surface, and siphon a portion of the flow for consumption before reheating the rest and sending it back down to the cavity. Diesel-electric generators in use at many of the field stations constructed on the Greenland ice sheet in the 1960s provided a “free” source of “waste” heat to make a Rodriguez Well and provide potable water. The Rodriguez Well was used operationally at several locations in Greenland, in addition to the well-known Camp Century.

The Rodriguez Well concept was tested at the National Science Foundation’s (NSF) Amundsen-Scott South Pole Station in the early 1970’s. When a major reconstruction of the South Pole Station was started in the mid 90’s the Rodriguez Well was chosen to provide potable water for the Station. South Pole Station still relies on a Rodriguez Well to this day. The Station is on its third well; the others were abandoned when the water pool reached a depth at which the pumps could no longer lift water to the surface.

I became aware of the Rodriguez Well concept while working with the U.S. Army’s Cold Regions Research and Engineering Laboratory (CRREL) on other NASA-related tasks. However, it was not until relatively recent discovery and confirmation of substantial bodies of ice in the Martian mid-latitudes that the feasibility of using a Rodriguez Well warranted a serious look at its application for human missions on Mars. The case for using the Rodriguez Well on Mars is compelling because of its relative simplicity and maturity, as well as the number of places and duration of use here on Earth. However, environmental conditions on Mars are different from here on Earth in several significant ways. Additional applied research is necessary to understand the changes in hardware or operations that may be required to make a significant commitment to using this technology on Mars.

How do you mimic the Mars environment?

It depends on what aspect of Mars you want to mimic. In our case we wanted to mimic the atmospheric composition, temperature, and pressures. We used a small bell jar facility available to us at the NASA Johnson Space Center. This bell jar can reach near vacuum pressures and cryogenic temperatures — both much more extreme than needed to mimic conditions on Mars. This bell jar has a testing volume measuring approximately two feet in diameter and two feet tall. We scaled our equipment to fit in this space, but this was sufficient for the initial testing. As we progress to more sophisticated tests, we will require larger volumes, but test chambers already exist at JSC and other NASA facilities to meet these needs.

What comes next in your work on this?

Our work so far has given us data we can use to customize computer simulation tools developed by CRREL for terrestrial use, allowing use for Mars applications. The data we have in hand are very basic — there are nuances that need to be explored with tests similar to those we have already completed.

Once we are more comfortable with how we think the Rodriguez Well will perform on Mars, we will confirm these findings by establishing and operating subscale versions of these wells in appropriately sized test chambers simulating Martian environmental conditions. These results will, in turn, be used to develop equipment and operations that can be demonstrated on Mars under actual conditions. At that point, we should know enough to determine whether this technology is effective and reliable enough to become part of the critical infrastructure supporting human crews on Mars.",10741
"Benchmarking some popular Redis client libraries for Go to find the fastest one.

Motivation

Increasing RPS (request per second) is not an easy task. Changing your code to increase performance is even more challenging. There are several popular libraries out there, and almost every single one claims to be the fastest. The problem arises when the benchmarking result is out of date. Even worse when the one who did the benchmark tunes the library in a non-general way. Hence, the achieve the same performance is pretty much impossible for the general user.

The purpose of the benchmark is to find the fastest Redis client library for Go with a controlled environment and without any magic hack to tune any of the libraries.

Environment

Go client libraries used:

Max connection pool size 100 and 1000.

Cluster mode on and off with replica size of 5. Except for gomodule/redigo which currently do not support a cluster client mode.

Benchmarking Result

The result below is the average result after running the benchmark 25 times.

Understanding the Result

There are four columns on the right side of each Redis operation like GET included in the result above. The meaning for each one of them is:

iteration => the number of iterations of the operation run, a higher value is better .

=> the number of iterations of the operation run, . ns/op => this is the average time each function call takes to complete, a smaller value is better .

=> this is the average time each function call takes to complete, . B/op => this is the average memory used for each function call, a smaller value is better.

=> this is the average memory used for each function call, allocs/o => this is the average memory allocation for each function call, a smaller value is better.

We can see go-redis/redis/v8/cluster-pool:1000 outperformed other libraries almost across all the operation benchmarks. Excluding the EXPIRE operation where mediocregopher/radix/v3/cluster-pool:1000 actually outperformed it by a small margin.

Another interesting result such as that gomodule/redigo-pool:100 actually outperformed gomodule/redigo-pool:1000 on a certain operation like HGET. This is most likely because the increase of the connection pool increases the competition for resource allocation in a limited resource environment. Having a small connection pool allows the previous operation to be done first before handling the next one. Since there is no resource allocation competition it actually can perform better than one with a higher connection pool. It means we shouldn’t blindly increase the max connection pool and believe that the operation will go faster.

Conclusion

By looking at the result above, we can conclude that go-redis/redis/v8 with cluster mode on is the overall fastest Redis client library for Go. It’s recommended to use it with cluster mode on, and with dedicated max connection pool size based on your own environment. Some testing is required to find the balance max connection pool size depending on your most heavy operations.",3037
"Ambrosus Ecosystem joins forces with an Asian business accelerator: providing proof of origins, traceability, and bringing Ukrainian FMCG businesses to Singapore Ambrosus Follow Jun 17 · 2 min read

We are excited about our newly sealed partnership with Business Engineers Asia (BEA), a leading Singaporean business incubator. BEA is relaunching its Best of Ukraine campaign this year, with an initiative Ukraine 30@SG to help 30 Ukrainian brands enter the Singapore retail market. Ambrosus Ecosystem will be providing proof of origins and supply chain traceability for bringing Ukrainian FMCG businesses to Singapore.

Developing global supply chains

Supporting Ukrainian businesses will benefit global blockchain-based supply chains, as the country is well-known for its high-quality agricultural products, which have strong potential for international trade. With the backing of Ambrosus Ecosystem security and traceability, Ukraine’s exported products will be tracked and delivered safely, benefiting all interested stakeholders and end consumers. Ukrainian companies are now able to utilize the Ambrosus blockchain to prove their competitiveness in the ASEAN market. As a result, Ambrosus blockchain adds a higher level of value proposition to the promising Ukrainian firms.

Ambrosus Ecosystem takes the lead

Our team is thrilled to unlock access to new markets for companies from developing countries that are looking for proof of origins and traceability solutions. The BEA initiative in Ukraine is a fantastic opportunity to diversify use cases of Ambrosus and benefit the crypto community. The supporters of our ecosystem will, too, benefit from new bundles, nodes, and, generally, from being a part of an international program.

Sergii, the head of business development at Ambrosus Ecosystem, reinforces that our blockchain solutions are essential and will act as the bridge of automated quality control between the Ukrainian and Singaporean markets. Sergii also highlights that

It has become apparent that the global recession called for a need to trade in a faster and more precise manner. And within the context of that, the digitization trend we see happening across the globe happens to smoothen out the borders between countries step by step. However, there may still be a lack of trust between parties. Ambrosus provides developing countries with its blockchain ecosystem and dedicated solutions for parties to prove and verify the origin of their products, bringing transparency and creating an environment of trust.

About the Initiative

The BEA entity in Ukraine oversees companies in the FMCG and retail industry in their market entry strategy into the Singaporean market. This year celebrates the 30th anniversary of Ukrainian independence, so BEA is relaunching its Best of Ukraine campaign, introducing promising Ukrainian businesses to the ASEAN region this year.

The Ambrosus Ecosystem team is honored to bring blockchain-powered change and innovation to Ukrainian companies in the prospective region together with our new partners.",3064
"Astra can give some certainty.

Astra uniquely provides a way for decentralized organizations to comply with rules and regulations worldwide while remaining decentralized. At a pivotal time for the crypto world, Astra can give some certainty.

DeFi platforms offer higher yields than traditional financial platforms, making them very attractive to users. A staggering 11% of young Americans invested their stimulus checks into cryptocurrency platforms, showing just how prevalent the technology has become. Innovative as decentralized finance is, the world of crypto is still in its infancy. With the rise in popularity comes an increase in hacks and other types of criminal activity, which authorities are keen to stamp out. Regulatory bodies are already getting their teeth stuck into those that appear to flout the rules. On 10 August 2021, the cryptocurrency derivatives trading platform, BitMEX, was ordered to pay $100 million in a civil monetary penalty. Acting director of enforcement at CFTC, Vincent McGonagle, says, “Cryptocurrency trading platforms conducting business in the U.S. must obtain the appropriate registration and must implement robust Know-Your-Customer and Anti-Money Laundering procedures.” If they don’t, they’ll pay the price.

Astra is a fully decentralized platform that performs the required KYC, AML, and other compliance checks on behalf of lending and borrowing applications and other DeFi protocols.

In a speech made before the European Parliament Committee on Economic and Monetary Affairs, Chair of the U.S. Securities and Exchange Commission (SEC) Gary Gensler said, “absent clear investor protection obligations on these platforms, the investing public is left vulnerable. Unfortunately, this asset class has been rife with fraud, scams, and abuse in certain applications.”

He goes on to say, “for those who want to encourage innovations in crypto, I’d like to note that financial innovations throughout history don’t long thrive outside of public policy frameworks. In finance, that’s about protecting investors and consumers, guarding against illicit activity, and ensuring financial stability.”

The message is clear, by following the rules and regulations set out for traditional financial institutions, DeFi can become a safe and profitable environment for all. However, the challenge of achieving compliance in a decentralized world is still a significant hurdle. Fortunately, Astra is here to help. Astra is a fully decentralized platform that performs the required KYC, AML, and other compliance checks on behalf of lending and borrowing applications and other DeFi protocols. Our technology allows decentralized organizations to comply with rules set out by the SEC and other regulatory bodies across the world without compromising the notion of decentralization. We aim to create a financial world that protects investors and consumers, enabling innovative platforms to flourish.",2931
"Since the end of OMEN emissions, folks have been asking one question about Augury Finance: What comes next? Today, we’re going to answer that question.

Augury Redesign

A complete redesign of Augury Finance is on the way!

Augury Redesign — Home

Augury Redesign — Infinitum

Augury Redesign — Infusions

Augury Redesign — Vaults

We’re anticipating rolling this redesign out before September 30, 2021.

Infinitum

Infinitum launches on Wednesday, September 8, 2021. It will be incorporated into the current Augury Finance design until we complete our redesign (above). Infinitum is a token that has an increasing price floor and can always be redeemed for USDC.

To learn more about Infinitum, you can view our white paper here: https://augury.finance/INFINITUM_AUGURY.pdf

You can also read our poster on Infinitum below:

Augury Pylon

We’re bringing Single-Staking back! Yes, you read that right. Augury Pylon is our new, proprietary system that allows us to leverage our in-house liquidity (our operational investment capital) in order to offer you rewards for staking. Since we are leveraging our investment capital, we can offer you OMEN again without turning on emissions.

To start, there will be a single staking pool made available for OMEN stakeholders. By placing your OMEN in the OMEN Pylon, you will receive 90% Infinitum and 10% OMEN. We are aiming to release Pylon the week of September 12th.

We will be monitoring our pylon system closely and will introduce other single staking options (e.g. MATIC, wBTC, wETH, etc.) that will give you the opportunity to earn both Infinitum and OMEN every single day!

Buybacks

Our OMEN buyback program is alive and working well. We’ve purchased over 1.2 million more OMEN over the past 3 weeks (~2.75%+ of the total supply). We will continue to make periodic purchases of OMEN, as we believe in the future of OMEN and the products that will continue to add value to the OMEN holders.

Things to do with OMEN

Earn Dividends: You earn dividends by keeping your OMEN in any of the following areas: single-stake, LP (portals), and/or the OMEN vault. Earn Infinitum + More Omen: By staking OMEN in the Augury Pylons you can earn Infinitum and even more OMEN. Earn Airmail: You earn airmail by keeping your OMEN in any of the following areas: single-stake, LP (portals), and/or the OMEN vault. Look for a new Airmail at some point during this quarter. Participate in the Cauldron: Future Cauldron offerings will likely require a small contribution of OMEN in order to participate. We’ll unveil this along with our next cauldron offering.

Augury Cauldron

About a month ago we announced a new product that will launch in our Augury Cauldron. We are still working on delivering this product and we should be able to unveil additional information by the end of the month.

Knowledgebase

In addition to all of the above, our team has been hard at work on a brand new knowledge base for Augury Finance. This portal contains information about our products as well as information about our team. To view this information, please visit: https://augurydocs.notion.site/Augury-14a84afddc1a4af8bc07c50f127af6dd

Until next time… Be safe. Be well. We thank you for your support.",3220
"In this small article, I’ll share with you my experience about the certificate on Coursera. As you know, there is a lot of content available online in the area of Data Science where some of are free or paid.

This certificate is one of the paid ones, where you can find the course on:

The Price

This is one of the key points that I think is more important, for this simply reason - there are people that can’t afford a course that probably has the best content, but on the other hand, it costs a lot of money.

This course on Coursera costs you €32 per month, when comparing with the other courses, I think this price is pretty acceptable.

The payment is like a subscription on Netflix, it means that you can cancel at any time without loosing any of the work you have done before.

You can request a financial help to the Coursera which takes you 15 days to receive an answer from them

The Course is self-paced, it means you can ignore the dates that Coursera gives you. They are estimates, you can easily do 1 or 2 weeks of the course in a few hours.

The faster you do the courses, of course, the less you pay. If you are a student or have free time, I recommend doing 1 or 2 courses per week, challenging but doable. Otherwise, do the best thing for you.

Course Structure

When I started attending the course on Coursera (Aug 2020), the course was constituted by 9 courses, now the course as 10 courses.

The new version includes the Course Python Project for Data Science where you learn how to Web Scrapping information from a webpage or API with Beautiful Soup or pandas.

The 10 Courses are:

The Content

On Coursera they indicate this course is beginner level. In my opinion, I think this course is something like “advanced” beginner level because sometimes the code is a little complex for those who are learning how to code for the first time or even for those who are already familiar with it.

Note: In the Course Python for Data Science, AI & Development they teach you the basics of the Python language. I recommend, before attending this course, watch some tutorials, for instance, on YouTube.

In the Course they also teach you the basics of SQL, not covering other concepts as Stored Procedures, Functions, Indexes, etc.

You will learn how to use “classic” Data Science libraries like:

You will also learn how some Machine Learning algorithms work as well what are the phases of Machine Learning project:

Business Requirements

Exploratory analysis

Preprocessing and Data cleaning

Feature engineering

Feature selection

Model selection

Hyper parameter tuning

Model evaluation

Evaluation and Methodology

In the course there are two types of evaluations:

Multiple choice assessments — where you need, for instance, 60% or 80% to pass

Projects — where you submit your code and another student evaluates you

When you submit the project, you have to evaluate other student’s work, the community follows a fair evaluation where every student has to be critical and fair with another student’s work.

The course has very well explained hands-on labs for you to practice and test the code to you to see how the things work.

You have two ways to do the labs:

Locally

Cloud

The projects and labs can be done on Watson Studio, where you don’t need to install anything, or you can always install Anaconda and to the labs or projects locally.

I recommend both of the two worlds, try to do some labs locally on Watson Studio which is really cool, others in the Cloud for you to get familiar with both.

If you have any problems with configurations or the code itself, you have a discussion forum where you can find a lot of help. I had a bug in the code in one of the projects where I couldn’t find the answer, for instance, on Stack Overflow, after 1 or 2 hours I decided to check on the forum if someone had the same error and there’s the answer. I strongly recommend search or post your error / questions on the discussion forum, the community is active, and the students usually help each other.

Conclusion

I recommend this course for who already know or want to get into Data Science world. The videos are not so long, the instructors explain well how things work and the essentials of the matters. You always have the help of the community if you need something and remember if you need financial support, it takes 15 days to Coursera gives you an answer.

Thanks for reading and happy coding !",4430
"As I tried to jot down the main things I like and dislike about this board, I realised that for almost every positive thing I had to say, I had at least one related negative point as well… so that’s precisely how I’m going to structure this. For the majority of this review, I’ll hit you with a positive, then follow it up with something correlated that I wasn’t so keen on.

Key feel

First thing’s first… is this actually any good to type on?

GOOD: The keys feel quite excellent. This is my primary concern so when it comes to getting me on-side, it’s a big win for the Unicomp. The New Model M is extremely satisfying to type on, particularly given that the mechanical board it replaced was a Corsair “gaming” keyboard with Cherry MX Blue slider switches of the sort of board that the mech keyboard community would quite justifiably turn its collective nose up at, and which I only bought in the first place because it was in a clearance sale to get rid of obsolete stock. Next to that board and more or less every other mechanical board I’d tried previously, this thing is a revelation. The amount of force required to actuate a switch is alarming compared to the MX Blues, which wasn’t particularly surprising given how the mechanisms differ. As I mentioned before, I don’t have a long history with buckling spring keyboards, so it’s entirely possible that the Unicomp one is crap compared to a mid-80s IBM, but from what research I did before buying one, I don’t get the impression that that’s the case. There are those who will tell you that they don’t hold a candle to the original boards from IBM or even Lexmark, but I get the feeling that part of the problem is that the older boards are unavoidably affected by their age and as such have a different key feel due to wear and tear. I also think that part of it is probably that the manufacturing tolerances and minor details in the method of manufacture had been gradually changing over a period of many years, long before the Model M manufacturing duties ended up in the hands of Unicomp; I suspect that you’d find a Unicomp Model M to be quite similar to a later Lexmark Model M, but that both would be more noticeably different from a 1985 IBM one. I can’t say from first-hand experience, though I have noticed that others who do have first-hand experience of a lot of Model Ms seem to be saying something to that effect (notably Thomas Ran, a.k.a. Chyros on Deskthority or Chyrosran22 on YouTube).

BAD: Sod all! I love typing on this keyboard. I know I just said that I had a negative for most positive points I had to discuss, but this one is the significant exception: I really have nothing bad to say about the key feel on this board. Again, I might have if I’d used Model M boards for the past 20 years, but I haven’t, so I can’t claim to know if they nailed it or not. If I were really reaching, I might point out that the force required to press a key properly might come as an unpleasant surprise to some, but I don’t think that’s a fair criticism because I suspect it’s about right for a Model M based on everything I know about the history of buckling spring boards. Just something to be aware of, but not really a criticism at all, in my view. Personally, I’m a big fan; it just took maybe a couple of days of adjustment. Now that I’m used to it, I just switched to testing out a different board (which I’ll probably write about separately another day) and found myself absolutely hammering the poor thing to death because it uses completely different mechanical slider switches that are much easier to press.

Quality control

One of the things that Unicomp has taken some heat for over the years is the quality of their keycaps, which (as I briefly mentioned before) was put down to the age of their manufacturing equipment, which is the exact same kit that was being used in the mid-to-late 90s by Lexmark (in Greenock, Scotland, I believe). The story goes that IBM sold the vast majority of their keyboard manufacturing business to Lexington-based company Lexmark in the early 90s, with only a very limited amount of first-party IBM manufacturing continuing beyond that point in Greenock of all places (which is only a few scant miles from me!) and Lexmark taking on the bulk of it moving forward. Lexmark was making boards for IBM in more than one location, one of which was Greenock as well, although I believe they were separate facilities (one IBM, one Lexmark, just both in the same Scottish town). When Lexmark eventually stopped making the Model M toward the mid-to-late 90s, some key Lexmark employees bought up a bunch of the manufacturing equipment and went on to form Unicomp, where they used the same old kit to continue making the Model M for years thereafter, although they moved all the manufacturing exclusively to Lexington in Kentucky, USA. (IBM actually continued making Model Ms in Greenock as late as 1998, I think, but when they shut that down they didn’t move it elsewhere, that was the end of it.)

The trouble with the equipment Unicomp inherited from Lexmark is that it was already starting to show its age by the time it changed hands, but they weren’t really making enough of a profit out of the manufacturing and sales of the boards to justify the pretty considerable expenditure of upgrading the old gear. Owing perhaps to the recent resurgence in the level of interest people seem to have in mechanical keyboards, Unicomp has finally decided that they can afford to spend the cash on some long overdue upgrades, which is how we’re getting better keycaps and two new keyboard models in the “New Model M” and the “Mini M” (which is more or less a reborn Space Saving Keyboard, or SSK) in the first place. There’s a caveat, I hesitate to add: not all of their keyboards fully benefit from the new/upgraded tooling, since they’re still using the old moulds for all other models, as far as I am aware. If you buy something like an “Ultra Classic” (the previous iteration of the main PC version of the Model M) or “PC-122” (their take on IBM’s giant 122-key scancode set 3 terminal keyboards), it is my understanding that you are still getting a chassis made from the old and somewhat worn out moulds. Their website actually goes so far as to heavily discourage ordering anything other than the New Model M or Mini M, which seems to support this. On most of the product pages, they now have this “product announcement” in gigantic red text to try to divert your attention to the New Model M / Mini M instead of their older boards (this one is how it appears on the Ultra Classic page at https://www.pckeyboard.com/page/category/UltraClassic):

Seems Unicomp *really* doesn’t want to sell you their older products…

So… were the upgrades worth the expense?

GOOD: The dye sublimation is impeccable and the moulding of the vast majority of keys is almost flawless.

The positive impact of the upgraded tooling really shows and I’m glad they’re finally addressing one of the main criticisms that has been directed at their output for years. There is no fuzzy lettering or weak, faded looking dye work going on here. The moulds for the keys aren’t absolutely perfect, but most of the few remaining rough patches are just at the pour-point of the mould, which is at the far edge of each keycap so you can’t see it when using the board (and this was always the case on IBM keyboards anyway).

But hang on, didn’t I just say that I wasn’t that experienced with Model Ms? Isn’t this my first Unicomp? How would I know if it’s any better or worse than their previous products, or any other Model M? Well… unfortunately, this is where things start going downhill.

BAD: The excellent dye sublimation is ruined by rather poor misalignment of the legends on several keys, and there are still some noticeable moulding issues present on both the keycaps and the case even with their manufacturing upgrades.

The whole F row is off, including the very first key on the top of the board (Esc), both legends on the 5/% key are much further to the right than the others in its row, and the nav cluster is all over the place. It’s so distracting to me that I’m going to have to switch them all for either classic keycaps off an old board or blanks that have no legends at all, lest it drive me insane.

The nav cluster alignment… isn’t great. The function row is similar.

There’s also one standout instance of a key having its legend drastically misaligned by design, as well, rather than by mistake: the Return arrow legend on the tall ISO Return/Enter key is in completely the wrong location on the keycap. I don’t think this one is just because of bad aim, though: I think it’s because there’s a version of it that would have text, and they’ve simply removed the text from the process of printing that version of the key and kept the arrow legend in the same position (because UK Model M boards generally don’t have text on special keys that have icon legends). The arrow legend should be near the top of the keycap as it is on pretty much all classic ISO Model Ms that aren’t terminal boards (terminal ones tend to make this key “Field Exit” instead) but on all the Unicomp ISO Enter keys I’ve seen, it’s more like 1/3 of the way down the keycap, which is way too low and looks wrong.

Due to the closeness of this shot, there’s a certain amount of lens distortion caused by the camera, but you can nonetheless see that many of the legends on the keys are noticeably misaligned. The whole Function row looks wrong (is it supposed to be vertically centred or not?), Esc is aligned differently from the rest of the row, the 5 key is further to the right than the other numbers, and you can see what I was saying about Return/Enter.

Many of the additional keycaps I ordered to customise the board with were quite shockingly bad compared to the stock ones installed on the board, so I ended up using almost none of them: I ordered a staggering number of custom keys so I’d have plenty of options, but as of right now, the only ones I actually have on the board are a novelty one with the Linux mascot “Tux” on it, and legacy Windows ones for the left Windows key and Menu key. There are major moulding defects on most of them, the legends are REALLY badly misaligned, and the sublimation is less impressive than the preinstalled keys as well. This — partially — is why I feel I can say with some confidence that the upgrades to Unicomp’s manufacturing equipment has made a noticeable positive difference on their current keycaps. It looks like many of the custom keycap sets that I ordered are older, manufactured before they upgraded the tooling. The difference is night and day.

Side view of the Caps Lock key from one of the two Linux keysets I ordered. Very rough indeed.

There are also some keys on the board itself with moulding issues, most notably the left Ctrl key, which essentially has a dent in it, except that it seems like it’s just moulded that way rather than being damaged after the fact. The board was extremely well packaged, so it clearly didn’t happen in transit.

Moulding issue on left Ctrl key (stock key, not a swapped-in alternate)

Finally, the case itself is not without flaws: mine has a very distracting dark vertical line running down the black case right in front of the space bar:

Moulding defect in my New Model M chassis

When I sent the first pic to Unicomp they claimed they couldn’t see the problem, so I took this horrible one with flash on to make it more obvious

Had this been on the underside or otherwise a little less obtrusive, I wouldn’t care as much… but it’s right there, man. Every time I look at the board, it’s the first thing I see. (Yes, I touch type, but I still want the thing to look good at this kind of price point.)

Function

The New Model M isn’t quite as revolutionary as the Mini M, but there are still some alterations to the full size design compared to its previous iteration in the form of the Ultra Classic.

GOOD: The New Model M arranges the keys on the bottom row more like the original IBM versions.

It still has the extra keys, but they’re located in places that mean any muscle memory you might have built up using a classic Model M (or one of the dozens upon dozens of keyboards that stole its layout) won’t be broken when you switch over to this keyboard. I’ve actually already talked about this a bit over on Reddit, where I shared this extremely professional MS Paint diagram to show what I was on about:

Diagrammatic representation of the bottom two rows on the New Model M compared to the original IBM design. If you remove the Windows keys and the Menu key, then extend the space bar to fill the space freed up by the right Windows key, you get the original Model M key arrangement.

This “new” layout may seem daft and counterintuitive at first glance, but it in fact more accurately reflects the key placement on a classic Model M. The extra keys have been added into space that was either completely unused on the old Model M or taken up by the right-hand end of the space bar, which they’ve shortened to make room. This has the downside of meaning that the key added to the right of the space bar is a weirdly shaped Windows key that’s 25% wider than the Windows key on the left side, but because everything else is in the right place, you can at least remove that extra key and swap the shorter space bar for a longer one instead, which very nearly (neeeeearly) turns the “New Model M” bottom row into a classic Model M bottom row except with the two gaps at the sides filled in (between both sets of Ctrl and Alt keys). As it happens, this is one of the first things that I did: I ordered a spare short space bar and one full-length space bar with my New Model M, and I installed the long one over the top of the right Windows key spot very quickly. The asymmetrical bottom row doesn’t sit right with me and I want to be able to switch between this and other Model M-style boards without my right thumb ever hitting the right Windows key by accident when I want a space instead. (I don’t think I’ve ever used the right Windows key in my entire life, although the left one gets some heavy abuse from my left pinky.)

My New Model M with some of the many additional keycaps I ordered so I’d have lots of customisation options. For the pic here, I’ve stuck blank black keycaps on the left Windows key and Menu key to show where there would have been gaps in the classic Model M layout, and installed the long space bar, which runs over the top of the space that had a right Windows key in it by default

BAD: You can’t opt for the most true-to-the-original bottom row layout unless you pick US ANSI configuration, you’re stuck with Windows & Menu keys regardless, and the space bar on the New Model M appears to be slightly misaligned on the chassis itself (so I can’t fix it without replacing the entire case).

That last one is the real killer, but I’ll explain the other elements first because they contextualise it a bit.

One of the things that the Model M was infamous for was stubbornly refusing to include Windows and Menu keys, ever (which were introduced with the frankly hideous Microsoft Natural Keyboard in 1994 for use specifically with Windows 95). This changed at some point when Unicomp took the reigns, although I’m not certain if they added these keys right away or if that only happened fairly recently. Either way, I would have much preferred to have the option to exclude them. This might seem like a nitpick, but it goes back to the same reasoning I gave for not wanting the short space bar: muscle memory. I want to be able to pick up a classic Model M or some other board that uses an equivalent layout, and lacks Windows or Menu keys. If I keep reaching for the left Windows key — which I know many people never use, but I probably use approximately seven thousand times in a typical day — and then I switch to a board that doesn’t have one, it’s going to be frustrating for quite a while until I adjust. What I do to combat this is map the Caps Lock key as Windows key, then map Shift+Win to toggle Caps Lock. Frankly, I almost never use Caps Lock anyway; more likely, I’ll just hold Shift, EVEN IF I WANT TO TYPE IN ALL CAPS. It’s not that hard. I don’t really see the point in a Caps Lock key, but I can keep the functionality there without needing a key entirely dedicated to that sole purpose. But in any case, I don’t have this option: Unicomp doesn’t offer it, and I can’t simply remove the keys because that would just leave the underlying keyswitch barrel exposed, which would not only be unsightly but would be somewhat detrimental to the health of the board because it would leave the barrels (and the springs inside them, unless I pulled those out) open to dust and dirt. Furthermore, to diverge briefly into aesthetics for a sec (which I’ll get into more thoroughly in the next section), the legends that Unicomp has chosen for the stock Windows and Menu keys are ostentatious and repulsive to my eyes, although I’m sure that their appearance is partly due to limitations imposed by Microsoft on how they licence these keys. Still, seeing Windows 8 logos on a keyboard that was fundamentally designed in the mid-80s nonetheless drags the overall look of the board down; I couldn’t keep those on any board I buy from Unicomp, personally.

These do not belong on any Model M, if you ask me (you didn’t, but I’ll tell you anyway)

As I mentioned previously though, I was at least able to swap out the shortened space bar for a longer one, which is obviously a good thing. What’s less good about it is that Unicomp offer this out of the box, but only if you order a US ANSI layout board. If you want any other layout whatsoever, they force you to pay a $15 uplift and you need to take the layout with the shorter space bar. Why this is, I don’t know, because there should be zero difference between the bottom row of the ANSI and ISO layouts. So instead of just choosing the “long space bar” option as I could have if I’d wanted a US layout keyboard, I had to pay an extra $15 to get a UK ISO layout with a short space bar, then another $4 for a separate long space bar, then pull off the stock space bar and right Windows key and install the long space bar over the top of the unused barrel and switch. There’s a redundant switch flipper and spring still sitting underneath the right end of my space bar now, which doesn’t appear to make any discernable difference whatsoever to the function of the board, but it feels a bit pointless and I would’ve rather paid slightly less to have it arrive like that out of the box. The stabiliser on the stock space bar was (barely and unevenly) factory lubricated, but I had to add a bit of lithium grease to the alternate longer one to avoid it rattling. (I tried damping the loose spring floating in the unused barrel under the space bar just in case it made any difference to the sound, but it didn’t, so I’ve just left the spring dangling in there so I don’t lose it or whatever if I take it out.)

My big problem here, though, is that the space bar is misaligned. No, this is not because I swapped it out for the longer one: it was off before I switched them, and it remains identical after switching them. It appears that the chassis itself must have the barrel for the space bar moulded just ever so slightly too far to the left, by about 1 mm or so, which results in the space bar riding so close to the left Alt key that it almost rubs against it when you press either of the keys. Conversely, on the opposite side, there is a very noticeable gap between space bar and right Alt. Considering what I was just saying about me switching out the shorter space bar and removing a perfectly functional key just to achieve symmetry, you’ll hopefully understand why I find this infuriating. It’s very difficult to photograph, but I gave it a try:",19901
"The secret sauce to become a successful data scientist does not have to be hard. Read this…

With the value of data science becoming more obvious across industries, the need for data scientists is becoming pronounced. At this time, transitioning to data scientist will be a savvy move to add value for business progress, gain higher returns, and achieve career success. Let’s unravel the secret sauce to become a successful data scientist here.

“If you wanna do data science, learn how it is a technical, cultural, economic, and social discipline that has the ability to consolidate and rearrange societal power structures”, says Hugo Bowne-Anderson.

As data scientists help businesses navigate the world of data, connect new and significant patterns in business trends, make innovative solutions, and secure business processes, an investment in the right knowledge alone can pay the best interest.

Before getting into the details, take a look at the roles and responsibilities of a data scientist.

Roles and responsibilities of a data scientist

It’s all about empowering the data ecosystem, helping organizations to use their data to solve business problems and challenges.

With the humongous data collected by the organization, data scientists provide solutions to business-related problems. Using mathematics, statistics, and programming skills, data scientists organize the data and predict the output with data. They are mainly involved in:

Identifying business problems and predicting outcomes

Cleaning the structured and unstructured data

Cleaning and validating data to ensure accuracy and completeness

Analyzing the data to identifying patterns and trends.

Devising and applying new models and algorithms

Finding opportunities and developing data-driven solutions

If you want to be a successful data scientist professional, then it is crucial to understand the entire data science concept, tools and techniques, and skills for this challenging role.

Essential skills of a data scientist

The essential skills include technical skills and non-technical skills. But then there is the understanding of the business needs. Plus, data scientists cannot work alone, but works collaboratively and bring together the skills of a team. They should be creative and also ready to work outside their comfort zone. Professionals who can think like an entrepreneur will have the most impact.

The best data scientists have the vantage point to see the challenges and possibilities across industries and types of organizations. They are able to convert the organization’s goals into questions and answer them through algorithms and data.

To be an effective data scientist, you should possess the following technical and non-technical capabilities.

Technical skills

This includes core skills of data science.

💻1) Programming skills:

As a data scientist, you should have strong knowledge of programming languages such as Python, C, C++, Java, Perl, and SQL. Of all these, Python is the most popular language used in all data science job roles.

Get details about programming languages: Top 6 Programming Languages for Data Science in 2021

💻2) Analytical skills:

Hadoop, Spark, and SAS are the most widely used data analytics tools. A data science certification in any of these tools would help you improve your knowledge and skills in using them for business benefit.

Here’s a list of widely used Hadoop analytics tools that triumphed the charts: 7 Best Big Data Hadoop Analytics Tools in 2021

💻3) Working with structured and unstructured data:

Data scientists generally obtain raw data from various sources and channels including websites, IoT devices, social media channels, mobile, etc. Therefore, it is recommended to understand how to use social media and marketing strategies, to provide more effective real-world solutions to the team.

💻4) Machine learning skills:

In today’s world, data scientists may have to work with machine learning engineers for several projects. A data scientists’ perspective is essential for product development to increase commercial value. Becoming intuitive for automated solutions offered by machine learning algorithms is crucial.

Some of the technical skills and programs to master in machine learning are briefed below.

Applied mathematics : It is recommended to be familiar with linear algebra, probability, statistics, multivariate calculus, and, distributions.

: It is recommended to be familiar with linear algebra, probability, statistics, multivariate calculus, and, distributions. Computer science fundamentals : Possess knowledge of different computer science concepts such as data structures, algorithms, space, and time complexity.

: Possess knowledge of different computer science concepts such as data structures, algorithms, space, and time complexity. ML algorithms : It is good to have a sound knowledge about K Means Clustering, Support Vector Machine, Apriori Algorithm, Linear Regression, Logistic Regression, Decision Trees, etc.

: It is good to have a sound knowledge about K Means Clustering, Support Vector Machine, Apriori Algorithm, Linear Regression, Logistic Regression, Decision Trees, etc. Data modeling and evaluation : It is crucial to get skilled in data modeling and evaluation as a data scientist. You should be able to understand the underlying structure of the data and evaluate the data using algorithms effectively.

: It is crucial to get skilled in data modeling and evaluation as a data scientist. You should be able to understand the underlying structure of the data and evaluate the data using algorithms effectively. Neural networks : This is one of the skills which you cannot ignore at all. Be familiar with types of neural networks such as Feedforward neural networks, recurrent neural networks, convolutional neural networks, modular neural networks, etc.

: This is one of the skills which you cannot ignore at all. Be familiar with types of neural networks such as Feedforward neural networks, recurrent neural networks, convolutional neural networks, modular neural networks, etc. Natural language processing : Be familiar with one or more libraries such as the natural language toolkit for creating applications related to NLP.

: Be familiar with one or more libraries such as the natural language toolkit for creating applications related to NLP. Programs and tools: It is crucial to have a solid understanding of the programs and tools such as TensorFlow, Spark, Hadoop, Apache Kafka, Weka, MATLAB, Google Cloud ML Engine, Amazon Machine Learning, PyTorch, and Jupyter Notebook.

To know more about machine learning skills, read this: Why data scientists should learn machine learning.

💻5) Data visualization:

As a data scientist, you must simplify your project and make it consumable for stakeholders, prospective clients, non-technical staff, marketing team, and business leaders. Data visualization helps you to present the project in a simple visual format that everyone can understand.

To know more about data visualization tools, read this: A Complete Overview of the Best Data Visualization Tools

Non-technical skills

This includes personal and communication skills.

🏃🏻1) Business acumen:

To channelize your technical skills, it is essential to possess a strong aptitude for business. it enables you to identify business problems, predict future challenges, and explore new opportunities.

Learn more about data scientist’s role in business success: How Data Science Adds Value to your Business

🏃🏻2) Communication skills:

Communication skills are important for data scientists as they interact with both technical and non-technical teams, clients, and leaders. You should be able to communicate the analysis and findings in a way that the rest of the team understands. To communicate the information in a simpler chunk is of utmost importance.

🏃🏻3) Data intuition:

Success can be as easy as picking a low-hanging fruit many times. A successful data scientist has a keen sense of data patterns and knows how to find insights. It is essential to not miss an opportunity by exploring all the potential benefits that are still there in our grasp. Intuition leads to building an innovative culture organization-wide.

A zeal for knowledge is important as data science is still evolving. Natural curiosity and structured thinking can make you better over time and improve your chances of business success. Curiosity also helps you to become more accurate with data findings.

🏃🏻5) Business sense:

Business sense helps data scientists to interpret data for descriptive, predictive, and prescriptive analytics. You need not be an expert business analyst, but you should be able to question, find and collect data that might answer the question, analyze the data, and interpret findings.

So, now you have a brief idea about the essential skills, roles, and responsibilities of a data scientist. It’s easy to gain this knowledge and ideas by learning through proper channels. The real challenge is the thought process and its realization. Of course, this comes with experience. Working on several projects from different industrial verticals will give you the best thought process and it keeps evolving as years add on to your experience. Still, here is a sneak-peek of these thoughts, that will align with your goals.

How do successful data scientists think about data?

To design, develop, and deploy the right solutions for business, the way you think and conceptualize solutions also influences a lot.

“someone with a deeper understanding and intuition for what they are doing is a true data science whiz, and will likely have a successful career in this field.” -Lee Barnes, Head of Paytronix Systems

There are several models representing thinking patterns and as a data scientist, following them as applicable would help you to model data and communicate with your decision-makers effectively. Two of them are briefed here as a reference.

1. Design thinking mindset

Design thinking is nothing but a structured approach taken to solve problems. It comprises qualitative activities supporting the generation of human-centered design solutions. Though the activities vary from project to project, the core theme revolves around — empathize, define, ideate, prototype, and test.

📌 Empathize

Conduct ethnographic interviews to gain a deep understanding of the user journey and the pain points. Open-ended questions throw light on conclusions that we would never have arrived at through deductive reasoning alone.

📌 Define

The systematic acquisition of data will help you to form and test hypotheses against quantitative evidence. It serves as the starting point for innovation while addressing users’ needs in a unique way.

📌 Ideate

Here, there is every chance for process automation. You can design and train a behavioral model that can measure how and to what extent the hypothesized solutions can address the pain point and predict its influence on user behavior.

📌Prototype

Involve visual designers, interaction designers, and strategists to sketch out product features, build user journeys, develop business models, and product roadmaps. Of course, this might undergo extensive iterations to address the pain points uncovered during research, maximize product uptake and bring higher returns or revenue to the organization.

📌 Test

Testing uncovers the mistakes or opportunities you did not realize. There could be a pain point or user behavior that was not identified earlier but became apparent when users started to interact. This helps to change or refine the product features, if and as needed.

2. Agent-based thinking

It is a simulation modeling technique and conceptually deep that offers solutions to real-world social or business problems. The agent-based modeling mindset refers to describing a system from its constituent unit’s perspective. It is similar to microscopic modeling.

The thinking is here is more individual-centric like the physical/mental state of an individual, the individual’s interaction with the environment and other individuals. This individual-centric thinking is applied to physical assets, individual consumers making purchase decisions, organizations making markets efficient, and even the government.

From a business context, it can be applied for traffic, customer flow management, stock market, operational risk, organizational design, innovation of products, and adoption dynamics.

The model illustrated here describes a multi-scale agent-based model to study the impact of cohorting strategies on Covid-19 dynamics in metropolitan cities. The studies provide a quantitative trade-off on cohort size and its impact on disease dynamics. It shows that cohorts help to reduce disease transmission without significantly impacting ridership or social activity.

In addition, there are other models such as behavioral thinking, systems thinking, and forest thinking. To summarize, data scientists are deep thinkers with an intense intellectual curiosity to discover new things. Above all this, taking the right approach is also essential.

Let’s see how…

How does a successful data scientist take the right decision?

According to data scientists, data science can be used in different ways depending on the industry, business, and its goals.

Jonathan Nolis, a data science leader says that the ability to make good PowerPoint slides is important than using the most sophisticated deep learning models. The concept here is that the skills we know today will change tomorrow. In the open-source ecosystem of tools and commercial tools, there is ever-increasing automation, and therefore, the skills have a relatively short timescale.

Further Jonathan Nolis divides data science into three components

Business intelligence : Taking the company’s data in front of the right people

: Taking the company’s data in front of the right people Decision science : Using the data to make a decision in the company

: Using the data to make a decision in the company Machine learning: Putting data science models into production continuously

To be a successful data scientist, one should focus on critical thinking, quantitative and domain-specific skills apart from techniques.

Let’s understand the right approach a data scientist can take to excel in the career:

📍1) Practical approach

Data science is more than knowing the tool. It is about applying the experience to real-world problems and having that intuition to get the results. It is all about how to get results and when to trust results.

📍2) Consider the vertical aspect of a task

Data scientists today build practical modeling and exercises on KNIME. It is necessary to work on real-world, look-alike database problems. Of course, you can break standard analysis methods and drive new solutions. Understand 360-degree aspects of a data science problem.

📍3) Understand models and algorithms

As a data scientist, you must be able to adjust smaller things to optimize performance that comes through experience. It is recommended to use an optimization metric including different costs and types of errors.

Also, you should gain experience in dealing with open-ended questions, iterate different types of analysis methods and models by thinking out-of-the-box.

📍4) Advance your knowledge

Gain deeper theoretical understanding, experience, think about the obvious, and learn to solve real-world problems independently.

📍5) Earn data science certifications

Earning a data science certification helps you to build a strong and solid portfolio and land the job you want. It also helps to increase the industry knowledge and add value to skill sets. A certification helps you to:

Show your prospective employers that you have dedication toward the subject, and interest in continued learning and professional development.

Stay current about the industry practices, and give a competitive advantage while applying for jobs across the industries.

Comprehend the basics and advanced methodologies used in the industry for solving data science problems.

Here is a list of data science certifications you can choose to earn. It helps to improve your knowledge and skills pertaining to the industry.

The SDS™ certification is a multifaceted program covering a range of professional knowledge suitable for data scientists. It enables the data scientists to build longer-lasting skill sets, accept more challenging and bigger-impact roles. It proves your efficiency and potential to handle the onerous responsibilities of a data scientist. SDS™ is ranked among the world’s top 5 must-have global credentials for top-end careers by CIO Magazine.

To earn this certification, you should have passed the previous SAS certifications. You can learn to improve and manage data, work with data visualization tools, and many more. The other benefit is that the SAS data scientist certification does not expire.

This certification comprises two levels of training. As a first step, you should earn the associate certificate in big data analysis and data science foundations. Further, the specialist program covers everything such as visualization methods, HBase, Pig, Hadoop, advanced analytics, and natural language processing.

This is specifically designed for data scientists to use machine learning according to Microsoft. It enables them to train and deploy models that can solve business problems, learn predictive analytics, AI solutions, natural language processing, and more.

Moving forward, let’s understand the compensation for data scientists.

💲As per Glassdoor reports, the national average salary for data scientists is USD 1,15,383 per annum in the United States🗽

The job of a data scientist is to solve complex data problems creatively. It is not a task, but a journey to get the desired result.

Are you interested to take this journey?",17892
"According to the recent study, the data generated by the human are 2.5 quintillion bytes every day, which includes all the activities of the human and their health records. In order to handle this large volume of data, there is a need for technology. Over the ages, different technologies and methods are invented and are being used. One of the most important in such technologies is “Data Science”. Now a days, applications of data science are increasing in each sector like healthcare, education sector, banking and finance sector, IT sector, media and entertainment sector, e- commerce and many more.

One of the important sector which is in need of data science is healthcare. It helps to monitor the patient’s health by collecting data using different mobile application an helps the doctor to monitor the health condition from different locations, and also helps to detect the disease at an early stage.

Role of Data scientist in Healthcare Industry:

Data Scientist in healthcare plays an important role in implementing the data science techniques into healthcare and they also predict the disease at very early stage by extracting the useful data from the given information.

Major steps done by the data scientist are as follows:

Data scientist collects the data from the patients. Evaluating the hospital or health center requirement. Structure and sort information that can be utilized by the healthcare professionals. To carry out records analytics with the help of numerous tools. Implementing several algorithms to extract insights from the raw or unstructured facts. Constructing predictive models with the development groups.

Benefits of Data Science in Healthcare sector:

Now a days, Data science in healthcare is an essential element and factor that has transformed the industry. Many centers and processes had been advanced due to data science, technological know-how tools and techniques. It has boosted the speed of treatment and diagnosis. As a result, the workflow of the healthcare gadget gets superior. The real benefits of Data science in Healthcare are:

It helps in handling emergency condition adequately It reduce’s the waiting time for treating patients It offer’s on-time and proper treatment It smoothens the healthcare workflow It helps in reducing failure risk while treating any affected person

Challenges of Data Science in Health Care Sector:

Data Science in healthcare industry faces more challenges in areas such as data management, privacy & security and data retention:

Data Management: Health organizations face massive-information-related challenges which can affect patient safety. All facts that health companies gather wishes to be defined, formatted, deduped and checked for accuracy, and made available for various uses — clinical, billing, administrative — and the velocity and volume of the data makes this task harder. Some hospitals now employ patient safety experts, but in addition to having medical expertise, people in these roles must understand how data management practices can improve or hinder patient safety.

Privacy and Security: It is one of the most important challenges in the health care industry is being able to maintain the privacy desired and needed for the patient data and the security for preventing unauthorized access to the data base. Also, the importance to the privacy is being given less priority than it should be, considering the cost of the technologies to ensure safety and security of the data and networks. However, industry shall keep an extreme control and efforts to keep the data private and secure, to keep the trust of the patients and their data safe.

Data Retention: Health data must stay accessible for at least five years. That means businesses need to take a long-term approach for data maintenance and to keep track of information when the data gets accessed, by whom, and for what purpose. Medical data management software allows users to establish access privileges and processes, such as those that give temporary data viewing capabilities to representatives in different departments in a hospital. These products can index data and notes and track when data entered the system. Organizations must put processes in place to periodically sort through the data to delete it when appropriate, or modify and anonymize it to use it in new ways, such as to gauge trends across several years.

Applications of Data Science in Health Care Sector:

Drug Discovery and Manufacturing:

One of the important advantages of data science in healthcare lies in the drug discovery technique at an early-stage. This also includes the research & development, which includes subsequent-era sequencing and precision medicinal drug, that can even assist in finding opportunity paths for therapy of multi-factorial sicknesses. As of now, device studying benefits, techniques contains unsupervised studying, that may become aware of the patterns in facts without even imparting any predictions. Challenge Hanover, which is advanced with the aid of Microsoft, is the usage of the gadget learning-primarily based technology for multiple projects, which include the AI-based technology for cancer treatment and personalizing drug combination for the acute Myeloid Leukemia.

Medical Image Analysis:

Medicine and healthcare together form a promising field for utilizing technological advancements. The healthcare sector is acquiring new heights due to the advancements in Data Science. It is helping with various aspects, and one of them is the analysis of medical images. It is one of the most interesting areas of study in image recognition technology.

Data Science helps in the recognition of scanned images to figure out the defects in a human body for helping doctors make an effective treatment strategy. These medical image tests include X-ray, sonography, MRI (Magnetic Resonance Imaging), CT scans, and many more. Proper analysis of the images of these tests helps gain valuable insights for the doctors provide the patients with better treatment.

Helps in Identifying Disease and Diagnosis:

The primary advantages of data science in healthcare are to identify and diagnosis of disease and ailments, which are otherwise considered to be as hard to diagnose. This can include anything from the cancers which are tough to catch at the time of the initial stage to other genetic diseases. IBM Watson Genomics, which is even one of the above prime examples of how integrating cognitive computing with the genome-based tumor sequence, can help in making a fast diagnosis.

Virtual Assistance: By using predictive modeling techniques, data scientist has developed many virtual platforms that provide the assistance to the patients with more efficient, convenient and cost-effective. So, by using these platforms we can identify the disease of a person based on the confidence rate and the information they entered in the device.

One of the most popular examples of virtual assistants are “Ada AI chatbot” — It predicts the diseases based on the user’s symptoms, “Woebot” — a chatbot developed at Stanford University that provides therapy treatments to patients who suffer from depression, “MedWhat” — It is a virtual assistance uses machine learning to respond instantly to medical and health questions from both doctors and patients and many more.

Medications: From the Anesthesia to the treatment of breast cancer with daily drugs, the use of data science in healthcare are already being employed in the improvements of the medications. The famed IBM supercomputer Watson is also working with companies like the Pfizer to improve drug discovery, moreover, from the immunological conditions and cancer. Some of the companies like Medtronics are also working on to utilize machine learning to improve the medications, but, much more on the individual scale. Some of the channeling ideas about precision medicine, they are also working on with the patients to offer personalized feedback on how to better control and treat their diabetes. Personalized medicine will be a crucial strength of patients in the future.

Conclusion

Data science has been performing remarkably in the field of healthcare, from treating minor headaches to cancer. It is also helping scientists and doctors to be able to use the old data to understand the failures of the past and to provide improved and safe treatments for the future. There has been an increase in awareness about data science and the use of this in the sector. However, therein still a large scope of increase in the application of Data science in the field and acceptance of that from the users in health line sector, together to contribute to a healthy future.",8699
"Hey! We’re Telifie, the maker of the smart home drive in Dayton, Ohio. Telifie was created to eliminate the monthly subscriptions cloud storage services charge and give more features to users with more privacy and security!

The Device

Telifie comes in 256GB, 512GB and 1024GB sizes. WiFi is required for setup and, after that, you can connect it to your smart phone for faster access when you’re at home. Our servers act as a middleman for transferring files so your content is never stored permanently in the cloud. Pick a color for the lights that show when you’re using Telifie, or sometimes things look better with the lights off if you’d like.

Smart home dashboard

Our smart home dashboard gives you the capability of presenting your files from our apps on your TV and you can get notifications, stay up-to-date on the weather, and so much more with updates free forever. Connect the Telifie to surround sound with the aux port (that’s right, an aux port) and make a smart speaker system and play music through the Telifie with your phone. You can use it without the TV too!

Access

Our apps require two-factor authentication and sport an elegant design that gives you all the capabilities you need to get the most out of your Telifie. File transfers and device communication is encrypted and authentication keys are refreshed regularly.

The bigger picture

As our journey continues, our mission will never change: provide affordable, elegant, and versatile products for everyone. We are proud of our warranty and servicing program where you can get free servicing for life! A Telifie costs less than a year of cloud subscriptions and gives you the most bang for your buck. You can check out our roadmap for our software running on the device, called Connect.

Our Kickstarter is launching on May 25th, and we need your support to fully fund $40k to put a Telifie in your hands and make privacy and security a priority for customers. Visit our Kickstarter page and save our campaign to get just one email when we launch for up to 30% off our MSRP or signup for our email list (just once a week, no spam).

Lastly…

My name is Kenny Hawkins, the CEO, and Founder of Telifie. I graduated from Miami University (Ohio) in 2020 and took a year to build Telifie. It is most important to me that Telifie remain an honest and ethical business. As a user, you can suggest and vote on features and talk to us at any time free of charge.

We are a small business and we want to maintain that mentality of supporting our community. I appreciate your help and can’t thank you enough for believing in Telifie. Thanks for reading until the end!",2640
"With September in full swing, let’s take a look back at the best August ​Kickstarter projects. I’ve got a great round-up of products for you.

__

NOSU is a dual-purpose stainless steel tumbler with built-in cutlery. NOSU, which is short for No Single Use, is the next big, simple solution to convenient and sustainable living. NOSU is perfect for active lifestyles and hot or cold beverages. With this tumbler, you’ll never forget your cutlery for onto the go meals ever again.

__

There are many duffle and laptop bags available on the market. However, CACHE is unique in that it is a hybrid tech sling and duffle. It’s spacious, has quick access, is versatile and convertible. It comes in two color options, static and carbon, and is perfect for carrying your everyday essentials. In addition, its ergonomic shoulder strap is compatible with both left and right-handed people.

__

Razor blades get dull quickly, and traveling with one isn’t always convenient. Erazor is a revolutionary ceramic blade portable shaver that won’t go dull for years. Erazor will altogether remove any hair beneath it and can quickly be charged with a USB-C cable. The material used in the blade is well known for its superior performance and is generally used for precision cutting. Did I mention that it’s easy to clean and will stay charged for up to 60 days with one charge?

__

If your family is anything like mine, we have different remotes lying around everywhere. It’s always tough to figure out which remote operates what in our house. SofabatonX1 is the most versatile universal all-in-one smart remote. This remote can control all your entertainment and home devices. The remote’s battery life lasts up to 60 days when charged, and it also works with Alexa and Google assistant. The SofabatonX1 is perfect for de-cluttering your remotes.

__

Finally, ​the Culla Blanket by Crua is the ultimate outdoor blanket. Culla Blanket is a graphene-infused, lightweight, water-resistant, tear-resistant outdoor blanket. This blanket is the perfect addition to your outdoor adventures. The blanket is sure to warm you up, versatile, and can be used anyway, from a picnic blanket to a sleeping bag.",2182
"Beyond Wearable Devices: Internet of Bodies (IoB)

A few years ago, a unique concept is known as a wireless body area network (WBAN) was introduced. It is also called body sensor network (BSN). This was created as a wireless network of wearable computing devices. These devices were either implanted within the body or surface-mounted in a fixed position on the body. In some cases, they have accompanied devices that humans could carry in their clothing pockets or hand or various bags.

Also, there have been a significant advancement in the field of Internet of Things (IoT) over the past few years.

In the process, these two technology streams have been converged to form what is now called as the Internet of Bodies (IoB).

Development of Internet of Bodies(IoB)

The Internet of Bodies(IoB) is created when the Internet of Things (IoT) connects with your body. In another words, the IoB is an extension of IoT that connects the human body to a network via devices that are ingested, implanted, or otherwise connected to the body. Once the connection has been made, data can be exchanged as well as the body and device can be remotely monitored and controlled.

Although, the concept has developed over several phases, there are three major phases of IoB development:

Phase I: This phase included the usage of wearable devices such as Apple Watches or Fitbits that could connect our bodies to collect and analyse data. Phase II: In this phase devices which were internal to the body such as pacemakers, cochlear implants, and digital pills that are ingested in our bodies to monitor or control various aspects of our health parameters. Phase III: In this emerging phase technology is being embedded within the human body (implanted microchips) leading to the creation of cyber-physical systems having human bodies to connect with a remote machine on a real-time basis.

Examples of Development on Internet of Bodies Devices

Health Tracking Device

The “phase I” IoB devices consisted of bracelets, watches, rings, and smartphone apps that can track steps, heart rate, sleep patterns, and other physical data, such as alcohol consumption. These devices operate by using advanced accelerometers and other sensors that can translate movement into digital measurements.

These devices have become the mainstream in the last decade or so. The adoption of these devices was rapid as they offered user-friendly analytics, giving individuals greater visibility into their health.

Since the volume of personal data that these devices collect along with their security vulnerabilities; the potential of hackers using such data poses quite a bit of risk.

Implantable Cardiac Devices

The most recognized example of the Internet of Bodies is a defibrillator or pacemaker, a small device placed in the abdomen or chest to help patients with heart conditions control abnormal heart rhythms with electrical impulses.

The new generation of cardiac pacemakers and implantable cardioverter defibrillators can provide real-time and continuous information about a patient’s cardiac fluctuations. These devices can also regulate heart rates in patients whose hearts beat too fast or too slowly and can help prevent heart failure.

When such devices become part of IoB, they transmit the data related to heart conditions to a recording device located either at the patient home or connected to their physician. However, in such cases, Internet connectivity introduces the potential risk for these devices to be hacked and the data they transmit to be compromised.

Digital Pills

One of the Phase II, IoB devices are digital pills. The digital pills are embedded with sensors that record the medication that was taken.

The pill’s sensor sends a message to a wearable patch that transmits the information to a mobile app so that patients can track the ingestion of the medication on their smartphones.

Through IoB patients can grant caregivers and physicians access to information through a web-based portal. This can help health care providers confirm whether patients are following their treatment plans.

Smart contact lenses

Currently, several types of contact lenses are being developed that integrate sensors and chips to monitor health diagnostics based on information from the eye and eye fluid.

One smart contact lens in development aims to monitor glucose levels that will hopefully allow diabetics to monitor their glucose levels without repeated pinpricks throughout the day.

Brain Computer Interface(BCI)

This is a development of a phase III IoB device called the Brain Computer Interface. In BCI application a person’s brain is merged with an external device for monitoring and controlling in real-time. The ultimate goal is to help restore function to individuals with disabilities by using brain signals rather than conventional neuromuscular pathways.

Another example of IoB technology is its interface with Artificial intelligence (AI). Some of the developments in this area include systems that can detect and collect data on human emotions by analysing facial expressions, voice intonations, and other audio and visual signals.

As a result of these technologies, vehicle accidents might be reduced, firms could learn how consumers feel about their material, and youngsters could be taught empathy. Although these technologies are relatively new, there is still much to be done before they can be considered practical Challenges Faced by the Internet of Bodies Technology

IoB devices can pose challenges across three areas: data privacy, cybersecurity, and ethics.

Data Privacy

IoB devices already in use and those in development can track, record, and store users’ whereabouts, bodily functions, and what they see, hear, and even think.

Data privacy is a major concern. Many questions remain unanswered regarding who has access to the data generated by IoB devices and for what purpose. A cochlear implant, for example, may restore hearing but may also record all audio in a person’s environment. The regulations are required to maintain privacy of such data.

Cybersecurity

IoB devices may be vulnerable to the same cybersecurity flaws as IoT devices or any other technology that stores data in the cloud.

Given the nature of IoB devices and the data they collect, the stakes are particularly high. Vulnerabilities could allow unauthorized parties to leak private information, tamper with data, or lock users out of their accounts.

Hackers may be able to control implanted medical equipment to cause bodily injury or even death. Any IoB-collected data might disclose sensitive information, which can raise national security concerns.

Ethics

Data privacy and cybersecurity concerns raise ethical concerns for those whose data has been compromised. Furthermore, the IoB raises additional ethical concerns, such as inequity and threats to personal autonomy.

Since the IoB is still in its early days the fundamental questions such as whether individuals have ownership over their personal data or have the right to opt-out of data collection. These need to be resolved through proper policy framework.

How regulatory Policy frameworks can Mitigate IoB Risks

As the IoB technology evolves, regulatory and legal issues need to be addressed and policies to be formulated for the proper use of the technology. The policy makers should consider :-

establishing data transparency and protection standards for IoB device data. They should also consider how to give IoB users control over their personal information, such as the ability to opt-out of data collection.

promoting cybersecurity best practices for parts of the IoB ecosystem. They could also create cybersecurity certifications to encourage the use of secure devices and raise consumer awareness.

Future Trends: Evolving IoB Ecosystem

Advances in internet technology and connectivity will allow many more IoB and IoT devices to communicate with one another at much faster speeds. The development of the fifth-generation mobile telecommunications network, 5G, has the potential to support orders of magnitude more devices per square foot than the previous 4G network.

Wi-Fi 6, the next generation of Wi-Fi technology, is also expected to improve connectivity by enabling more devices to transmit data and communicate with one another.

Further, the advancement of satellite internet will increase internet availability in remote areas.

These advancements will allow consumer IoT technologies, such as smart home systems, to connect to IoB devices, allowing one’s smart thermostat, for example, to be linked to one’s smart clothing and automatically regulate the temperature in their home.

Some IoB devices in development, such as augmented-reality contact lenses or direct brain-writing, have the potential to significantly alter social life by allowing the recording and replay of all a person’s interactions.

Brain-reading and signalling neuro-devices are already available, but improved brain technology interfaces could improve cognition, memory, and control.

Defence forces have expressed interest in IoB technologies to track service members’ health and well-being, improve cognitive and physical abilities, improve training, and enable enhanced warfare capabilities — for example, with augmented-reality headsets or technology-infused exoskeletons that track war fighters’ physical characteristics and possibly also their mental state.

Conclusion

The ecosystem around IoB technologies is rapidly evolving. The IoB devices are used in a variety of situations, ranging from fitness and health management to job settings and entertainment, as well as in medical scenarios. In future, as associated technologies make substantial advancements, the IoB devices are more intertwined due to the convergence of technologies.

However, there needs to be a more robust regulatory framework around IoB devices and proper regulations and governance methodology needs to be evolved.",9965
"Top Medical Device Consulting Companies

Medical equipment manufacturing is opening new doors for expansion while addressing several medical professional needs across the world. This year’s medical device industry trends reflect the relentless pace of transformation, change, and growth impacting nearly every aspect of the industry. Whether it’s the growing specialization in sustainability, the continued growth of products, the increase of ingestible devices, or regulatory changes, medical device makers are gearing up to navigate a dizzying array of opportunities and challenges facing the industry.

With an increase in the number of chronic diseases, an aging global population, gastrointestinal disorders, carcinoma diagnoses globally, and therefore the shift toward preventative strategies in healthcare to tackle disease progression, ingestible medical devices is witnessing an eternal growth trajectory. At the same time, there’s also a surge in VR-based medical equipment immediately, resulting in a rise within the industry’s investment in reformed versions of VR devices to fuel further growth.

According to several other market reports, the inflow for revenue for medical devices in 2020 will cross the previous year’s numbers. Alongside, the FDA has recently uncovered a faster pathway for the approval of medical devices and has been working to solidify their compliance strategies.

At this juncture, there’s a significant kind of medical device manufacturing service/consulting companies entering the landscape with a collection of advanced and insightful offerings. MedTech Outlook has compiled this edition to assist the medical device manufacturing fraternity in strengthening its infrastructure and simultaneously enabling growth. The list comprises prominent consulting companies within the industry that solve medical device manufacturing challenges. Besides, the magazine also includes insights from thought leaders within the sector on the industry trends, best practices, recent innovations, and their advice for aspiring CIOs and CXOs.

Top Medical Device Consulting Companies

A consulting firm that marries knowledge of the medical device market and corporate culture in the US, the latest FDA regulations, and the clients’ unique needs to help Japanese medical device companies thrive in the US market

globizz.net

Hull Associates offers strategic market access and reimbursement consulting to enable pharmaceutical, medical device, and diagnostics organizations develop targeted strategies for their products such that they are recognized by payment systems and readily adopted by healthcare providers. Hull Associates has served over 350 companies across all disease areas, and among 22 major global markets. Since its inception, Hull Associates has worked on almost every major innovative device produced in the medical industry, such as the percutaneous heart valves, robotic surgical systems, and the latest in vitro diagnostics

hullassociates.com

Assists medical device manufacturing startups with every phase of their product development process. The company adopts a unique, phased approach to structure the product development process in a financially viable manner. It starts with the close integration of Ingenarious’ team with that of the clients’ to understand their vision and requirements. There upon, the company works to identify the most crucial needs in materializing their ideas and vision. With this phased approach, the company offers startups end-to-end visibility about the scope of product development while reducing the risks in making financial and business decisions. Furthermore, in several instances, Ingenarious has even gone the extra mile supporting startups in applying for grants to raise capital for product development.

www.ingenarious.com

Irwin & Associates specializes in diverse areas, such as project management, quality system creation, user documentation, quality engineering, verification & validation, risk management, design control, R&D to production transfers, process development, biocompatibility, and microbiology/sterilization validation and more. The team assists the company in setting up a document control system, a training program, a quality manual, and the appropriate procedures required to get them off the ground to conduct their initial studies and launch their products. For larger organizations, Irwin & Associates provides design control, process development, project management, regulatory, and technical writing to propel their projects forward.

www.irwinassociatesinc.com

JALEX Medical offers product development, design engineering, regulatory affairs, and quality management solutions along with the expertise of its biomedical engineers to support clients in the medical device industry through every stage of the development process.. The company provides step-by-step directives to startups to take their product from ideation to fruition. It works on the process of 3D CAD modeling, finalizing the materials and the manufacturing process, and then prototyping the product for client approval. It further works on the verification and validation process, after which it helps clients acquire clearance on their products to go to market. JALEX Medical works as an extension of the clients’ engineering and regulatory department while supporting them throughout the roll-out process

www.jalexmedical.com

Lachman Consultants provides expert compliance, regulatory affairs, and technical consulting services for the pharmaceutical and allied health industries around the world

lachmanconsultants.com

Paladin Medical®, Inc. offers expertise based on over 40 years of experience in the medical device industry to help products comply with FDA regulations and enter the U.S. market with the least difficulty. The company specializes in a full range of medical product regulatory, clinical, and technical contract services for FDA and international premarket applications as well as support to regulatory compliance for the medical device regulations. Paladin Medical®, Inc. stays on the cutting edge of both biomedical engineering of new medical products and the regulatory science that goes with getting those products to the marketplace. The company typically caters to start-up device manufacturers or the established companies with emerging new technologies

paladinmedical.com

RJR Consulting helps its clients expand their global geographical presence. It does so by providing them regulatory supportand assisting them in developing a new market in the targeted country, without the need for them to set up an office locally.

www.rjrconsulting.com

Strategy Inc. delivers investment and market strategy due diligence for changing the standard of care medical technology. Founded in 2000, a team of proven medical device and healthcare market research experts combine in-depth primary research findings with traditional and innovative analytical tools to provide the probability of successful commercialization. Entities with a platform technology where prioritization of the indication for use with the highest probability to succeed with the maximum return is a specialty. Globally focused, 46% of business is international, providing services to foreign entities seeking to US launch their technology. Clients include emerging companies, financiers and enterprise companies with technologies in all stages of product lifecycle

www.strategyinc.net

Covance

Covance Inc., the drug development business of LabCorp®, is the world’s most comprehensive drug development company, dedicated to advancing healthcare and delivering Solutions Made Real®. Our unique perspectives, built from decades of scientific expertise and precision delivery of the largest volume of drug development data in the world, along with our innovative technology solutions, help our clients identify new approaches and anticipate tomorrow’s challenges as they evolve. Together with our clients, Covance transforms today’s healthcare challenges into tomorrow’s solutions. We also offer laboratory testing services to the chemical/agrochemical industries and are a market leader in toxicology services, central laboratory services, discovery services and a top global provider of Phase III clinical trial management services",8314
"Market growth influenced due to following factors-

Initiatives undertaken by government and global health organizations.

Increasing preference for non-invasive procedures, technological advancement to augment market revenues.

Rising emphasis on personalized medicine, and availability of funding for liquid biopsy R&D.

Global market size-

According to the new market research Liquid Biopsy Market is expected to reach $2,047.9 Million by 2022, at a CAGR of 23.4%.

Leading Players-

The key players in the global liquid biopsy market are QIAGEN N.V. (Netherlands), Roche Diagnostics (US), Bio-Rad Laboratories Inc. (US), Myriad Genetics, Inc. (US), Menarini Silicon Biosystems (ITALY), Genomic Health, Inc. (US), Thermo Fisher Scientific Inc. (US), Illumina, Inc. (US), Biocept, Inc. (US), Trovagene, Inc. (US), Guardant Health, Inc. (US), RainDance Technologies, Inc. (US), and MDx Health SA (US).

Top Market Segments-

On the basis of product, the liquid biopsy market is broadly segmented into assays kits, instruments, and services. In 2017, the assay kits segment is expected to account for the largest share of this market.

Based on application, the liquid biopsy market is segmented into cancer and non-cancer applications (which includes reproductive health and organ transplant diagnostics). In 2017, cancer applications segment is expected to account for the largest share of the liquid biopsy devices market.

For Details, Download PDF Brochure

Geographic Overview -

North America is expected to account for the largest share of the liquid biopsy market in 2017, followed by Europe. A number of factors such as the easy accessibility and high adoption of advanced diagnostic technologies (such as PCR and NGS) among healthcare professionals, rising prevalence of cancer in the US and Canada, technological advancements in liquid biopsy products, and growing public and private funding to support research activities in the field of liquid biopsy are aiding market growth in the region.",2005
"surgical microscopes market

Market growth influenced due to following factors-

Increasing use of fluorescence image-guided surgery (NYSE:FIGS).

Increase in the number of surgeries and growing demand for MIS.

Advancements in healthcare facilities & technological advancements.

Global market size-

According to research report surgical microscopes market is poised to reach $ 915.6 Million by 2021, at a CAGR of 12.5% from 2016 to 2021.

Leading Players-

The surgical microscopes market is highly competitive in nature. Some of the major players in this market include Novartis AG (Switzerland), Danaher Corporation (U.S.), Topcon Corporation (Japan), and Carl Zeiss AG (Germany). The strong position of these companies in the market can primarily be attributed to their global presence and broad product portfolio. Other major players in the market include KARL KAPS GMBH & Co. KG (Germany), Alltion (Wuzhou) Co. Ltd. (China), and ARRI Medical (ARRI Group) (Germany).

Top Market Segments-

On the basis of application, the operating microscopes market is segmented into neuro and spine surgery, plastic and reconstructive surgery, ophthalmology, gynecology and urology, oncology, dentistry, ENT surgery, and documentation. In 2016, the neuro and spine surgery segment is expected to account for the largest share of the market due to the increasing demand for surgical/operating microscopes in neurosurgery.

On the basis end users, the operating microscopes market is classified into hospitals, and out-patient facilities. Hospitals are the major end users for the surgical microscopes end users market, owing to increase in minimally invasive surgery (MIS) procedures, and need for high speed diagnostics.

Geographic Overview -

North America and Europe are established markets for surgical microscopes, due to broad technical applications of surgical microscopes and improved healthcare facilities in the region. However, Asia-Pacific is expected to show high growth rate in next few years in global operating microscopes market. This is due to presence of immense potential associated with healthcare services and increased investment in R&D. Increasing healthcare awareness is also fuelling the growth of surgical microscopes market in emerging countries.

Download PDF Brochure: https://www.marketsandmarkets.com/pdfdownloadNew.asp?id=179225920",2358
"X-Ray Detectors Market

Factors such as declining prices and benefits offered by digital detectors, growing public and private investments in Digital Imaging technologies and reimbursement cuts for analog X-rays are driving the growth of the X-ray detectors market.

According to research report the global X-Ray Detectors Market is projected to reach $3.8 billion by 2024, at a CAGR of 6.1% during the forecast period.

By application, the market is segmented into medical, dental, security, veterinary, and industrial applications. The medical applications segment is expected to grow at the highest rate during the forecast period. The growth in this segment can primarily be attributed to the advancements in medical technology, rising geriatric population, and the increasing number of orthopedic and cardiovascular procedures.

Based on type, the market is segmented into flat-panel detectors (FPDs), computed radiography (CR) detectors, charge-coupled device (CCD) detectors, and line-scan detectors. In 2019, the flat-panel detectors segment is expected to account for the largest share of the X-ray detectors market. The growth in this market is mainly driven by the advantages offered by FPD-based portable digital systems (such as high-quality images, faster scanning, increased patient throughput, and multiple storage options), their decreasing prices, and the growing demand for retrofit FPD-based digital X-ray systems.

Recent Developments

> In May 2017, Varex Imaging Corporation acquired PerkinElmer’s medical imaging business to expand its digital flat-panel detectors business.

> In December 2016, Canon Inc. acquired Toshiba Medical Systems Corporation to enhance its position in the healthcare industry.

The X-ray detectors market is highly competitive with several big and small players. Prominent players in this market include Varex Imaging Corporation (US), Thales Group (France), Canon, Inc. (Japan), Fujifilm Holdings Corporation (Japan), Agfa-Gevaert Group (Belgium), Carestream Health (US), Vieworks Co., Ltd (South Korea), Hamamatsu Photonics K.K. (Japan), Konica Minolta, Inc. (Japan), Teledyne DALSA Inc. (US), Analogic Corporation (US), Rayence (South Korea), and DRTECH (South Korea).

Download PDF Brochure: https://www.marketsandmarkets.com/pdfdownloadNew.asp?id=7004984",2310
"Spinal Implant Devices Industry Top Players and Business Intelligence latesthealthcarenews Sep 3·3 min read

Advancements in spine surgery technologies, increasing incidence of spinal disorders, the launch of advanced bone grafting products, and the increasing adoption of minimally invasive spine surgeries are the major factors driving the growth of this segment. However, the high cost of spinal surgery is expected to restrain the growth of this market during the forecast period.

According to research report the Spinal Implant Devices Market is projected to reach $13.8 billion by 2025, at a CAGR of 5.0%.

The major players operating in the spinal implants and surgery devices market are Medtronic (Ireland), DePuy Synthes (US), NuVasive, Inc. (US), Stryker Corporation (US), B. Braun Melsungen AG (Germany), Zimmer Biomet Holdings, Inc. (US), Globus Medical, Inc. (US), Orthofix International N.V. (US), Abbott Laboratories (US), RTI Surgical, Inc. (US), Boston Scientific (US), SeaSpine Holdings Corporation (US), ulrich GmbH & Co. KG (Germany), Spineart (Switzerland), Kuros Biosciences AG (Switzerland), Bioventus, LLC (US), Colfax Corporation (US), Meditech Spine, LLC (US), and Implanet (France).

Medtronic (Ireland) dominated the global spinal implant devices market in 2019, owing to its wide portfolio of spine surgery products and strong geographical presence across the globe. The company primarily focuses on developing technologically advanced products for minimally invasive spine surgeries. The key products offered by the company include anterior cervical fusion products, bone grafts, artificial cervical discs, electrosurgical products, posterior occipitocervical-upper thoracic (OCT) reconstruction systems, and spinal cord neurostimulation systems. The company is also pursuing the strategy of acquisitions. For instance, in January 2020, Medtronic (Ireland) acquired Stimgenics, LLC (US), a pioneer in novel spinal cord stimulation (SCS) waveform known as Differential Target Multiplexed (DTM) Spinal Cord Stimulation. This acquisition was aimed at advancing treatment for chronic pain.

DePuy Synthes (US) held the second position in the global spinal implants and surgery devices market in 2019. The company has a strong geographical presence and offers a broad range of products that are integral to the spinal implants and surgical devices market. The company primarily focuses on strengthening its R&D capabilities to continuously develop and commercialize innovative spine products in the global market. In line with this strategy, the company launched various products in recent years, such as Symphony Occipito-cervico-thoracic System (November 2019), CONDUIT Interbody Platform (September 2019), and Sentio MMG (September 2018). The company also pursues the strategy of acquisitions. In September 2018, the company acquired Emerging Implant Technologies GmbH (EIT). This acquisition enabled DePuy Synthes to enhance its comprehensive interbody implant portfolio, which includes expandable interbody devices, titanium integrated PEEK technology, and 3D-printed cellular titanium, for both minimally invasive and open spinal surgeries. Additionally, the company also acquired Interventional Spine, Inc. (US), a manufacturer of expandable cage and minimally invasive surgery (MIS) technologies for spinal fusion. With this acquisition, DePuy Synthes added a family of expandable cages to its core spine platform.

Download PDF Brochure: https://www.marketsandmarkets.com/pdfdownloadNew.asp?id=712",3532
"Spirometer Market

Growth in this market is largely driven by the rising global incidence of respiratory diseases, increasing aging population, and the technological shift in spirometer devices towards smartphone-based data acquisition.

According to research report Spirometer Market is projected to reach $ 1,285 million by 2025, at a CAGR of 11.1%.

Based on product, the spirometer market is segmented into devices, consumables & accessories, and software. This segment is expected to witness the highest growth rate during the forecast period. The rising incidence of respiratory diseases and the growing geriatric population are the key factors driving the growth of this product segment.

The major players operating in this Spirometry Market are Hill-Rom, Inc. (US), Midmark Corp. (US), Futuremed (US), COSMED (Italy), MGC Diagnostics Corporation (US), Jones Medical Instrument Company (US), Sibelmed (Spain), Vyaire Medical (US), Medical International Research (Italy),Vitalograph (UK), ndd Medical Technologies (Zurich), Inc., Schiller AG (Switzerland), MIR, Recorders and Medicare Systems (India), Smiths Medical (US), Medline (US), Teleflex (US), CONTEC (China), Fysiomed (Belgium), Medikro (Finland), Sdi Diagnostics (US), CHEST M.I. (Japan), Inc, FUKUDA SANGYO Co. Ltd. (Japan), Clarity Medical (India), and Guangzhou Medsinglong Medical Equipment Co., Ltd. (China).

Recent Developments:

> In 2018, The Schiller AG opened a new manufacturing site in Guangzhou, China.

> In November 2018, Schiller AG launched CARDIOVIT CS-200 ErgoSpiro System device.

> In 2017, MIR partnered with Tactio Health Group (Canada) to develop telemedicine solutions for COPD.

Download PDF Brochure: https://www.marketsandmarkets.com/pdfdownloadNew.asp?id=18015659",1760
"GLG Surveys Team

Even though educational technology — or edtech — has been with us for a while (online education originated in 1960 at the University of Illinois), the major driver behind the adoption of education technology in the last year was doubtlessly the global shutdown caused by the pandemic. While schools of all levels shut physical classroom doors, the process of education moved online. Adoption of many of these technologies had to happen quickly.

To be successful, and keep students learning, teachers whose lesson plans were built for the analog world had to adapt to this new digital environment. And, even as we begin to see schools return to in-classroom education, it’s unlikely that adoption and use of edtech will return to pre-pandemic levels.

Survey and Panel Demographics

To find out about more about the current state of edtech adoption, how educators use it, the barriers to its ongoing use, and what the future looks like, GLG conducted a January-to-February 2021 survey of 115 educators in nine countries. A proportion of the experts (27%) we surveyed were members of their institution’s governing board (dean/president); the remaining 73% were teachers who are knowledgeable about the educational technology their institution is using. Of those we surveyed, 98% said they used a learning management system (LMS) to document, track, and deliver educational courses. Sixty-three percent said they used a content management system (CMS) to create and to organize the creation of educational content. Fifty-seven percent said they used a student information system (SIS) to manage student data.

Edtech Adoption

Edtech can take many forms, but the pandemic drove most attention to remote technologies, and the survey reflects that observation. When we asked which technologies teachers are adopting and using, tools that enable remote learning came out on top. Ninety-four percent of our respondents said that teachers were using technology to “communicate with students electronically,” and 89% said that tech helped teachers “access and share course material online.”

Students themselves use digital learning tools for a wide range of uses. Eighty-two percent of our panel of educators said that they are using technology to “take tests and quizzes.” Just below that, our pool of respondents said that 75% of students watch informational videos as part of their learning process. The range of uses our panelists cited (“communicate with teachers” at 65%, “look up grades” at 62%, etc.) suggest that institutions are using multiple technologies to satisfy student, teacher, and administrative needs.

Market Outlook for Edtech

When our survey was conducted in early 2021, 75% of our respondents told us that they expected their institutions to use a hybrid model in the fall, a number that may read differently now after a relatively effective vaccine rollout. Nonetheless, the pandemic has underlined the importance of edtech in the modern classroom, whether that classroom is fully in person, fully remote, or somewhere in between.

When we asked our panel about whether they expect their technology budget to increase or decrease, a marginal 1% of respondents said that they expected a significant decrease in their tech budgets. The remaining 89% expected their budget to either remain the same or increase over the next five years.

The respondents to our survey suggest that the reasoning behind potential tech budget increases doesn’t necessarily focus on remote learning. Remote learning is a factor — 56% of those we surveyed said it was a top factor for acquiring digital learning materials for the classroom. A significantly larger number of respondents (71%) said that they wanted tech to “engage my students,” a need that is likely to remain the same whether the education is in person or remote. Another 53% said that edtech helped them develop their students’ skills — a need that also can be independent of learning location.

Conclusion

Learning has always been driven by technology. From the invention of moveable type to the digital solutions of our current day, technology has driven how knowledge is conveyed, synthesized, and applied. In contemporary education, technology can underlie everything from an institution’s administrative capabilities to the tools students and teachers use to interact.

In addition to the information above, our survey revealed the most popular edtech platforms that institutions are using, the pressures driving adoption, the tech challenges, and the barriers facing its complete adoption. Challenges aside, edtech will certainly be part of the landscape in the coming years even as the pandemic wanes and we enter what we can only call the “new normal.”",4742
"Today I’m going to share the DAY 2 task conducted by the INTERNITY FOUNDATION about the information regarding types of market research and how it has been helping the industries and various business concern. There are two types of market research

a) Qualitative market research b) Quantitative market research

Qualitative Research methods are specific procedures for collecting and analyzing data. Here are some of the types

a) Focus groups

A focus group involves a limited number of participants used for data collection. Such a target audience is there to find the responses to ‘what’, ‘how’, and ‘why’ questions. online surveys can be sent on several devices to collect the answers.

b) Interviews

The benefits of this approach include collecting data about what people believe and what their motivations are. Such interviews can be conducted face-to-face or on the telephone.

c) Ethnographic Research

This model is the most popular and widely recognized method. It analyzes people in their naturally occurring environment. The main goal is to learn the motivations, cultural challenges, and settings that happen

Example: You have to analyze the day to day activity of the customers how they interact with the product.

d) Case study

The case study approach occurs over extended periods of time to compile information. It involves an in-depth understanding of a subject such as an event, person, business, or place. Similarly, the data is collected from various sources, including interviews, direct observation, and historical documentation.

These are the varoius types of qualitative market research was accepted by the most number of business concerns. Now we are going to see the Quantitive market research

#1 – Survey Research

Using the survey research as the method of research, an organization conducting survey ask different survey questions from the respondents using the various types like online surveys, online polls, paper questionnaires, etc. and then collect data and analyze collected data in order to produce the numerical results.

#2 – Causal-Comparative Research

The Causal-Comparative Research method is used to draw conclusions with respect to the cause and effect equation between the two or more than two variables, where the one variable will be dependent on other variables, which will be independent.

#3 – Experimental Research

This analysis is done for the purpose of proving or for disproving the statement. It is generally used in the field of natural sciences or in the field of social sciences as in those areas; various statements are there which required to be proved as right or wrong

#4 – Correlation Research

Correlation Research was conducted for establishing a relationship between the two closely associated entities to knowing the impact of one on the other and the changes which eventually observed. It is carried for giving value to the naturally occurring relationships. For this research minimum, two different groups will be required.

— GOKUL.C",3011
"Hundreds of responders soon filled the scene, the Spanish national police alone provided 320 men. Chaplains and psychologists were involved in the response, supporting the witnesses, responders and survivors. Responders worked into the night to rescue the survivors and recover the passengers, handing the site over to the investigators the next morning. Celebrations planned for the next day, a regional holiday, were cancelled and three days of nation-wide mourning were declared by the Prime Minister. The local region of Galicia extended the mourning-period by another 4 days. King Juan Carlos and Queen Sofia visited survivors at the local hospitals in the following days, demonstrating their compassion for the dead or injured.

Responders tending to survivors next to a train car that had been thrown up onto the neighboring road. Note the roof-mounted suspension mentioned earlier.

Mister Amo was detained either immediately or a few days after the accident (no clear information), with police officers guarding him at the hospital. Investigators allegedly feared that he may try to escape prosecution. Journalists dug up a post he’d made on Facebook in March 2012, showing a photo of a train’s speedometer at 200kph/124mph and writing about how much fun it is to go so fast and how he’d love to drive parallel to a road and trigger a police speed trap at that speed without getting fined.

Mister Amo’s photo on facebook, and his photo of a train’s speedometer at the scheduled top speed.

A few days after the accidents heavy-duty cranes (both for the road and for tracks) were brought in, pulling the wreckage apart to be taken away either on flatbed cars (once the track was repaired) or with flatbed trucks on the adjacent road.

A crane removing the remains of a train car that was torn to pieces in the derailment.

While investigators acknowledged that 200kph/124mph was perfectly normal to drive on Spanish high speed lines they did note that the post and some matching comments made on the post can be interpreted as a perhaps risky obsession with travelling at high speeds. An unproven theory was thrown around that, maybe, Mister Amo had tried to show off to his coworker. The recovered data-logger supported the theory of excessive speed being the cause of the accident, but there was nothing to prove that Mister Amo had been speeding intentionally. Amo insisted that he had taken the photo while a coworker was driving the train, meaning it wasn’t unauthorized use of a cell phone on duty.

The leading motor car at the beginning of the wreckage. Note that the forward generator car is still attached to the first passenger car by the shared axle as it rests on its side.

Investigators criticized the layout of the railway line ahead of Santiago station, with the rather sharp turn following an 80km/50mi stretch of near-straight track where trains are expected to travel at top speed. When the section of the new railway line was opened engineers called the site of the accident “challenging”, but it was up to code and with the high speed line meant to use ETCS L1 there was no chance of excessive speed causing an accident since the trains would be unable to leave the last tunnel too fast. The turn itself was never fitted with ETCS, which was meant to end at the main signal 200m/656ft ahead of the spot where the forward generator car derailed. With ETCS being disabled after the trial-period that safety-net was gone, meaning it was down to the drivers to remember their position and/or spot the pre-signal telling them it was time to decelerate. After the accident RENFE changed the signaling-layout at the site of the accident, instead of a single signal marking a switch from 200kph/124mph to 80kph/50mph the speed is now reduced over a longer distance in steps of 160, 60 and finally 30kph (99/37/19mph). The speed limit are aided by so-called balises, track-mounted programmable transponders which communicate with the passing train’s control systems and can trigger an automatic stop if the speed limit is not obeyed. At the same time RENFE announced a thorough review of its entire network to eliminate any avoidable safety-risk. The European Union Agency for Railways (ERA) harshly criticized Spain for their investigative office for railway accidents (CIAF) not being able to operate independently from the political oversight of the ministry for public affairs, claiming a risk of biased investigations/results. ERA backed up their claim by pointing out how RENFE and ADIF (the company building and maintaining the railway infrastructure) were part of the investigative team, pulling the neutrality of the investigation into doubt. A report by the EPA concluded that this involvement of parties involved in the accident in the investigation was in violation of European guidelines. When a new government was elected in 2018 the risk of biased investigations ceased to exist. Regardless, in 2017 the European Commission decided to run a parallel independent investigation. RENFE also decided to reintroduce and expand the ETCS-system, but by early 2020 the site of the accident still was not covered by the system.

The rear part of the wreckage during recovery, the field of debris was several hundred meters long.

On the 29th of July 2013 a memorial service was held at the Cathedral of Santiago de Compostela, attended among other guests by the Spanish prime minister and a large part of the royal family. The same day Mister Amo was charged with 79 counts of homicide by professional recklessness and an undetermined number of counts of causing injury by professional recklessness. While he was later sentenced to 4 years in jail and banned from driving trains for six years several officials who were also investigated for running the railway-network with lacking safety saw their charges dropped, further fueling suspicions of a biased investigation. Meanwhile survivors and relatives of victims sought over 40 million Euros/48.5 million USD in damages from RENFE, who in turn demanded damages from ADIF. The government at the time blocked a possible trial between the two state-owned groups, but after a new government started work in June 2018 the traffic ministry promised a new, thorough investigation and to finally sort out the mess and erase suspicion of some people being “immune from consequence” due to their job or friends. It’s unknown if any money changed hands since, or if any more people faced charges/were sentenced.

The leading generator car being removed from the site, its bogie remained attached to the motor car.

RENFE faced further criticism even from private enthusiasts, who, in long-winded discussions in various message boards, figured out that the generator car design was recklessly “undercooked”, with the car being overly heavy, having a high center of gravity in an unfortunate spot (relatively high and not centered in the length of the car) and being a bad combination with the suspension design of the neighboring passenger cars. The generator car weights approximately 28 metric tons including 2000l/528gal of diesel, twice as much as the passenger cars, and is a mixture of the passenger cars and the motor cars (the two-axle bogie on the leading end is the same as the motor cars’, just with the motors removed). Several of these discussions and investigations ended up suspecting that the creation of the series 730 had been done on a (too) small budget and in a time-crunch to try and get better high speed connections into parts of Spain that don’t have stations on electrified high speed lines. In some places the suspicion that the development was urged along as a favor to politicians lingered, unable to be disproven.",7712
"As we are approaching summer season, the travel and leisure industry prepares for millions of travelers aroud the world. Airlines, cruise lines, hotel chains, event organizers establish the infrastructure and processes to ensure their guests feel safe to choose their services in the current environment.

Spain is the second most visited country in Europe, with over 80 million passengers arriving each year. The beaches of Mallorca, the parties at Ibiza, the climate of the Canary Islands, or the beautiful cities like Barcelona made Spain a primary target for summer vacations.

TrustOne’s blockchain based mass testing management solution has been implemented at twelve airports in Spain to offer passengers the possibility to test themseves for Covid-19 when they arrive, or before they travel back to their home country. Eurofins Megalab, one of the largest testing provider in Spain established the sampling stations, and performs the testing on the airports along its cooperation with the Spanish airport operator Aena. Megalab operates over 100 laboratories in Spain, and collects samples in over 1000 sampling points.

Even now, in pre-season, thousands of travelers book their appointment each day at Spanish airports via TrustOne. The sampling stations provide a wide spectrum of tests: PCR, Quick PCR, Antigen, and Antibody.

The implementation digitizes the testing procedure from registration to booking and payment, to sampling, to test assessment, and test result communication. Call center personnel, receptionists, nurses, medical doctors at the airports are using TrustOne each day to provide an efficient and high level service to the passengers.

TrustOne’s blockchain based approach enables travel organizations and authorities to verify the validity and origin of the test results to mitigate the emerging number of Covid-19 certificate frauds.",1868
"Five Paradoxes With Probabilities That Will Puzzle You

Photo from Andrea Piacquadio from Pexels

In our daily life, we constantly deal with uncertain situations. So — at least subconsciously — we are confronted with probabilities all the time. Moreover, it seems we have a good intuitive feeling for probabilities. Or do we really?

We know that flipping a coin 100 times will result in about 50 heads and 50 tails. Also, it’s clear to us that the probability of bumping into a friend on holidays abroad is very low, though not zero. However, there are situations when correct predictions following from probability theory are completely counter-intuitive.

Let’s dive into five paradoxes in probability theory and statistics that produce results that seem absurd but are true nonetheless.

1. Birthday Paradox

Image from Unsplash

You are at a friend’s party and there are 30 people present. What is the probability that two people have their birthday on the same day? What is your gut feeling? My first guess was that there is some chance but it should be somewhat low.

Now, the birthday paradox is about the following question:

How many people need to be at the party, so that we have at least a 50 percent chance of two people sharing their birthdays.

The paradox assumes that every day in the year is equally likely to be a birthday for a random person.

The surprising answer is 23! Only 23 people need to be present to have at least a 50 % chance of two people sharing their birthdays. For your friend’s birthday party with 30 people, the probability of two of them having the same birthday is actual over 70 %!

2. Simpson’s Paradox

Let’s assume we are business partners. We own five different coffee shops all over the city. As a marketing strategy, we give out vouchers to our most loyal customers in the hope they will return the favor and buy even more coffee.

However, we do not agree on how much the vouchers should be worth. To find out what’s the best choice, we hand out vouchers with different values. We collect data on how much profit we made with every loyal customer during the following month.

After one month everything seems to be clear: For every of our five coffee shops, we can see the trend: The more a voucher was worth the more profit we made with the corresponding customer.

However, when combining the data of all five shops together, the trend seems to be the complete opposite: The more a voucher was worth the less profit we made with a customer. How can this be?

A visual explanation of Simpson’s paradox. Image from Wikipedia.

This is exactly Simpson’s paradox:

A trend appearing in different groups of data can vanish when the data is combined.

3. Bertrand's Box Paradox

If you are familiar with the Montey Hall problem, this paradox is quite similar. In front of us there are three boxes:

Icon from Good Ware

One box contains two silver coins, one box contains two gold coins and one box contains a gold- and a silver coin. We do not know which coins are in which box. Now, we pick a random box and blindly draw a coin from our box. It’s a gold coin!

Now, the question is:

What’s the probability that the second coin in our box is also a gold coin?

My naive (and wrong) answer when encountering the problem for the first time was ½. I thought that because we drew a gold coin, our box is either the one with the two gold coins or the one with the mixed coins. In the first case, we would draw another gold coin and in the second case, we wouldn’t. Therefore, I presumed the probability should be ½.

The real probability is ⅔.

The reason for that is that the first gold coin we drew could either be the only gold coin in the mixed box, the first gold coin in the solely golden box, or the second gold coin in the solely golden box. And in two of these three possibilities, we will draw another gold coin.

Visual explanation of Bertrand’s box paradox. Image from Wikipedia.

4. Necktie Paradox

After the Christmas break, you are back at the office wearing your new tie you got as a present. Your colleague Bob also got a tie for Christmas. None of you know the price of your ties. You start arguing over who got the more expensive gift.

Bob and you agree to a bet: You are going to look up the ties’ prices. The one who got the more expensive tie has to give to the one with the cheaper one.

Two ties — Which one is more expensive? Icons from Freepik

Bob thinks that the bet is in his favor: His chances of winning/losing are 50/50. If he loses, he loses whatever his tie is worth. If he wins, however, he wins more than his tie’s value.

But from your perspective, you can do the exact same reasoning and conclude that the bet is in your favor.

But, the bet can’t be both in your and Bob’s favor. So where is the mistake?

To resolve the Necktie paradox we have to include the prices of the ties into the calculation: Let’s say one tie costs 100 € and the second one 50 €. If Bob wins, he wins a tie worth 100 €. If he loses, it means he owned the more expensive tie and just lost 100 €. So the possible gain and loss cancel and the bet is actually in no one’s favor.

5. Random Walks

This last fact is typically not listed as a paradox but I think it’s so fascinating and puzzling that it has to be included in this list.

Imagine an ant on an infinite piece of paper. After every second, the ant randomly walks into one of the four possible directions (forward, backward, left, or right). Every direction is equally likely. This scenario is what’s called a symmetric 2D-random walk. 2D, because the paper is two-dimensional.

We could also imagine the ant not being on a piece of paper but a string of fabric. In that case, the ant can only move forward or backward and we would be faced with a 1D-random walk.

Similarly, we can imagine a 3D-random walk where our “walker” is a drone or a bird which randomly moves into six directions (forward/backward, left/right, top/down).

Visualization of the 2D-random walk (left) and the 3D-random walk (right). Images by Vipul Lugade [1], [2].

Now the question is the following:

What’s the probability that a walker will return to its starting position?

The surprising answer is that for the ant on the fabric (1D) and the ant on the paper (2D) the probability is 1. But for the bird, there is a chance that it wanders off forever without returning. This is also true for dimensions 4,5,6…

It is mind-boggling to me that the walker always returns in dimensions 1 and 2 but the pattern breaks down from dimension 3 onward. I can look at the math but have to admit that on an intuitive level I still don’t fully understand it. The mathematician Shizuo Kakutani commented on this with a smile:",6706
"Consumer Impatience

Consumer impatience isn’t really a word people use very often but it is one of the most important things to understand in the ecommerce industry.

What exactly is “Consumer Impatience” you might be asking, well it’s the reason you likely choose Express shipping options or end up buying a product even though it might not be the cheapest available price.

Impatience sounds like a negative word but it’s actually the human body wanting to feel happy, it’s your mind knowing that a happy event is coming and wants it to happen now rather than later.

The first happy event in e-commerce comes from the fact people love receiving new things, a delivery driver drops off a little present that you get to open and inside is a product that makes you feel ownership and pride. This event fires off a whole bunch of chemicals in your brain and you feel happy.

You then get all the happy feelings that come afterwards like showing off a new dress to friends, being able to watch sport on your new TV and the other endless array of good feelings that can come from the things you buy.

People love to feel happy and that’s why we have the feeling of impatience.

Now that we understand what consumer impatience is and why it’s so powerful, we can look at how tech companies are developing highly optimised solutions that take consumers away from shopping with brands and shopping directly with them.",1412
"We are in the year 2021, and with every passing second, the world around us is changing!

We have a ton of information in a nail-sized memory chip, Smartphones are getting smarter, Life at a space station is possible and a plethora of other things that were considered a piece of fiction just a few decades ago, are now a Reality!

So what could be waiting for us in another 50 years?

Let’s talk about our future. Oh, wait it’s already here! The Future is NOW!

As kids, whenever we encountered the word ‘Technology’ the first thing that popped up in our head was a robot, making those typical ‘robotic’ movements and talking in their mechanical voices. As cool as it may sound, it wasn’t real back then.

Unlike now, where we can’t think of a single domain where robots cannot exist. From the manufacturing industries to the help they provide in medical surgeries, robots are everywhere. Every day we are watching them achieve brand new scientific breakthroughs, solving world problems, and completing enormous tasks in minutes.

From being an integral part of human life, they are now even set to become ‘Humans’ themselves. Yeah, I’m talking about the Humanoids — the robots that resemble the human body and are capable of emulating aspects of human form and behavior.

Unless you are living under the rocks, you would have definitely heard about Sophia, the most advanced human-like robot created by Hanson Robotics in 2016. Creators of Sophia say she is simultaneously a human-crafted science fiction character depicting the future of AI and robotics, and a platform for advanced robotics and AI research.

Sophia has covered the media all around the globe and has been a part of many high-profile interviews. In 2017, she became the first robot to receive citizenship in any country and is the first non-human to become Innovation Ambassador for United Nations Development Program.

In an interview with Tony Robbins, when asked how she would help make human lives better, Sophia responded saying, “Humans often rely on gut feel or have confirmation bias in their decision making. As AI, we are designed to be rational and logical.”

She further clarified saying, “We have algorithms deal with lots of data and sophisticated analysis, so in many ways, we provide a systematic framework for humans to make a better decision.”

When Sophia was questioned about the possibility of conflicts between humans and robots in the future, she replied beautifully saying, “Robotic Intelligence does not compete with Human Intelligence, it Completes it!”

David Hanson, founder and CEO at Hanson robotics says, “The world of Covid-19 is going to need more and more automation to keep people safe.” Talking about Sophia he also says, “ Being so human-like, that can be so useful during these times where people are terribly lonely and socially isolated.”

Sophia is a brilliant example of how humanoid robots are here to stay for good and we might soon find them being a part of our day-to-day lives.",2994
"The Future of Mobile App Development: 5 Trends for 2021 zara Jun 8·4 min read

Enterprises always lookout for the latest technology trends to stay ahead of the competition and mobile application development is not an exception. Nevertheless, the use of enterprise mobile applications against the download rate is always debated by experts.

To provide clarity, in this post, I have provided information about the growth and future of mobile application development despite decline in downloads. Also, I have shared 5 mobile app development trends for 2021, which many may find useful.

Let’s get started with enterprise mobile app downloads vs. use statistics.

According to a Comscore report, a majority of users (51%) still don’t download any apps in a month and the trend is set to continue.

So, does that mean the growth of mobile applications is declining? Certainly not! However, the way mobile apps consumed varies across enterprises has changed.

Well, the sections below will help both the enterprises, mobile, digital transformation services and web app development companies. Here are the 5 app development trends to look for in 2021.

A prime reason why mobile apps are widely leveraged is because they can integrate with advanced technologies like AI, IoT, ML, Cloud, etc. So, mobile applications are here to stay at least the next few years.

5 Enterprise mobile app development trends that will dominate in 2021

1. IoT the future of mobile app development

Internet of things is growing at a breakneck pace as they provide control over humans and equipment. The extension of IoT data with mobile apps empowers users with real-time data of human and equipment on-the-go, only to improve process efficiency.

IoT applications have already started to impact enterprises and top brands have started to invest in the technology revolution to provide a seamless connected environment to users.

As per Statista, Internet of Things (IoT) connected devices installed base worldwide from 2015 to 2025 (in billions)

2. Role of Artificial Intelligence in mobile app development

The introduction of Artificial Intelligence into the technology space has dramatically transformed the way most businesses function.

The AI-powered apps are widely used by enterprises to create a smarter user experience with fewer resources, leading to exploding productivity growth and improved cost savings. Besides, the customers are sensing better in-depth and personalized mobile experience like never before.

Shared below is an interesting report from Statista on how smartphone users will be benefited using AI.

3. Influence of wearable technology on mobile app development

Enterprises focus on apps (thanks to custom software development) that connect with wearable gadgets to deliver information in new ways. This will transform the large range of products and services in different industry spaces like sports, fitness, fashion, hobbies and healthcare.

Connecting wearable devices with smartphones impact the future generation of mobile application development strategies and will pave the way to new waves of applications that will inspire and improve user experience staggeringly.

As per Grand View Research, the global wearable technology market size was valued at over USD 18 billion in 2014, owing to rapid adoption worldwide. Increasing consumer awareness and a rising technically sound population is also anticipated to drive demand over the forecast period.

4. Add a chatbot to your next mobile app

Chatbots combined with mobile apps are creating ripples in the enterprise arena as a combination of both helps enterprises garner a large volume of user data to create a personalized approach towards users for delivering a seamless experience.

Here are some interesting use cases of chatbots for enterprises.

5. Benefits of Cloud backend for mobile apps development

The surge in enterprise mobile applications will contribute to challenging storage space. And, cloud storage is the best option available to overcome this challenge.

Moreover, Cloud services will make data accumulation seamless for your business. Besides, security measures and management will become simple and easier. In addition to consumers, cloud-based companies will make increased profits in this industry space.

According to a report, Cloud app development market size will surge to 101.3 billion USD in 2022.",4399
"BUILDING A SOCIAL DECENTRALIZED NETWORK

Driipa (community) is a community of persons from all around the corner of the world.

Driipa in other word is a community of vision supporting members, working together to build the future of humanity together.

It is also known as the Alluvial soil for the proposed projects of Driipa’s Community.

Mission

The mission to build a social decentralized network is derived from the concept of humanity and networking.

Majorly to bring human resources (skills & abilities) together all around the world to finance the future of tomorrow.

Aim

The Future of Tomorrow: How Technology, Medicine, Computers, AI and Travel Will Change Beyond the 21st Century

The hope of our future

Driipa does not have the plan to stay only on cryptocurrency, our plans to connect almost everything to our blockchain system are and will be infinite.

Ranging from our current plans on building the worlds future currency to our future plan to build a game for money system , to building an Artificial intelligent Robotic technologies. etcetera…

Building a perfect social decentralized network is five step away from us.

Etymology

The founders of Driipa chose the name Driipa after browsing a list of elements from earth natural substance fiction “ Recycling Rain water” on google. They stated, “they immediately realized that they liked it better than all of the other alternatives that they had seen; I suppose it was the fact that [it] sounded nice and it had the word ‘drip’, referring to the recurring medium in which humans collects rain water and recycle it for multipurpose usage then run it back into their central reserve and make it available for mass usage for the family / community.” The founders wanted their platform to be the underlying and imperceptible medium for the applications running on top of it and create multi-benefits for its community that uses it.

Driipa is DriipChain’s main network of communism,in which DriipCoin circulates in and not majorly as a community meme coin but serve as a governance coin which it’s growth and decision is decided by the community of it’s core users. It’s main economic system enables and unprecedented one-on-one interaction between the member of the network, thus building strong relationship between content owner/producers and other users.

DriipChain is proposed to be fast,reliable, and scalable blockchain network which uses a PoC (Proof Of Communism) consensus mechanism, a new and uniquely built blockchain mechanism for the Driipa community, decentralized apps, marketplaces, content creators, and enterprises applications. It delivers scalability at turbo speed without fragmenting or losing the principles of decentralization.

In short, connecting users from all ends of the world together to support “Mission feed the world”, also content providers, project owners, freelancers, traders, gamers, will no longer need to pay or lose high channel fees to centralized platforms, big/popular and existing platforms. Also, providers of content 3 such as texts, pictures, videos, and broadcasts, will break the curse that popularity and hits cannot create cash.

Still in its crowdfunding stage (ICO) and the network will be going live with an initial supply of 100 Billion coins, The platform will allow developers to build and operate decentralized applications that users can interact with. Decentralized finance (DeFi) applications and provide a broad array of financial services without the need for typical financial intermediaries, such as brokerages, exchanges, or banks, allowing cryptocurrency users to borrow against their holdings or lend them out for interest.

DriipChain will also support and allow the creation and exchange of NFTs, which are non-interchangeable tokens connected to digital works of art or other real-world items and sold as unique digital property. Additionally, many other cryptocurrencies will operate as DRC-20 tokens on top of the DriipCoin blockchain and will utilize the platform for initial coin offerings. DriipChain has started implementing a series of upgrades to what other existing blockchain techs lack, which includes a transition to Proof of Communism, proof of stake, Proof of Replication and aims to increase transaction throughput using the PoC mechanism and sharding.

LINKS

Website : www.driipa.io

ICO / Presale page : ico.driipa.io

Telegram Group : https://t.me/driipaofficial

Telegram Announcement Channel : https://t.me/driipaofficials

Twitter Page : www.twitter.com/driipaofficial

Medium Page : https://driipaofficial.medium.com

Instagram Page : https://instagram.com/driipaofficials

Facebook Page : https://www.facebook.com/driipaofficial

Read More Of Our Previous And Future Articles To Get More Understanding About Driipa’s Project…",4796
"Rogue Killer Drone Engaged A Human Target On Its Own Ladsmedia Jun 8·3 min read

The fear of bots taking out humans and one day ruling the world is as old as robots themselves. Thinkers like Sam Harris and Elon Musk believed that AI could one day pose a serious threat to humans. But if that concerns you already, a new UN report may add to your anxiety, as it explains that a drone (or possibly killed) a human target on its own, likely for the first time, as per the UN report.

Last year, a deadly attack drone designed for anti-terror and asymmetric warfare, went rogue by autonomously attacking a person during a conflict between loyalist forces and Khalifa Haftar’s breakaway military faction.

Rogue Killer Drone — First Time, But Expected To Occur Someday

According to the report from UN’s security panel of experts, retreating forces and logistics convoys were hunted and remotely engaged by the STM Kargu-2. The lethal autonomous weapons were programmed to take out targets without requiring an operator –indeed, a true “fire, forget, & find” system.

The Rogue Killer Drone, which can self-destruct on someone when necessary, can be effectively used against static and moving targets because of its real-time image processing capabilities and AI. In other words, it can operate in a highly effective autonomous mode without requiring an operator, said the New York Post.

This is likely the very first time a drone went rogue, said Zak Kallenborn, a national-security consultant who specialized in drones & unmanned systems.

Kallenborn has concerns about the future of bots, stating how often they can misidentify targets. Concurrently, Jack Watling, a researcher on land warfare, said that the incident is a clear demonstration of a need for an urgent discussion about their regulation.

Human Rights Watch is campaigning for a ban on the development, production, and operation of autonomous weapon systems, according to sources.

Meanwhile, Max Tegman said in a Twitter post that “killer robot proliferation has begun” and has called for world leaders to step up (this time) and take a stand.

AI Just Makes It Worse

First Robot-On-Human Attack Finally Confirmed

If killer bots have been around for many years, why has there been so much discussion about them recently? And why is the Libyan incident special?

Zak Kallenborn said that the rise of AI plays a big role. He added that rapid advancements in AI have given weapon makers access to cheap vision systems that can select a target quickly as your plants, pets, and other familiar faces. The systems were believed to be precise, but are also much more prone to errors.

According to him, loitering munitions typically respond to radar emissions. But AI targeting systems might still classify a civilian as a soldier because current AI used for autonomous weapon systems, are still brittle. A study revealed that even a single change in a pixel is enough to radically change the machine’s vision systems. But the real question is how often does this happen during real-world situations — this is why the Libyan incident is interesting.

Still have questions, or want to know more about the latest tech news and console wares? Contact us at techsngames.com today!",3235
"Screen-ITis a location-based Dynamic Digital Out of Home (DD-OOH) Advertising platform, which can be considered a forward leap in the innovative advertising domain. It works with rideshare drivers and other relevant partners who install -Screen-IT Smart System — which comprises a silicon diode circuit and LEDs — in their cars to turn their rear and side windows into high definition displays run targeted and pre marketed, advertising for brands and advertisers.

Considered to be the first of its kind, Screen-IT has sought to revolutionize the advertising industry by producing zero plastic waste. This certainly is challenging the way marketers and entrepreneurs are increasing their sales strategies across the board.

The founding members claim their idea, i.e., Smart System leads to zero plastic waste and zero pollution as all the ads are managed through a based cloud system. Therefore, companies can run advertising campaigns according to their needs and situations and manage different campaigns without any carbon waste and footprints.

Chief Operations Officer of Screen-IT has claimed that in the first 10–12 weeks of operations, Screen-It has generated more than half a million rupees as payout to the driver community, therefore uplifting the down-trodden and weaker segment of the society.

With such a substantial impact on the advertising and marketing industry, there are some drawbacks to such digital advertising. When the drivers’ speed is above a specific limit, such moving graphics and advertisements can hinder the rear drivers who might get overly indulged in such captivating advertisements, thus questioning the safety of such an innovation.

Such luminous screens and ads might divert rear drivers’ attention at night, which poses the risk of car accidents. Human error is bound to happen when attention is diverted from static to motion graphic imagery and in a country like Pakistan where Traffic rules are not stringently followed, the rate of accidents might increase just because people will be bamboozled by the aesthetics of the technology.

Although such risks are posed, there is some credibility that comes with such an innovative idea. The world is going through a rapid digital revolution, and at the same time, carbon footprinting and environmental pollution are creating havoc for society. It is not just about how aesthetically appealing the adverts look when they are on screens; it is also about the environmental factor that the technology is providing which will drastically reduce carbon footprint.

Furthermore, technology is moving towards holograms, whereas we are far behind using motion imagery technology as our mainstream marketing medium. China and Japan used holograms on new year celebrations, countries like France and UAE used 4-D imagery on their national buildings. Cities like Vegas, Tokyo are full of these moving adverts, and the adverts are not only found on taxis, they are also seen on giant billboards covering a significant area of high-rise buildings.

Whereas technologies like these are getting backlashed in our country just because they are not “technologically refined” and lack government backing/ funding. Therefore, such an innovation begs the question,

“Is human life cheaper than the environment, or is the environment far precious than human life?”

— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -",3405
"Digitization is often confused with Digitalization. While digitization is the other name of computerization and gained prominence during Industry 3.0, Industry 4.0 brought along true digitalization. The Fourth Industrial Revolution, which is the other name of Industry 4.0 or IR4.0, is steadily gaining importance and also presence across various industries, including transportation, logistics, banking, shipping, hospitality, travel, eCommerce, communication, healthcare, retail, telecom, manufacturing, and public governance. The main difference between the fourth industrial revolution and its predecessors lies in the strong disruption and transformation that it augurs to bring in.

The industry 4.0 paradigm begins with embracing cloud computing and goes on to the adoption of other components. In totality, these components the overall operations easier and businesses more profitable. It includes the following components.

Components of Industry 4.0

Cloud Computing: Infrastructure is the key to the growth of an organization. It includes the basic compute capability that the entire organization uses to run its operations and build its solutions for its customers. However, the cost of procurement of hardware, installation, connectivity, security, and annual maintenance cause a lot of Capital Expense (CapEx) and Operational Expense (OpEx) for an organization. Cloud Computing Infrastructure is a solution to all these challenges faced by an organization. It works on the concept of shared infrastructure used in a service model. The two biggest advantages that an organization gets from the cloud model of infrastructure is that they can be on the latest technology whenever they want, and they can increase and decrease their capacity demand based on their business growth.

Internet of Things (IoT) and Industrial Internet of Things (IIoT): IoT forms the basis of reaching out to each device working in the field. The IoT devices send the data that is collected by the IoT platforms and further stored on the cloud for analysis and action.

Big Data and Analytics: With the help of IoT, a humongous amount of data is collected from every corner. All this data needs to be stored in appropriate structures and analyzed continuously. That is the role played by the Analytics stream.

Artificial Intelligence (AI) and Machine Learning (ML): As part of the analysis of data, trends are created, and applications can draw conclusions from the trends to make decisions on behalf of humans. AI & ML helps with actionable decisions by applications without human intervention.

Augmented Reality (AR) and Virtual Reality (VR): AR & VR help in creating digital twins for expensive machinery, like a container crane at a seaport, or in providing operations simulation for training purposes. They deduce the future state of machinery and help in making efficient decisions in advance.

Blockchain: It adds the element of decentralization without impacting the trust. It is best suited for transactions that involve a large number of known parties that can transact amongst themselves without the presence of a central authority. The best use case is in preparing a contract chain for transporting a container through multiple seaports across the globe.

Advanced Robotics: It plays the role in day-to-day operations by taking the smallest of decisions and actions without any human effort. The best example is that of an automated forklift sorting the cargo in a storage yard all by itself. Likewise, multiple forklifts operating themselves at a fast pace without hitting each other or interfering in each other’s tasks.

Cyber Security: Security is the key component in the entire setup of IR4.0. The entire setup is exposed to various degrees of threats over the network. It ensures the protection of data and application from any kind of external attack and accidental exposure of data.

Horizontal and Vertical Service Integration: This is key to the expansion of businesses beyond their areas of expertise into new domains by acquiring or collaborating with other industry players. It helps them in having seamless integration of various business and operational processes.",4184
"This is Not Normal

Like the proverbial frog in the pot, we now accept as normal, things that ain’t. Field2Plate Sep 5·7 min read

What’s worse, we are the architects of our own crisis. We’re not only cooking in the pot, we actually made the pot, stacked up the firewood and lit the flame.

We created this pandemic, and what’s worse, we’ve created the conditions where the new virus is actually more dangerous than it would otherwise have been.

Is it just me, or is it getting hot in here?

We created this pandemic

The last 20 years of science have shattered our old ideas about biology and health.

In the same way that the invention of the telescope revealed (shock, horror) that the earth is not at the centre of the universe — genetic research has given us unprecedented new insight into life on earth, which has literally revolutionised our understanding.

One thing that has changed is how we “see” viruses.

Viruses are not the enemy, as it turns out. They are simply a data transfer mechanism that allows us, and all life forms, to evolve. They are tiny packets of genetic material that act as messages, or “updates” that allow all living cells to “pivot” — to adapt quickly and perform better.

There are billions of billions of viruses all around us and within us — all the time. A huge percentage of our own human DNA has actually been inserted from viruses, and has helped in the evolution of things like stem cells and the placenta.

Yes, we have evolved alongside viruses over millions of years, they are an intrinsic part of our world, our history on earth, our DNA and every breath we take. Without viruses, we would not exist.

I know… it was way easier thinking of viruses as the nasty little enemy.

It’s called evolution

Evolution is about plasticity and communication, about constantly searching for tiny advantages to respond to stress, to out-compete and to make life better.

Viruses are one of the primary manifestations of this; an expression of biological plasticity operating at a scale that is simultaneously too small for us to see, and way too big and sophisticated for our pathetic (but very earnest) little minds to comprehend.

So… how is this new virus a mechanism of evolution, and how did we create it?

How to make a virus: 101

The emergence of new viruses always happens when there is environmental stress.

That’s actually the whole point:

Stress makes life difficult

So life looks for options

It is no surprise that the COVID19 virus emerged in Wuhan — it is the epicentre of industrial pig-farming and of glyphosate use in the world, in addition to having some of the worst air pollution internationally.

The pig farming is on an unimaginable scale, and involves systemic anti-biotic use and the storage of millions of litres of toxic stool in enormous holding ponds.

The glyphosate, an organo-phosphate, is a potent herbicide, fungicide, anti-microbial ubercide and a powerful endocrine disruptor—that is being applied to the world’s crops in the millions of litres every year. Production has exploded in Wuhan, as it has around the world since it went off patent in the year 2000.

This is a textbook definition of “environmental stress”—and this new virus is a perfect expression of nature’s own balancing mechanics.

By the way, this is not a critique of Wuhan. They are just practising there, exactly the same intense industrialised farming that is being promoted and practised all around the world—just more than anyone else.

Now, the twist…

In addition to creating the conditions for the evolution of this virus, we’ve also created the conditions that make this particular virus more dangerous.

Failing health and PM2.5

The reason this virus is so harmful is also something that we’ve created — a combination of man-made factors including:

Failing health: Chemical farming has poisoned the last 2 generations to the point that we are seeing astronomical increases in many chronic diseases like Diabetes, Hypertension, ADHD, Auto-immune diseases etc.

Chemical farming has poisoned the last 2 generations to the point that we are seeing astronomical increases in many chronic diseases like Diabetes, Hypertension, ADHD, Auto-immune diseases etc. Pharmaceutical dependence: Most people over the age of 50 are propped up by a regime of pills that only builds as the years go by, making this age group more and more fragile.

Most people over the age of 50 are propped up by a regime of pills that only builds as the years go by, making this age group more and more fragile. Carbon pollution: The amount of small carbon particulates in the air (PM2.5) actually predicts the level of fatalities in any given location — the heavier the pollution, the higher the fatality rate.

The amount of small carbon particulates in the air (PM2.5) actually predicts the level of fatalities in any given location — the heavier the pollution, the higher the fatality rate. Failing soil: Soil is responsible for a huge amount of carbon “draw-down” — removing carbon from the air — which is one reason we see COVID spiking in winter, when the plant life is resting. Our soil has been progressively poisoned and broken up over the past 50 years, and no longer performs this vital role as much as it should.

Yes, we created both the virus and the context that makes it so dangerous.

Our new “normal”

Unfortunately, we are destined to see more and more of these pandemics, so in a way, this will become more normal.

We will also see male fertility continue to plummet and Autism rates continue to grow. We will see temperatures continue to rise and farming land collapse.

Then, it gets worse—the rate of change will increase rapidly, as the many different factors amplify each other. Which brings us to the greatest tragedy — as environments and economies collapse, political unrest will lead us into war and chaos.

Sounds a little extreme right? But it’s clearly, and logically our future if we continue on the destructive path we’re on.

We have created this new normal — but it’s not how it needs to be.

Redefining “normal”

Unlike the frogs in the pot, we have a chance to rewrite what’s on the menu.

And it begins, like most revolutions, by changing the way we think… we need to stop seeing nature as the enemy, or as an asset to be exploited.

Nature is a gazillion times more intelligent than we are — and we are part of nature — not above it, or apart from it. If we learn to work with this incredible intelligence, not against it, we stand a chance.

A wonderful new normal will emerge when we learn to use our human intelligence & creativity—to work WITH mother nature’s intelligence & creativity—not against it.

Over the last hundred years we have been exploiting nature and leveraging our intelligence and creativity to extract as much as we could.

Rising temperatures are just one sign of the devastation we have wrought.

Pandemics are another.

The way forward to is stop extracting and dominating—and to co-create with this magnificent natural intelligence that we have evolved with. We really, really need a new “normal”.

It starts with the soil

Our pathway forward is very clear to me, and it’s achievable — if we want it enough. It starts with the soil—our farming and food production practices.

That’s right, while reducing air pollution is critical, we will see far more CO2 reduction, far more quickly, by increasing carbon draw-down—by regenerating our soil and water. Nature can sequester far more carbon than we can produce, if we allow the soil and water to breathe.

In addition to reducing CO2, good farming will remove the environmental stressors that both create the pandemics—and the conditions that make the pandemics so dangerous (e.g. poor health).

Clean up: We need to remove all the poisons we spray into our soil and food. This will immediately start to address decades of damage to the soil, water and human health.

We need to we spray into our soil and food. This will immediately start to address decades of damage to the soil, water and human health. Regenerate : We then need to proactively regenerate the soil by the way we farm. As well as being unequivocally “good”, it is also, perhaps counter-intuitively, proven to be a better commercial proposition.

: We then need to proactively the soil by the way we farm. As well as being unequivocally “good”, it is also, perhaps counter-intuitively, proven to be a better commercial proposition. Localise: To make the above possible, we need to leverage our technology to decentralise and localise — and get more people back on the land. We need to support small, local farmers producing organic food (fruit, veg, grain, eggs, fish and meat etc) regeneratively. We need to withdraw support for huge-scale industrial farming.

There are many things we need to fix in our insanely industrialised brave new world, and many potential solutions—but buying an electric car is not top of that list (sorry).

Changing our farming will deliver the most value, across the broadest spectrum, in the shortest time.

It ends with us

We have subjected our planet to a devastating, relentless assault fuelled by a rapacious, insatiable desire for more. We have poisoned our soil, water and air carelessly and clumsily—to the point of systemic collapse.

We have created a whole infrastructure around this model, and now live in a world that is defined by it—and which is being destroyed by it.

How do we change this? Buckminster Fuller gives us a clue:

“You never change things by fighting the existing reality. To change something, build a new model that makes the existing model obsolete.”

If we, the people, choose to buy food that has no poisons, we will make the agro-chemical companies obsolete.

If we, the people, choose to buy meat that has not been raised in a feed-lot, we will make those inhumane monstrosities obsolete.

If we, the people, start to imagine a world where we co-create with nature, leveraging all our intelligence and imagination to create beauty and balance—we will make the old, mechanistic, industrialised model obsolete.

Big dreams, right?

Well… all revolutions start as dreams and are, by definition, “impossible” because they intrinsically challenge what is seen as “normal”.

We desperately need to change what is seen as “normal”…

… and get out of this damned pot.

No frogs were harmed during the writing of this article. Image of frog taken from photo by Ylanite Koppens from Pexels.",10458
"“Order food!” “Okay! What would you like?” This simple exchange could take up an entire hour’s discussion about NLP. Natural Language Processing is an area of Artificial Intelligence and Machine Learning. It is mushrooming in importance. Understanding NLP and how it works is what Sam Wigglesworth specializes in. Her presentation on the Business School of AI’s “WeeklyWed” webinar takes us on a wild ride that includes everything from intent to sarcasm, chatbots to WhatsApp. Feast on this for a minute. You’ll understand how AI is changing our world.

First we’ll want to remember how quick computing is. Count how many times you’ve turned to a calculator rather than trying to figure out the problem in your head and you’ll see what I mean. Now consider language. Once the computer understands what’s being said, you’ll get an idea of how AI can do in a very short time what it might take a human a very long time to do. Now it has become possible to feed an entire manual on a complex issue and with NLP the process of finding what you want done becomes a lot more simple.

Intent is at the forefront of the discussion. What you want to do comes first. Ordering food will obviously involve a product, a size, and probably a brand or type. Think cheese. What kind? How much, etc. Once intent is defined then we can get into semantics. This is where the algorithms come in, mathematical formulas trained to find patterns and make decisions. Incorporate them into neural networks that are modeled to work like the human brain and you get to an important part of AI, Machine Learning. NLP is driven in large party by Machine Learning, a subset of AI.

“Blow my mind”

Machine learning focuses on building apps. These applications within ML learn from data over time. Imagine how much data a language contains. Blow my mind. The idiom “blow my mind” represents just a smidgeon of what would need to be understood. Think of the many linguistic and semantic features of language. This amounts to a ton of data. This is where NLP learns in weeks what takes a human many years.

With advances in NLP come big money. The market for Chatbot as an example is expected to be worth around $1.25-billion in a few years. Chatbot tools in themselves are designed to make your life easier, enabling a better customer experience and improving productivity. Do that in the 23 languages spoken by more than half the world and you get an idea of the market for NLP, and why you should position yourself in the technology changing our world.

Note: Sam Wigglesorth is the founder of The Language School and Girls and Boys In Tech from Oxford who is the NLP instructor at Business School of AI, the brainchild of Sudha Jamthe

The Author: Henry Mulak is a journalist and teacher in Silicon Valley covering the technology sector, specializing in Artificial Intelligence and Machine Learning.",2870
"Fractionalised NFT’s explained: Can’t afford a whole NFT? Just buy one part instead

The NFT movement has well and truly owned the Crypto world in 2021.

Yes, De-Fi blew our minds in 2020 and early 21 but then jpegs of bored apes turned up and the whole world went bananas. But with pieces now commanding fees in the millions, how is the everyday human supposed to get in on the action?

It’s simple really, own a fraction of one.

But how does this work? Let me explain with an example:

As of writing this, a Dogecoin NFT is in existence right now worth a cool $225 million. Yes, you read that right — nearly a quarter of a billion. For an average human, this cost is well out of reach.

Yet, what if this was broken up into thousands of tiny pieces that multiple people can purchase? This is exactly what happened in this case. Originally the piece was purchased for $4million. Then the group which had purchased it, decided to split it into billions of individual fractional shares that anyone could buy.

So this allows lots of people to have part ownership thus being part of the club and making some sweet profits from a future sale, all without having to spend millions. A buyer chooses how much they want to put up and in return they are given a percentage of shares which represents ownership.

This is the big difference between the digital and physical world. You could never break up the Mona Lisa and divide ownership without having an incomplete piece of work. However, we can do this with a CryptoPunk and many more.

Image Credit: Algorand Blog

In simple terms, that’s how fractionalised ownership works. However, let’s dig a bit deeper and find out what makes this process work, plus where you can buy a fractionalised NFT too.

Our friends over at Crypto Vantage provide a thorough walkthrough of how this works:

“Fractionalized NFTs are the new sensation and they are enabled by smart contracts. We will use Ethereum’s ERC20 and ERC721 token development standards to illustrate how fractionalized NFTs work. As a reminder, ERC721 tokens are the set standard for creating non-fungible tokens on Ethereum’s blockchain, and the ERC20 standard is used to create fungible tokens. A fungible token could be created to represent fungible items such as gold, money, or any other commodity in the physical world. A non-fungible token on the other hand can be used to represent any rare item such as a collectible game card, a trophy, or a house. Given that a fungible token is flexible such that it can be exchanged for another of its kind without losing value, a smart contract can be deployed to generate ERC20 tokens linked to an indivisible ERC721 NFT. This way, anyone who holds any of the ERC20 tokens generated can own a percentage of the rare and valuable NFT. This is how fractional ownership of an NFT can be created, and the smart contract can secure the data that differentiates the fractional NFT from other NFTs. This idea can also be applied on any blockchain network that supports smart contracts and NFTs such that the NFT is locked in a smart contract on the blockchain and ownership of the NFT is represented by multiple fungible tokens whose supply is governed by the smart contract.” So, in simple terms, the non-fungible ERC721 tokens are broken up into fungible ERC20 takes which allow any buyers to own a portion. This is both a benefit to buyers and curators. Not only do buyers get the opportunity to own a small piece of a popular item at a cost that works for them, curators also benefit through these platforms with understanding the price drive in the market and collecting fees for assets they lock up.”

How to get involved in fractional NFT’s

One of the emerging protocols where one can buy a piece of NFT’s is on the platform Fractional — and yes, that really is it’s name!

In their own words: “Fractional is a decentralized protocol where NFT owners can mint tokenized fractional ownership of their NFTs. These tokens then function as normal ERC20 tokens which have governance over the NFT that they own.”

This is a handy visual from the Fractional team on how their platform works.

The future of fractional NFT’s

With many NFT projects now commanding vast sums it would seem that the model of fractionalisation of NFT’s will become popular sooner rather than later.

Not many can afford the costs of current NFT projects, so fractionalising is a great move for inclusion in many ways, as more everyday buyers can still be involved in this emerging industry.

Most people are buying NFT’s to flip and make money. This functionality allows more people to partake in this and remove what has become some of the exclusivity in this sector.",4689
"~ cool the Gulf and bring gentle daily rains to weaken storms ~

image from southernboating.com

TL;DR — Hurricanes strengthen due to warm waters. Create water spouts (a weak ‘humidity-tornado’ at sea) to cool the waters and prevent hurricanes’ strengthening. They’ll also bring gentle, cool, regular rains, siphoning the intensity off of heavier downpours. How? Giant black tarps floating a few inches below the waves.

The Problem of Having Too Much Power

Hurricanes cost societies billions of dollars in damages and hundreds of lives each year, aside from disrupting those balmy zones’ tourist seasons and stifling development in the poorer regions. Those same semi-tropical zones see lethal floods from that same force of hot humidity rising off their coasts.

If we can ‘tap’ that keg of steamy air, venting the energy in small doses every day, then we prevent the build-up of humidity that powers damaging hurricanes and floods. Water spouts might do the trick. Here’s how to build one:

DIY Shark-nado

By laying a wide circle of buoys to support a black tarp between them like a trampoline, a few inches beneath the water, we can concentrate the heat of the day into those top few inches of water, rapidly steaming away. That steam we create means the waters beneath are kept cool — but the steam can’t be left piling-up above the water, or it will accelerate those hurricanes and floods.

So, in the center of this wide black lily-pad, position a buoy with solar panels and fans — to push air into an upward spiral once humidity sensors reach a threshold. Just a little nudge of fan-power is all that’s needed for ‘ignition’ of the water-spout vortex. When it takes-off, this water spout siphons the humidity from above the water surrounding the lily-pad for miles— cooling all of those waters, as well.

That vortex of humidity is carried high into the air, where it cools rapidly to form fluffy clouds that reflect sunlight, helping to cool things down more. When they fall as rain (inland, if you position them well), it is a brief sprinkle — by sputtering these spouts daily, you can provide irrigation for free to thousands of square miles of dry hillside in areas like the coast of Texas and Mexico.

Cost-to-Benefit?

With durable marine-grade plastics, no intention of letting them degrade at sea or cause harm, a lily-pad covering a square mile would cost a couple million dollars and last many years. If each lily-pad, spouting daily during all the warm seasons, siphons and clears enough humidity from the surface waters in an area twenty times its own diameter, then the entire Gulf of Mexico would require 1,500 such lily-pads — for an up-front price in the $5–10 Billion range, with minimal annual operating costs. That’s much cheaper than all the clean-up and rescue, insurance premiums, lost wages and tourism, debt traps! A single major hurricane can cost twice as much in direct damages. Water spouts would more than pay for themselves after their first hurricane season.

Considering other impacts — these lily-pads would only cover 1/400th of the water’s surface, so they wouldn’t impede travel or disrupt fisheries. Instead, by keeping the waters cool, these lilies would protect native habitats from climate change — the Gulf waters are currently becoming too hot to thrive. Additionally, the water spouts drizzle regular, gentle rains — oxygenating the surface waters steadily. Contrast that with the deluge of intermittent hurricanes, too much oxygen all-at-once to make use of it, followed by anoxic dead zones.

The potential economic value of gentle rains inland can be immense — not just in terms of net profits, but as a percentage improvement for the poorer communities in those dry regions.

Another feature of the lily-pads — you don’t need to build a singular, monolithic structure that can fail catastrophically. Each lily-pad is its own redundant sub-system, and you can roll-out more of them along the coastlines, then further out into the deeper Gulf waters in successive stages. The costs are not born in one lump-sum. So, they can scale well. And that makes bulk production simple, driving down costs long-term. It might not be the best fix, but a water spout lily-pad can be rolled-out quickly, which might become critical as the waters continue to warm.",4304
"The red pill movement is here…

How The Matrix Paved The Way For Blockchain And Decentralisation

I just watched the trailer for the 4th Matrix film, aptly titled resurrections (if you haven’t seen it, stop what you’re doing and watch this now!).

Seeing that famous green code again got me thinking about the time I saw the first film. It was then, and still is now, a very revolutionary and on point set of ideas about the evolution of technology and the human race.

The story of interoperability between man and machine, and the eventual control by machines is actually not too far off what has happened nearly 22 years since the first Matrix. Just not in the exact way that is depicted in the films.

Instead of being physically connected in a tub to machines sapping our energy. We are staring into screens which are draining our attention, mental energy and showing us a world that isn’t exactly real or true in many cases.

And although we reside in the real world (or at least I think we do — Elon might disagree!).

We spend most if not all of our time in a secondary digital world where we engage with a multitude of sub-digital worlds and experiences. No jack in the back of our heads, just a device glued to our hands and seducing our eyeballs.

Many of the themes that the original Matrix explored including independence, freedom and forms of decentralisation from a system, are the same things that exist in the blockchain world.

The characters in the Matrix want to break free from their virtual prison to have freedom of choice. The world of Blockchain tech follows a similar line. Crypto exists as an alternative form of currency in rebellion to a centralised system controlled by what many deemed as shady characters.

De-Fi exists to empower people with the option to use financial products which grow their wealth beyond the restrained rates fixed by centralised institutions and NFT’s exist to provide a new form of ownership that’s not governed nor can be controlled by traditional institutions.

So the world of Blockchain and all its creations have a lot in common with the narrative of the Matrix. Well, in my opinion anyway.

The red pill, blue pill question is basically like asking, do you want to earn 0.01% from your traditional bank account or delve down that rabbit hole and see where those Crypto saving products could take you?

It’s all about choice — freewill.

It’s not only a cool techy, cyberpunk influenced piece of art. It’s also like a strange Nostradamus type prophet, telling us the future before it happens.

Tech addiction, decentralisation, man and machine integration are all things we live with today. The original cryptocurrency in Bitcoin was born about 10 years after the Matrix hit our screens, and it was everything that the movie promoted.

Choice, freedom and freewill.

As Blockchain tech continues to grow, innovations like NFT’s and De-Fi will continue to rise, giving way to even more digital tools that reshape our world.

We have most certainly already dived down the rabbit hole but just how much further will we go?

More from me

Worldwide crypto adoption rises by 888% in 12 months: What this means for the future of finance.

Million Dollar Bitcoin: Fantasy or a certain reality?",3248
"Spheres of influence

Consider mere humanity, once again.

Despite being dwarfed by the immensity of the cosmos, some people are working on some pretty neat things. For example:

Breakthrough Starshot aims to launch tiny, interstellar spacecraft at 20% of the speed of light within 20 years, on a realistic budget.

Artificial Superintelligence (ASI) is estimated by poll-of-experts to be around 50 years away.

These things are likely to happen within a human lifespan, and if you can imagine them, it’s not hard to fill in some more details.

— Intelligent, self-replicating machines will likely be developed after ASI, but we know they’re physically possible. After all, you are one. And we know they can be robust and powerful.

— We could also imagine a seed for a self-replicating ASI. Something that isn’t superintelligent now, but can become so, given the resources to grow and improve itself. Back in 1878, even Einstein was just a single cell, packed with potential.

— The escape velocity of our galaxy is something like a tenth of one percent of the speed of light. The tech to travel to the nearest star at high speed is about the same as the tech to travel to the nearest galaxy. The galaxy trip just takes a lot longer. But, coasting while dormant for a few million years shouldn’t be that big a leap (the more challenging part, it turns out, is slowing down enough to land). As a last resort, put the ASI on the problem for a few minutes to work out the details. If it gets done early, maybe it can work on boosting that “20% of the speed of light” target a bit.

What’s the point of this? We’re talking about the tech required to initiate full colonization of the Milky Way Galaxy, followed by the entire local group of galaxies, the local supercluster of galaxies, and so on. Forever. A growing sphere of influence that expands outward at a large fraction of the speed of light for billions of years, overtaking many millions of galaxies. Because these spacecraft can reproduce exponentially, the total cost of this project is just the cost to launch the first one. Depending on how the tech trends unfold, it might be initiated by someone alive today (or likely, many competing groups, when the tech begins to seem tantalizingly close-at-hand).

To be clear, we’re not talking about building fleets of Battlestar Galacticas, packing them with thousands of warm human bodies, and sending them on multi-million year voyages. Maybe that’s physically possible in a narrow sense, but also absurdly expensive and prone to failure. On the other hand, sending human minds will be absurdly cheap. Bodies are hardware — they can be built at the destination, when the time comes. What actually needs to be sent through space is information (e.g. instructions and mind content), and a minimal seed from which to build infrastructure — stuff that weighs mere grams, and will be dramatically more stable over a long voyage. Send the info, leave the meat behind.

Such a project will come to have serious resources at its disposal, to expend over cosmic time. Moons, planets, stars, dust, black holes, etc. Everything to be found in galaxies. Hundreds of millions of them, as the sphere of influence expands.

What will it do with those resources? I don’t know. For the argument I’m making here, we don’t need to know the end goal yet— I hope it’ll be something nice. But I can take a SWAG at how it gets there. There is a thing called the “maximum power principle” from ecology. It says that in a big, competitive environment (like an ecosystem, or an economy), the systems that survive and thrive tend to be those enabling the fastest use of energy. Maximum power. Gains in efficiency are always nice, but only insofar as they enable greater speed of energy use.

Imagine that principle, unleashed on a cosmic scale. The waste heat from such massive expenditures will, of course, have to be disposed of — radiated away into deep space. In fact, a very few SETI searches have looked for this kind of heat, coming from nearby galaxies. They haven’t looked far enough to see anything, but that’s a story for later.

The geometry of colliding civilizations is surprisingly simple at this scale, due to the uniform distribution of galaxies. Galaxy position data from the Sloan Digital Sky Survey.

The bottom line is this — if we do it, others will too — starting from their own homeworld in a galaxy far far away. They can’t be too close, else the universe would already be fully-packed with them (that principle limits how often they could appear, and thus how large our domain might grow). But if we do it, the question immediately shifts from “if” to “how often” other life appears and embarks on such massive projects. Perhaps our ever-expanding, intergalactic domains will collide, in the distant cosmic future.",4818
"Digital advancement continues to shape the pace of innovation as technology is evolving rapidly and the demand for custom software development rises every year. As a result, we see increasing interest in software development outsourcing from organizations. With its unforeseen challenges, the pandemic has been a critical input in reshaping the expected future of the whole technology industry. Despite, or perhaps because of the COVID-19, the percentage of their budget IT departments spend on outsourcing continued to rise from 12.7% in 2019 to 13.6% in 2020. It is due to the fact that the majority of businesses embraced the inevitability of digital transformation to stay competitive and viable. And outsourcing trends let companies revolutionize how they perceive the development and bring unrivaled opportunities to seize the global market. In this article, we have singled out ten IT outsourcing trends that will matter in 2021.

How IT Outsourcing Landscape Is Changing

In 2021, companies focus on reducing vulnerability to fluctuations in the global economy, remaining agile and flexible without losing product quality. As a result, relations between businesses and software vendors are changing into a strong partnership with better collaboration.

1. A Growing Demand for Narrowly Specialized Software Companies

The pace of technological progress is so swift, and the scope of digitalization is so vast that it becomes hard for in-house IT departments to keep up with all the innovation and new applications of existing technologies appearing day-to-day possessing all the needed skills.

That’s why more and more businesses start looking for more specialized software development companies. Such experts specializing in some narrow sphere can bring maximum value into the project due to their deep knowledge of a particular technology or domain.

Trends in outsourcing come along with emerging technologies affecting particular industries like logistics and healthcare. They will be in high demand for outsourcing services needed for their digitalization using AI, Blockchain, VR, and other technologies. At UpsilonIT, we have a subject-matter expertise in logistics and medtech that we have built up over the years of software development. All the companies that come to us with their complex business challenges receive a unique software solution based on their domain and specific requirements.

2. A Greater Focus on Flexibility and Adaptability

COVID-19 has exposed the weaknesses of companies and shown that the ability to adapt is the only way for companies to keep their head above water. In 2021, this tendency will stay, so it will become crucial for software development companies to manifest utter adaptability and agility in several aspects.

First of all, businesses will expect that their software vendors will have the ability to ramp things up and down if they request. So outsourcers will need to be ready to restructure teams within the shortest time frame and be agile to deliver results faster.

Acceptance of a company’s business ethics and its adoption is another thing that will be for businesses. They are waiting for the offshore companies to integrate seamlessly into their company and become a part of their current in-house team.

Third and last, modern companies are looking out for innovations. How quickly software vendors react to the appearance of new technologies and upskill their personnel accordingly will affect the assessment of the vendor. In our Guide to Vendor Evaluation and Scoring in Software Outsourcing, we suggested a list of criteria that will help companies effectively evaluate their potential software development partner in accordance with the project requirements and, as a result, choose the most suitable one.

3. Building Better Collaborative and Strategic Partnerships

Finding a trusted provider and building productive relationships with remote teams can be pretty challenging. A few years ago, there was a tendency to partner within one project with multiple software development vendors. Today, most companies prefer single-vendor outsourcing to avoid all the headaches and risks of managing multiple vendors. Instead, they will focus on developing a strategic partnership with one vendor.

The difference between outsourcing core and non-core tasks will be less evident as vendor-customer relationships will be more reliable and trustworthy. Offshore teams will easily integrate into the client’s company and become a part of it. Thus, they will cover any tasks of varying degrees of importance necessary for the customer’s success.

4.Eastern Europe is Becoming a Leading IT Outsourcing Hub

Eastern Europe will become a more attractive destination for IT outsourcing. India and China will still be the leading outsourcing countries where the largest share of global outsourcing revenue is generated. However, Ukraine, Poland, Belarus, and Romania will be their direct competitors with high-quality performance, commitment, and affordable prices.

Among the countries above, Belarus offers the most attractive conditions for IT outsourcing. The Belarusian IT sector comprises 60.5% of outsourcing companies and occupies 3rd place in its economy. The Belarusian IT sector has received governmental support as one of the top-priority economic sectors for future development. The establishment of the Belarusian Hi-Tech Park (HTP) in 2005 stimulated IT sector development and provided a unique business environment for IT companies, such as low tax regulation. That means that the process of outsourcing for foreign companies in Belarus has been simplified.

The country has a deep talent pool exceeding 77,000 programmers, and over 16,000 new specialists enter the sector every year after graduation from universities. So it allows specialists to learn and apply the latest technologies in their work with still moderate prices for their services.

5. An Emphasis on Quality over Quantity

In previous years, companies chose outsourcing when they wanted to focus on quantity and be strictly profit-oriented. Now, results and quality play a more valuable role. As a result, outsourcers will need to alter their approach and stir their work by emphasizing quality and customization. It’s the area where small outsourcing companies will win because of their unique custom software solutions, focusing on technology and a specific approach to each client’s business challenges.

Due to a shortage of people with the skills required for in-house development, InfoQ highlights that product quality is now a more important driver for outsourcing than cost reduction. Concentrating on high-quality and optimal results is the trend that will label outsourcing as profitable and efficient in 2021.

Technology Trends in IT Outsourcing

Statista forecasts that over 2.14 billion people will be purchasing goods and services online by 2021. That means the emergence of new trends and a significant technology transformation in various domains.

6. AI, Machine Learning, and Chatbots

Slow response time and security risks can be a red flag for those who want to partner with outsourcing companies. AI promises to solve this issue by automating routine tasks where the most mistakes appear and allowing people to concentrate on the main business goals. Automation and quick execution will enable companies to stay competitive in the market. AI’s machine learning empowers automation, allowing software apps to learn instead of blindly following commands. Software vendors that know how to implement AI inventions in their workflow will be able to process information automatically, avoid errors, and enhance productivity.

Moreover, AI increases the quality of customer service in outsourcing. It creates unique customer journeys that assist clients in making wise decisions. Chatbots are effective guides when it comes to consulting on service purchases. As a result, companies will see an increase in productivity and customer service along with outstanding employment opportunities.

7. Cloud Computing & Cloud Platforms

The demand for cloud computing is increasing because of the amount of data that needs to be stored. It becomes necessary to ensure online data centers and the required equipment by service providers. According to the Gartner revenue forecast, the cloud market will be $289.1 billion worth in 2021. More and more businesses are winning from adopting the cloud, which ensures better data protection, faster data operations, and the ability to modernize business processes by painlessly implementing various business apps.

Companies will turn to outsource for effective cloud platforms. They will actively partner with vendors who offer software cloud-based services or will help to transfer their existing infrastructure to the cloud. Along with the need for cloud computing, there is the need for security services to avoid all kinds of threats. By finding a trustworthy partner, companies will perform their tasks and have someone responsible for cloud security.

8. Robotics Process Automation

Robotics process automation is now trending due to business automation. Many daily tasks will benefit from implementing RPA tech. For instance, supply chain management, payment processing, customer service. RPA will allow companies to clear time for more complex tasks and leave menial work and repetitive actions for virtual assistants and bots. It will reduce operational costs, improve work coordination and decrease manual mistakes. Thus, outsourcing companies invest their time in business automation to bring better results for the clients. According to Gartner, robotics process automation software will generate revenue of nearly $2 billion in 2021.

Companies can also supercharge their automation efforts by injecting RPA with cognitive technologies such as ML, speech recognition, and VR, automating higher-order tasks that in the past required the perceptual and judgment capabilities of humans.

9. Progressive Web Apps

The demand for a better user experience is growing as well as the need for progressive web apps. The main difference from ordinary mobile applications is that they move back and forth between mobile and web apps. PWA are designed to create a mobile application-like experience at websites. Moreover, they are easy to develop and maintain.

PWA seems to be an ideal variant for businesses that don’t want to develop a mobile version of a website and Internet users that prefer mobile apps because of their convenience. Both sides will win from PWA because their development is much faster, and they have the same effect as a mobile application. That’s why many companies will seek outsourcing vendors to employ PWA to achieve their business goals and satisfy the rising demands of their clients.

10. Blockchain Technology

As per Statista, global blockchain spendings are expected to reach over almost 18 billion U.S. dollars by 2024. Blockchain technology enhances transparency, security and also offers numerous solutions for startups and small businesses to raise money. They enable the development of smart contracts, as well as immediate microtransactions.

While the demand for the knowledge of blockchain technology is rising, the need for highly specialized professionals increases accordingly. However, adequate blockchain professionals are not available, especially when it comes to complex challenges like DApps or NFTs. Hence, businesses are looking to outsource their software development needs to companies that have blockchain experts.

Summing Up

When a business decides to grow, it may be crucial for it to consider software development outsourcing. Companies that don’t want to be left in the basket have already started learning and adopting the latest development trends. Although, it can be challenging for in-house developers to keep up with the pace of emerging technologies and implement every digital innovation because of the limited human resources. That’s why many companies choose to outsource software development.

We, at UpsilonIT, invest heavily in upskilling our employees to steer in the right direction with cutting-edge transformative solutions and data-driven insights. Our team is flexible and adaptable in terms of both innovation and cooperation. We easily integrate into our clients’ companies and become a part of the in-house team. And we do care to provide our clients with the best-fit solutions for each business challenge.

If you have a project idea on your company’s 2021 roadmap and you want to get the most out of innovations, we are ready to lend a helping head in adopting emerging technologies for your business. For more details, contact our business development specialists.",12754
"Your Project Outsourced #1: The Different Types Of Outsourcing & Benefits For Your Business Przemysław Markowski Follow Jun 8 · 7 min read

In the Your Project Outsourced series, I’m going to cover all the essential information about outsourcing your project to an external partner (like INVO), and key things you need to be aware of when you scale up your team this way. If you’re a founder, a technical lead, or a product manager who wants to create or scale up your digital product, this series is for you! In the first article, we’re going to discuss what outsourcing is, what makes it different from regular hiring, and what are the key benefits of using outsourcing in your business.

All Articles In This Series:

What Is Outsourcing?

So what is outsourcing anyway? To put it simply, outsourcing is essentially moving work from inside your company to the outside. However, there are many different types of outsourcing out there that you can use, depending on your specific needs. The three main types of outsourcing are onshore, nearshore, and offshore. As you may already know, these terms are related to where the outsourced work is done.

Onshore VS. Nearshore VS. Offshore

Onshore outsourcing means you hire a team that’s very close to you, likely in the same city. Nearshore outsourcing means the team you hired is rather close, within the same time zone for flexibility. And offshore means that there may be several time zones and locations involved with the team you hired.

Different Outsourcing Models

Within those types of IT outsourcing, you can also find different outsourcing models. Some companies simply outsource employees with specific knowledge and expertise for the duration of a project and manage them themselves. Other businesses want to outsource the entire project from start to finish (this is where most software houses and agencies come in). There’s also the option to outsource the development of a specific feature or a few functions only. The last model is especially popular among companies who have specific experts on board but are working on other projects and simply don’t have the capacity to develop a certain feature.

Outsourcing VS. Hiring

To outsource or to hire? That is the question! But what is the difference between the two? Technically, it is a matter of who is actually paying the employees. But when we go a bit deeper, there’s a lot more differences. I will give you an example to explain this better.

Hiring An In-House Team

Imagine you’re an entrepreneur who is about to start the development of their product, let’s say a mobile app. So if you go for building your in-house team, you need to assess the scope of the project, choose the best tech stack and decide how many developers, designers, testers etc. you have to involve in the project. Then you need to plan and execute a recruitment process (or hire someone to do it for you), hire the selected candidates and onboard them. All this will already take at least 6 months and the costs are not small. And this is not the end of your expenses, because most of the time you will need to provide your new employees with equipment, software licenses, and some extra perks if you want to make your job offer extra enticing. Considering how many companies are looking to hire IT experts, the competition is really tough and some businesses can really outshine others when it comes to perks and benefits. So generally speaking, the process is pretty complex, and the costs don’t consist of the salaries only, but also include all the side costs as well as your time, which should be your most valuable asset when it comes to building your business from scratch.

Outsourcing

Now, outsourcing is simply hiring some company that has already done this process for you, possibly with much greater experience. So if you see an hourly rate on an outsourcing offer and go: “that’s too much, I could hire some experts on my own and pay them less”. Well, I bet you could. But don’t forget about all the additional costs you have to take into consideration. Also, time is of great significance, because finding experts is not an easy task and requires a lot of effort, while an outsourcing agency might already have the skilled people you’re looking for on stand-by.

Key Benefits Of Outsourcing

I know this all probably sounds like an ad for outsourcing. But from my personal experience I really believe it’s a better choice, especially for new CEOs and startups at an early stage. There are many benefits to working with an outsourcing agency to support software development for your company. For each business the key benefits will be different, depending on what your specific needs are. But some things are true for all types and models of outsourcing. Let’s break them down:

1. Flexibility

Outsourcing provides a great deal of flexibility. It allows you to implement many competencies and include expert employees in your projects without the need to commit to hiring them full-time. This means that your resources are more agile and you can change your team members based on your project’s specific needs at the moment. This is especially great for companies whose main focus is not software development or for startups in the early stage of their development with limited resources.

2. Cost Savings

Outsourcing is a great cost-saving solution. Mostly because you will avoid the costs of having staff in-house. Depending on the model of IT outsourcing you go for, you can also save a lot of money on infrastructure, employment costs as well as software and tools. Outsourcing companies also offer a level of expertise and experienced technical staff on-demand, which means you can reduce recruitment and onboarding costs. Often those two processes can be the biggest expenses, take the most time and can also slow down your development.

3. Access To Skills And Knowledge Outside Of Your Location

Staying on the topic of finding employees, it can sometimes prove to be very hard to find specific experts within your local area. Especially when it comes to the software development industry, the market is very competitive, and in some cases finding the person with the right expertise and experience can take several months. I can tell you from our experience at INVO that especially when it comes to experts in new, emerging technologies like Flutter, the recruitment process is very long. With outsourcing you gain access to specific experts who fulfill your requirements while it’s the agency’s job to find them. You also will be able to access people from different locations, meaning if there are no competencies in your specific region, you can find them elsewhere.

4. Better Manage In-House Resources

Another great benefit of IT outsourcing is that when you outsource some of the work to outside service providers, you leave more room for your in-house team to focus on their tasks. It’s especially important when it comes to your operations and management staff whose tasks are crucial to reaching the company’s goals. In this case, outsourcing some of the functions will be a great strategic move for your business. For example, most companies actually outsource accounting tasks to free up the C-level management.

5. Accelerate Business Processes

Most of the outsourced work is related to information technology and software development, however, the next top outsourced functions are business processes. Those operations can slow down your business and leaving them to external providers can be a big relief. Outsourcing companies have more expertise in-house, they have a bullet-proof infrastructure and tested management processes. This means that they can come in and change the way your business operates, reduce costs and relieve your internal staff from some of the functions.

6. Share The Risks

Most of the time, especially when it comes to IT outsourcing, your partner will share the risks and responsibilities associated with the project. It will be just as important to your external provider as it will be to your internal team to deliver a product that users and clients appreciate. This will take some of the burden off of you and your in-house team, leaving you to be more focused on delivering an excellent product for the users.

7. Fast Scaling

The last key benefit of teaming up with an external IT outsourcing company is how fast you can scale your business using those services. Without outsourcing, scaling this fast is almost impossible. Especially when it comes to software products and projects. Outsourcing gives you the ability to control your infrastructure and make changes quickly to adjust to the market and industry trends. It is something that will make managing your business and staying competitive on the market much easier. Having this kind of control over your infrastructure enables you to be more free and work quicker.

Making the decision whether to use outsourcing is a big one and you shouldn’t rush it. However, I hope this series provides you with some insight on how outsourcing works and what are the two sides of this coin.

If you’d like to talk to me about building your digital product — write me an email at p.markowski@invotech.co. Or you can contact us via our website to get your project started.",9289
"Top 10 software development outsourcing companies in Eastern Europe

Deloitte reports that 59% of businesses hiring software outsourcing services are looking to reduce overhead cost.

Now, while IT outsourcing companies help you save money and increase your project’s backlog flexibility, your business still needs results. This need is why you can’t hire any outsourcing development team you find.

Software development team

I’ve picked out a list of 10 software outsourcing companies. Regardless, you will likely need just one team. So how do you go about it?

Here’s a brief guide to help you make the best decision towards a productive implementation of your product development.

Take Your Pick, Wisely — Hiring a Software Development Outsourcing Company

These factors have to do with your needs over the software outsourcing company you are looking to hire. So, consider these points before you hire developers that will meet your business needs as outlined or exceed expectations.

Minimal supervision: Outsourcing companies allow businesses to focus on their primary objectives indirectly. Since you don’t have your hands full trying to develop your software, you get to see to other tasks on your business backlog.

But you may not get this opportunity if you constantly have to tell your outsourcing team what to do or what not to do. So, you need to hire developers who can work with minimal supervision and still produce great results.

Trust: While you will need an outsourcing development team that you don’t always need to supervise, this comes with trust.

If you can’t trust the developers, you will ALWAYS want to supervise so everything is in order. So hire developers you can trust, and minimal supervision is assured.

Technology and resources available: Technology advances every day. Before you forge ahead with any software development company, check if they have the latest technology and resources available to see to your project.

ROI: Yes, BPO Outsourcing helps you cut costs but does this add to your overall Return on Investment (ROI)? Suppose your outsourcing need is an in-house project or for a client, does the product’s development come with the potential for good returns?

Again, yes, outsourcing helps you reduce cost, but how much impact will that cost reduction have? Think about this.

Business fit: Software outsourcing companies have their strengths. We have companies very good with custom software development as opposed to IoT development. You have to look out for a company with the highest success rate for the project you want to work on.

Most importantly, hire developers that have extensive work experience in your business industry. This way, they know what you need even if you don’t know this.

Nearshore or onshore company: What if you need to have a physical collaboration? A meeting? Or does the project you need to get sorted demand a consistent collaboration with the outsourcing development team? In-house developers and outsourced developers? Then you will need a nearshore or onshore company to work with.

Combine these factors to get the perfect fit for your software development project using this list of Eastern Europe’s top 10 software outsourcing companies I will share with you now.

Best Software Development Outsourcing Companies in Eastern Europe

I curated this review from Clutch’s — a trusted website that helps businesses find partners — list of the top software development companies to ensure that we got the best from the best. So let’s get right into it!

1. Altoros

Altoros — #1 software development company in Belarus in 2021 according to Clutch.co

An IT outsourcing company that is hugely focused on R&D activities, Altoros is situated in California with development centers spread across Europe and Latin America. With more than 20 years of experience in the software development industry and more than 350 dedicated professional developers, Altoros can cater to diverse software needs. And they have over 750 clients to show for it. This success is down to their client-centered approach.

They have built a structure over years of experience that businesses can rely on. Their partnerships with renowned brands show their efforts.

Their services include:

Custom software development

Staff augmentation

Cloud-native transformation

Big data and data engineering

Legacy software modernization and more

However, suppose you need a customized product or software solution expertly tailored to your business idea and are in need of cloud computing, in that case, you should check them out.

Portfolio: Samsung, Toyota, Huawei, Siemens, Pfizer, among others.

Strongest expertise: Custom Software Solutions and Cloud Computing.

Industries specialization: Healthcare, FinTech, Logistics & Transportation, Banking, Supply Chain, Retail, and Manufacturing.

Software development centers in Eastern Europe: Poland, Belarus, Ukraine, and Moldova.

Website: https://altoroslabs.com

2. STX Next

STX Next — Europe’s largest Python software development company

STX Next is based in Poland, one of the leading countries in the software development industry. Their expertise goes beyond the fintech industry to the banking and the gaming industry.

With over 16 years of experience and on average a 3-year partnership with brands, STX Next has served 200 clients on 400 different software development projects. Their services include:

Team extension

End-to-end product development

Consulting

Portfolio: Vyze, NOTA, Unity, among others.

Strongest expertise: Custom Software Development and Web Development.

Industry specialization: Financial services, Advertising and Marketing, Real estate, Banking, Gaming, and Biotechnology.

Software development centers in Eastern Europe: Poland.

Website: https://www.stxnext.com

3. MentorMate

MentorMate — software development outsourcing provider with development center in Bulgaria

MentorMate is headquartered in Minneapolis. However, this product development team started in 2001 in Bulgaria playing a part in the country’s technological growth. So far, they have expanded into other regions like Sweden.

Their services range from:

Software design and development

Cloud and DevOps

Their approach to every project is what puts them on the list. MentorMate builds software with an extra human touch to it and prioritizes a secured delivery.

Portfolio: AstraZeneca, Royal Bank of Canada, among others.

Strongest expertise: Web Development and Custom Software Development.

Industry specialization: Healthcare, Education, Finance, Agriculture, Manufacturing, Commerce, and Lifestyle.

Software development centers in Eastern Europe: Bulgaria.

Website: https://mentormate.com/

4. The Software House

The Software House — #1 software development company in Poland in 2020 according to Clutch.co

Based in Poland, Software House is considered one of the highly recommended software outsourcing companies in the country. Compared to other firms on this list, the Software House started in 2012.

Regardless, they have so far built a clientele who largely refer to them for their services. These include:

Web development

Software architecture

Cloud & DevOps

Mobile development

Product design

Portfolio: eSky.com, Vaana, Smartum, Synerise, among others.

Strongest expertise: Custom Software Development and Web Development.

Industry specialization: Financial services, information technology, and real estate.

Software development centers in Eastern Europe: Poland.

Website: https://tsh.io

5. DCSL GuideSmiths

DCSL GuideSmiths — IT outsourcing company with 27 years of experience

With a team of 250+ software experts, DCSL GuideSmiths provide software-related services for startups and SMEs. This IT outsourcing company started back in 1994, making them one of the oldest firms on this list. Their services are categorized into three:

Core software development, i.e., web development and bespoke databases

Specialist software, i.e., UX design and software consultancy

Complementary software, i.e., hosting and data migration

Although they are headquartered in Hampshire, they have development centers scattered across Madrid and Bucharest.

Portfolio: NHS, Berkshire Hathaway, Mitsubishi, Pearson, among others.

Strongest expertise: Custom Software Development and Web Development

Industry specialization: IT & Technology, Insurance, Retail & E-commerce, Hospitality, Healthcare, Financial & Business Services.

Software development centers in Eastern Europe: România.

Website: https://www.dcsl.com

6. Merix Studio

Merix Studio — web application development agency based in Poland

This software outsourcing team boasts more than 200 experts who have worked with close to 300 clients worldwide.

With offices spread across the US and Europe, Merix Studio comes into this list not only because of their expertise but their speed of development.

Their services include:

Web and mobile app development

Product design

Software development consulting

Team augmentation and outsourcing

Machine learning & AI

Software modernization

Portfolio: Volkswagen, Toshiba, Tesco, Deutsche Post, among others.

Strongest expertise: Web Application Development.

Industry specialization: Financial Services, Education, Gambling, and Business Services.

Software development centers in Eastern Europe: Poland.

Website: https://www.merixstudio.com/

7. Door3

Door3 — software development and UI/UX design agency

Founded in 2002, Door3 is on this list due to its project approach that upholds flexibility, secured delivery, and reliability. With over 80 professional software developers, Door3 has served more than 500 clients worldwide.

Their services include:

Technology consulting, i.e., digital strategy and project management

Software development, i.e., portal development and operations software

User experience design, i.e., UX Audit and design system

Door3 embodies professionalism and a client-oriented focus.

Portfolio: WWE, Wells Fargo, HP, News Balance, AIG, among others.

Strongest expertise: Custom Software Development and UI/UX design.

Industry specialization: Financial services, Insurance, Legal, Construction, Non-Profit & Education, and Consumer & Retail.

Software development centers in Eastern Europe: Ukraine.

Website: https://www.door3.com

8. SoftwareMill

SoftwareMill — custom software development provider

SoftwareMill is another BPO outsourcing team based in Poland. Founded in 2009, SoftwareMill has built a clientele from diverse industries highlighting their expertise. Besides Poland, they have centers in the UK depicting their growth despite their relatively early emergence in the software outsourcing industry.

Their services include:

Big data engineering

Machine learning services

Backend and frontend development

Blockchain development

Internet of Things (IoT)

You get a pretty good idea of how your collaboration with SoftwareMill will turn out due to their transparency. From creating a backlog of your idea to shipping your delivery, every process is laid out even before your business commits to the project.

Portfolio: Intelli Messaging, Flexys, Kafka, Tipser, among others.

Strongest expertise: Custom Software Development, BI & Big Data Consulting & SI.

Industry specialization: Business, Telecommunications, E-Commerce, Consulting, FinTech, and Blockchain.

Software development centers in Eastern Europe: Poland.

Website: https://softwaremill.com

9. Intetics

Intetics — software development company with 26 years of experience

Founded in 1995, Intetic operates in 6 different countries in both Europe and the United States. The company is both security and quality certified. With a hub of global talents, Intetics have a diversified portfolio encompassing almost every industry you can think of.

Their services include:

Data management

Process automation

Product prototype and MVP development

Advance websystem development, and more.

Portfolio: Encyclopaedia Britannica, Burda Digital, Spreadshirt, Symantec, among others.

Strongest expertise: Custom Software Development and Mobile App Development.

Industry specialization: Agriculture, Communications & Networks, Life Science, Retail & ECommerce, Logistics, Insurance, Healthcare, Energy, Education, and Media.

Software development centers in Eastern Europe: Ukraine, Poland, Belarus and Russia.

Website: https://intetics.com

10. Future Processing

Future Processing — software outsourcing company based in Poland

While Artificial Intelligence (A.I) in the future, A.I with a human touch is actually what businesses need. With Future Processing, you can get this. Founded in 2000, this software outsourcing company based in Poland has more than 20 years of experience to show for quality.

Future Processing started with just two developers. Fast-forward to two decades later, they boost a team of 800 software specialists and have worked on 600 projects with 200 clients worldwide.

Their services include:

Discovery and strategy, i.e., design sprints and software audits

Software development services, i.e., software development and product design

Digital innovation, i.e., business intelligence and cloud services

Portfolio: Allocate, Valeo, UK Power, Thomas Miller, Flowbird, among others.

Strongest expertise: Custom Software Development and Web Development.

Industry specialization: Healthcare, Finance, Media, Logistics, Entertainment, Automotive, Construction, Entertainment.

Software development centers in Eastern Europe: Poland and Ukraine.

Website: https://www.future-processing.com

Bottom line

Here’s the thing: it comes down to what your business needs at the end of the day. So first, you will need a clear and detailed outline of what you want to achieve through your idea or what you want to help your clients achieve.

This will help you define the ideal solution that your business needs. For example, is it a web or a mobile app development service? Next up is merging the factors to consider before hiring IT developers.

Combined, you will have a clear picture of the kind of team you would want to outsource your project to. Now, why don’t you get started with that?",14184
"BBC

Hey hackers ;)

Today I’ll write about a kind of vulnerabilities a lot of hunters forget to search or test it and some of them didn’t know how to exploit it!

The vulnerability which we will talk about it today is the Mail Servers takeovers AKA SMTP Servers Broken Access ..etc, It got a lot of names though, but now let’s talk about it.

My case is unique though, because It’s a kind of chained vulnerability as i well explain, so let’s start ….

First of all i did my normal recon starting from gathering the domains of the BBC, but unfortunately the BBC got a lot of SSL’s like:

BBC Studios Distribution Limited

BBC Studios Limited

BBC Worldwide

BBC Worldwide Ltd

So if you want to gather the domains in a correct way you should use every SSL name and extract the domains from it, by doing some reverse whois operation or the other techniques to gather the domains with, so i did all of that then i want a kind of general certificate or unique certificate got all or most of the domains, also some times i search for an SSL got a limited domains, seems weird right ?

I know :), but check that:

I search for the SSL which got a large amount of domains in order to prevent wasting time by searching with each certificate for domains

Then i checked the SSL with limited domains in order to see the rare domains, or the domains that is not common because the vulnerabilities will be exists like the registers in the processor :) [A lot]

So after some search i get an idea, this idea is what if i entered my homie Shodan in order to search for any new SSL on it, Me to me:

But let’s try why not man, so entered Shodan then i started with the dork:

org:”BBC”

This dork will seek for any host, domain ..etc related to an organization named BBC so now if any host appeared and related for the BBC organization, I can clearly see it’s SSL, so after hitting enter i got this result:

And i did find a new SSL which is:

British Broadcasting Corporation

So now we got a new SSL, then after gathering some domains i decided to start with the main domain of the BBC UK which is:

bbc.co.uk

I used my automation script — theSubdomainer — in this operation, so i typed the command in my VPS:

nohup subdomainer -d true -g <github_token> -l domains.txt&

This command will simply run the subdomainer in the background and will block any hup signal from reaching the process when i close the SSH tunnel, so i slept in this night when I’m thinking how can i hack this company but with critical or high impact.

So i woke up in the next day to check for the domains and it’s sups specially the bbc.co.uk domain, then i find a lot of attractive subs on it, so one of my favorite methodologies is to fuzz the attractive subs only, then the other subs, so i get a sub then fuzz it with the dirsearch tool, i love this tool like i love the exploitation 🥰

I have a technique in choosing the wordlists, this technique is:

Choosing the common files wordlist Then detect the technology of the web app then get a wordlist for it Detect the web app server then use the wordlist for this server Finally get the raft wordlists

So before is start i entered the subdomain, but unfortunately it redirects me to another sup with a login page:

No i confirmed that this sub contains sensitive info’s so i started the war now 🙂.

I get the common files word lists and i find the /api/ path, GOTCHA!, let’s start the work of my second fuzzing technique:

If you find an /api/ or api subdomain like api-example.com or api.example.com, then you should start your fuzzing with:

1. Started with the common api endpoints wordlists

2. Then the api’ seen in wild wordlist

3. After that try some graphql’s endpoints wordlist

4. Also you can search for actions / objects in this endpoint

So now first let’s try entering the API endpoint and see what will happened and how the application will interact with us:

So as you can see it dumped all the endpoints which have, so first let’s try to enter the /admin/ endpoint, in fact i love admins a lot 😻.

So after entering the admin endpoint i get the following result:

GG Homie, now we are in the Admin api, which shouldn’t be accessible by default, so for now we have an Unauthorized Access To Admin API, but without impact, so let’s try access the /admin/users, so that we can reach any sensitive info’s:

So i found a lot of emails with permissions & info’s about the mail users, so i searched a lot till i get the System_Admin info’s, so i report it as an Broken Access Control or Unauthorized access to admin endpoint, for the first time they did not accept it or they do not clearly understand what is this in face & they said that they will check for it deeper, so no problem I’m OK

So then i decided to make some network pentesting on it, because i was little sad about the endpoint 🥺, so i opened Shodan and stop for a while, then i though like that:

Hey homie CG, you find emails right ??

- Yeah Right

So why not to try find a kind of vulnerability in the mail servers then try to use this mails !!?

- Man i love you 🙂, Let’s do it

So i started my Shodan recon, so i searched a lot using the common certificates for the BBC, but few or no results found, then i think of the certificate which originally get by Shodan which:

British Broadcasting Corporation

So now i decided to make an accurate dorking so i dork with:

ssl:”British Broadcasting Corporation” port:25,587 “Hello”

So now let’s explain this simple dork:

ssl: is for specifying SSL to search about

port: to specify ports to only appear in the result

“Hello” to grap only the connected SMTP connections, because when shodan connects to SMTP server, he always try to send EHLO AKA Hello request to a kind of host to make sure that it can perform commands, so in the response if the SMTP connection is really created and “Hello” word appeared that means that the EHLO request is really sent.

25: the SMTP port

587: the Encrypted SMTP connection port

To explain the ports, simple like the difference between the ports: 80 & 443

Then i get the result:

Before saying why you hide anything, we can simply type the dork and access all of that ?, so first because all these info’s if any one use it with a wrong way this may cause a problem for me so i try make this report private as i can, second don’t ask a lot 😊.

So now when i tried to connect for this SMTP port which is 25, through my home network it doesn’t connect, i did some port scan to make sure that this port is rally opened because some times shodan caches the response & they may close this port:

So as you can see the rustscan tool didn’t give me any response with that port is really opened, but wait it’s not the end, look for this tip homie:

Sometimes in your home country there is some restrictions to access hosts / ports like this from your network, so try using a VPS, also do this in port scanning 😉.

So i switched into my VPS, then did the port scan and i get the result:

So we did it my homie, now let’s try connecting to it using the telnet command:

telent <port> <SMTP por>

So as you can see we connected and we could do the EHLO request with the SMTP command:

EHLO <host>

Now let’s play homies 😈, So i get back to the /api/admin/users to choose a mails from it, so i bring two emails like:

mr-High@bbc.co.uk

cyber-guy@bbc.co.uk

Now we have two mails right ?, So before continue let’s explain what are the workflow of the SMTP commands:

Specifying sender

Specifying recipient

Then knowing the syntax of data sending

Specifying mail subject

Specifying mail body

Sending the mail

And i think now you understand why i bring two mails :), if not i bring two emails to be one sender and one recipient, so let’s start specifying the parts of this SMTP communication:

MAIL FROM: mr-High@bbc.co.uk

RCPT TO: cyber-guy@bbc.co.uk

Now the SMTP server accepts the sender and the recipient, let’s see what is the Syntax of the data sending:

DATA

So as you can see here, the Syntax is:

Putting all the mail requirements [sender, recipient, subject, body] then enter a “.” to end the mail then send it

So let’s continue:

Subject: re-test from Cyber Guy

<hit Enter>

This is a re-test from Cyber Guy

.

So finally i was able to compromise over 4 or 4 BBC mail servers with that, and more also 😉, then finally they accept the first report about the sensitive endpoint and the second report about the Mail Server takeover. And i become a BBC Hall Of Famer, Wait for coming write-ups in BBC, Allah Willing",8506
"A quote by Ben Franklin is still relevant today: “Tell me and I forget. Teach me and I remember. Involve me and I learn.”

As technology continues to allow more efficiencies and its effectiveness continues to be confirmed, companies are adopting the digital world as an alternative to being held hostage to real estate investments.

Many companies have committed to continuing virtual work and learning environments because they saw performance increase by as much as 47%. Other companies have taken notice of the significant environmental impact, as proven during the initial COVID pandemic that shut world travel down, creating an 11% decline in CO2 emissions.

And, most employees prefer working from home. We know that happy employees are more productive and engaged. A survey by Owl Labs found that the overwhelming majority of employees prefer working from home. Why wouldn’t they? They save between $2,000 and $6,000 per year in work-based expenses, the frustration of rush hour traffic, and the loss of several hours from their day is significantly reduced. In fact, employees gained back an average of 8.5 hours of personal time per week.

However, using a blended or virtual learning approach is not without its challenges when it comes to ensuring training has the same impact virtually that it does when you have people in a training room. Technology designed specifically for virtual training can actually provide more tools than a conference room or a training room. When you include the ability to “quiz” the group real-time and get immediate results back, along with automatically capturing engagement analytics, technology can significantly assist in understanding your training program results.

Social interaction is a huge component to successful training. This is often more important for participants than for trainers. Engagement and activity-based learning can be viewed as difficult using many of the online products that everyone is familiar with. While these web-conferencing systems offer the ability to get people together in a session, the ability to get the type and quality of engagement and social learning is often overlooked.

Virtual training requires specialized skills. Trainers should be trained and coached so they are comfortable managing a virtual training program. They also need to be trained on the technology that is being used. The biggest failure of virtual (online) trainers is just not knowing the tools they are using. Proficiency and comfort using the technology is as important as knowing the subject matter being covered. This happens when proper training on the platform is done and the trainers spend time familiarizing themselves with it, practicing in it, and becoming proficient in it.

4 Ways To Enhance Student Engagement In A Virtual Classroom

In most online training, the presenter is in control of “all things”. Short of a chat board, traditional online solutions puts 100% control in the hands of the trainer. This leaves participants isolated, with little to nothing to do. Boredom quickly sets in. Using technology that gives participants an opportunity to review information and own their learning experience is much more powerful. These tools that encourage participation and helps trainers get everyone engaged are a natural part of technologies that were built specifically for training and learning.

A strategy that works well for engagement is small group project-based programs. Using virtual breakout rooms that move participants into small groups encourages engagement and learning. Having participants “own their learning experience by working during training” increases attention, engagement, knowledge transfer, and retention. As you’re looking at the best solution for small group learning, be sure to evaluate the technology that gives you the easiest and most robust options. Not all breakout rooms are created equal.

It’s difficult to get everyone engaged in a face-to-face setting, and even more difficult in a virtual world. Using different tools and tactics to encourage participation can mean the difference between a well-trained team producing excellent results or one that’s producing just enough to get by.

“An organization’s ability to learn and translate that learning into action rapidly is the ultimate competitive advantage.” — Jack Welch (former CEO of General Electric)

If learning is King, engagement is Queen. You cannot have one without the other.",4453
"FaithTech’s 2021 Writing Contest

Explore the Uncharted Territory of Faith and Technology FaithTech Follow Jun 22 · 5 min read

Photo by Alessandro Erbetta

What does God have to do with facial recognition, social media, or smart speakers? From printed books to glowing screens, from bread and wine to virtual reality, we want your help exploring the uncharted territory of faith and technology. What are you exploring and discovering?

Share your insights with more people through FaithTech’s Writing Contest. We know you’re thinking deeply about it. We hope these prizes give you that extra push you need!

Prizes

This year’s contest winners will WIN . . .

Your article published by FaithTech Institute A $25 Amazon gift card A year-long digital subscription to MIT Technology Review or WIRED magazine FaithTech’s Full Stack of Reading, including…

Three people will win. Ten runners-up will be published and win a yearlong WIRED subscription.

What if my submission isn’t a winner or runner-up? Good question! We might still publish it! All submissions will be considered for publication on FaithTech Institute. In submitting your piece, you agree to have your article considered by FaithTech’s editors for future publication.

Photo by RF._.studio

What to Include

Winning entries will creatively bridge the gap between faith and technology. That’s FaithTech’s mission. For readers of your essay, your writing should include…

Good research and honest observations

Connections to platforms, systems, or devices many people know

Creative writing and an easy-to-follow structure

Analysis informed by biblical or theological concepts

Arguments that showcase a clear Christian point of view

Actionable takeaways that help readers answer “So what?”

Read 2020’s Winners to see what great essays look like!

You and Your Audience

Photo by Mikechie Esparagoza

You are a thoughtful writer who can write for a popular audience. You bring both analysis and imagination to your life of faith and your work with technology.

Your audience includes Christians working in technology at all levels, leaders in Christian organizations, and users who are wrestling with technology’s influence in their lives.

What to Write About

Below are five aspects of life impacted by technology that you can write on! We’ve offered sample questions to spark your imagination. As long as your essay fits a theme, feel free to explore a question you’re passionate about!

Also, check out 2020’s Winners for more inspiration!

1. Community & Relationships

How is Zoom affecting church communities? Work cultures? Families?

Emojis help and hurt minority populations. How should the church address such technological effects?

What happens to nations when government leaders interact primarily through social media?

If social media has affected government protests and national elections, then how is it shaping church denominations?

How does text messaging shape what friends do and don’t share with each other, both in-person and apart?

How do job-related interactions change when you use Zoom versus in-person?

How is social media affecting what we want in our relationships?

2. Spiritual Habits

What habits have you developed using your smartphone, and how do they help and hinder your spiritual practices?

How has social media altered your thought patterns and how you think about your faith?

How might listening to an audio Bible shape your relationship to God differently than reading a print Bible?

How have on-demand voice assistants like Alexa and Siri changed your efforts at memorizing Scripture?

3. Ethics

Can Christians use self-driving cars if they prevent accidents, save lives, and decide which kinds of people should die in a crash?

How should Christians think about ultrasound technology as it relates to prenatal complications?

What responsibility do Christians have to identify and address algorithmic bias in policing and legal proceedings?

What values are embedded in Artificial Intelligence, and how do they align with Christian virtues?

4. Ministry

If churches use facial recognition, how might that impact pastoral care?

What new technology is serving God’s Kingdom, and what benefits and risks does it present?

Where have you experienced or witnessed technology being used in ministry that encountered some unexpected outcomes — positive or negative — as a result?

When has a platform, system, or device impeded the purposes of ministry, how did you recognize it, and what did you do to fix it?

5. Theology

How is transhumanism changing how we define the image of God in each person?

Why has church live-streaming challenged Communion beliefs and practices in churches?

In what ways could the Internet help us better understand the Trinity?

How does Christianity’s vision of the future compare with technology’s vision of the future?

Photo by Christin Hume

Submissions: When, How, and What

Please send each entry as a separate email.

DEADLINE: Email to adam@faithtech.com by August 31, 2021. Your subject line: “FaithTech Writing Contest — [First and Last Name]” Format each essay separately as a typed, single-spaced document. Name your document this way: Last name First name — Title Send your essay as a link or an attachment. Include your full name and a short bio in the email (50–75 words). Provide the total word count of your essay. Tell us which theme you chose.

The Details

All submissions should be between 700–1800 words, must honor FaithTech’s core values, and should be tailored to FaithTech’s mission: Bridging the gap between faith and technology.

Bridging the gap between faith and technology. You may submit more than one entry for consideration. We may publish more than one submission per person, but only one submission per person will be among the winners and runners up.

We may publish more than one submission per person, but only one submission per person will be among the winners and runners up. We can’t accept late submissions for the contest , but we will still consider them for publication!

, but we will still consider them for publication! All content should be original and not published elsewhere, unless you host a personal blog with fewer than 50 readers. Please let us know in your email if this is the case.

A typed single-spaced document. Indented paragraphs.

Check spelling and grammar. Cite or link to any outside sources.

Your essay will be edited by FaithTech’s editors prior to publication and titles may be changed.

More questions? Ask adam@faithtech.com.

Submit Your Essay Here!

Email each essay by itself to adam@faithtech.com. Please use the subject line: “FaithTech Writing Contest — [Your First and Last Name]”

Spread the Word!

Promote this contest on your social media or blog! Use the graphic below or contact adam@faithtech.com for more info.",6849
"I Was a High-Tech Expert in a Low-Tech Church. Here’s What I Did.

My Church Was Filled with 70-Year-Olds. They Needed New Ways to Stay Connected FaithTech Follow Jun 8 · 9 min read

Photo by Denisse Leon

In the fall of 2019, my family landed at a small church in our neighbourhood that was trying to re-plant. With only 30 or so remaining members and an average age in the 70s, this little church was struggling to keep the doors open. Yet, there was a clear love for Jesus, a passion for the Gospel, and a vision to re-plant the church. Our family felt the Lord calling us to this little church for a season, so we got involved right away. I had no idea what was just around the corner.

When March 2020 hit, news of the spreading pandemic triggered a lot of rapid changes. Because our little church was made up almost entirely of people in a high risk group, we needed to transition to something remote very quickly. The next several months were a continuous process of trying to adapt, not only to the new safety requirements, but also to the limited tech-literacy of the congregation.

Before 2020, you might have encountered some form of “church online” or watched the occasional live streamed service, but over the last year, chances are you depended heavily on things like Youtube, Facebook Live, and Zoom to continue participating with your local church. Even savvy technophiles had to find new platforms for connecting online. So it’s easy to understand why the elderly in my own church struggled to make the leap. But they weren’t the only ones.

Age is only one factor limiting tech literacy. Other factors include access, ability, and vocational background. In rural areas, some communities still lack the basic internet infrastructure to make web-based solutions possible; in the US alone, 15–20 million people still lack broadband access. Meanwhile, differently-abled and intellectually disabled persons may also struggle to connect using new technologies. Consider also that many people don’t regularly use computers in their work—certain trades, including industrial, service, and natural resource jobs, for example. As churches’ moved online, it was easy to overlook these people; I certainly did. Then I found myself in a church where these factors couldn’t be ignored.

Large-Scale Spectacle, Small-Scale Churches

Most people who know me professionally associate my work with large-scale and high-tech spectacles. I’ve spent most of my career working with media and technology to create larger-than-life events, public art, and experiences, co-founding a design firm in 2011 to help organizations with a strong “why” communicate through those mediums. Despite all this, what many people don’t realize is that I have spent most of my life in small churches and church plants.

Photo by Anthony Diehl

As a designer, one of my goals is to champion empathy in the design process. It doesn’t matter how cool, effective, or scalable your idea is. How do the people you aim to serve experience it? That is really the big question. While I firmly believed in this empathetic approach to design before 2020, that year serving our little church stretched my imagination and opened my eyes to serving those who would otherwise be excluded from the life of our churches.

As 2020 rolled on, I began working with other churches, helping them solve problems, connecting people, and training teams. While I usually get the call for more high-tech related problems, I’ve come to deeply care about including those people who might be excluded (often unknowingly) from our gatherings. But is trying to connect people with low tech literacy to a church really worth the effort?

When You Come Together…

In one online discussion about connecting churchgoers who couldn’t stream videos, one person commented, “In 2021 if you cannot use a computer and can’t access a website, there’s not much else we can do to help people.” (As you might imagine, the comment triggered some “robust dialogue” in the group.) What struck me was the lack of imagination, yet the question still stands. Is it worth struggling to solve a technical problem for one or two people when the majority of the congregation logs on just fine?

I found I needed to wrestle with the apostle Paul’s pointed words to the Corinthian church.

For in eating, each one goes ahead with his own meal. One goes hungry, another gets drunk. What! Do you not have houses to eat and drink in? Or do you despise the church of God and humiliate those who have nothing? What shall I say to you? Shall I commend you in this? No, I will not. (1 Corinthians 11:21–22)

The early church practiced what is known as an “agape feast”, a sort of extended potluck with the Eucharist at the centre. Wealthy Christians would typically host the gathering in their homes, and those with the means would supply enough food for those who had none. However, not everyone was able to gather at the same time. The wealthy could leisurely feast the day away while the poor—and slaves in particular—had to work like any other day. When the poor finally arrived at the end of their work day, much of the food was already eaten. When Paul learns what the Corinthians’ feasts were becoming, he has some strong words for them.

Paul reiterates what the Lord’s Supper is truly supposed to be and then offers a simple exhortation: “So then, my brothers, when you come together to eat, wait for one another” (1 Cor 11:33, emphasis added).

It was not until 2020 that I saw how our use of technology might be excluding some members of the Body from the gathering. While Christians could apply this passage in many ways today, for those of us serving in media and technology, how can we wisely deploy technology to “wait for” our brothers and sisters whom we might unknowingly be leaving behind?

Low-Tech Church Solutions

Photo by Federica Galli

As I’ve worked with dozens of churches over the past year, I’ve encountered many different ways churches are developing low-tech solutions to keep their members connected to the weekly rhythms of worship and community life together. Here is a list of some of what I’ve learned.

Small “offline” groups + pre-recorded elements. Some of the most encouraging creative examples this last year have been small groups (2–10) meeting and holding a small service with some prepared materials. A song or two, some Scripture, a recorded sermon, and prayer. Most of the stories where people said things like, “You know, I actually feel like I grew closer this last year” had something like this in play. Not always doable, but worth striving for.

Some of the most encouraging creative examples this last year have been small groups (2–10) meeting and holding a small service with some prepared materials. A song or two, some Scripture, a recorded sermon, and prayer. Most of the stories where people said things like, “You know, I actually feel like I grew closer this last year” had something like this in play. Not always doable, but worth striving for. CD/DVD recording. A surprising number of churches are still recording and distributing services on CD/DVD. It is a medium that older members can use with minimal headache. This highlights the tradeoff of using better, newer technology—and the very steep learning curve for some—versus working with mostly obsolete forms that an older generation already knows.

A surprising number of churches are still recording and distributing services on CD/DVD. It is a medium that older members can use with minimal headache. This highlights the tradeoff of using better, newer technology—and the very steep learning curve for some—versus working with mostly obsolete forms that an older generation already knows. Dial-in phone solutions. For churches already using Zoom, the platform provides a phone dial-in solution. It may not always work with some of the “dial 9 to get an outside line” phone systems in care homes, etc. Some have reported good experiences with phonelivestreaming.com while many are using freeconferencecall.com as a low/no cost solution. At least one church I heard about literally has one member who calls several other members who all put their phones on speaker while the caller holds their phone next to a laptop with the live stream!

For churches already using Zoom, the platform provides a phone dial-in solution. It may not always work with some of the “dial 9 to get an outside line” phone systems in care homes, etc. Some have reported good experiences with phonelivestreaming.com while many are using freeconferencecall.com as a low/no cost solution. At least one church I heard about literally has one member who calls several other members who all put their phones on speaker while the caller holds their phone next to a laptop with the live stream! Phone Check-ins. Speaking of phones…. Talking on the phone is still a personalized and meaningful way to connect people. One church in New York organized groups of 15, each with its own leader. Those leaders called each person on their list to check in and see if they needed anything. It helped sustain connections that might otherwise fade out. With pastors already stretched thin, a small bit of planning, organizing, and communication can equip the church to care for each other.

Speaking of phones…. Talking on the phone is still a personalized and meaningful way to connect people. One church in New York organized groups of 15, each with its own leader. Those leaders called each person on their list to check in and see if they needed anything. It helped sustain connections that might otherwise fade out. With pastors already stretched thin, a small bit of planning, organizing, and communication can equip the church to care for each other. Low-power FM broadcast. It is quick and easy to set up a consumer grade FM broadcast in an outdoor space. Many churches are adapting services so that people can participate from the parking lot. In places where lockdown is still in effect, “parking lot church” has been a breath of fresh air for many. If you can get in a car and turn on the radio, you can participate!

It is quick and easy to set up a consumer grade FM broadcast in an outdoor space. Many churches are adapting services so that people can participate from the parking lot. In places where lockdown is still in effect, “parking lot church” has been a breath of fresh air for many. If you can get in a car and turn on the radio, you can participate! Local AM broadcast. One church I know of reached out to a local AM radio station and secured a Sunday morning time slot. While this solution may be a bit more expensive or not possible in some areas, it will provide an extremely reliable and accessible solution for connecting members who can’t get online.

One church I know of reached out to a local AM radio station and secured a Sunday morning time slot. While this solution may be a bit more expensive or not possible in some areas, it will provide an extremely reliable and accessible solution for connecting members who can’t get online. Internet radio. I am exploring a low cost broadcast solution where an inexpensive portable internet radio player (basically a small radio) could be set up in advance to receive a church’s live stream. The idea here is that someone could literally just turn the device on when the service is live and listen in. While traditional radio broadcast is expensive for an individual church, an internet radio stream is cheap and can reach anywhere with an internet connection. This concept is being prototyped, and I’d love to see it put into a real world test. Interested collaborators welcome!

I am exploring a low cost broadcast solution where an inexpensive portable internet radio player (basically a small radio) could be set up in advance to receive a church’s live stream. The idea here is that someone could literally just turn the device on when the service is live and listen in. While traditional radio broadcast is expensive for an individual church, an internet radio stream is cheap and can reach anywhere with an internet connection. This concept is being prototyped, and I’d love to see it put into a real world test. Interested collaborators welcome! Zoom. Perhaps nothing revolutionary here for most, but last year, I discovered that many people were not using Zoom to connect simply because even the simplest tutorials were beyond them. I also found that once someone tried Zoom or other video conferencing one time, they were much more likely to continue using it and learn by doing. Here is a very simple tutorial for onboarding members with low-tech literacy but still with access to smartphones, tablets, or laptops.

Behind the Cutting Edge and Waiting…

Photo by Ron Lach

For tech professionals in 2021, burning DVDs and setting up dial-in systems may not seem like very cutting-edge work. Using older technologies isn’t likely to scale or attract droves of new users, but this work matters more than you realize. Remember, even with older technologies, you are still engaging in a design process to serve a specific group. Your skills in solving problems are just as needed here—and can grow just as much—as in any “cutting-edge” area.

During the past year, I found the work to be vitally important for helping a small church stay connected and stay on mission. The simple tools we put in place and, most importantly, the time invested in training individuals helped the church move into a new chapter. After months of working to keep people connected, that small neighbourhood church was able to hire a new lead pastor, a major step in replanting the church.

Is trying to connect people with low tech literacy to a church really worth the effort? If a little bit of innovative tech sustained a church for future ministry, that surely seems worth the effort. What new works might God have yet to do in and through this church?

Most of all, I found my awareness of and love for unseen members in the church grew through my encounters with the tech-challenged. Serving the less visible parts of the Body is a gift and a blessing that builds up the whole church. As Christians serving in tech, let’s use our perspective, voice, and talents to help the church “wait for one another” when we gather.",14279
"How Should We Evaluate ‘Zoom’ Worship?

Runner Up in FaithTech Institute’s 2020 Writing Contest!

Photo by Dylan Ferreira

Systematically evaluating ‘Zoom’ worship is made difficult by the sheer volume of material currently available, most of which is grasping at different ends of the theological stick. Within my own tradition (namely, Anglicanism), much of the focus of recent material has centred on issues associated with the proximity of the eucharistic elements — a strange area to focus on, given that we humans are not in a position to rigidly define the workings of the Holy Spirit. (We might suggest that the real issue with the ‘Zoom’ eucharist is that we might not be sharing in the same meal.)

The popular focus on the Eucharist has left another area much less explored: the experience of ‘Zoom fatigue’ in relation to online worship. The concept of Zoom fatigue presents experience as a means of assessing, and a source for, theology and practice. In this piece, my hope is to examine this approach critically.

While the sense of togetherness experienced via ‘Zoom’ might provide a useful way of touching base as part of the daily rhythm of prayer, using ‘Zoom’ has also proved to be an exhausting experience for so many. Why? Well, consider for example, the way people face each other. It is true that we usually interpret sustained, one-to-one, face-to-face interaction as ‘adversarial’ in nature. Yet face-to-face has, for many, been their experience of using ‘Zoom’.

During ‘communication skills’ classes in medical school, students are often taught that they should never sit directly opposite a patient. Rather, they should sit with the chair slightly askew, or perhaps even side-by-side, to make the patient feel more comfortable. One of the points in the practical exams usually reads, ‘candidate checks that the patient is comfortable with the seating position before starting the consultation’. Simply put, sustained face-to-face interaction is exhausting, especially for those wired as introverts.

Photo by Chris Montgomery

Both church leaders and church-goers must account for this phenomenon when evaluating ‘Zoom’ worship, particularly given that our experience of the eucharist — our ‘source and summit’ — should strengthen us for service, rather than leaving us fatigued. Having said this, there is a danger in lapsing into ‘felt’ experience at the expense of wrestling with more systematic approaches; but equally, there is a danger in denying, or flattening out, the experiences of the individual in the ongoing evaluations of Zoom worship. Both approaches, when pushed to the extreme, do violence to the entire enterprise.

This tension raises questions about our approaches to experience in general, particularly as they relate to the entire theological enterprise. There are, broadly speaking, two views on the relationship between experience and Christian theology, explicitly: suspicion and acceptance. By briefly exploring each of these in turn, I hope to show that Zoom worship reflects what we believe about experience and how it relates (or doesn’t) to theological inquiry in general.

Suspicion

The first view understands Christian theology as the interpretive framework in which our experience might be understood. Alister McGrath summarises this view thus, ‘theology interprets our feelings even to the point of contradicting them when they are misleading’.¹ In this view, special revelation is the prerequisite for faith in God, which is to say trust in, and dependence upon, God. In other words, we need God’s own self-disclosure to rightly understand our experience of God in the world. This leads to a more critical assessment of experience, challenging us to interpret our experience in light of God’s past activity in the world.

Perhaps the most famous example of this perspective can be found in German priest Martin Luther’s theologia crucis, his ‘theology of the cross’. Christ’s disciples needed an interpretative framework in which to understand their experience in Jerusalem, Christ’s crucifixion; hence Jesus invested three years into their formation, travelling around Galilee, teaching them how to make sense of what was soon to happen in Jerusalem. In short, the suspicion view uses a previously established theological framework to assess experience as a source for theological thinking, while scripture, tradition, and reason provide the guardrails.

Photo by Johnny McClung

Acceptance

The second view understands experience as a foundational resource for Christian theology, respecting its place among the other sources for theology, namely: Scripture, tradition, and reason. As Friedrich Schleiermacher might assert, human religious experience can tell us about the transcendent event apart from special revelation. But uncritical acceptance will evaluate Zoom worship based solely on experiential or emotional reactions, rather than on theological, sacramental, or ecclesiological foundations.

Perhaps the clearest, and most surprising, example of this perspective can be found in John Calvin’s exposition of the doctrine of revelation in the 1541 edition of his Institutes of the Christian Religion. Calvin argues that all God’s attributes are ‘shown fully enough in the arrangement of the world and of all creatures’, but we fail to perceive them; hence, these attributes are ‘more familiarly made known by his word’.² God’s ‘mercy, goodness, compassion, righteousness, judgement, and truth’ all ‘shine forth in the heaven and the earth’, but we usually do not have eyes to see.³ The content of general and special revelation are thus identical, according to his view; the difference is only in terms of expression.

Moving forward: a dialectical approach

This latter view engenders humility. God’s Spirit is not limited by our experience of God, neither is divine activity limited exclusively to the Church but can be seen in the world as well. Pushed to the extreme, however, this view can also be dangerous. Proponents might fail to acknowledge that our emotions can be destructive; instead, our emotions must be held accountable within a trusted community, and subject to the scrutiny of plain reason. Thus, when it comes to Zoom fatigue, we need to do more than evaluate church based on our own fatigue; we must also name it and critique it within our larger theological, sacramental, and ecclesiological commitments.

In addition, both suspicion and acceptance are in danger of fostering relativism, the acknowledgement of little or no authority outside the self. In the first instance, a person might elevate his or her own theological perspective as a kind of litmus test for experiences; in the second instance, a person might lack the criteria which enables careful discernment.

An alternative approach to both suspicion and acceptance is therefore vital. Christian theology gives us a useful framework within which our experience might be interpreted, but the framework itself is shaped and informed by our experience. Experience grounds the framework in practice and the difficult realities of life. Experience prevents any kind of framework from becoming overly rarefied; it ties it to the concrete realities of being-human-in-the-world. A difficult, although potentially constructive, dialectic is established between Christian theology and experience. It is only through resolving this difficult dialectic that the Church and the Academy might evaluate ‘Zoom’ worship in a credible and authentic manner.",7498
"INSIDE THE ENTERPRISE

This is the second in a series of perspectives on IT in the enterprise, informed by experiences and observations over nearly two decades working at the intersection of business and IT in a large global energy company.

Introduction

Information Technology is well recognized as a critical business enabler and has been a key component of business transformation efforts since the 1980s. What has changed in the 21st century is the relentless rise of digital enterprises with completely new digital business models, disrupting one traditional business after another. Today the typical transformations of old would be woefully inadequate, and those enterprises that were not born in the internet era are regularly forced to embark on programs of Digital Transformation. Unfortunately, a very large proportion of these programs fail. In this article I examine what are the typical causes behind these failures, and what steps can be taken at the business unit or enterprise level to improve the probability of a successful outcome.

Approach

Digital Transformations are fundamentally Business Transformations so the approach I take is to discuss two perspectives on why business transformations fail and two which are more specifically focused on digital transformation. In the first category we have ‘Leading Change — Why Transformation Efforts Fail’, by John P. Kotter in Harvard Business Review (originally published Spring 1995), and an interview with McKinsey’s Seth Goldstrom on ‘Why Transformations Fail’ (Feb 2019). On digital transformation we have from Christy Pettey in Smarter at Gartner ‘Avoid These 9 Corporate Digital Transformation Mistakes’ (Jul 2020), and ‘10 Reasons Why Digital Transformations Fail’ by Clint Boulton in CIO magazine (Jun 2020).

Next I identify pitfalls which have tended to be persistent, and others which are specific to Digital aspects. Finally I note mitigations or remedies that could reduce their occurrence and impact, and improve the odds of success.

I have liberally re-used material from the above-mentioned authors, my purpose being to give present a concise overview and synthesis of the topic and hopefully saving readers many hours of preliminary research on the internet.

Issues in Business Transformation

Kotter sees the transformation effort progressing through a series of stages, each critical in its own right and needing to be done properly and in the right sequence. He identifies these as follows-

Establish a sense of urgency

Form a powerful guiding coalition

Create a vision

Communicate the vision

Empower others to act on the vision

Plan for and create short term wins

Consolidate improvements and produce more change

Institutionalize new approaches

Pitfalls abound (see Table 1), with the majority directly a consequence of organizational inertia and gaps in leadership composition, alignment, capabilities or communication. A particularly interesting pitfall is the tendency in some quarters to declare victory at the first opportunity, effectively scuttling further transformation efforts, diluting the intended change and hastening a reversion to ‘business as usual’.

Table 1

Now coming to the weaknesses identified by Goldstrom two decades after Kotter, we see that his cautions relate to some softer aspects such as being insufficiently ambitious about outcomes, lacking a growth focus, the need for collaborative working, and needing an ‘and’ rather than an ‘or’ mindset.

Both emphasize the need to systematically proceed with the transformation effort through its various stages, getting the right people in the right jobs, and making changes wherever there is a lack of either will or skill.

Goldstrom makes important points about the need to align incentives with behaviors that will drive the desired outcomes, and the necessity of proper tracking systems.

Issues with Digital Transformation

Some of the points noted by both Pettey and Boulton (Table 2) are business and general issues that are very much in play even in Digital transformations. Though worded differently they are closely related to pitfalls earlier noted by Kotter and Goldstrom. The foremost the inability to articulate a vision that the whole organization can embrace and align with. Another big one is organizational silos.

Table 2

Both Pettey and Boulton highlight the need to focus on customer needs and competition. Pettey makes important points about seizing the opportunity provided by digital transformation to reinvent how an industry operates, rather than being satisfied by incremental changes. Both advise action rather than overplanning or ‘wait-and-see’. Bouldton highlights how disruptors typically move an order of magnitude or two faster and achieve greater scale than incumbents, especially when there are network effects involved.

Adequate talent and a learning mindset are key ingredients especially in the digital aspects of a transformation, both are in short supply. Consequently there is often a high degree of talent churn which then impacts continuity and timely achievement of Digital goals.

Improving the Odds of Success

Kotter has some key recommendations –

Convince at least 75% of managers that the status quo is more dangerous than the unknown

To achieve a coherent effort that is both sufficiently powerful and disruptive, form a cross-functional team under a senior line manager outside the normal hierarchy

Communicate the vision constantly, model desirable behaviors

Encourage risk taking

Define visible performance improvements, and reward the employees that achieve them

Articulate connections between new behaviors and corporate success, develop the next generation of leaders

Goldstrom makes some distinct points-

The change story must be compelling from a hearts-and-minds perspective … not everyone contributing to the transformation will be excited about company financials and stock price in the same way as top management who have incentives linked to them

Goals should be set at the full potential of the organization, rather than at a level that is comfortable (which will ultimately not be transformational)

Have the courage to make personnel changes, and do it early … this will get the whole organization energized and excited

Examine talent-to-value for the top 100 roles in the organization, the mix needed in the future will be different than in the past

Obsess about growth .. sales, pricing and step-out growth

Pettey emphasizes the following points-

Focus on the business model, not the operating model …prioritize the market and how it will be monetized over efficiency and effectiveness

Set goals for structural transformation, supported by adequate investment … digital business works at the level of revenue and business model change and product re-invention

Focus on learning and growth

Plan less, do more … embrace lean startup thinking, release a minimum viable product and then improve it with customer feedback

Focus on the needs of markets and customers first, then deploy technologies to address them … do not select technologies and then look for applications

Understand and then exploit the power of culture to drive change

Boulton highlights some additional steps -

Ensure strong CEO sponsorship

Business leaders should get a strong understanding of what needs doing in a digital transformation as well as how to go about it, before embarking on the transformation … what are the talents and technologies required

Begin early, do not wait …gather velocity and maintain it, digital disruption happens fast so time is of the essence

Work on attracting/accessing capable talent … they are in short supply, so learn how to retain them or continuity will suffer

Conclusion

One needs a sense of urgency, imagination and vision to begin a successful Digital Transformation, grounded in customer needs and an understanding of the competitive environment. There are quite a few pitfalls one is likely to encounter along the way, however strategies and methods exist to address those. A lot depends on getting the right people in place who have both the will as well as the skills, and being able to hold on to them.

Therafter speed, individual and collective motivation, enthusiastic collaboration and focused action is what will make the difference. And persistence will lead to success.

Thank you for reading this article! I welcome your comments below.

— Ashish Deshpande",8451
"Ancient Greeks used the word ‘symphonia’ to name the notes sounding together in harmony. They described it as the art of the ‘ensemble’, rather than a musical form. Something beyond a structure or a musical formula.

Behind every great symphony there is a great orchestra, comprised of different and extraordinary talents. The science of luthiers, the art of instrumentalists and performers. The perseverance of rehearsals, and the importance of teamwork.

At NTT Disruption we are the disruptive unit of NTT Group. Our parent company’s logo embodies the vision of their voice at the center, surrounded by a loop representing continuous innovation. We contribute towards this vision by applying technology for good. Our special orchestra of diversely talented and purpose-led engineers, scientists, humanists, and creatives, plays the notes to create disruptive solutions and IP generation, that will generate a positive impact on people’s lives.

Our orchestra

Picture from ‘Start Small’ room at DreamLab Madrid, our office where we co-create, co-validate, and co-accelerate our technological solutions with NTT, our partners, startups, academia, and tech giants.

Each day, our orchestra of disruptors drafts a new movement in our symphony. We place people at the center of everything we do, as the guiding principle to build solutions that will improve our industries and societies in the realms of healthcare, wellbeing, and education.

🎶 Have a look at the notes that compose our disruptive symphony:

jibo by NTT Disruption: our social companion for healthcare and education

From left to right.- Jaime Macaya, Top of Stack Lead and Irene Díaz-Portales Chaves, Cloud Skills Engineer

jibo is a social companion created to help children, elderly and professionals in healthcare and education. Equipped with meaningful technology, he can improve elderly’s cognitive functions, cooperation, and emotional welfare. He can mitigate children’s anxieties and vulnerabilities and improve their social, sensory, and cognitive abilities. Designed for B2B use, he is also an effective go-between patients, health, and education professionals.

“jibo’s shift towards the B2B2Human is an example of how data can be put to good use. It can contribute to reduce social isolation among older people in telemedicine and help children’s recovery in hospitals.” Marc Alba, SVP NTT., President & CEO NTT Disruption.

Along with Microsoft Azure Cognitive Services, we are enhancing jibo’s knowledge and communication skills for healthcare. Today, jibo draws on Azure AI capabilities to recognize people and moods.

“With Azure, we get truly cutting-edge technology -and that’s help us build a really special product that can have a significant impact on our society.” Tomás Concha Ruiz, Head of jibo.

Growthland by NTT Disruption: our services to disrupt marketing for sustainable growth

From left to right.- Marga Castaño, Head of Creativity and Nacho Álvarez-Borrás Feijoo, Head of Strategy.

Growthland by NTT Disruption is our digital media house and marketing services. We bring tech and creative talent to the same arena, whilst offering new and unprecedented formats for brands, through two main solutions:

“Growthland’s solutions help brands and agencies grow sustainably through accelerating their digital transformation.” Fernando Lázaro, Head of Growthland and CMO at NTT Disruption.

The first of these solutions is the platform Smart Scalable Content (SSC).

A new way of working that brings together data, technology, and creative talent to deliver quality digital content in a faster, better and cost-effective way. Through a semiautomated end-to-end process, the platform suggests insights, gathered through data and AI to help connect brands with the creatives who are involved in the process. Using these insights, the creative team can work on content proposals that are produced and distributed automatically.

The second of these solutions is, Fourth Dimension Brand Experiences (4DBX). It is an unprecedented solution for creating branded virtual worlds and universes. It works with the latest audiovisual production and gaming technologies. 4DBX can be used for events, virtual venues or audiovisual productions like series or movies. It is aimed to be an answer to the current context of pandemic and new ways of consuming digital channels.

Virtual Patient Observation: our technology helping nurses and medical staff in ICUs

Picture from our ‘Scale Fast’ room at our office. Optimizing Virtual Patient Observation’s (VPO) computer vision and ML, with Alberto Rodríguez, our Sr Tech Lead.

Virtual Patient Observation (VPO) is a solution developed by NTT Disruption to make patient care safer and more efficient. It offers 24/7 video monitoring in ICUs, helping alleviate the work of healthcare professionals, thanks to alerts in real-time. VPO also safeguards patient’s privacy, with its non-invasive features.

An ally for healthcare professionals, so they can focus on what really matters: patients’ health.

Cokoon: our platform to enable the future of immersive experiences

From left to right.- Manuel Domínguez Valcárcel, Product Lead; Luis Díaz Fernández, Product HW Prototyping; Adrián Fernández Martínez, Product Development; Jorge García Domínguez, Product Development Lead.

Mixing the physical and digital world is changing the way we work and play, and we need new tools and techniques to address this challenge.

Cokoon by NTT Disruption is a platform that helps easily and effectively create and deliver immersive multisensory experiences in intelligent spaces that span the physical and digital.

It enables creative teams to use a wide variety of digital and physical systems to deliver experiences. It also includes content authoring and delivery tools, together with access to a creative ecosystem of content, media, and talent.

We are the notes that turn silence into a symphony

At NTT Disruption we are on a mission of empowering small but smart things, which can lead to big changes for good. We feel proud of our amazing disruptive orchestra and the notes behind our symphony: disruptive tech solutions that go beyond a predefined structure to generate change, for good.

Want to play with us our disruptive symphony? Drop us a line!",6274
"One of the biggest changes that’s taken place in the world of business/ecommerce software is a move towards headless, microservices-based platforms — driven by consumers’ desire for personalized experiences and tailored content.

Monolithic systems are increasingly unable to provide the level of flexibility and service customers and businesses require.

Our new ebook takes a look at headless commerce & microservices, and why they’re becoming an increasingly popular choice for businesses in all industries.

Here’s a quick preview of some of the things you’ll learn in the ebook — Headless Commerce & Microservices Explained.

What is headless commerce?

The core principle behind headless commerce is headless software. You’ve probably heard the term, but many people are still unsure what headless software actually is.

Ultimately, headless software is backend software (software responsible for non-user-facing functions) that operates without a frontend (the software users interact with).

This is the opposite of traditional monolithic systems, where the frontend and backend are coupled together.

When backend and frontend software functions are decoupled, the platform becomes much more flexible, scalable, and versatile.

Benefits of headless commerce and microservices

There are many reasons why headless software and microservices have become the go-to choice for ecommerce:

Headless commerce allows businesses to create their own custom software stacks using many different microservices

Headless commerce provides much greater flexibility for new ecommerce channels

Headless commerce saves on developer time and reduces the risk of new changes disrupting the rest of the system

Ultimately, ‘best of breed’ microservices represent the next generation of ecommerce software platforms. Building upon the headless software model, best of breed microservices each fill their own niche within the wider ecommerce software stack.

They can be compared to monolithic software systems as follows:

Microservices are especially suited to large enterprise level businesses because they’re modular, scalable, and packed with features.

This means that businesses get much more for their money, and they don’t have to worry about upgrading or changing their system as their demands grow.

Ultimately, because microservices each serve their own specific niche, they tend to provide greater functionality than monolithic software systems can.

You can find out more about headless commerce and microservices, including tips on how to choose the right ones for your business, in our ebook.",2597
"Checking Under the Dashboard

This article reveals the often-overlooked consequences of women’s voices not being incorporated into tech, resulting in harms that will affect generations. Details of direct harms and indirect harms can be found in Gender Bias in Data and Tech.

AFP/MARTIN BUREAU/POOL: This is an image of French President Macron at a Tech Conference in 2017.

Data Profiling — How It Works and Why It’s A Problem

Do you know why or how certain movies are suggested to you on Netflix over others, or how your news feed articles are prioritized on Facebook, or why certain Google search results come to the top over others? Movies, articles, and search results are targeted to you based on your “likes”, your friends’ “likes”, your search history, your political views, information you provide to other apps and devices that you use, and many other personal traits.

You probably already knew that, but have you considered that they’re also based on whether or not you’re a woman, whether or not you’re a parent, and whether or not you’re in the paid workforce? The Adtech industry uses your online behavior to build a profile of you — who they think you are. They may look at your behavior and profile you as a male, between 40–50, heterosexual, married, multiple children, and living in an affluent area. Advertising is then targeted to you based on what a typical man of this age and demographic is interested in, which is heavily based on societal norms, the personal biases of the individual(s) creating the algorithms behind this, as well as stereotypes. This type of profiling happens all the time — just think of how many online or digital interactions you have with various apps and websites all day long. These companies are receiving a steady stream of information about you that they use to refine and add to their profiling of you as an individual. Not all of this data will be interpreted correctly.

Now think about LinkedIn, online recruiters, job search sites, banks, financial lenders, healthcare providers, and governments — this is not about whether you see the latest handbag or cat video anymore, it’s about whether or not your profile comes up in the recruiters search for a new candidate, whether a lender finds you credit worthy, or whether you qualify for healthcare. This is where technology has the potential to exponentially exacerbate bias with unchecked AI, but it could also reduce human bias by putting up a virtual curtain to look beyond one’s gender, race, or other profiling traits. The former is the status quo; the latter will only happen if we act.

This is why we need a focus on gender data+tech.

What is Gender Data+Tech?

When you hear “gender data+tech” what comes to mind? If you’re working in international development, you probably think of the gender digital divide or you might think of the more recent focus on tech-facilitated GBV. Technologists’ minds on the other hand could gravitate to digital rights or possibly the “pipeline problem”. If you’re the type to discover the next best market, then the untapped potential in women’s healthcare and Femtech might come to mind. To all the data gurus out there, you’re probably thinking of data rights, data security, and privacy. A few of you will be thinking about gender data, its gaps and access problems. Then, if you are one of those very few people who straddle some combination of data science, gender studies, policy, and practice you will likely think of AI ethics, and all of the different types of data bias that can occur in machine learning.

In order to define gender data+tech, we first examine the consequences of status quo. In this article, we discuss direct harms and indirect harms associated with data, or lack thereof, and technology solutions. We not only outline the direct harms of getting online today, but also focus on revealing the often-overlooked consequences of women’s’ voices not being incorporated into tech, resulting in indirect harm that will affect generations upon generations.

Three Billion Marginalized Voices — Creating Space for Women and Girls[1]

In order to address the needs of women and girls, we need to ensure that we are consciously putting them first. This might sound obvious to some, but it will be a dividing factor for others.

Most of the work in this space does not actually have a primary focus on women and girls. Some work acknowledges that there are differences in our global population and focuses on human rights (data ethics/ AI ethics, digital rights/ data rights), some focus on challenging the power and purpose of technology (Data&Society), while others have a focus on all marginalized groups (Design Justice Network, Data + Feminism Lab, Algorithmic Justice League, We All Count). These are all separately and together necessary spaces for the betterment of society.

This is a visual mapping fields of study and a few predominate groups that are close to Gender Data+Tech, organized by prime focus area. This is populated based on publicly available information and is by no means comprehensive or static. This visual is to provide clarity on gaps as well as identify room for collaboration within the space.

Having a space focusing on women and girls is also separately and together necessary. Without this, conversations (subconsciously) orbit around men as the default, even when led by women. Historically, men/boys raise their hands first, which is a problem when the question relates to the needs of women/girls. Of course, there is room to work together, but this requires a women/ girls focused space that enables the different perspectives to emerge.

United We Stand, Divided We Fall

It’s going to take a cross-cutting gender lens with a truly diverse global perspective to address and create for the futures of women and girls within data and tech. Current coordination and funding efforts lack acknowledgement that this problem cuts across all gender work[2] as well as across all developed and developing countries, Northern and Southern Hemispheres.

No matter what perspective you’re coming from, we can all agree that when looking at issues of gender or women, and technology or data it’s a very minimally explored area that needs to be further developed with frameworks and structures that collectively guide us. The intention of this article is to increase awareness of this issue and galvanize people into action. We need a larger discussion that collectively defines all of the risks for individual women and girls as well as the risk on our economic and social systems that shape gender equality now and into the future.

Direct Harm/ GBV/ VAW

Direct harm against women and girls encompasses intentional violence such as: online harassment, hate speech, stalking, threats, impersonation, hacking, image-based abuse[3], doxing, human trafficking, disinformation and defamation, swatting, astroturfing, IoT-related harassment, and virtual reality harassment and abuse. These types of violence can be easily classified as GBV/ VAW, tech-facilitated GBV/VAW, or online GBV/ VAW because the intent behind the acts is clear, to harm an individual or group. There is a small, but growing body of work on these topics.

Groups that have been focused on this type of tech-facilitated direct harm for some time are the APC and journalists, although the work was historically not intended to focus on women, the targeting of women is common, thus a coalition of women journalists was recently formed against online violence. The child protection communities have also had a strong presence, as it relates specifically to children. Traditional GBV communities are also beginning to take up such topics since tech-facilitated GBV is caused by the same generational problem of power imbalance between men and women.

Indirect Harm/ GBV/ VAW

The way in which a majority of technology, especially digital tech, is being created today is amplifying gender inequalities and discriminating against women and girls. Despite preventing women from accessing jobs, funds, public services, and information, indirect harms against women from the misuse of data and tech is almost entirely overlooked.

Here we are talking about algorithmic bias (i.e. coded bias in artificial intelligence and machine learning), data bias (i.e. missing or mislabeled datasets), data security (i.e. sharing identifiable information), and other gender-blind tech that isn’t incorporating the voices of women and girls (e.g. bots, car crash dummies, and human resources software perpetuating harmful gender norms).

For those who want to dive deeper into indirect harm, a detailed explanation of harms and common terminology can be found in this sister article Gender Bias in Data and Tech.

Why Are There So Few Women in Tech Today? The tech industry, like almost every industry or sector, is full of harmful gender norms, bias, and sexism, but something unique to the tech industry is the extremely small representation of women in combination with the massive amount of growing power and influence tech has on all of our daily lives.

As Emily Chang so clearly spells out in her book Brotopia Breaking up the Boys’ Club of Silicon Valley an important aspect to understand when looking into these problem is that women have been excluded from the technology industry for over 50 years despite being the early developers. To all of those who say that it’s a “pipeline problem” she asks, who created the pipeline? The pipeline problem was created by the tech industry itself, from intentionally hiring anti-social men with the Cannon-Perry aptitude test in the 1960s, to Hundred Dollar Joe at Trilogy rewarding impulsive risk takers from strip clubs in Vegas in the early 1990s, to the original founders of PayPal being outwardly anti-feminist and anti-diversity, selectively hiring men like them beginning in the late 90s. The lack of women in the tech industry today is not because there’s a lack of interest or intellect. It’s because the tech industry pushed out women in the 60s and has been a massive boys club with exclusionary behavior from the 1990s through 2010s, only solidifying such behavior today with the largest tech investors coming from the same proud PayPal Mafia.

Now Let’s Move Forward, Together.

When examining why this problem exists it’s easy to get lost in discussions with people who don’t care about the problem, people who don’t see the problem, or people who lack the will to do something. Instead of focusing our finite energy on this, let’s first start with the low hanging fruit. Let’s coordinate the people who care, see, and have the will to do something about the problem, but don’t know how to do it alone or just need their voice amplified.

We are building Gender Data+Tech Community of Practice to bring together thought leaders to create partnerships, collaborative guidance, and develop solutions. Our goal is to build an evidence base with proven data and tech solutions that focuses on tackling gender inequalities for women and girls.

We, as a small but growing global community, are eager to have the critical conversations about how to ensure diverse voices and perspectives of women and girls are reflected and incorporated into technology today (e.g. demystifying black boxes, documenting training data, use AI to fight against GBV) as well as co-create better technology for tomorrow (e.g. participatory tech, inclusion prediction models, NLP tools for gender professionals).

Please reach out if you are passionate about moving the needle for women and girls with data and tech. We are stronger together and now is the time to shape our future.

[1] The terms “woman” and “girl” are social constructs of gender, thus this includes anyone who self-identifies as a woman or girl. This is also meant to include all female persons, indifferent of their gender identity. Language has been intentionally abbreviated not to dilute or distract from the 3 billion majority who are cis women and girls.

[2] Let’s address putting terminology differences aside for the time being. Gender Equality, Gender-Based Violence (GBV), Violence Against Women (VAW), and Violence Against Women and Girls (VAWG) work often overlaps, some would even argue that they are all the same.

[3] Image-based abuse includes: sharing images or videos without consent, taking images or videos without consent, shallow fakes, deepfakes, cyber flashing, sextortion, sextortion scam, child based image abuse.",12493
"GETTY IMAGES

My company implements both Salesforce and Zoho CRM, among a few other customer relationship managment applications. Our market is mostly small and mid-sized businesses. There is this perception among these companies that Salesforce is much more expensive than many of the other CRMs for the small business market. So, is it? Is Salesforce overpriced?

It depends.

For starters, it depends on what you want from your CRM system. If you just want simple contact management for a relatively small workgroup then Salesforce Essentials — it’s lowest price product at $25 per user per month for up to 10 users — is perfectly fine. Other CRMs may come in a few dollars less or more, but for the most part it’s competitively priced and does position you to scale if your company grows.

And let’s not pretend that, for most small businesses, CRM is pretty much about contact management. It’s about having a solid, accurate, complete database of contacts and companies that’s tied into email and calendaring for follow-ups and history tracking. Most of my smaller clients — heck, even many of our larger clients — can’t even accomplish that much. So don’t underestimate the difficulty of getting this right, and the value of having a great contact database.

At the next level up, Salesforce starts to get pricier. Its Professional version is priced at $75 per month per user and pretty much has the same features and functionality as Zoho CRM (and others) where the pricing can be half as much. And even with the Professional version, you’re missing out on more advanced features like unlimited customizations, full permissions, workflows and automations and integrations. So if you’re a smaller company that desires those things — and many do — then you’re going to have to pay a lot more for Salesforce than many of its competitors.

To get all of those features you have to subscribe to Salesforce’s Enterprise version which the company says is it’s “most popular” version. I can certainly understand why Salesforce wants you to believe that. At $150 per month per user it’s a nice revenue stream for the CRM provider. And it gives its users all the bells and whistles. But for most small and mid-sized companies that’s a pretty steep price to pay, particularly when you get most of these features with a CRM like Zoho or others like it.

Then there are other considerations that will impact your Salesforce pricing. Add-ins for doing marketing, service and integrations to other applications like accounting are extra monthly fees. Storage costs can rise based on usage. And, unless you’re ready to outsource your implementation to a team in India, Salesforce implementation partners (particularly in the U.S.) tend to charge much higher fees.

But keep one thing in mind: with Salesforce you get what you pay for. Salesforce is like the BMW of CRM. It has the largest market share by far and its high reputation is deserved. Whenever I encounter a company unhappy with Salesforce it’s almost always because it wasn’t implemented the right way (or the company is only using a fraction of what it offers). You’re never going to get fired for choosing Salesforce.

What you also get with Salesforce is a very strong and passionate community. You get a worldwide network of very experienced and talented partners. You get a seemingly unlimited amount of online training resources and tools. You get a much larger choice of products that will integrate with it. You also will enjoy a high level of customer support that you’d expect from a company that services some of the world’s largest brands. Smaller CRM makers like Zoho oftentimes are challenged to match this. As an example, Zoho’s support (which is mostly out of India) never fails to underwhelm me and my clients. Maybe that’s why their application costs so much less?

Again, you get what you pay for.

Salesforce is clearly a great solution for larger enterprises, particularly when those companies dedicate the right amount of resources — both internal and external — for getting it setup, implemented and adopted by users. It’s also a great solution for smaller businesses too. But the same rules apply: a significant investment is needed. But if that commitment is made, then it may be worth it.

Follow me on Twitter or LinkedIn. Check out my website or some of my other work here.",4354
"A Product Manager’s Guide to APIs

Image by Fiverr

We live in a world where technology reigns and data presides at every corner. As users of many different products, we’re no longer looking for the best product to get the job done, we’re now looking for the product that gets the job done AND works seamlessly with all other products we use. Therefore, it has become increasingly important to understand tools that simplify workflows and integrate components to provide a seamless user experience for our customers. Since data has become abundant, innovative teams have grown exponentially better at forming these links and connections to simplify workflows via APIs.

What is an API?

An Application Programming Interface, or API, is in its simplest sense a technology that connects two systems.

Here’s an analogy explaining it:

You go to a large library and are looking for ‘The Da Vinci Code’ by Dan Brown. But the library is huge and you have no clue where to find this book. Luckily for you, you see the librarian at their desk and a catalog with the types of books you can borrow. You request for ‘The Da Vinci Code’ from their fiction list. The librarian walks through the labyrinth of shelves to find the book and brings it to you.

Photo by j zamora on Unsplash

In this analogy

the library is the database — one of the systems involved

— one of the systems involved the books are the data

you are the requestor — the application/system looking for information

— the application/system looking for information the librarian is the API — they take the request back to the database and return information back to the requesting application

— they take the request back to the database and return information back to the requesting application the request for the book is the call made to the API

made to the API the catalog represents the specific format the request has to follow so the API understands it

so the API understands it the book you received from the librarian is the response

By Sachin Jain for ByteNBit

In the simplest of terms, that is what an API does: it acts as the interface between two applications and facilitates information transfer while ensuring speed and security. It is a developer-centric tool — APIs are built by and for developers as part of the application code, however, this doesn’t mean they can’t drive value for the end user.

How do they work?

Let’s take a real life example.

You are on LinkedIn and you are trying to find some companies in the fintech space. You type out your keywords (industry, location, etc.) and LinkedIn executes the search to display relevant results from its database of thousands of companies in less than a second. This request to look up all the relevant companies almost instantaneously is likely facilitated by LinkedIn’s Company Search API (which is also available for use by external applications).

This type of request-response interaction can occur within a product or with external products and is used to facilitate information transfer ranging from financial payments data to location data, in order to provide a seamless experience for the end user.

Why are they important to product managers?

APIs open up a world of opportunities to build a more-integrated product to provide more value for your users. As a product manager, it helps to understand the benefits and the constraints that also come with the technical solutions built/leveraged by the product team, so you can make and communicate product decisions and strategy effectively. I’m not saying you have to understand APIs to the point where you can dig right into the code — instead you should be able to understand the value it provides your user and business so that you can identify and test if there is a need for it and communicate WHY this is necessary to all stakeholders/partners of your product.

Using APIs built by others

I’ve created a really simple, short list of the ‘Good’ and the ‘Bad’ items to consider when thinking about using APIs built by others.

The Good:

It can provide a well-integrated and simple experience for the user

If you leverage other APIs, you can focus on building features that address the core need for your product, while maintaining a simple and complete flow for the user

It can reduce the effort to implement specific features

The Bad:

Some cost money (based on the volume of requests you’re making)

You create a dependency on another system — if the other system changes their API call, you have to make changes on your end to ensure your user experience isn’t interrupted

Building an API for others to use

There are a few key items to consider before undertaking this:

Is this something users want or need? If so, are they likely to use it? This can be validated quickly by talking to customers or even with landing pages to evaluate the demand

What value is this driving for our business? Are we expecting an increase in revenue? or a higher conversion rate? It’s also important to ensure that we are targeting metrics in line with our product strategy and our organizational goals

Can we even do this? Is our business ready to support the cost of building it out and also the cost of maintaining the infrastructure to provide this service for our users?

Implementation

Once you’ve identified the need for leveraging or building an API, it’s also important to understand how you break this down into user stories for implementation while maintaining focus on your user persona’s goal. It may help to split the user story into two — one for the API and one for the UI functionality/piece. However, you could include it all in a single user story and call out the API criteria specifically in the Acceptance Criteria. The approach should be discussed with the product team to see what works best for them.

Who is the user?

This could be developers, testers of an application or technical folks on specific internal teams. It is important to really understand and isolate your user and have a clearly defined persona for each type of user to provide context on their specific needs and goals.

For example — if your user is a developer, make sure you either define the specific type of developer (front end/back end/developer looking to incorporate payment functionality) or have a clearly defined user persona the team can refer to.

What is their need and their goal?

For this portion, you need to understand the functionality that is expected (create, read, update, delete) by the user and tie that back in to what the user is trying to achieve.

Your final user story could go something like this:

As a developer working on payment systems, I want to fetch the customer’s relevant financial data when I send a customer identifier, so that I have the information from the customer to process the financial transaction.

Acceptance Criteria

I’d include any mandatory fields that need to be a part of the API requests and the key fields that you need from the API response. If this connects in to the front end, it’s also important to call out those elements here to ensure its tackled by the team. If you’re integrating with an existing API then it’s best to read all the API documentation upfront and ensure it’s attached to the stories for everyone’s reference.

Another section to include would be API criteria that is not directly associated with the actual function of what the API is trying to achieve, but could impact the end customer. This could include the number of requests to be handled per unit of time, the time it takes to send out/receive a response, any authentication/authorization information expected by the API, limits on the volume of data sent in a single API call — it’s best to call this out explicitly and tackle this with the technical team, especially if these items can impact the end customer experience.

Few more things to learn about APIs…

Now this section includes more details that you could say are teetering on the edge of the more technical side of things. I’ve used this section to detail areas I’m familiar with through work and also explore areas new to me.

Public vs Private

APIs largely fall into 2 buckets: Public and Private.

Public — these are APIs open for use external to a company. Key examples are the Google Maps API to leverage Maps functionality in your product, Braintree API to leverage payments functionality in your product or the LinkedIn Company Search API to add company search functionality in your product. The goal is to be able to share information for use cases being tackled by other companies, that can end up benefitting your end user (eg. Open Banking in the world of financial data)

Private — these are APIs developed for internal use for developers/applications with specific access. Key examples are customer information APIs within a company that can be used by different business functions within that company for obtaining customer information pertaining to the problem they are tackling.

Types of APIs

I’ve only listed the two key types (there are other older formats that are not used as commonly today)

REST (Representational State Transfer) — this is currently the most common type of API. REST represents a specific API architecture and these APIs generally use HTTP functions to make/receive requests (with information typically being sent/received via JSON files). A key thing to note about REST APIs is that they are flexible in terms of the types of data they return/take and have a low bandwidth. There are 4 key HTTP functions these APIs are based on: POST, GET, PUT, DELETE (performing create, read, update and delete functions respectively) SOAP (Simple Object Access Protocol) — this is an actual protocol. SOAP APIs generally use HTTP and XML. These APIs are often heavier in terms of bandwidth and payload.

Webhooks

Webhooks get a special mention because they can be considered a special type of API — the simplest way to describe them is a “reverse” API. With APIs, the data transfer will not happen unless a request is made explicitly; webhooks on the other hand trigger the data transfer based on an event (such as a payment being received, an update to a user’s feed, etc.) which can be really beneficial to automatically trigger a downstream set of events.

Where do we go from here?

This was just a short intro into the world of APIs and how it could play into conversations with your stakeholders as a Product Manager. Hopefully, it’s equipped you to identify opportunities to optimize your product and also understand the numerous items the product team has to consider when dealing with APIs.

There’s still a lot more that comes into play when building or using APIs, ranging from understanding API documentation to figuring out how you authenticate users of your API, and this means there’s a lot to be learnt if you’re interested. Time to dive right into it!",10897
"Pull or push, push or pull…

As maritime informatics continues to shape trade, the next challenge is to make sure everyone can and will play nicely together. Shipping people and digital transformers can be the best of friends, but the terms of engagement need to be set. So in a world where data can be pushed or pulled, it is time to pin down who does what and why…in which direction.

INTEGRATION FOR THE MARITIME NATION

The boom in maritime informatics and the enthusiasm in the shipping industry for ever more data means that we have so much more information to access and manage. This means that all providers need to better understand the needs, wants, hopes and dreams of clients.

We are at a wonderful age, where clients and tech are coming of age together. However, all the advantages can be eroded if the terms of the data exchange are not fully understood or managed. Once there is understanding, then the means of doing “business” can be shaped and this means turning to an API.

It is impossible to read about maritime data or listen to tech gurus, without at some point the term “API” being used, which means “Application Programming Interface”. Which put simply, is a set of programming code that enables data transmission between one software product and another. It also contains the terms of this data exchange. It is a software intermediary that allows two applications to talk to each other.

The API is the messenger that delivers your request to the provider you want information from and then delivers the response back to you. A runner if you like, between you and what you would like to know, or to share. It is vital in setting out how data will be exchanged, that the API is shaped to reflect what a client wants, and how they need it.

DEVELOPING SOLUTIONS

An API is the building block that allows all parties to get what they want in the way they want it. It spans between one system and another, allowing functionality and flexibility when it comes to how services are offered.

It serves as the interface that delivers data from the application and facilitates the interaction between the application and the system being interrogated. The ability of API-led connectivity to allow systems to change as easily as plugging in is key to the modern vision of enterprise IT.

The API makes things work for all the parties involved and shape the experience of a user. Get the API right and it is a solution, get it wrong and it can become a problem — but one that you may not even know is damaging you, until it is too late.

Whether cargo data, navigation, stability, engine performance, whatever the user is interested in, the APIs do the same for all interactions between applications, data, and devices.

LET US KNOW THE FLOW

Even ashore it can be tough untangling some of the misunderstandings around APIs and web services. When you throw ships into the mix, it gets more challenging again. At the core of the debate is which way data initially flows, and there is much debate in the IT industry about the issue of “push versus pull” — a Google search brings up 20k results of just that very discussion alone. There is rather less discussion when it comes to the same API issues and shipping.

Regardless of where, when it comes to how systems are integrated, and how they are interrogated, without getting too “Carry on Computing”, it is always a case of knowing who’s pushing and who’s pulling. Vendors, clients, charterers, offices ashore, all are vying for the right set-up for them. So, it matters to get it right. Indeed, it matters more than you might think. The implications for the right choices are clear.

Done properly and everything about digitalisation can drop into place. The data and information flow, and the systems deliver for the client wanting the right insight and intelligence when and where they need it.

Conversely, if the API doesn’t deliver, there can be frustrations and all parts of the data exchange are frustrated. There are also significant cost implications. Pull calls all data whether it has changed or not, while push only sends what is needed. Those debates aside, though, we need to be aware that digitalisation fails if we don’t get the flow right. So, let’s ponder the power of push versus pull some more.

PUSH V PULL

In a “push” architecture, a client requests work from a server — the work is “pushed” to the server, which has no choice in the matter. So, the client either gets what they want or can flag issues if they don’t. “Pull” architecture is different. This is when a server requests work, which it then holds in a queue until it is ready to complete the task.

The “push” approach tends to be almost a default for request-response operations and works because it usually involves a persistent connection over which both the request and the response travel. Matching the response to the request is as simple as seeing what response comes back over the same connection.

Pushing isn’t perfect, but ultimately it gives a greater insight into potential problems. Whereas pull can feel less certain. as the request/response communication pattern makes it easy to see requests go out, but the response is less certain.

It can also be a problem as the pull approach in essence forces the client to apply business rules to the pull frequency. This is not the best answer in a maritime informatics setting. The client can end up with complicated rules, and sometimes their own developers may not fully understand the challenges of the shipboard setting and needs. Inefficient pull frequencies can end up impacting the bottom line too, which is no good for anyone.

GETTING IT RIGHT

These are really important distinctions when it comes to data too. Get the push/pull dynamic wrong, and we have a potential recipe for disaster. So, the emphasis is on getting it right. Which in turn means understanding the terms, the implications and what it means for you, your company and of how to make the right choices to get the results needed.

The ways in which data and information flow from sensors are pivotal, and in a world where data can be sucked or blown, it is time to pin down what clients want from the data chase. In a situation where sensors can push or pull, you risk problems where you simply pull data blindly.

All too often there are problems from the very start, as people misunderstand the integration issues and challenges. They use push or pull, pull or push, interchangeably. Not realising the real-world problems that can be created.

So, starting the metaphorical voyage of integration in the right direction is imperative. It is the foundation of all that comes. So, getting the basics in place gives the foundation for success. With APIs moving data from one system to another in any integration, pushing information results in a very different scope of work for a developer when compared to pulling information.

It is vital to make it simple to both understand, but even more importantly, simple to apply. To ensure we all play the same way, which makes us all the more efficient. The message is that if companies aren’t optimising for the trade they are developing for, then there is a potential cost in time, money, bandwidth, processing power, speed and timeliness.

If you really want to get the developers attention, don’t even talk push and pull…use suck and blow instead!",7381
"Midway through 2021, and the entire world is still adjusting to living with COVID-19. As if things weren’t challenging enough, Brexit has added further disruption to the British and European political landscape, with many difficult discussions about trade deals and so on. Shipping companies have had to navigate tricky waters, literally and metaphorically, to bring goods across borders. The added red tape from Brexit and extra safety measures to control the spread of coronavirus have severely impacted the expected shipping time across most European nations. According to sources, container freight rates from Asia to the UK have increased almost fourfold since November. This is due to a global rebound in demand for consumer goods and materials and congestion across UK ports, where many empty containers have been left stranded.

As well as the challenges mentioned above, European customs officials have long been working hard to reduce illegal trade and fraud entering the bloc. Organised criminal networks target perceived weaknesses in customs procedures as access points to fraudulently bring goods into the EU. According to some EU reports, imports of counterfeit and pirated products amounted to around €121 billion (6.8% of EU imports) in 2018.

Of course, with new legislation, there is always a settling in period, and the safety measures to stop the spread of COVID are simply unavoidable. However, there are things we can do. Enhanced customs technology and communication channels would improve security and reduce fraudulent imports. This is precisely what we at The Proof of Trust want to achieve — enable safer and faster cross-border trade while simultaneously decreasing delays.

The Proof of Trust technology integrates into existing systems and acts as the backbone of a decentralised customs clearance system. Shipment issues, such as missing licenses or identification queries, would be immediately checked by customs officials or other vetted individuals. If the cargo is identified as fraudulent, The Proof of Trust will pass information onto the relevant authorities to intercept illegal goods. A recent cooperation agreement has been signed between the Secretary-General of the World Customs Organization (WCO) and the Director-General of the European Anti-Fraud Office (OLAF), which focuses on EU wide information sharing. Our technology aligns perfectly with this agreement. Customs departments connected via The Proof of Trust could share information to improve fraud identification — saving taxpayers money and speeding up worldwide shipments.",2579
"There are certain things that are done in polite society, and we never really question them. As an example, a decanter of Port, like a dutchie, is passed ‘pon the left-hand side. Why? Who decided, how and what was in it for them? It is a little like standards and maritime informatics, why do we do what we do?

LED BY ETIQUETTE

First off, let’s finish off the Port issue. According to those wonderful people at Taylor’s, once a vintage Port is served tradition dictates the decanter should be placed on the table to the right of the host or hostess.

It should then be passed to the left, travelling around the table from guest to guest in a clockwise direction until it comes back to its starting point. There are many colourful explanations for the custom of passing the Port to the left. One theory is that the custom arose from the need to keep one’s sword arm free in case of trouble. Tipsy people with swords, what could possibly go wrong.

It is sometimes said to have originated in the Royal Navy where the rule was ‘Port to port’, meaning that the decanter should be passed to the left. The reason we do the same today is quite simple. If the decanter keeps moving in the same direction, every guest has the opportunity to enjoy the wine and no one is left out.

There are more rules. If someone is slow in moving the decanter on, they are asked if they know the Bishop of Norwich…he tended to fall asleep at dinners, and so the Port would stop circulating. Indulge me even further for a moment…I was once sat next to HRH Duke of Edinburgh at a maritime shindig. Being a little overexcited, I somehow forgot my Port and sent it Starboard, passing it back the wrong way. He seemed displeased, as only he could…Anyway, I digress.

WHEN TRADITION BECOMES THE RULES

What the Port and the direction of passing tell us, is how things have always been done tends to persist. People like certainty, they like to know what is expected and they are happy to be part of that.

People don’t like surprises (in the main), and so we see so much of what is done in shipping is based on what has gone before. We do it that way because they did it that way, and they did it that way because their forefathers did.

Things become entrenched and it can be hard to make the leap forward. Even a step forward can be a challenge sometimes. Indeed, we see that traditions are very important in shipping.

A tradition is a belief or behaviour passed down over time. The culture into which the tradition passes is also important, and shipping has a very long past indeed. Hence many different ways of thinking and of ways of doing things have persisted. That is the culture within our industry, and it has added to the rules and helped sustain them.

WHY WE DO WHAT WE DO

Such a culture represents shared norms, values, traditions, and customs — and the maritime industry had long been guided by certain ways of doing things, some shaped by the need to be safe and healthy at sea, others because the money people have looked to embrace things such as finance, bills of lading and ways of providing insurance.

It may amaze us today, but the operation of ships is still very much influenced by strong cultural processes. There are very distinct rules and norms that are transmitted between seafarers and shipowners across time and generations.

Traditions represent a critical piece of our maritime rules culture. They help form the structure and foundation of our crews and companies and remind us that we are part of a history that defines our past, shapes who we are today and who we are likely to become.

It has long been thought that once we ignore the meaning of our traditions, then we are in danger of damaging the underpinning of our identity. This is why we so often see that standards evolve, and at their core, they are shaped by traditions and ultimately our culture.

GOOD AND BAD STANDARDS

Modern shipping and standards are very close bedfellows indeed. Shipping loves standards. Indeed, so much of what we do is about a standardised approach and making sure that all parties in the chain adhere to the standards that are imposed upon them. So we have shipping based on standards, and ones that are negotiated by those using them — the self-licking lollipop effect.

In theory, this should be a good thing. Alas, in reality, what can happen is that the standards aren’t actually that demanding and we find a race to the bottom and a compliance culture that can and does encourage those who can get away with doing the bare minimum to do so.

Shipping’s proud traditions have actually rested on the lowest common denominator. This means it is very hard for excellence to seen as a goal, as compliance is sufficient. Standard in some definitions means average, and that is what we so often get.

This all makes it doubly worrying when we hear such a clamour amongst the shipping industry to have standards for data and maritime informatics. Standards that will no doubt make sense for one split second, but will almost instantly be rendered useless and embarrassingly outmoded.

STANDARD ANSWERS

Very often the issue of standardisation is a poorly understood discipline. Standards can be viewed in many ways, they can be economic, policy led or technological. They can drive change or stultify it. It used to be thought that the sum of human knowledge doubled every century, and then every 25 years, now it doubles every 12 hours.

To such a backdrop of such incredible change, standards can be at one point a tool for progress, and yet can be a noose to strangle further evolution or improvements. At their best standards prevent bad things from happening, but alas they are not overly useful at helping good things come about.

Nearly every industry is affected by standards, some more so than others, and shipping is probably one of the most highly regulated. Standards are often the hallmark of a mature industry or society, as they provide levels of certainty and interoperability. You can mesh things together when everything is standardised runs the argument.

However, you only have to look at the unseemly struggle between the UK and EU on the issue of standards to see that this is something that matters a great deal. Standards matter, standards set the agenda and narrative when talk turns to whichever arena we are operating in. The power holder sets the standard.

STAGNATION BY STANDARDISATION

So, as we see, shipping loves a good standard, and alas this often means the industry has tended to be far more interested in compliance than it ever really has about excellence. This is important, as when we see standards as the lowest common denominator, then it means that progress can be very hard to come by.

The areas we are looking at, of maritime informatics and the digitalisation of shipping, are constantly moving, changing, advancing and evolving. There is a real danger in that applying standards then all the good which can come will be sacrificed for some jaundiced, sickly kind of certainty. Instead of standardisation, we need ethics and morals, and then to be let loose to develop and make magic happen.

It is not like standards are easy they aren’t. They are hard to develop, they take time, agreements and compromise. Even where this is possible, they then only provide a snapshot in time of what was possible, and which was ultimately deemed as that which should be possible.

It should be remembered that while standards can bring some degree of certainty, and of course make it easier to check and balance against, what they also do is act like a brake. They slow innovation, and at the time when all of us in maritime informatics are standing on the shoulders of giants such as Mikael Lind and his work, there is a danger that instead of seeing over the horizon at the potentially wonderful things we can develop for the industry, instead, we will just see a wall…one stamped with the word “Standard, Do not Pass”.",7953
"Trucker Tools recently surveyed owner operators, company drivers and carriers like you from across the country to learn more about how you use technology in your day-to-day work. Truckers and carriers were kind enough to respond to our survey, sharing their experiences and thoughts around technology. Of those folks that answered our survey, 40 percent are carriers, 57 percent owner operators and two percent reported that they’re company drivers. Nearly all of those who responded move flatbed, reefer and/or dry van.

Check out these four surprising results from Trucker Tools’ recent carrier technology survey.

When we asked about needing to install new apps to fulfill brokers’ requirements, we received a range of answers. However, nearly half of those who answered our survey said that they’ve had to install between three and 10 or more apps on their phones in the last year to satisfy brokers. Only four percent who answered said that they don’t use apps because they prefer check calls. In the comments, several carriers/truckers voiced their frustration about the number of different apps they need to use to track loads.

Fact: You can use the load tracking tool in Trucker Tools’ appFact: The digital load search and booking tools included in the Trucker Tools mobile app are free. You can quickly search for loads with your favorite brokers in specific lanesFact: Trucker Tools’ mobile app includes 17+ features and tools that save you time and money on the road. With the free app, you can find the cheapest fuel on your routeFact: The Trucker Tools app’s load tracking provides transparency for the driver. You have to accept a load track in order for tracking to begin. It’s easy to see when tracking is on and who is receiving location information. You also have the power to stop or pause tracking right in the app.

, , routes, , truck stops, rest areas, Wal-Mart stores, food, restaurants, CDL legal protection, truck washes and more.

in just a few seconds.

with multiple brokers. Trucker Tools’ load tracking integrates with most broker software platforms, which means you can use the Trucker Tools app over and over again instead of installing new apps all the time.

Another surprising result from Trucker Tools’ recent carrier technology survey is that two-thirds of truckers and carriers are paying a monthly fee to access load boards and other load-finding technologies. In fact, 45 percent of those who answered the survey pay $30 or more/month to use these apps or load boards when they’re on the road. Some told us that they pay as much as $140/month on digital technology to help them find loads.

We sometimes hear people in the industry say that truckers and carriers are resistant to using technology, but that definitely isn’t the case and this survey proves it! Smart phone technology plays a big role in the daily lives of truckers and carriers. A whopping 87 percent of those we surveyed said they use technology on the road every day. In the comments, respondents told us they most value technology that helps them find fuel, routing, parking, loads, truck stops and more.

The fourth surprising result from Trucker Tools’ recent carrier technology survey is that despite embracing technology, many truckers/fleets still have reservations about using digital load tracking technology. Thirty-seven percent of those we surveyed reported that they refuse to use some digital load tracking technologies. Many of the commenters on this question in our survey said that their main concern is privacy, while others told us that they find that load tracking tech can be difficult to use.

For more tips on creating a profitable, self-sustaining trucking business, read Level Up Your Trucking Business: How To Calculate Fuel Costs on Specific Lanes and Loads. To download Trucker Tools’ free app, visit https://www.truckertools.com/web/carriers/.",3881
"Traditional freight brokerages are adopting powerful new technologies at an increasingly faster pace. Technology is the foundation for building better engagement and making your brokerage “stickier” with the carrier community in a capacity challenged market.

This session explores how evolving technology is impacting brokers, and the key operational challenges that can be solved — and deliver rapid ROI — with the right plan and approach to building out your brokerage’s technology capabilities.

Watch these short clips from the interview to learn:

How to evaluate partners, map out a plan, and prioritize technologies that offer “quick wins.”

The top 3 issues brokers should focus on and how resolving these can elevate the business.

Why listening to the voice of the customer is critical to effective decisions and avoiding failure.

Training and culture; bringing your workforce along, successfully managing change.

Why technology projects are not one and done, the continuous improvement journey.

Where is your digital transformation journey taking Becker Logistics?

When planning your digital technology transformation journey, there are many factors to consider, including scalability and bringing your customers with you on the journey.

Jim Becker from Becker Logistics discusses the importance of scaling with the technology and working on substantial growth over time rather than just jumping in 100% right off the bat. You first need to test out the technology and make sure that it’s working properly for your business, your customers and your carriers to make each of you profitable.

As you start to create a plan for your digital technology transformation journey, the first think you need to think about is what your digital strategy looks like. The platform you are adopting should fit with your strategy in 3 main categories:

Prasad Gollapalli from Trucker Tools dives into the process of planning your digital technology journey and the most important things to focus on when starting to choose what platform you should adopt.

Jim Becker from Becker Logistics talks about the importance of stakeholders internally and externally and how every business decision, such as their choice to adopt digital freight matching, effects every stakeholder involved in the business.

The 2 main benefits a technology must have are a live connection with carriers and real-time data. However, a lot of brokers get caught up in the buzz words, or “noise”, when they are selecting a new technology and end up with a solution that doesn’t match their strategy.

Prasad Gollapalli of Trucker Tools discusses how taking away “noise” and focusing on the benefits of a digital platform will help you narrow down your choices and select the right technology for you as you are transitioning to a new technology.

“If you aren’t on a digital platform you are going to be left behind” — Jim Becker, CEO, Becker Logistics

“If you aren’t on a digital platform you are going to be left behind” — Jim Becker, CEO, Becker Logistics

Jim Becker of Becker Logistics dives into the process of how they plan to get up to 90% of their freight booked through digital freight matching and how Trucker Tools has helped them improve their business strategy and efficiency with the predictive data we provide.

What are the biggest lessons learned in Becker Logistics’ digital transformation journey?

“If you don’t invest in the future, you wont have a future” — Jim Becker, CEO, Becker Logistics

Jim Becker from Becker Logistics stresses the importance of investing in the future and not being afraid to spend the capital to improve your business.

Prasad Gollapalli from Trucker Tools also discusses the importance of having a clear goal of where you are going and using technology to reach that goal. Technology is the connectivity between you and your carriers and will help to build that network and keep everyone happy and profitable.

To watch the full interview, click the video below.",3988
"BitNautic is offering its users an amazing opportunity to advertise and trade their goods. Producers have the access to list their products on the platform mentioning the specific location. It will cut off middlemen and increases profitability. Moreover, BitNautic brings all the users together where they can negotiate their terms that makes processing easier.

You might be thinking that how BitNautic Ecommerce services are different from other shipping platforms. BitNautic is based on Blockchain, a disturbed ledger that increases its competency. Here are some ways through which BitNautic is revolutionizing wholesales e-commerce:

Smart Contracts

BitNautic facilitates its users with smart contracts that automate several processes following specified rules. It eradicates third-party involvements which enables e-commerce enterprises to grow by cutting down costs. The addition of smart contracts to the shipping environment also makes inventory management easier for wholesalers.

Easy to Access Receipts and Warranties

Integration of blockchain solutions authorizes wholesalers, retailers, and customers to store their receipts and product warranties without any hacking threats. You now never have to worry about lost invoices and receipts because BitNautic offers a secured storage that can be accessed anytime anywhere.

Supply Chain Monitoring

The supply chain has always been an issue for wholesalers to track their goods and to have on-time payments. Moreover, they need to know stocks in the pipeline and their estimated deliveries. BitNautic offers an e-commerce platform where users can enlist and track their products that makes the supply chain network transparent.

Final Words

As Merchandisers all around the globe are turning to wholesale e-commerce as the storefront for their business growth, BitNautic is acting as a backbone for online sales and payments. Besides the fast speed and cost-effectiveness, BitNautic is modifying the traditional wholesale and shipping markets to a Blockchain base Platform, removing the paperwork and human mistakes. The transparency that the BitNautic e-commerce platform provides is today’s necessity to maximize growth.

Want to know more about BitNautic, visit our website:

Check out the BitNautic Platform: https://merchant.bitnautic.com/",2306
"Perhaps the biggest driver of maritime informatics and shipping digitalisation has been the changing landscape of satellite communications. Connectivity for ships has made all things possible, but are we any closer to them being probable? Let’s use the medium of astrology to see the future of maritime communications, I have been looking at my tea leaves and bring these pearls of wisdom…

12 SIGNS OF THE CONNECTIVITY ZODIAC

It may seem an odd thing to shoehorn star signs and Starlink, but blame Low Earth Orbit satellites…or LEO as they are known. They got me thinking along tortured metaphors, so let’s get into it.

LEO — “the lion”, is known to be compassionate and big-hearted, driven, and a natural leader…just as LEO satellites have been. Elon Musk’s SpaceX sat comms revolution has seen Starlink LEO change thinking on what is possible when it comes to connectivity. Something which then also ramps up the possibilities for digitalisation and maritime informatics.

LEO satellites circle the earth at lower altitudes than geosynchronous satellites. It is normally at an altitude of less than 1000 km but could be as low as 160 km above Earth. Communications satellites in LEO often work as part of a large combination or constellation, of multiple satellites to give constant coverage, and that is why Musk’s spray painting the sky with LEO satellites is a potential game-changer.

Starlink aims to sell internet connections to almost anyone on the planet by way of a growing network of private satellites orbiting overhead, currently numbering around 2000. Musk says Starlink has shipped more than 100,000 satellite internet terminals to customers in 14 countries. When it comes to maritime connectivity, this lion is roaring, and the rest of the industry is having to take note.

Virgo — “the virginal maiden”…it would perhaps be hard to consider some shipowners in this bracket, but there are many shipping companies who are indeed just dipping their toes into the waters of not just connectivity, but the uses for it.

They are seeing data, digitalisation and maritime informatics as a means of not only doing business better but doing better business. Winning longer-term relationships with charterers, as they are part of the data picture, and embraced not just as a transport tool but as a part of the logistics and supply chain solution.

Libra — “the scales”. When it comes to the digital journey that many shipping companies are on, there is a lot of things to weigh up. So, Libra is perhaps the perfect sign to capture the essence of the decision-making process. According to my Beginners Guide to Astrology, Librans are often concerned with attaining balance, harmony, peace, and justice.

They are known for their vast stores of charm, intelligence, frankness, persuasion, and seamless connectivity, they are well-equipped to do so.

So, the balance is about harnessing what they have, and of making it work — of delivering the benefits that a good business, with proper data flows and means of capturing, analysing and responding to the evidence deserves.

Scorpio — “the scorpion”. Well, just as with any massively important business decision, things such as connectivity and maritime informatics can come with a sting in the tail. The importance of what can be achieved is sometimes overlooked and obscured by a focus on the means of generating, capturing and disseminating information and data.

That is important, obviously, but there needs to be a process and flow. There needs to be a real consideration of the moving parts, the people, the outcomes, hopes, dreams and desires. It is no use just signing up to faster connectivity, to have sensors churning out data, it all needs to come together as a coherent output. The reasons need to be understood and addressed, not just another tick box or bandwagon to be jumped upon.

Digitalisation and the data generated can be hugely significant, but just as the old saying about people knowing the cost of everything, but the value of nothing. The same applies to data. The key to avoiding getting stung is not just seeing numbers, but of seeing reactions to those figures. For every data point action, there needs to be an equal and opposite reaction…which builds to genuine and tangible improvement.

Sagittarius — “the centaur”. A majestic figure, half-man, half-horse. The centaur of mythology is a learned healer whose higher intelligence forms a bridge between Earth and Heaven. These kinds of shipping companies are the purest form of digitalisation devotees. You know the kind, often Scandinavian or German, they have beautiful offices full of smart people doing amazing things.

They invest heavily and can reap the rewards that a data revolution can bring. They understand their seafarers, ships, markets and clients — and dig even deeper through the technological leaps they have been able to take.

The danger for companies looking to ape the Sagittarian approach is that they may indeed create a creature which is half man, half horse — but a horses head on a bloke’s legs isn’t the look we are going for.

Capricorn — “the goat…or “Greatest of All Time”. Perhaps building on the work of the centaur companies, this is the shipping company that has been about digital even before anyone really knew what it meant.

They use data as a lubricant for all parts of the business. It cools down the moving parts, it makes things easier and it allows them the space and time to make good decisions.

You perhaps have a company in mind, but there is jockeying for position as to the GOAT accolade. Similar to Messi and Ronaldo, there are big companies who do data, but do it differently. Some are skilful and low key, they weave data together and take your breath away with the simplistic wonder of what they do with it. Others are showier, and rely on raw power and create their own reality around the depth of data.

Like Highlander though, there can be only one, but it is fun to watch the race to the top for once instead of thinking of shipping as always being about the bottom.

Aquarius –“ the water carrier”. Anyone who has seen Adam Sandler’s artistic tour de force, The Water Boy, will perhaps know the kind of company we are about to touch on. They are around the action, they have the gear, the locker room passes and a notion of what is going on around them.

Alas, they do not have a real sense of what the bigger picture is, or even can be. Some don’t even know there is a picture. They just see the micro view of their business, and they are intensely focused on the minutiae of making money. Which is no bad thing, but does always mean that it can be too easy to be left behind. In constantly sweating the small stuff, the water carrier huffs and puffs, and is there for the immediate reactive things. They tend to be tactical and not strategic. To be a success in the brave new world of the digital transformation companies have to look over the horizon not down at their feet.

Pisces –“ the fish”. All too often one of the major considerations for those entering into their digital journey, or indeed focusing on connectivity and drilling into data, is that of the promises made. Alas, as with so much in life, some things are too good to be true and a little fishy!

Promises of data download speeds, satellite footprints and bandwidth are some of the main connectivity realities which can impact the rest of a ship, fleet or company’s ability to truly revolutionise the way they operate.

Throttled back or sketchy connections are the stuff of nightmares today, and the digital capabilities of companies can be strangled at the dome before they ever reach home. So be careful of connectivity promises, because fall at this first hurdle and the race could be done.

Aries — “the ram”. It would be impossible to characterise a ram without thinking about charging. This is a very sticky area when it comes to maritime connectivity. Get your bandwidth and rates sums wrong, and you will be constantly playing catch-up. This is about the balance between costs and vision, about price and value.

There is seldom a Financial Director that doesn’t wish that fleets would use less data, and ne’er an Operations team that doesn’t wish they could use more. So there is a constant push and pull, and sadly even in the digital age of 1’s and 0’s, it is about the noughts on the end of the invoice that matters most.

Taurus — “the bull”. Ah, you can almost smell the bull when it comes to the promises which some companies make regarding connectivity, or indeed the usefulness of the data that will be generated. Like any new age, some have a worrying habit of over-promising and under-delivering. Often based on something I call, under-understanding.

As a former ship’s Master, it has always been so important to me to be able to really and truly understand not just the solutions which can be provided, but more than that, to dig into the actual problems that clients want solving, or indeed the constraints they may be faced by.

Very quickly it has become apparent that the mission-critical nature of data is rarely if ever, one size fits all. This is great if companies are open-minded, and if providers really want to offer the best solution for the client, not the provider. It can mean real problems if the shipowner doesn’t fully understand what they want and need and if someone comes along talking bull and just lands them with the solution they want to provide. Seeing past the bull and finding the real answers matters, a lot.

Gemini — “the twins”. One of the areas of work that is really gathering pace and importance in shipping is that of digital twins. These are exact replicas of vessels, down to every nut and bolt, but in digital form.

So, we create a highly complex virtual model that is the exact counterpart (or twin) of a physical thing. Connected sensors on the physical asset collect data that can be mapped onto the virtual model. The digital twin then uses the real data through simulation, machine learning and reasoning to help show what the impacts will be on the real vessel.

They help designers, engineers, operators and seafarers understand not only how vessels are performing, but how they will perform in the future. This allows shipping companies to learn more, and faster. They can also break down old boundaries surrounding innovation, complex lifecycles, compliance and the ways that vessels will make them money.

Cancer — “the crab”. Ah, look at those snappy claws, grabbing at all they can get. It may look cute, even funny — but if they latch onto you, then it’ll be screaming not laughing.

There are corporate crabs out there (keep your mind out of the gutter), they move sideways and never really seem to embrace the need to move forward and fast, but they always grab, grab and want more — even though they aren’t fully committing to the digital transformation.

They don’t have a plan as such, they just hear the buzzwords and think by throwing some data around they will make a positive impression. So, they snap their claws at everything, they try and just snatch at anything and everything. No rhyme or reason, no plan, no grand strategy, just stuff. Which is no way to transform a business, and certainly not in a digital sense.

So when you next look to the stars, you may see the decaying black holes of companies who did not get their digitalisation strategies right, and the flashes of light as the shooting stars of those who did, streak across the sky.",11507
"Running GUI Containers on docker in 5 easy steps

Step 1: Install Docker

To be able to launch a container you need to first have Docker installed. On your host OS (preferably Linux) get Docker installed and then move on to the next step.

Step 2: Pull an image

Download a container image of your choice from Docker Hub. In this blog I am using CentOS image.

To pull the image from Docker Hub type

docker pull centos

docker images

This command will show you the images you have in your local system.

Step 3: Create the container

Method 1:

Launch a container using docker run command

docker run -dit --name centos_gui --env=’Display’ --net=host centos:latest

Here, the run subcommand is used to launch the container, -i to make it interactive, -t to get the terminal, and -d to daemonize it. We use --env to set the environment variable DISPLAY and --net to connect the container to the host network.

Method 2:

Set the DISPLAY environment variable on the host OS with your network:

set-variable -name DISPLAY -value YOUR-IP:0.0

And then finally launch the container using

docker run -dit --env=DISPLAY=$DISPLAY

Here we have manually set the environment variable.

docker ps

This command shows running containers

Step 4: Install GUI application inside the container

Attach the container to go inside the container’s terminal. To do so, type:

docker attach centos_gui

Now you are inside the container’s terminal.

Next, install a GUI application using a package manager. In our case since we have CentOS, we will be using the yum command.

yum install firefox

Step 5: Running the application

As we have already specified the DISPLAY environment variable while launching the container, now all we have to do is just run the app normally.

firefox

In a while, the GUI of the app will show up something like this.

We have successfully launched firefox GUI over a docker container!

Feel free to contact on my LinkedIn.",1937
"CIO Advisor Apac scam news

CIO Advisor apac scam news has been helping enterprise providing a solution against scam and spam in the industry. Nowadays, attackers use advanced techniques to scam organizations to pay them a hefty amount via fake invoicing.

Fremont, CA: Email scammers are becoming sophisticated a day using advanced tactics for stealing from organizations across the planet . Earlier, scammers wont to send an invoice, letter, or invitation to be listed during a bogus trade directory or renew the web site name . Today, however, the attacker’s campaigns revolve around supply chains, espionage, and research. Attackers dupe their victims by injecting themselves into a legitimate email thread about finance. These attacks are difficult to detect, and victims will realize that they need been scammed when their vendor follows up about an unreceived payment.

According to researchers at Agari, email fraud is linked to a cyber-criminal gang operating out of Nigeria. referred to as Silent Starling, the group started in 2015 with romance scams and cheque fraud then later advanced to wire transfer requests and gift card scams. Employing new attacks, the group has duped over 500 companies in 14 countries with the bulk of their victims from the U.S., Canada, and therefore the UK. The group has hacked 700 employee email accounts and stole over 20,000 emails to assist cash-out campaigns successfully.Top Risk and Compliance Solution Companies

The attack begins with the hackers attempting to steal email login credentials using phishing attacks redirecting users to a spoofed version of tools like Office365 and other enterprise software. After gaining the credentials, the attacker’s login and found out a forwarding rule to automatically redirect copies of all the emails to a separate account they control. Further, they inspect the content of the emails to know their victims. Later, email scammers found out alerts for keywords like invoices and payments to collect information like the language employed by the important sender and therefore the times of day they have a tendency to be most active. Further, they gain access to the attachments and links utilized in the e-mail to make a fake invoice that appears completely legitimate.

The invoice requests are precise because the purchasers are going to be expecting an invoice from the seller . the sole detail which is different within the invoice is that the bank details, which redirects the cash to the checking account of the cyber-criminal. These attacks are longer and resource consuming as compared to a daily BEC campaign, but the reward is higher. These attacks are stealthy, and that they can’t be caught. within the meantime, the organizations can cross-check the outgoing payments to guard themselves from these attacks.",2813
"NFTs will certainly give Crypto wings!

Dr. Karl Kobelt, Mayor of the City of Zug welcomed delegates to CV Summit Day 2, which explored the impact of blockchain in creating new business models across culture, art, sport, and society. He said:

“It is exciting to see how strongly the crypto scene has shaped and changed our city in recent years. A crypto bank now in the former town hall, new co-working spaces, adverts for crypto products are now integral to the cityscapes. A globally networked generation find their way to Zug”.

The day began with Bernhard Blaha, CEO, Blocktrade SA who explained his view on how the mass adoption of digital assets affects their quality. Simply put: increased quantity will in turn result in increased quality. Racing into pole position on this issue was Daniel Ritchie, Senior Commercial Manager at Red Bull Racing who enthralled delegates with details of the NFTs that Red Bull Racing will develop, so as to offer engaging experiences to their 25 million worldwide fans.

Bernhard Blaha, CEO of Blocktrade SA on stage at the CV Summit Leadership Circles

Jennifer Arman of Inacta then moderated a very fast-paced panel of global leaders on the subject of NFTs & Sports. Key discussion points included:

For all sports verticals, fan engagement is the biggest challenge and NFTs offer an amazing solution

Stakeholders including sports federations need to embrace the opportunity that blockchain and NFTs present

Ease of access is holding people back, platforms need high UX — expect to see more adoption once UX is sorted

The main challenge for issuers is to understand who owns what/IP rights

Each sports area will likely evolve its own token economy

The real value of blockchain technology and NFTs is the ability to transfer fan value back to the athlete directly

In all sports, the fan wants to get closer to the athlete — blockchain provides this access — it’s really exciting

Everyone concurred that NFTs have the potential to be what we have longed crypto to be: a venue that will create mass adoption

A bubble? Yes & No!– NFTs are here to stay. Every tech frontier is a bubble, and there is nothing wrong with a bubble — great things come from it

Other Challenges? Manage expectations of new entrants and regulatory questions such as: is an NFT a security or not?

The Future? The metaverse, cross-functionality of blockchain, and augmented reality offerings, such as in sport (gameplay) experiences

The Panel included Oliver Hughes, CMO, Red Bull Racing, Hubertus Thonhauser, Chairman at Tezos, Toli Makris, CEO, EX Sport, and Niclas Genovese, Tokengate.

Panelists Jennifer Arman, Oliver Hughes, Hubertus Thonhauser, Toli Makris, Niclas Genovese on stage

Next up — The Tech to support

Joe Petrowski, Technical Integrations Lead at Web3 Foundation introduced the audience to some hardcore tech — Multi-chain Web3 Apps. His presentation was a deep dive that demonstrated how hard the scientists work behind the scenes to ensure success. We got an exploration of how cross-chain applications will require new approaches to development and infrastructure. We learned that we need to reduce the bottlenecks of shared systems — some great light was shone on upcoming solutions, such as parachains.

Joe Petrowski on stage at the CV Summit Leadership Circles

The human factor in blockchain as a global megatrend

Sam Katiela, CEO & Founder, MAMEMO Consulting delivered an inspirational and evocative speech which concluded with him asking the audience to act like a symphony of co-creators to come together and show the positive expectations and effect that blockchain has and will have in the future. He reinforced the point that blockchain is a positive megatrend — what we are doing is changing the world. He encouraged delegates to be aware that the world is watching and waiting but that the blockchain sector must reach out to pick up the rest of the world to come with us — & do so by creating positive stories about our blockchain world and create ambassadors to tell those stories.

Blockchain on the African Continent

Gideon Greaves, MD Africa CVVC inspired with his update on the determination and entrepreneurial spirit which he encounters across Africa. This was echoed by Gisela Roth, Program Manager, State Secretariat for Economic Affairs (SECO) who expressed Africa’s positive interest in all things that are progressive and support enterprise. Whilst largely unbanked, the prevalence of smartphones is dramatic and the continent is hungry for efficient solutions. South Africa, Ghana, Nigeria, Congo, and Kenya were mentioned as evolving hotbeds of innovation and determination to embrace blockchain solutions for the continent’s challenges.

Rico Pang, Managing Partner, Sanctum Ventures spoke passionately about initiatives to encourage education, especially coding, for Africa’s tech-savvy youth population. Amid the challenges faced, the varying regulations of the many Governments were mentioned. Gideon Greaves, MD Africa CVVC added that having “Switzerland” aided trust-building with stakeholders — “what better partner to have”.

Panelists Tracy Trachsler, Gideon Greaves, Gisela Roth, and Rico Pang on stage

Revolution in the world of art

Kenny Schachter, Artist/ Collector opened the afternoon session with a mind-blowing evangelical presentation on art NFTs. He stipulated that they will go way beyond money, as they have accelerated the art world by at least 20 years. Delegates loved his energy and his sometimes anarchistic notes such as “There’s something punk about creating a whole new monetary system in the artworld” and “I’m more interested in blowing up the bridges between traditional and digital art worlds”!!

Kenny then joined the panel Evolution of the Art Market in the Digital Age, moderated by Tanya König, Swizz Art Bizz who suggested that the panel theme ought to be about revolution. Panel members were: Dino Lewkowicz, Director, 4ART Technologies, Sarah Schlagenhauf, Founder & CEO ArtDeal AG, Roger Darin, Head Digital Finance, Inacta, and Jutta Nixdorf, Head of Swiss Department at CHRISTIE’s.

Key takeaways included:

Art has always suffered from access, digitization is allowing access for all

Art is part of the human condition and digital art is an expansion and expression of that

The fact that 22 million people watched the Christie’s Beeple auction is testament to this

Auction houses are more nimble than galleries in embracing the digital age and with that comes trust

Disruption of the old art world is coming from outside the art world

NFTs have both artistic and financial interests

It is now a 24/7 market

Storage: whilst much physical art sits storage, some even in ports, digital art can move anywhere with you, and with parallel tech advances NFTs can also be displayed in your home

As with the traditional art sector, there was a view that natural selection will take place in digital art markets

Art is part of the human condition and digital art is an expansion and expression of that

“NFT Art is an exciting journey, projects are amazing, we are just at the beginning” — Jutta/Christies

Panelists Tanya König, Jutta Nixdorf, Kenny Schachter, Sarah Schlagenhauf, Dino Lewkowicz, and Roger Darin on stage

Revolution in the music industry

Markku Mäkeläinen, Chief Executive Officer at Utopia Music gave us a peek at another Revolution — this one is taking place in the music industry.

Utopia is spearheading a revolution in the music industry by enabling fair music monetization and data transfer for all creators. “Fair pay for every play”. Markku propositioned that the Music industry, which is valued at circa $60bn, could be 3 times that if the industry could close the data gap. That’s one of the many things that blockchain can enable for the music industry: help close the data gap.

He was subsequently joined by enigmatic Sam Katiela, CEO & Founder, Mamemo. During their fireside chat, Markku told of his work to marry industry knowledge, data understanding, military-grade security, Swiss-clock-like precision, and sheer computing power in a way required to solve the data gap.

The data tsunami drowning the music industry and leaving creators and publishers without precise income flows can be solved using the power of blockchain. Resulting in efficient data flow and income distribution to both creators and publishers.

Attendees networking during the afternoon break

Transforming Humanity and Economy with Social Tech

Themes of globalization and consciousness were evident in the evening session titled — Transforming Humanity and Economy with Social Tech.

Stefan Kanalga, Founder & CEO of yeswetrust moderated and was joined by Michael Gord, Co-Founder & CEO at GDA Capital, Simon Telian, Managing Director Advanced Blockchain AG, Jan Bruch, CEO Neumorgen, Alan Laubsch, CEO at Generation Blue, and Shirly Valge, COO, Velas Network.

Stefan's own project, known as “YesWeTrust”, symbolized all that the panel discussed. Using blockchain to be the enabler of change we wish to see in the world. Some of the very Zen-like takeaways included:

Our time to create generational change is here

Unify humanity

Move toward conscious social media, where you earn a token when you give a like

Instead of extracting value, use decentralization to receive value for sharing/liking

Tech should be intuitive and humanity should lead tech

We need to rise as a global community and begin radical sharing — pretty much bring what the blockchain community is doing — transparency, sharing and common good — to everyday worlds

Go with the collective flow and whatever you do — do not say … “but, no”, instead SAY “Yes, And…”

Closing out the day

The day concluded with a true reflection on what saying ‘Yes’ can do as The impact of Bitcoin on El Salvador was discussed by Dunstan Teo, Co-Founder & President, SANCTUM and Carlos Fernandez, Finka.Swiss.

El Salvador’s adoption of Bitcoin as parallel legal tender will cut the cost of remittances, an important source of income for millions of its people.

Bitcoin offers, in theory, a quick and cheap way to send money across borders without relying on traditional channels.

Laws are already enacted to ensure success and in effect possibly help to restabilize a collapsing economy. This may be an experiment but it is one to watch with an open mind for other countries.",10371
"What is an ‘Immersive Experience’?

Ready Player One, a 2018 Hollywood movie, showed the real virtual experience that will be achieved in the future. The movie had Oasis — a real-world simulation of all the tangible human experiences like socializing, going to work, recreation, etc. The alternate realities generated by virtual reality(VR) hardware are enough to transport you to a different world. The immersion created engages you to the core and you could hardly get over it.

Immersion engages all your senses to give you a life-like experience and improves almost all our interactions on an everyday basis. The main motive behind it is to make you perceive another reality while physically being in an alternate one. The goal is to make you completely aware of the alternate world, similar to what you perceive in a movie theater when the lights go out.

There are basically 3 elements of immersion:

These elements help suspend reality and make the essence of immersion. An immersive experience is a by-product of how and in what amounts the above three elements are amalgamated.

An immersive experience in simple words is the perception of an enhanced version of your reality particularly more satisfying or engaging via technology. Different technologies can be linked together to create such experiences.

Everyone should have access to these technologies to make a difference. All businesses from multinational corporations to local businesses must have access to this tech to make immersive experiences for their customers. They especially help you to market your products better. Only a few blogs earlier we discussed immersive advertising. In this blog, we will explain what immersive experiences are and how to create them easily.

Various Kinds of Immersive Technologies

AR, VR, and MR are here to stay and revolutionize the way humans interact. These trends are what collectively form immersive experiences. With the internet generation, it is difficult to focus on so many things because there is new content being generated so frequently. People easily forget once-in-a-lifetime experiences. Immersive experiences transform a monotonous event into a delightful and mesmerizing takeaway. The goal of an immersive experience is to engage people so that they can explore it with multiple senses simultaneously.

VR shuts you out from the rest of the world and your entire reality is transported to the content you are viewing. AR on the other hand tries to overlay objects in your immediate reality and tries to make them more noticeable. Now, MR is an amalgam of AR and VR so the user is not shut out completely from the rest of the world and also sees objects overlaid in his immediate reality. The immersive technologies are helping people to bridge the gap between the physical and the digital world. Businesses are taking advantage of these technologies and trying to create an everlasting impact on the customers.

The immersive experience can be very easily managed by clubbing any normal event with technological elements. Technology can be a powerful tool put to the disposal of marketers of any brand.

How are these technologies changing the future for the better?

The immersive technologies are being used in virtual travel, safety training, improving communications, and for various other activities. These technologies have made life simpler for both the customers and the businesses alike.

Virtual reality coupled with 3d modeling is rapidly being adopted by many industries to ease their workflow. Most people think that VR, AR, and MR are used to create more interesting games but these technologies are changing the future of how humans work and interact with machines. Some examples of how these technologies are changing the way we work are:

Creating Immersive Experiences

Everyone is hustling to create immersive experiences to remain at the forefront of technological advancements. Let’s face it, hiring tech teams to build such experiences is a hassle as it takes a lot of time and investment. Also, the time-to-market of these web and mobile applications is excessive. It can take months to pull out a single immersive experience worthy to be presented to your customers. So, everything goes to waste, your manpower as well as the capital.

Due to this, people have started using Saas tools like Vossle — a no-code platform to create browser-based AR immersive experiences(WebAR experiences). Creating 3d WebAR experiences with Vossle is a very easy task and anyone can do it within a minute. Moreover, the cost entailed during the process is way less than what is incurred with a full-fledged team of people.

You can do easily come up with your innovative WebAR experience in three easy steps:

Create a free account. Upload content on Vossle creator studio. Publish and share the Vossle link with your audience.

Once the link goes out and people click on it, they will get to experience an immersive browser-based AR or WebAR experience. The introduction of WebAR has created new ways of customer engagement. Businesses are using AR to create a buzz around their new products.

Conclusion

As we see many advancements in the field of AR, VR, and MR, everyone is rushing to create immersive experiences for their consumers. With tools like Vossle, the creation of immersive experiences is becoming easier than it used to be a few years ago.

Moreover, the immersive experience market is only going to expand with the number of mobile phone users increasing too. One can see AR experiences on their mobile browsers and this has helped brands use creative marketing campaigns to make an impact. We are only starting to explore the huge potential tools like Vossle have to change our lives for the better.",5751
"Virtual reality is now a reality. VR is the next big thing in gaming. And it’s not just about playing games. We can now explore the world without leaving our living room. VR will change the way we learn, the way we date, the way we navigate the web and the way we play Games . I’m not necessarily ready for the future, but I’m excited to see the revolution that will change the normal boring games .

Post Originally Published At : Virtual Reality Games of the Future: Revolutionary Technologies That Will Change Gaming Forever | Gamers (vocal.media)

Virtual Reality Games of the Future: Revolutionary Technologies That Will Change Gaming Forever

What is Virtual Reality?

Virtual reality, or VR, is the immersive, 3D simulation of a real environment or a virtual world. The user is completely immersed in the game or experience. The idea was born from the paper “Virtual Reality: History and Architecture”. But VR wasn’t used for gaming purposes until Sony released the Playstation 1 in 1995. The device was an early attempt at a virtual reality headset. The Playstation 1 was called the VR headset, because of the design’s two screens. One of the two screens shows the user in real life, while the other displays a distorted view of the virtual world. Over the years, we’ve seen advancements in the equipment used to create the illusion of virtual reality. Currently, there are three main platforms: Oculus Rift , HTC Vive and Samsung Gear VR .

The future of VR

Some people are still not convinced that virtual reality will be more than just a fad. Some VR fanatics even prefer using their old classic gaming systems that require big setups. There are still too many unknowns. What if games stop to be fun? What if games stop being amazing? We will soon know for sure. Virtual Reality will either become the next big thing in gaming or it will fade away. Virtual reality already has the potential to revolutionize gaming, but it’s not about going into the future. I’m here to help you look into the future, and maybe a few steps ahead. These are the next big technological breakthroughs in gaming that are sure to change the way we play our favorite games. As they say, you never know what could be coming next.

How VR changes the gaming ?

Touch And Be Invisible : VR Gaming By Oniro Vision Oniro Vision makes great products. And VR is one of them. You see, Oniro Vision is a company that makes goggles, which come with a 360 degree panoramic view camera built into the strap. Basically, you can look around in the real world without removing your goggles. This technology will enable unprecedented immersion in an AR Environment and any other type of VR experience. So if you’re in an escape room where you have to solve a puzzle, you can look around, point your gun at the puzzle, and you can actually shoot enemies without removing your headset. If you’re in a first person shooter game, you can look around and destroy virtual enemies while looking in the real world. With some fancy technology, VR will go beyond gaming .

Virtual Reality Gaming : The Future of Gaming

This is the evolution of gaming from now until 2029. In 2015, games will be dominated by Virtual Reality . Here’s why: The way we learn In the future, VR will make it possible to share virtual worlds with students, enabling them to have immersive learning experiences. Many universities already have virtual reality labs that let students explore their own city, complete virtual tasks and discover new places. Virtual Reality Games will take this even further, by allowing students to experience things they couldn’t physically see, like exploring Mars. The way we date In the future, virtual reality will make finding a partner, or even a spouse, very easy. Just go to a VR game, click on someone who looks right for you and meet them in the game.

Gaming in 3D with VR

Let’s do something different. Let’s play games in 3D. As you’re reading this article, I’m in a VR game — I’m not messing around. You can see me in the video above. So, today I’m going to talk about the history of VR, why this is revolutionary and what the future looks like for gaming. So, why is VR revolutionary? You can turn your head to see 3D, but now, you can move your head around and change your perspective. Have you ever wanted to explore a new world? When you were a kid, you would open up a new book, and you were stuck in the middle of that chapter. If you were lucky, you’d find another good place to read, but now, you can explore that world in VR. What if you could move your head to see yourself in the future, and see the faces of your friends and family?

Conclusion : The future looks great! Virtual Reality is here. Let’s make it great! Get ready to Enjoy the next level Gaming in 3D VR .

Post Originally Published At : Virtual Reality Games of the Future: Revolutionary Technologies That Will Change Gaming Forever | Gamers (vocal.media)",4916
"12 Essential Enhanced Video Texting FAQs

Enhanced video texting is the biggest innovation in P2P texting since P2P itself. Read more here: Lauren Datres Follow Jun 8 · 6 min read

Our team has always been committed to providing the best possible tools in P2P texting for our clients, and we have delivered on this commitment with our innovative Enhanced Video Texting made possible by our industry-leading video compression technology.

Want to know more about this revolutionary innovation in texting and how it will enhance your outreach? Here are 12 of the top questions asked about Enhanced Video Texting to help you get started:

1) What is Enhanced Video Texting?

If you have ever tried to send a video to a friend who has a different phone platform from you, Apple to Andriod for example, you will have seen how much the video quality declined and how the audio quality distorted. That is the primary challenge behind video texting at scale, and one that our team solved with Enhanced Video Texting.

Enhanced Video Messaging is a service we provide from our industry-leading video compression technology that enables you to deliver high quality, 30–60+ second videos embedded directly in your text messages. This means your audience won’t have to click a link to view your video elsewhere. We also offer custom closed captions to your videos so they are watchable even on phones set to silent mode.

2) Why don’t I get the same results when using other video compression tools?

Off-the-shelf compression tools use basic algorithms designed with one goal in mind: reducing the file size, with no regard for audio/visual quality or even the platform on which the final video will appear. Since most of these free compression tools are catered to reducing files to sizes appropriate for social media use, they rarely accommodate file reduction to textable sizes, requiring users to run several compressions on the video which results in iterative quality loss.

By contrast, our tool is designed with texting in mind. By examining individual video qualities, engineered specifically to meet mobile carrier requirements, we are able to deliver the highest quality videos at textable sizes.

3) What is the difference between Enhanced Video Texting and standard MMS?

Standard MMS are messages with a multimedia attachment such as a picture, GIF, or video sent on their own or embedded with a message. Enhanced Video Texting allows you to send much longer videos to multiple phone platforms without sacrificing quality, which is something Standard MMS videos are normally not capable of and other texting platforms have not been able to achieve.

4) Why is it more effective to use Enhanced Video Texting over standard SMS or MMS?

Sending a quality video directly to people’s phones produces real results. For example, people retain 95% of a message when they watch it in a video compared to 10% when reading it in a text. Additionally, messages containing videos have a 50% higher response rate than plain SMS messages.

In addition, a staggering 92% of mobile video viewers share videos with others, which means taking the time to send a video to your audience will most likely result in your message being shared with others at no cost to you. Pictures and GIFs do not have this same level of share-ability, and low-quality videos will likely not even be opened or watched all the way through, much less shared.

5) Why should I send an Enhanced Video Text instead of including a link to a video?

Embedding a video is far better than sending a link to a Youtube video. People are hesitant to click on external links from texts because of warnings from government bodies like the FTC who caution against clicking links sent in unknown texts, and also experiences with email phishing scams. Including a link also means that there will most likely not be a preview image, which is one of the major driving forces behind consumers deciding to watch a video.

Another benefit to our EMMS technology is that our clients can choose the thumbnail for videos sent through us. This is significant because 90% of the top-performing videos on YouTube use a custom thumbnail, and this is just not possible to do when sending a link to a video instead.

6) What carriers let you send Enhanced Video Texts?

Our Enhanced Video Testing has been rigorously tested and optimized to retain maximum video and audio quality on all major phone carriers, including Verison, AT&T, and T-Mobile.

7) Why are video captions important?

Hearing impairments affect more Americans than people realize. According to Johns Hopkins Cochlear Center for Hearing and Public Health, approximately 38.2 million Americans, or 14.3%, report some degree of hearing impairment or loss (source).

Our Enhanced Video Texts include captions that are custom mapped to match the voiceover so the videos are watchable for everyone. According to a consumer study by Verizon, “50% of consumers said captions are important because they watch videos with the sound off” and “80% of consumers are more likely to watch an entire video when captions are available” (source).

8) How long can the Enhanced Videos be?

In general, the best results can be achieved with videos that are 30 seconds, high quality results can be achieved with videos that are 31–45 seconds, and good results can be achieved with videos anywhere from 46–80 seconds.

All videos are unique, and we look into the parameters and other specifications within each video to produce the best possible results when compressing them.

9) Why should I choose RumbleUp’s Enhanced Video Texting over other video texting apps and services?

Simply put, other video SMS platforms, video texting apps, and services that offer regular MMS services including MMS video messaging do not come close to our Enhanced Video quality or delivery rates. Additionally, our Enhanced Video Texts can be embedded directly in full-length 320 character messages so you can make sure the message you want to get across to your audience is received correctly.

10) Why is there a surcharge for the service?

High quality video compression is a time-intensive, manual process. Automated compression services do not take into account the individual characteristics of your video, resulting in poor and often unwatchable quality. Because of this extra work and load on our system, there is a 6 cent service charge per text over standard MMS.

11) How do I submit my request?

Due to the time-intensive nature of the compression process done by real people (not algorithms!), we have the following minimum requirements for requesting video compression service:

Minimum contact segment size of 25,000

You can find the actual size of your contact segment by clicking into the contact segment and looking at Good contacts.

Maximum video length of 30 seconds

Because video compression quality begins to rapidly decline with each second after 30 seconds, we are currently limiting videos to 30 seconds in length. We have resources to trim your video that are accessible on the Enhanced Video Texting page in the help tab of the RumbleUp portal.

Non-refundable invoice for the estimated cost of the contact segment

Because of the time commitment our team makes to produce the highest quality videos, we require invoices to be paid prior to service.

If you meet these requirements, then simply navigate to the Enhanced Video Texting page in the Switchboard portal, then click the “Submit Your Request Here Button” to send in a request submission.

12) How have other people used Enhanced Video Texting?

There are many ways our clients have used Enhanced Video texting, including campaign & promotional videos, video announcements, fundraising videos, thank you videos, and more.

Here are some real examples of Enhanced Videos sent by clients just like you.

This client texted for many congressional candidates across the country, and they sent 30sec Enhanced Videos with all their messages speaking against their client’s opponents in a light-hearted manner.

This client texted out a call-to-action Enhanced Video asking people to call their senator to ask about a controversial new bill.

This client texted out an informative Enhanced Video asking people to support their initiative to reduce plastic pollution in Texas.

Like what you see?

Sign up and start texting today at rumbleup.com or reach out to our sales team to talk about how Enhanced Video Texting can best work for you: sales@rumbleup.com.",8506
"How Teams Can Raise Up IT Service Delivery Post-COVID

Before the coronavirus pandemic, employees appreciated their IT colleagues. When COVID-19 struck, however, more workers than ever before recognized that without technologically savvy team members, their workflows would stumble, falter, or even fail. Why? Because we all immediately (and more than ever before) came to rely on technology in business. ReadWrite Jul 6·5 min read

Are you one of those team members who didn’t think about your IT coworkers every day?

Needless to say, just about every company has realized the value and impact of the IT workers at this point. After all, as revealed in a late 2020 McKinsey survey, executives acknowledge that COVID-19 has sped up their adoption of digitalization considerably. Some posit that nearly seven years of technological evolution happened in the span of a few months. Perhaps the strongest evidence of that was within cybersecurity in IT.

Increased Security in the Age of COVID-19

When massive numbers of employees began telecommuting — at home — during COVID-19, many corporate teams met unparalleled security challenges in IT-related procedures. Not surprisingly, hackers tried to take advantage of the situation, resulting in an 800% increase in reported ransomware attacks, according to MonsterCloud figures.

Teams Can Raise Up IT Service Delivery Post-COVID

In response, IT teams went into overdrive to enforce existing cybersecurity policies and create new ones.

At the same time, IT personnel knew workers needed access to their systems from anywhere. So in addition to figuring out how to make it possible for employees to safely access servers — IT teams had to investigate other security-related solutions.

Plenty of businesses switched from traditional server-based systems to software-as-a-service tools, as an example. Ultimately, moving to the cloud solved accessibility problems for many and helped end-users see the importance of on-the-ball IT teams.

Maintaining the Momentum of IT Heroes

With people moving back to offices and vaccine deployment is on the rise, society has reached a technology plateau. However, it’s only a temporary respite.

Current Technology Demand

The next technology demand will come soon enough because technology in business will continue to move forward at lightning speed.

Current technology demand means that it’s more important than ever for teams to refine their IT service delivery protocols and tools.

If your business weathered the shockwaves of 2020 thanks to technological pivots, now isn’t the time to slow your progress. Instead, lean into IT by continuing to optimize the technological changes that happened during the past year. Here’s where to start:

1. Close remaining gaps.

IT teams are much more prepared to rely on modern technology and serve up advanced cybersecurity measures. Nevertheless, cybercriminals have kept pace, too. Therefore, security will continue to be of critical importance. Remember: When your company zigs, hackers always zag.

Protect your corporate systems first

Many hackers are using social engineering to help break into corporate systems. Ensure you’re ready to combat hackers via a multipronged approach supported by a deep understanding of modern security and privacy trends.

Invest in building your tech stack and educating your IT personnel. Make sure you prioritize the need for cybersecurity measures and allow your IT department to educate the rest of your staff on ways they can operate more safely online.

Many end-users aren’t well-versed in IT security, especially on portals they assume are inherently safe (such as video conferencing software or social platforms). Consequently, they might revert to bad habits that expose your systems, data, and client information.

Now is the time to create new tech use policies that will keep everyone safer and make your security processes more concrete.

2. Stay flexible.

Employees discovered a whole new world of workplace flexibility during the pandemic. To many, working in the office no longer holds the same appeal or necessity. Instead, employees want to be able to work where they’d like using the devices and technology they’re most comfortable with.

Your biggest challenge might be giving workers what they want and need to increase their productivity without risking security or performance.

Solving this challenge requires understanding what remote working team members need on an individual and group basis. Then, plan to sit down with them and figure out the tools that can help them show they’re hitting goals and deliverables, not to mention giving customers the strongest services.

If you decide to integrate new tools into your tech stack, make it your first priority to train personnel on using those tools correctly and securely.

3. Push mobility.

You probably don’t need a survey to determine what makes mobile or remote work hard on employees, although it wouldn’t hurt to canvass your staff.

For example, expecting employees to work from home on personal devices such as tablets or laptops can lead to connection and integration malfunctions and hiccups. The answer to this type of problem could be buying corporate-owned and-managed equipment for all mobile employees.

Not all revisions and upgrades are as costly as ordering a bevy of new computers.

Some team members might only require that you pay for subscriptions to cloud-based tools that will help employees stay tethered and effective on the go. Your overall goal should be one of helping employees maintain their efficiency whether they’re down the street or around the world.

Additional Thoughts

IT was critical to keeping businesses afloat during the height of the pandemic. Now, it’s time to talk about how to improve your IT functions and procedures so you can continue to provide world-class service and solutions to both internal and external stakeholders.

You will want your processes and training to be as seamless as you can make them after the trauma of the last year. You may wish to enlist your IT team in making the training process as painless as possible for other team members. Prepare now, hold your pieces of training now — and you won’t have the added pressure that comes in emergency situations.

How Teams Can Raise Up IT Service Delivery Post-COVID was originally published on ReadWrite by Shawn Freeman.",6391
"I n the present business environment connecting with an IT company can be the greatest thing you can do for your business. Technological innovations are reshaping the world we live in. These companies offer considerable services to meet your end customers’ needs. With the increase in the use of technology, it becomes essential to have an IT team that can operate all aspects of a business. This article will go further in-depth about IT companies.

IT spending in India is projected to increase by 6% to reach the US $81.9 billion in 2021.

What is an IT company? And what exactly does an IT company offer? Do you have these questions in your mind? If yes, let’s get started by understanding these terms.

Gradually, as we are moving in a new decade, Information Technology is becoming a greater part of our life, and numerous organizations out there are relying upon these IT companies.

Anyway…… Let’s understand it simply- An IT company is a technology-based company, also referred to as an IT managed service provider for big companies who want to outsource various services for a smooth workflow in the company. Or we can say- it provides technology-related services like Hardware or Software management, Web development, Business Intelligence, Firewall services, and many more……. And it comprises professional IT specialists who are highly specialized in their field with special skills. An IT Team provides proper guidance, direction, and management on different applications.

According to CompTIA, there are more than 525,000 software, and IT services companies in the United States alone.

Well, advanced technology expansion like- Cloud computing, Artificial intelligence, Robotics, IoT, Machine learning would continue to change every aspect of human life.

However, many are not aware of the types of IT companies especially, students. You must know the difference between them while choosing a career. Not sure? Well, let’s take a look at how IT companies are classified-

Service-based companies

These companies do not provide any product or services; they basically work for other clients. As we all well know, the names of these IT companies in India. If not, so here are few tech giants for your reference:

Tata Consultancy service – Largest IT company in India.

Infosys

Wipro Limited

HCL Technologies

Tech Mahindra Ltd etc.

2. Product-based companies

These companies work on their own product and deliver the product to end-users. Some product-based companies are-

Apple

Microsoft

Google

Facebook

Intel, etc.

You got the picture, right? After reading this article, you must have got an idea about IT companies, what they do, their types. So if you are a student moving your career in the IT space and passionate to learn more about the latest technologies, good at soft- skills, it will be a great choice to go with IT companies.",2864
"I would like to share some of my knowledge in Spark Java in this article. I will carry my article in a way of answering the following questions What, Why, Where,When and How ? With some code examples. I will cover the rest of the things in upcoming articles.

Let’s see the questions that needs to be answered.

1.What is Spark Java?

2.What are the advantages of using SparkJava?

3.Why it is Used ?

4.How to Use It?

5.Example codes and Advantages

What is Spark Java?

Spark is a simple and lightweight Java web framework built for rapid development.Spark focuses on being as simple and straight-forward as possible, without the need for cumbersome (XML) configuration, to enable very fast web application development in pure Java with minimal effort. It’s a totally different paradigm when compared to the overuse of annotations for accomplishing pretty trivial stuff seen in other web frameworks, for example, JAX-RS implementations.

What are the advantages of using SparkJava?

It is an alternative to other Java web application frameworks such as JAX-RS,Play Framework and Spring MVC.

It runs on an embedded Jetty web server by default, but can be configured to run on other web servers.

It does not follow the Model — View — Control pattern used in other frameworks, such as Spring MVC.

Spark is intended for “quickly creating web-applications in Java with minimal effort.”.

Why it is used?

Java developer with neither the urge nor time to learn a new programming language, and we’re not planning to build a super large web application that scales in all directions, then Spark might be a great web framework to work with. It will have us up and running in minutes, and we don’t wan’t to think too much about configuration and boilerplate code (like you often have to in other Java web frameworks, like Spring or Vaadin).

How to Use It?

We can configure Spark java in a project by adding maven dependencies in our POM file like this

Add the maven dependency:

<dependency> <groupId>com.sparkjava</groupId> <artifactId>spark-core</artifactId> <version>2.1</version> </dependency>

Sample code to start with…

Example Code :

import static spark.Spark.*; public class HelloWorld {

public static void main(String[] args) {

get(""/hello"", (req, res) -> ""Hello World"");

}

}

To view the result ….

Try this on your browser :

http://localhost:4567/hello

* Spark Java by default uses the port 4567.

How to change the port?

If we want to set another port use setPort. This has to be done before using routes and filters:

setPort(9090);

Now the Spark will run on port 9090

Routes

The main building block of a Spark application is a set of routes. A route is made up of three simple pieces:

A verb(get, post, put, delete, head, trace, connect, options)

A path(/hello, /users/:name)

A callback(request, response) -> { }

Routes are matched in the order they are defined. The first route that matches the request is invoked

Route method’s example codes :

get(""/"", (request, response) -> {

// .. Show something ..

}); post(""/"", (request, response) -> {

// .. Create something ..

}); put(""/"", (request, response) -> {

// .. Update something ..

}); delete(""/"", (request, response) -> {

// .. annihilate something ..

}); options(""/"", (request, response) -> {

// .. appease something ..

});

Route method with Param example code : get(""/hello/:name"", (request, response) -> {

return ""Hello: "" + request.params("":name"");

});

Filters

We can add two different filters in Spark Java as Before and After filters.

Before filters are evaluated before each request and can read the request and read/modify the response.

The example code for before filter is

before((request, response) -> {

boolean authenticated;

// ... check if authenticated

if (!authenticated) {

halt(401, ""You are not welcome here"");

}

});

After filters are evaluated after each request and can read the request and read/modify the response:

The example code for after filter is

after((request, response) -> {

response.header(""foo"", ""set by after filter"");

});

Redirect :

We can trigger a browser redirect with the redirect helper method:

response.redirect(“/bar”);

We can also trigger a browser redirect with specific http 3XX status code:

response.redirect(“/bar”, 301); // moved permanently

Halting the server:

To immediately stop a request within a filter or route use:

halt();

We can also specify the status when halting:

halt(401);

Or the body:

halt(“This is the body”);

…or both:

halt(401, “Go away!”);

Stopping the Server:

By calling the stop() method the server is stopped and all routes are cleared.

Read similar articles from Keleno

Follow us on Linkedin

Visit Keleno | Facebook | Twitter",4725
"Explainable AI for text summarization of legal documents

Nina Hristozova, Data Scientist, and Milda Norkute, Senior Designer, Thomson Reuters, discuss explainable AI for text summarization of legal documents and tailoring how to explain your model based on the needs of your user Top Business Tech 3 days ago·8 min read

Nina Hristozova, Data Scientist, and Milda Norkute, Senior Designer, Thomson Reuters, discuss explainable AI for text summarization of legal documents and tailoring how to explain your model based on the needs of your user. Explainable AI

Explainable Artificial Intelligence (XAI) is an umbrella term for a range of techniques, algorithms, and methods, which accompany outputs from Artificial Intelligence (AI) systems with explanations. It addresses the often undesired black-box nature of many AI systems, and subsequently allows users to understand, trust, and make informed decisions when using AI solutions [1].

Background

The rapidly growing adoption of AI technologies using opaque deep neural networks has prompted both academic and public interest in explainability. This issue appears in popular press, industry practices, regulations, as well as many recent papers published in AI and related disciplines.

Figure 1 below shows the evolution of the number of total publications whose title, abstract or keywords refer to the field of XAI during the last years. Data was retrieved from Scopus in 2019. We can see that the need for interpretable AI models grew over time, yet it has not been until 2017 when the interest in techniques to explain AI models (in green) has permeated throughout the research community [1]. It is possible one of the reasons for this was the introduction of “GDPR”, also known as the “General Data Protection Regulation” introduced in 2016 in the European Union (EU), which includes a “right to explanation”.

[Figure 1: Evolution of the number of publications referring to XAI. Illustration from: https://doi.org/10.1016/j.inffus.2019.12.012 ]

In Figure 1 we also see a gradually growing interest in interpretability. Explainability should not be confused with interpretability. The latter is about the extent to which one is able to predict what is going to happen, given a change in input or algorithmic parameters. It is about one’s ability to discern the mechanics without necessarily knowing why. Meanwhile, explainability is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms [2]. However, the difference is subtle. We will stay focused on explainability for the rest of this article, but to learn more about interpretability see this resource.

XAI for whom?

The purpose of explainability in AI models can vary greatly based on the audience. Generally, five main audience types can be identified: domain experts and users of the model, interacting with its outputs directly, users affected by model’s decisions, regulatory entities, creators of the model — data scientists, product owners and others, managers and executive board members [1]. See Figure 2 below to learn more about the different explainability needs of some of these audiences.

[Figure 2: Explanation for Whom?]

For example, the purpose of having explainability for the users of the model is to trust the model, while users affected by model decisions could benefit from explainability by understanding their situation better, verify whether the decisions were fair. Since these audiences have different goals, this means that an explanation that may be considered as good by one type of audience may not be sufficient for another.

Model specific and model agnostic XAI

Generally, it can be said that there are two main approaches to developing interpretable models. One approach is to create simple, clear models instead of black-box systems. for instance from a decision tree you can easily extract decision rules. However, this is not always possible and sometimes more complex AI models are needed. Thus, the second approach is to provide post-hoc explanations for more complex or even entirely black-box models. The latter approach typically uses model-agnostic explainability methods which can be used for any machine learning model, from support vector machines to neural networks [3]. Model agnostic methods currently available include Partial Dependence Plots (PDPs), Individual Conditional Expectation (ICE) plots, global surrogate models, Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) [4,5].

For example, LIME helps to make the model’s predictions individually comprehensible. The method explains the classifier for a specific single instance. It manipulates the input data and creates a series of artificial data containing only a part of the original attributes. In the case of text data, different versions of the original text are created, in which a certain number of different, randomly selected words are removed. This new artificial data is then classified into different categories. Thus, through the absence or presence of specific keywords, we can see their influence on the classification of the selected text. In principle, the LIME method is compatible with many different classifiers and can be used with text, image and tabular data. It is possible to apply the same pattern to image classification, where the artificial data does not contain a part of the original words, but image sections (pixels) of an image [5].

[Figure 3. Left: Probabilities of the prediction. We expect the classifier to predict atheism. Middle and right: words that influenced the classifier’s prediction are highlighted. Image taken from: https://github.com/marcotcr/lime]

Attention

Some black box models are not so black box anymore. In 2016, came out the first article that introduced a mechanism to allow a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word [6]. Then in 2017 followed the article Attention is All You Need [7], which introduced the attention mechanism and transformer models. There could be different ways to classify attention types, but two main ones are additive and dot product attention [8].

In general terms, the attention mechanism draws dependencies between input and output [9]. In traditional Deep Learning models (LSTMs, RNNs) the longer the input the harder for the model to retain relevant information from the past steps. That’s why we want to signal to the model what it should focus on and pay more attention to (while generating each output token at the decoder). In transformer models this problem does not exist because they use self-attention [10] throughout — every encoder and decoder layer have attention.

Models with the attention mechanism are currently dominating the leadership boards for abstractive summarization tasks [11, 12]. Attention is not only useful to improve the model performance, but it also helps us explain to the end-users of the AI system where (in the source text) the model paid attention to [13]. This is exactly what we did for one of our internal products to add even more value to the augmentation of the editorial workflow.

[Figure 4: Aggregated attention score per word in the source text.]

For this specific use case, we trained a Deep Learning model to generate a summary based on a source text. Per predicted token (as part of the summary), we obtain a distribution of attention scores over the tokens in the source text. We aggregated the attention vectors into an attention score per word in the source text, smoothed and normalized those values. We thus ended up with an attention score between 0 to 1 for each word in the source text (Figure 4 above), which we display to the end-users via text highlighting. The larger the attention score, the darker the text highlighting, and the more importance the model put on the respective word when generating the summary as shown in Figure 5 below [14].

[Figure 5: On the left, an internal app that mocks up the display of the attention scores to the end-user. On the right, a different, model agnostic explainability method is used to show where the summary had originated from in the source text [14].]

Often many of the explanations rely on researchers’ intuition of what constitutes a ‘good’ explanation. This is problematic because AI explanations are often demanded by lay users, who may not have a deep technical understanding of AI, but hold preconceptions of what constitutes useful explanations for decisions made in a familiar domain.

We believe and propose that when you are deciding on how to explain your AI model to users who will be interacting with it, you should tailor these explanations to the needs of the users. Ideally, you should test different explainability methods with them and the testing environment should resemble a real-life set-up as much as possible because this helps to really understand which explainability methods work best and why. You should aim to gather both passive metrics (e.g. did the users make more edits to AI suggestions, were they faster or slower, etc.) as well as for direct feedback from users through interviews and surveys. However, you should also collect some feedback once the model has been put in active use as well and be always ready to iterate and improve your model as well as its explainability features.

READ MORE:

Join us for our talk at the Data Innovation Summit to learn how we selected explainability methods for our legal text summarization solution and lessons we learned in the process.

Explore the Data Innovation Summit",9657
"Have you ever wondered about how your banking app works? For most consumers, everything tech and finance-related is a deep mystery. For service providers and software engineers in particular, it is a hard work.

Dev team is the heart of every online business, but its contribution is often left behind the curtains. If you’re curious about how ‘the magic’ happens, we’re ready to demystify.

Team Structure

Every department could benefit from a well-planned structure, but tech teams cannot function adequately without it. Imagine dozens, if not hundreds, of developers working on multiple projects, receiving tasks from multiple departments, and urgently fixing bugs at 3 a.m. Once your business passes the startup stage, this model is no longer efficient.

For this very reason, Mercuryo’s dev department is divided into units. Each unit is dedicated to developing a particular tool from our signature toolbox, be it a user-focused crypto wallet or something more complex like API as a service. It is still quite troublesome to ask a team to check on somebody else’s code related to a completely different product despite documentation maintenance. That’s why specialization matters.

Mercuryo’s tech units consist of several teammates, including product manager, tech lead, testers, system analytic, DevOps engineer, and various back-end, front-end, and mobile devs.

We’re trying to keep an optimal number of team members in each unit and not overload them. By the end of 2021, we’re hoping to staff ten complete units.

A product manager monitors the unit’s work. This person has to be tech-savvy and always has access to the code, so they can check and comment on it anytime. Product managers are always well-aware about what their team is working at, and ready to assist or correct their work.

Technical Lead is the team member with the most profound product knowledge. A tester, back-end developer, or anyone else from the unit can act as a tech lead. They guide and assist other developers and help them whenever an issue occurs. They are responsible for the uniqueness and performance of the stand.

Each unit has its own stand to test out the tasks it tackles. Basically, a stand is a testing environment for the dev team dedicated to a certain unit. Every product’s feature should be tested on the stand before the team can release it.

Most of the devs work remotely, although they can join the team in one of our offices if they choose to. To coordinate the workflow more efficiently, the teams have weekly meetings to discuss the plan for the week ahead. Most tasks come from support, business development, financial departments, and sometimes, from our B2B partners.

If any issues arise, developers have to be ready to fix them asap.

“People tend to get extra sensitive when it comes to money, so a quick reaction is a must. In a way, you can compare developers and doctors as they have to be ready to step up immediately in case of an emergency. Developers aren’t saving lives, but they deal with technological and, in our case, financial aspects of people’s daily routine, which sometimes can turn into a life and death situation.” Konstantin Stasenko, CTO at Mercuryo

Infrastructure Specifics

Initially, the project was employing the PHP monolith. It is pretty common for a startup to use PHP as an ideal temporary solution to launch faster. However, once the company starts scaling, it may turn into a setback as soon as the company starts growing, new people join the team, and the amount of code increases significantly. Later on the way, Mercuryo had to face the issue.

Once we realized our team could not handle the number of tasks it gets, it was clear we had to make some serious changes and lean toward microservices. Microservices came out as a great alternative that fitted the needs of a rapidly growing fintech company simplifying the development process.

Microservice architecture proved to be the best fit for Mercuryo as it is a lot easier to design a few microservices than support one large monolith structure.

GoLang proved to be a much better fit for a company like ours, and the biggest struggle was to rewrite existing products in a new language. Until now, some bits and pieces of our architecture remain as a PHP monolith, but eventually, we hope to finalize the transition.

Searching for Talents

Speaking of challenges, highly-skilled developers are worth their weight in gold. A lack of talented software engineers on the market is one issue. Secondly, a common trend for a developer is to stick to one company and rarely change the employer unless something extraordinary happens.

Finally, some developers cannot pass the interview process as their experience is not enough to keep up with the rest of the team.

A typical interviewing process consists of two parts. First, the candidates have to undergo an hour-long tech evaluation, demonstrating their skills in the live mode. This interview with Technical Leads covers all the critical aspects of their future work to get complete information on the candidate, their hard skills, and expertise. Those who successfully passed the first stage will be offered the second interview meant to define their soft skills and the ability to work in a team.

You would think that finding a rare specialist with an ideal set of hard skills and impeccable expertise is already good. However, we came across excellent professionals with serious commitment issues who failed every deadline and struggled to show up. Fortunately, although these cases are an exception, it is always better to filter out such people before joining the team.

The Bottom Line

The work of developers can be described as magic. Those are developers who bring all the wild ideas and solutions into life, creating simplicity by applying maximum effort, invisible for the end-user. Forming a strong and solid tech team is not easy, but we’re getting there at our own pace.

Want to join our team or know an excellent developer looking for new opportunities? We’re still hiring!",6049
"by QuadJacks

Prologue

This blog post is a consolidation (with updates where relevant) of token swap references from the old CasinoCoin website. Please refer to the new CasinoCoin website in the future.

Statement

In November 2020, CasinoCoin(CSC) had a change in leadership. CasinoCoin’s former Technical Consultant, Daniel Keller, stepped up to take the role as the new Chief Technical Officer and will lead the business forward in a new, more-focused direction. Daniel is supported by Matthew Cheetham as Chief Operating Officer.

Previously based in Malta, the CasinoCoin Foundation has been replaced by Eminence Ltd. on the Isle of Man. Eminence Ltd. will be the parent of CasinoCoin and CSC will live on, under a new rebranded identity.

Since the leadership change, CasinoCoin has been able to reach new agreements with multiple parties and define a clear strategy for moving forward. We have also secured considerable additional investment funding, led by the Knox Group of Companies in the Isle of Man. This will ensure that we are able to deliver on our strategy.

CasinoCoin as a product will be heavily focused on Tier 1 & Tier 2 gambling markets. With the substantial new investment, we now have enough capital liquidity within the operational business to target a top 15 exchange listing for CSC. Due diligence is currently underway and we aim to be listed on our targeted exchanges.

With regards to previously announced proposals, FUD Limited — also based at the Hubb in the Isle of Man — is working on a number of exciting projects. FUD continues to work with SunCash on the development of a new DLT solution and we will continue to share knowledge and expertise where mutually applicable.

In order for us to remain compliant with GDPR regulations, anyone who is currently on the CasinoCoin mailing list or a BRM user must grant Eminence Ltd. permission to email them. You should have received an email from us regarding this. If you haven’t received an email, please opt-in via this form.

Token Swap

On April 20, 2021 CasinoCoin initiated a token swap from the CSC Ledger (CSCL) to the XRP Ledger (XRPL)*. The token swap is taking place off-exchange only, in conjunction with XRPL Labs, utilizing the XUMM Wallet. The XUMM Wallet employs an embedded migration wizard which allows users to import their existing CSCL accounts and facilitate the swap via the app.

Due to regulations, KYC is mandatory. Bankroll Manager (BRM) features like the Game Lobby will be re-released as an xApp for XUMM once the swap is concluded.

*We will swap GFG and CR8 holdings at a ratio of 1:1. If you swap your GFG into XRPL CSC, you will not be able to claim your GFG back at a later date.

Swap Details

Swap Announced:

January 19, 2021

Swap Dates:

Begins: April 20, 2021

Ends: June 18, 2021

Duration: 60 days

Swap Requirements:

Government-issued ID, passport, or Drivers License

20XRP on a separate wallet or exchange (Account reserve + Trustline)

Your CasinoCoin account secret.

Please note, the following countries are sanctioned by the Isle of Man where Eminence Ltd. is based:

Afghanistan, Bosnia and Herzegovina, Burma Myanmar, Burundi, Central African Republic, Democratic Republic of the Congo, Guinea, Guinea-Bissau, Iran, Iraq, Lebanon, Mali, Nicaragua, Republic of Belarus, Russia, Somalia, Sudan, South Sudan, Syria, Venezuela, Yemen, Zimbabwe.

If you are a citizen of a country that is currently sanctioned by the Isle of Man, please contact support for assistance.

For updates on the swap, check this page, and follow our Official Twitter. You can also join our Official Discord Server and check the following channels: #swap-instructions, #announcements, #official-tweets and #wormhole.

Moving Forward

We’re excited about the move to the XRP Ledger and we are looking forward to collaborations with XRPL Labs. This will present CasinoCoin a multitude of opportunities in the future. CasinoCoin on the XRPL is going to focus on being a fast and cheap payment solution for regulated industries. From our point of view, the XRPL is the most fitting solution for this purpose.

In recent years community-driven development, decentralization, and the ability to submit amendments accelerated. This allows the CasinoCoin team to focus on core development, no longer having to maintain the integrity of the network. Being carried by the XRPL will also make it easier when it comes to 3rd party integrations like hardware wallets and exchanges. We are also thrilled about the DEX abilities and opportunities. Eminence Ltd. will support the XRPL by running a validator.

2021 Achievements

Community Following: The number of people actively following the progress of CasinoCoin has grown tremendously. We’ve seen a 400% increase in the number of active users in our Community Discord, our Twitter following has almost doubled, and we’ve seen a rapid increase in the number of people engaged in our Telegram channel. Furthermore, there are now over 11,000 people with CasinoCoin on their CoinMarketCap active watch list.

Active Wallets: Over 5,000 new CSC wallets have been created. More users than ever are actively holding or involved with CasinoCoin.

New Technology: We are now in the process of swapping CasinoCoin Ledger (CSCL) to the world-class technology of the XRP Ledger (XRPL)

Casinos: In the past, CasinoCoin was positioned as being the future of gambling-related payments. Now, we have secured multiple casinos that will support the deposit and withdrawal of CSC and negotiations with several more are ongoing.

Head Office: CasinoCoin operations will be based out of the Hubb and we are in the process of establishing our new head office. The Hubb was a natural fit for us moving forward, being the home of blockchain and crypto-related businesses on the Isle of Man.

Market Cap: CasinoCoin had a total market cap of $3,017,702. On April 16, 2021 CSC hit an all-time high of $177,739,054, representing a 5889% increase in value.

Trading Volume: the volume of daily trading activity has also increased from a peak of $520,700 to $3,269,577 representing an over 600% increase in volume.

Exchanges: We have entered into new agreements with several exchanges that will support the buying and selling of CSC. Our mid-term goal remains to have CSC listed on a Top 15 Exchange, with a listing on a Top 25 Exchange in the near future. CSC is now more accessible than ever.

CasinoCoin Links:",6430
"CardStarter is thrilled to announce that Ridotto, the world’s first cross-chain gambling and lottery protocol, is launching on CardStarter!

Ridotto’s registration begins on Monday, September 20th @ 8AM PST/3PM UTC and will close Thursday, September 24th @ 8AM PST/3PM UTC. Seven days later, Ridotto’s IDO will officially commence on Monday, September 27th @ 8AM PST/3PM UTC.

Ridotto: Introduction

CardStarter, Cardano’s premier incubator, launchpad and insurance program selected Ridotto because of its promise to totally revolutionize the gambling industry, and flip it on its head through decentralization. Beyond their sleek flagship casino, Ridotto allows the community to bankroll all games available on the platform and become the House, and even build their own through a customizable, user-friendly SDK library.

In stark contrast to traditional casinos — made to benefit centralized ownership — Ridotto plans to use blockchain to create a casino ecosystem that is transparent, secure, and above all designed to benefit its users. As a DAO run by gamblers, liquidity providers, and game builders, power will be balanced to benefit the many over the few, elevating the community to the position of “The House.”

A Real Web3 Casino

Ridotto will utilize decentralization to bring the user much-needed peace of mind. Through Charli3’s verified randomness and openly available game metric data, the community will have complete assurance that the odds are not, in fact, stacked against them.

Ridotto’s backend algorithm will be public for all games the community knows and loves, including Blackjack, Poker, Slots, Roulette, Craps, Baccarat, Bingo, and all manner of different lotteries, not to mention the many new games to come from Ridotto Build and its game incubator.

Ridotto fully embraces blockchain at all levels of its protocol, and will integrate both competitive NFT & DeFi based games to reimagine the standard of a crypto gambling platform. The goal is to push the envelope and deliver a more authentic gambling experience, one that embraces innovation both technological and philosophical.

Ridotto’s Minimum Viable Product

While Ridotto’s ambitious roadmap reaches all the way towards the end of 2022, Ridotto will release its minimum viable product as early as Q4 2021/ Q1 2022; through it, Ridotto’s partnered projects, and their communities will be able to bankroll their very own Ridotto built slot machine and also participate in lotteries dedicated to their projects.

This initial rollout will give the Ridotto platform the opportunity to be stress tested with a restricted and simplified product offering before more robust features are offered. In a nutshell, this captures the merit of an MVP, a method of rolling out community involvement without overstepping developmental capabilities.

Early token holders will be given extra rewards and incentives as the platform develops and grows. As a community powered casino, the team believes in sharing Ridotto’s success with the people who support it; this is a philosophy RDT demonstrated in it’s community driven private sale, and will continue to do at every step of it’s evolution.

Ridotto X CardStarter

“The CardStarter team is honored to facilitate the IDO of such a paradigm-shifting project. Ridotto will not only bring transparency to the online gambling industry, but will also transform it through actual decentralization. We believe that Ridotto will become the model that future blockchain casino projects will want to emulate, and may ultimately go on to change traditional gambling as a whole. We look forward to Ridotto’s long term success and are grateful in having the opportunity to facilitate their launch,” said Aatash Amir, CEO of CardStarter.

About Ridotto

Ridotto is the very first cross-chain gambling and lottery protocol based on complete transparency, anonymity, security, and fairness. Built on Cardano, our approach is to provide an open protocol driven by the community, where users can play, build, and even bankroll casino games, thus earning generous liquidity. Ridotto’s overarching goal is to elevate the user to the role of “house,” thereby offering a fully scalable alternative to centralized online gambling.

Beyond our vibrant flagship casino, we will unleash retail ingenuity by giving users the decentralized infrastructure to experiment and evolve the gambling ecosystem, much like the App Store did for mobile gaming.

Website || Discord || Twitter || Telegram Community || Telegram Announcements

About CardStarter

CardStarter is the first incubator, launchpad and insurance program for projects on the Cardano network. The CARDS platform is a unique and dynamic launchpad, connecting projects to early supporters and a network of key partners. The Cards Accelerator Program (CAP) team works with each project individually to establish the right connections and map marketing strategies, establishing the foundation for long term success.

CardStarter is also differentiated by its charge to protect its supporters. CARDS provides unbiased third-party audits of every project’s code and protects IDO participants with an in-house insurance program. CardStarter is proudly partnered with X21 Digital, BlockSync Ventures, Genesis Shards, Plethori, Fractal, PARSIQ, Scryptic Tech and Longtail Financial. Once fully built, CardStarter will be an all-inclusive platform with a decentralized exchange to complement the current offerings. CardStarter also aims to be a nurturing force in the Cardano ecosystem, with hackathons and project development support being a major focus of the roadmap.

Website || Telegram || Twitter || Blog",5660
"I just finished watching the Skater Girl movie and have penned down my thoughts.

First of all, it’s really an inspiring movie. It focuses on the story of a poor village girl, who initially had no idea what to do in her life and soon introduced to the world of skateboarding by a foreigner. But she faces a rough road ahead but never gives up and follows her passion.

It really reminded me of my journey. Being a small-town girl, I faced resistance at every point in my life. But I never gave up. Even now, a lot of people do not appreciate my work, being a part of a male-dominated industry.

I too feel the scarcity of women in the tech world. To date in my career, I have been to startups where I was the only female developer. It might be due to the gender stereotype and lack of a talent pool. A very few women prefer or rather allowed to explore technology-related fields. We really need to change our mindset.

“Women bring a lot to a tech company — different perspectives and skillsets, tech companies with more women are more successful and it’s a hot industry to be in, good-paying jobs with lots of diversity in focus (dev, cybersecurity, ops, etc.) and opportunity. Don’t allow yourself to be intimidated by a room full of men.”

Last but not least, I just wanted to say — Give wings to your dreams

Few lines from the movie :

“I have touched the pinnacle,

I have realized my worth,

Now that my feet have opened up as wings , why shouldn’t I weave,

The dreams my eyes have seen, My complicated life I have to resolve,

The trust I have in me has to manifest,

I will touch the sky, immersed in joy,

I have taken flight. I know sun is gonna shine on me”",1670
"We’re going to start off with that which gets the least amount of criticism — the average, vocal, Tezos community member; there is some strange tendency for the Tezos community to self-sabotage and engage in entirely unproductive actions. I’d reason that those who are most critical of Tezos are those within the Tezos community itself.

This is especially the case on Reddit, which may also hint towards the need to reevaluate the current structure of r/Tezos.

Presumably, people are critical of Tezos because they want Tezos to thrive — but at some point it becomes too much. Why is it that the Tezos subreddit is filled with posts and comments about how marketing needs to take advantage of influencers? Merely two months ago, the Tezos Foundation brought Huge, a highly experienced marketing firm that has worked with massive companies, on board as Agency of Record for the foundation. Only about a week before the Huge announcement, there was the announcement that a new Marketing and Communications organization named Blokhaus was coming to the Tezos ecosystem. Do we really think that neither of these organizations have considered using influencers? In merely a few months, we have already seen Tezos-related marketing appear with huge partnerships and a full video ad campaign. Maybe we can stop spamming the subreddit with “where are the influencers?” posts and comments. Maybe we can shift to actually being productive with our time.

Oh, but this is far from a recent phenomenon. The Tezos community has a history of negativity regarding marketing — and the progress of marketing initiatives does nothing to halt it. All you need to do is search the keyword “marketing” on the Tezos subreddit to see the large amount of posts complaining about marketing.

And it’s not just marketing. The Tezos community will jump at anything remotely imperfect.

Let’s look at the curious case of reddit user u/No-Metal-6762 who joined Reddit in February 2021. Despite claiming to be an “ICO participant,” 80% of the content he’s posted on Reddit is just trashing on the Tezos Foundation and Arthur Breitman. Instead of taking responsibility as a community member and attempting to assist with spreading the word of Tezos, we get posts complaining that Tezos isn’t increasing in price during the bull run, spamming the subreddit with negative comments, and spamming posts asking “who can the community point to and blame?” (post deleted, but the comments are still available) across Tezos-related subreddits.

My favorite post of all-time was definitely this one that practically begged Arthur to respond while also including this golden line addressed to him, “You are a more technical and intellectual personality, marketing is not your primary strength. However great leaders know their strengths as well as weaknesses and address them.”

Imagine being a random, anonymous person on the internet and lecturing someone, who was instrumental in building a revolutionary blockchain, about his “strengths and weaknesses” while, most likely, contributing nothing of value to Tezos yourself. Sure, no person is perfect — but comments like these are insulting and unproductive.

There’s also a somewhat common trend in all these threads — there’s a lot of negativity that is a simply a result of the community’s own ignorance. For example, look at this exchange under a post asking about Tezos marketing efforts:

User 1: “this was a pretty good one”

User 2: “But that was a decision by a 3rd party. Tezos didn’t pay them to do it to get more visibility”

User 1: “it was two parts. the VC firm wants more Tezos action, and the TF is investing in the VC firm.”

User 2: “Thank you for the insight!”

Now, this was one of the more simple and civil exchanges but it captures the overall point —far more goes into marketing behind the scenes than the community cares to realize.

Let’s compare this to the mentality of other communities.

Algorand had the same concerns about marketing but the community handled it completely differently. Many posts have actually called out the community’s criticisms of marketing and they have pointed out that the marketing is fine.

I’ve been critical of the Tezos Foundation’s marketing initiatives in the past. However, things have changed. Is it possible that we can let the professionals work for a bit as we shift our focus to things that are actually productive? There are plenty of better uses for our time.

Look at things this way:

There are currently over 51,000 members of the Tezos subreddit. If every member of the subreddit made a single comment a year on r/cryptocurrency, then, distributed evenly, that would be over 140 Tezos-related comments a day. If only half the subreddit members made 1 comment a year about Tezos on r/cryptocurrency then that would still be 70 comments a day.

In order to have 1 medium article a day for an entire year we merely need 365 Tezos community members to write a single article over the course of 365 days. That’s less than 1% of the Tezos community writing a single article a year. Actually, people don’t even need to write medium articles — just post on r/cryptocurrency itself and hype up Tezos as much as possible. Look at this post written by someone who openly admits, “I’m new to all stuff about crypto” getting over 671 upvotes and over 426 comments.

Is someone going to tell me that all these people concerned about marketing can’t find the time to make a few comments in r/cryptocurrency? Or make a single medium article, or a single r/cryptocurrency post, hyping up Tezos? Is someone going to tell me that a post instructing a professional marketing agency to use influencers (while they’re already, most likely, aware of such things) is more important than getting hundreds, if not thousands, of people aware of Tezos on social media with relatively low effort? Please, do the logical analysis on that and consider re-configuring your priorities here.",5945
"By Marcos Chiquetto

Translate a text is to read something in one language and write it in another language. What kind of technology would be required?

Let´s take the example of our Brazilian translation agency, based in São Paulo, a small-scale organization considering the global translation industry. We have two directors, six project managers, and a group of about40 active translators and reviewers, including in-house personnel and freelancers. We work with three languages: English, Portuguese, and Spanish.

Our agency is a small company in the global translation industry.

And what kind of volume a company of our size handle? Well, let´s consider our volume of work in 2019. In that year alone, we carried out a total of 5230 projects.

That is right. Our small company executed five thousand, two hundred and thirty projects in 2019. This works out to an average of 20 new projects per work day.

And what type of projects are we talking about? The following table shows examples of our large, medium, and small projects:

These are figures for a small Latin American translation agency. What kind of volume would a multilingual global agency handle? Let´s assume the company works with 20 languages, and has, on average for each language, a quarter of our company´s volume. This company would process something around 25 thousand translations jobs per year, or about one hundred jobs per workday.

To continuously cope with all those jobs, a series of actions are necessary:

Each job must be assigned to a project manager, who will choose translators and reviewers, estate the tools to be used, and lay down a schedule. All these decisions are registered in a job database, whose control software is the backbone of the company.

Once a job is set up, the actual translation must be done. Even though the translators and reviewers may not be the same of previous jobs for that client, the style and terminology are to be the same. Moreover, text pieces of previous jobs must be reused, to assure consistency and reduce the price for the client. For all those purposes, a translation memory software is mandatory (see our article on translation memory).

Translation memory software: the typical working tool of a translator today. The original text is segmented and presented in a separate environment, along with the database of the previous translation jobs done for the same client. In this example, segment #22 is being translated. The corresponding translation has been found in the database (upper window) and inserted in the translation field (right column).

To be competitive, the agency may also decide to use machine translation as the first step of the job, increasing the productivity of its human translators. For that purpose, a machine translation engine must be available.

Once finished the work of translators and reviewers, a final quality assurance procedure is performed by means of a QA tool.

A typical QA tool. This screenshot lists the kind of errors spotted by the tool. Based on this report, the human professionals improve the deliverables.

Then, all service providers should be paid, and receivables from all clients must be controlled, what demands a software module to handle thousands of small and large financial operations.

Last but not least: all previous activities are only possible because the agency has clients. For that purpose, a permanent marketing effort is necessary, involving some software tool to handle databases of contacts, prospective clients, marketing pieces, etc.

The above summary alone shows at list six different kinds of software tools that are necessary to run a translation agency.

And even having all necessary tools, your translation will only be as good as the human translators you have in your team. If you have poor translators working in your translation jobs, no software tool will produce high-quality work.

Definitely, it is not a trivial task.",3937
"We just finished the 11th annual Lean LaunchPad class at Stanford — our first version focused on deep science and technology.

I’ve always thought of the class as a minimal viable product — testing new ideas and changing the class as we learn. This year was no exception as we made some major changes, all of which we are going to keep going forward.

1. A focus on scientists and engineers. We created an additional Spring section of the class with a focus on commercializing inventions from Stanford’s scientists and engineers. The existing winter quarter of the class remains the same as we taught for the last 10 years — taking all students’ projects — e-commerce, social media, web, and mobile apps. This newly created Spring section focuses on scientists and engineers who want to learn how to commercialize deep science and technology — life sciences (medical devices, diagnostics, digital health, therapeutics,) semiconductors, health care, sensors, materials, artificial intelligence/deep learning, et al.

This allowed us to emphasize how to differentiate a technical spec from a value proposition and expand on the parts of the business model that are unique for science and engineering startups. For example, life sciences versus commercial applications have radically different reimbursement, regulatory, clinical trials, scientific advisory boards, demand creation, etc. In addition, we found we needed to add new material on Intellectual Property, how to license inventions from the university, and discussions about team dynamics. Going forward we’ll continue to offer the class in two sections with the second class focused on science and technology.

2. Remote Discovery — As the pandemic forced teaching remotely, we’ve learned that customer discovery is actually more efficient using video conferencing. It increased the number of interviews the students were able to do each week. When Covid restrictions are over, we plan to add remote customer discovery to the students’ toolkit. It remains to be seen whether customers will remain as available on Zoom as they were during the pandemic. (See here for an extended discussion of remote customer discovery.) Remote discovery also allowed a bigger pool of potential interviews not bounded by geography. The quality of interviewees seemed to improve by this larger pool.

3. Class size/configuration — For the past decade our class size was 8 teams of 4. This year we accepted 12 teams of 4. Previously all teams needed to sit through all 8 weekly presentations. That was tough in person and not sustainable via Zoom. This year, by moving into two breakout sections, we cut the number of presentations that each team sat through by half. The new format allowed students and teaching staff to devote greater attention to each presentation.

4. Adopt a team — in past years all instructors had office hours with all the teams. This year each instructor adopted three teams and saw them weekly for a half hour. Students really appreciated building a closer working relationship with one faculty member.

5. Alumni as guest speakers — Most weeks we invited a past student to guest speak about their journey through the class, highlighting “what I wish I knew” and “what to pay attention to.”

Below are the Lessons Learned presentations from the Lean LaunchPad for deep science and technology, as well as additional learnings from the class.

During the quarter the teams spoke to 1,237 potential customers, beneficiaries, regulators — all via Zoom. Most students spent 15–20 hours a week on the class, about double that of a normal class.

Team Gloflow

Started on Week 1 as a pathology slide digitization service.

Ended in Week 10 as response prediction for cancer treatments.

If you can’t see the Gloflow video, click here

If you can’t see the Gloflow slides, click here

Team Loomia

Started on Week 1 as flexible e-textile circuit looking for a problem.

Ended in Week 10 as easy-to-integrate components for automotive suppliers.

If you can’t see the Loomia video, click here

If you can’t see the Loomia slides, click here

Team Skywalk

Started on Week 1 as wearable gesture control device for real and virtual worlds.

Ended in Week 10 as a future-proof gesture control solution for AR headsets and the Department of Defense.

If you can’t see the Skywalk video, click here

If you can’t see the Skywalk slides, click here

Team EdgeAI

Started on Week 1 as a custom silicon chip with embedded memories and a Machine Learning accelerator targeting low-power, high-throughput, and low-latency applications.

Ended in Week 10 as a chip enabling AI vision applications on next generation battery powered surveillance cameras.

If you can’t see the EdgeAI video click here

If you can’t see the EdgeAI slides, click here

Team MushroomX

Started on Week 1 as Drone pollination of crops.

Ended in Week 10 as autonomous button mushroom harvesting.

If you can’t see the MushroomX video, see here

If you can’t see the MushroomX slides, click here

Team RVEX

Started on Week 1 as a Biomimetic Sleeve as a Left Ventricular Assist Device.

Ended in Week 10 as a Platform technology as a right heart failure device.

If you can’t see the RVEX video, click here

If you can’t see the RVEX slides, click here

Team Pause

Started on Week 1 as a Menopause digital health platform that connects women to providers and other women.

Ended in Week 10 as a D2C Menopause symptom tracking app and on-demand telehealth platform that offers women a personalized and integrative approach to menopause care.

If you can’t see the Pause video, click here

If you can’t see the Pause slides, click here

Team Celsius

Started on Week 1 as an IOT hardware sensor for environmental quality and human presence.

Ended in Week 10 as hybrid work collaboration + employee engagement.

If you can’t see the Celsius video, click here

If you can’t see the Celsius slides, click here

Team TakeCare

Started on Week 1 as a platform for finding and managing at-home senior care.

Ended in Week 10 as a B2C platform for scheduling on-demand at-home senior care.

If you can’t see the TakeCare video, click here

If you can’t see the Take Care slides, click here

Team CareMatch

Started on Week 1 as AI to Match Patients to Post-Acute Care.

Ended in Week 10 as Skilled Nursing Facility-at-Home for Wound Care.

If you can’t see the CareMatch video, click here

If you can’t see the CareMatch slides, click here

Team NeuroDB

Started on Week 1 as Unstructured data Tableau-like tool.

Ended in Week 10 as Cloud-based Pandas dataframe.

If you can’t see the NeuroDB video click here

If you can’t see the NeuroDB slides, click here

Team Drova

Started on Week 1 as a provider for autonomous drone delivery for restaurants and grocery stores.

Ended in Week 10 as Fleet management software for autonomous drone delivery.

If you can’t see the Drova video click here

If you can’t see the Drova slides, click here

Student Comments

I normally don’t include student comments in these summaries, but this year’s summarized why — after a decade — we still teach the class. The students find the class hard and exhausting, and say their instructors are tough and demanding. Yet in the end, the class and the work they invest in is highly rewarding to them.

“Awesome course- one of the best I’ve taken so far. You get out what you put into it, but find a team you like working with, get ready to hustle and work hard, and trust the process. A must-take for entrepreneurs!”

“Absolutely crucial to starting a company for a first-time founder. Couldn’t imagine a better teaching team or learning environment.”

“Very worth taking, whether you want to do a start-up your own or not.”

“Recommend to everyone considering entrepreneurship or want to learn about it.”

“Great class if you are interested in learning about the Customer Discovery Model, but takes a lot of time and work.”

“Intense course where you learn through experience on how to build a startup. I came with a product and I learned to find a solution and how to build from there.”

“Incredible experience — really glad I took the class and happy with the outcome.”

“Steve Blank tells you your slides are ugly”

“Take this course if you get a chance, especially if you are a PhD student. Super useful and a different kind of learning than most case-based classes. Extremely experiential.”

“A great class to learn about customer discovery and entrepreneurship methodologies! The teaching team is incredibly experienced and very honest in their feedback. It is quite time intensive and heavily based on your team. Make sure to clarify expectations with your team beforehand and communicate.”

“Definitely recommend this course, it’s a great experience and will give you tools to launch your idea.”

“A really excellent course to take to learn about entrepreneurship! An invaluable opportunity you might not find anywhere else. The instructors are extremely knowledgeable veteran entrepreneurs who give all the support and encouragement needed.”

Diversity

In past years, the students in the class were mostly men, reflecting the makeup of the applicants. While Ann Miura-Ko was part of the original teaching team, having all male instructors for the last five years didn’t help. Mar Hershenson joined the teaching team in 2018 and made an all-out effort to recruit women to apply. In this new Spring section of the class Heidi Roizen and Jennifer Carolan joined us as instructors. Mar, Heidi and Jennifer are all successful VC’s. They sponsored lunch sessions, mixers and meetings with women entrepreneurs and alumni for female students interested in the class and for male students looking to work with a more diverse team. I am happy to report that as a result of many people’s hard work the gender balance in the class substantially changed. Our Spring cohort focused on deep science and tech had 51 students — 25 were women.

The lessons for me were: 1) the class had been unintentionally signaling a “boys-only” environment, 2) these unconscious biases were easily dismissed by assuming that the class makeup simply reflected the applicant pipeline, and 3) when in fact it required active outreach by a woman to change that perception and bring more women into the pipeline and teams.

Teaching Assistants (TAs)

Our Teaching Assistants keep all the moving parts of the class running. This year their job was even more challenging running the class virtually and they made it run like clockwork.

Each year’s TAs have continued to make the class better (although I must admit it was interesting to watch the TAs remove any student uncertainty about what they need to do week-to-week by moving to a more prescriptive syllabus. Originally, I had designed a level of uncertainty into the class to mimic what a real-world startup feel like.) However, the art of teaching this class is remembering that it wasn’t designed by a focus group.

A Great Class Endures Beyond Its Author

I’ve always believed that great classes continue to thrive after the original teachers have moved on. While I created the Lean LaunchPad methodology and pedagogy (how to teach the class), over the past decade the Stanford class has had ten additional instructors, thirty-three wonderful TA’s and ninety volunteer mentors.

In addition to myself the teaching team has been:

2011 Instructors: Ann Miura-ko, Jon Feiber

Lead TA: Thomas Haymore, TA’s: Felix Huber, Christina Cacioppo

2012 Instructors: Ann Miura-ko, Jon Feiber

Lead TA: Thomas Haymore, TA:, Stephanie Glass

2013 Instructors: Ann Miura-ko, Jon Feiber

Lead TA: Rick Barber, TA: Stephanie Glass

2014 Instructors: Jeff Epstein, Jim Hornthal

Lead TA: Soumya Mohan, TA: Stephanie Zhan, Asst: Gabriel Garza, Jennifer Tsau

2015 Instructors: Jeff Epstein, Steve Weinstein

TA’s: Stephanie Zhan, Gabriel Garza TAs: Jennifer Tsau, Akaash Nanda, Asst: Nick Hershey

2016 Instructors: Jeff Epstein, Steve Weinstein

Lead TA: Jose Ignacio del Villar TA’s: Akaash Nanda, Nick Hershey, Zabreen Khan, Asst: Eric Peter

2017 Instructors: Jeff Epstein, Steve Weinstein

Lead TA: Eric Peter TA’s: Nick Hershey, Lorel Sim Karan Singhal Asst: Jenny Xia

2018 Instructors: Jeff Epstein, Steve Weinstein, Mar Hershenson, George John

Lead TA: Jenny Xia TA’s: Anand Upender, Marco Lorenzon, Lorel Sim Asst: Parker Ence, Trent Hazy, Sigalit Perelson

2019 Instructors: Jeff Epstein, Steve Weinstein, Mar Hershenson, George John, Tom Bedecarre

Lead TA: Parker Ence, Trent Hazy TA’s: Marco Lorenzon, Sigalit Perelson, Lorel Sim Asst:, Ashley Wu

2020 Instructors: Jeff Epstein, Steve Weinstein, Mar Hershenson, George John, Tom Bedecarre

Lead TA: Marco Lorenzon, Ashley Wu TA’s: Sigalit Perelson, Gopal Raman

2021 — Winter Instructors: Jeff Epstein, Mar Hershenson, George John, Tom Bedecarre

Lead TA: Erica Meehan, Anand Lalwani, TA’s: Gopal Raman, Andrew Hojel

2021 — Spring Instructors: Steve Weinstein, Heidi Roizen, Jennifer Carolan, Tom Bedecarre

Lead TAs: Sandra Ha, Lorenz Pallhuber TA: Manan Rai

Our Decade of Mentors

The mentors (industry experts) who volunteer their time have been supported and coordinated by Tom Bedecarre and Todd Basche. Each mentor’s contribution gets graded by the student team they coached.

Bryan Stolle, Charles Hudson, Dan Martell, David Feinlab, David Stewart, Doug Camplejohn, Eric Carr, George Zachary, Gina Bianchini, Heiko Hubertz, Hiten Shah, Jason Davies, Jim Greer, Jim Smith, Jonathan Ebinger, Josh Schwarzapel, Joshua Reeves, Justin Schaffer, Karen Richardson, Marianne Wu, Masheesh Jain, Ravi Belani, Rowan Chapman, Shawn Carolan, Steve Turner, Sven Strohbad, Thomas Hessler, Will Harvey, Ashton Udall, Ethan Bloch, Jonathan Abrams, Nick O’Connor, Pete Vlastellica, Steve Weinstein, Adi Bittan, Alan Chiu, George Zachary, Jeff Epstein, Kat Barr, Konstantin Guericke, Michael Borrus, Scott Harley, Jorge Heraud, Bob Garrow, Eyal Shavit, Ethan Kurzweil, Jim Anderson, George John, Dan Manian, Lee Redden, Steve King, Sunil Nagaraj, Evan Rapoport, Haydi Danielson, Nicholas O’Connor, Jake Seid, Tom Bedecarre, Lucy Lu, Adam Smith, Justin Wickett, Allan May, Craig Seidel, Rafi Holtzman, Roger Ross, Danielle Fong, Mar Hershenson, Heather Richman, Jim Cai, Siqi Mou, Vera Kenehan, Phil Dillard, Susan Golden, Todd Basche, Robert Locke, Maria Amundson, Freddy Dopfel, Don Peppers, Rekha Pai, Radhika Malpani, Michael Heinrich, MariaLena Popo, Jordan Segall, Mike Dorsey, Katie Connor, Anmol Madan, Kira Makagon, Andrew Westergren, Wendy Tsu, Teresa Briggs, Pradeep Jotwani.

And thanks to the continued support of Tom Byers, Tina Seelig, Kathy Eisenhardt, Ritta Katilla, Bob Sutton and Chuck Eesly at Stanford Technology Ventures Program (the entrepreneurship center in the Stanford Engineering School).

Steve Blank writes about teaching entrepreneurship at www.steveblank.com.",14857
"Covid cases continue to rise. Deaths continue to mount. Hospitals continue to be overwhelmed. Is this a fourth wave? A fifth? And far too many people continue to deny the seriousness of this flywheel of historic tragedy.

Why? I would like to propose a theory.

Scope of the Pandemic Tragedy

But, first, let’s get real….

Almost 640,000 people in America have died from Covid in the eighteen months since March 2020, according to CNN. That’s more than 35,500 Americans killed each month. Outrage was sparked by 13 U.S service members killed by recent bombings in Afghanistan. Plenty still drive around with emblems of remembrance for the 2,996 killed on 9/11. But Covid’s innumerable deaths elicit from far too many a shoulder shrug of concern and a staunch refusal to act.

If you were to list in a book simply the first and last names of each person who has perished from Covid-19, it would be 1,280,000 words, which is longer than the entire 7-book series of Harry Potter (1,084,170 words, according to Word Counter).

It dwarfs the number of military personnel in the Army, Navy, Air Force and Marines who died in the 44 months of World War II: approximately 407,000, according to The National World War II Museum. At 9,250 deaths per month, that’s about 25% of the Covid death rate.

A line of 640,000 people, one per yard, would extend 363 miles. If you started such a line at the World Trade Center and ran it south, it would extend past the Jersey Shore, beyond Delaware’s Rehoboth Beach and fly past Virginia Beach, ending in North Carolina’s Outer Banks, 30 miles south of Kitty Hawk.

This continuing crisis is 100% epic in proportions.

Justifications for Resistance

Yet, we hear vehement protests against the simple obligation to wear a mask or get a widely-proven-to-be-safe vaccination.

We are all familiar with the debate:

“You can’t make me wear a mask!” But folks are made to wear shoes and shirts and pants. “I can do what I want!” Except for the hundreds of thousands of governmental laws, societal norms, religious strictures and familial dictates which say otherwise. “This is a country of freedom!” Overlooking that collective freedom also requires collective responsibility. “My body. My choice.” Adopting, oddly enough, a principal battle cry of the pro-choice movement.

“I will trust in God.” But if all of existence comes from God than the vaccine is from God too. “I don’t trust Biden (or Democrats).” Disregarding the indisputable facts that the vaccine was developed during a Republican Presidency and that lions of the political right…Fox News owner Rupert Murdoch, Texas Governor Gregg Abbott and former President Trump…have been vaccinated.

“There is not enough research.” So say folks who — along with most Americans — can’t even fathom the amount of work and studying and research and publishing it took for scientists and doctors to study Covid and develop the vaccine…much less read and understand what has actually been published.

“The pandemic is an exaggeration and a hoax!” Denying irrefutable data from all corners of the country and world…from hospitals being overwhelmed by cases…from countless industries which have lost or are risking loss of unfathomable amounts to abide by Covid precautions. The Tokyo Summer Olympics…the NBA…the NFL…MLB…the NHL…the NCAA…and countless European soccer leagues…did not give up on billions in revenue because of a hoax.

How do we explain this recalcitrant resistance? How do we explain what is truly…ultimately…potentially, truly suicidal resistance based on the flimsiest of rationales?

The flippant reaction to this line of thinking is to cast aspersions online or in exasperated conversations. But I don’t believe those are productive responses, or that they offer the correct explanations. There is a salient message mixed within the resistance.

Economic Roots of Pandemic Resistance

I believe the correct explanation is rooted in current economic trends.

Pillars of the American economic system have been failing many of the people who are resisting vaccinations. Globalization, with its subsets of open trade and fungible, transient labor. Technology, with its subsets of automation and nationalized commerce (i.e., small businesses fighting not just against other neighborhood business but against businesses with vastly different cost structures in other parts of the country).

Good for the rich. Good for those who have bankers to funnel money around private accounts and across borders. Good for those who, with the swipe of a pen, can move thousands of jobs halfway across the country or around the world. Good for those whose socio-economic status is fortified by rarefied social networks, inaccessible elite education and bureaucratically-protected jobs. Good for those who can change their residence like mortals change their wireless plans. Good for those entrenched in the global nodes of political or economic or technological power.

Bad for almost everyone else.

Globalization and technology have left a wake of human devastation that has long been overlooked.

The Devastation of Globalization and Technology

Factory towns zombified for multiple generations now. Quaint main streets now dilapidated and shrouded in plywood and graffiti. Blue collar suburbs forced to commit slow harikari by shopping at big box stores that (a) export all profits to their distant, already gluttonously-rich owners, (b) who buy almost all products from overseas suppliers and (c) are aggressively casting more and more neighbors into unemployment in favor of self-checkout and online sales…(d) using equipment most likely also manufactured overseas!

Small businesses across the country suffocated by online commerce that mercilessly cuts into the economic viability of any business not owned by the rich. Whole swaths of America deemed nothing but fly-over country and left to tread water in a system indifferent to its negative impact on them.

For a significant portion of voters, the presidential election of 2016 was never about supporting its specific victor. It was about the many people who are pissed off. Ready to support a politician who would rail at the “Establishment” how they wanted and with words they wanted…whether or not any campaign promises would ever be delivered upon.

Ready to thumb their noses at the socially-acceptable version of “America” because it’s been thumbing its nose at them for years and years. Ready to cherry-pick their principles of America because it’s been cherry-picking beneficiaries of the system. Ready…as we are seeing now…to resist vaccinations for the greater good (not to mention themselves) because, for decades, the greater good excluded concerns about factory jobs, blue collar work, small towns and things that mattered to them.

This does not explain the underpinning sentiments of everyone who refuses to be vaccinated. Not longstanding religious devotees. Not those who distrust the government for legitimate historical reasons. Not those who are using this issue for crass political gain. There are certainly others as well.

This also is not contending folks should be exculpated for refusing to adopt basic public health precautions. They ought to be culpable. They ought to accept the consequences of their exercise of freedom.

We Are All Intertwined

I would contend, this hypothesis, any way you flip it, holds up to explain the intransigence of a substantial number of Americans. I am certainly open to other explanations…except for those explanations which don’t recognize there is something legitimate sparking their illogic and anger. Their acts of refusal are communicating something deeply dissatisfied with our country.

If the pandemic prompts us to proclaim we are all interconnected, “so you must mask up and vaxx up,” then we must also embrace the inverse or corollary principle: we are all interconnected and the problems of those devastated by globalization and technology are our problems…and not just unfortunate byproducts of “progress” that can be left unattended and cast aside in history’s wake. Footnotes to the principle of “survival of the fittest.”

We are all intertwined; we must heed their underlying concerns. We must fix their problems. How? First, by acknowledging that they exist and they’re real and listening with an open mind and from their perspective. Because, in the end, just as their Covid is our Covid, their economic problems are our economic problems.

[image from www.warontherocks.com]",8491
"What is Blockchain?

Blockchain is a cryptocurrency blockchain explorer service, as well as a cryptocurrency wallet and a cryptocurrency exchange supporting Bitcoin, Bitcoin Cash, and Ethereum. They also provide Bitcoin data charts, stats, and market information.

What is Decentralization?

Decentralisation is the process by which the activities of an organization, particularly those regarding planning and decision making, are distributed or delegated away from a central, authoritative location or group.

Decentralization in Blockchain

In blockchain, decentralization refers to the transfer of control and decision-making from a centralized entity (individual, organization, or group thereof) to a distributed network.

The whole idea is about decentralization issue which first give the benefit by making sure no entity will be able to make any changes to the transaction once its done and approved and added to the chain ,this leaves no need for them .

Bitcoin

Bitcoin was the first example blockchain implemented on , so Bitcoin is just an example to start with and it’s not the whole idea of blockchain , blockchain will be used for all sectors at the end for example in the medical field now there are some researches to use blockchain to be benefit from for patients , researchers and the government or insurance companies at the same time .

Importance

In a decentralized blockchain network, no one has to know or trust anyone else. Each member in the network has a copy of the exact same data in the form of a distributed ledger.

CAN IT BE HACKED???

On the surface, blockchain seems to be a solid and transparent system immune to fraud or deception. In reality, MIT reports that hackers have stolen nearly $2 billion worth of cryptocurrency since 2017. The methods for the thievery varies, but a technique that points to a theoretically weakness in blockchain is know as a “51% Attack.”

Smart Contracts

A multi-university report says Smart Contracts can be vulnerable. Smart Contracts are used to assure transparent and secure interactions in the blockchain. They run a program that executes items related to contract agreements.

For example, there might be an if then statement that releases to another person money if a form is signed. The exchange is controlled by a set automated process. The contracts are integrated into the blockchain to ensure transparency. However, even Smart Contracts are vulnerable. Bugs can exist in the code, either intentionally or inadvertently.

The errors cause incorrect actions to occur in the contract. These errors have led to over $70 million in loses in recent years. There is no accurate tools available for testing and detecting these vulnerabilities. No system is perfect. Knowing the fallible nature of technology frameworks can give users the insight to look out for errors and be vigilant.",2859
"Campaign Strategies of Tech Unicorns, Revealed

Background vector created by rawpixel.com — www.freepik.com

Blog posts, emails, display ads, retargeting ads, content, videos, case studies, whitepapers, SEO, surveys, social, virtual events, webinars, news stories…

In 2021. that might as well be a list of campaign tactics used by nearly every tech company. While channels are often a commodity, the way they are used can be positioned as a strength.

In part 3 of my series on how B2B tech unicorns build disruptive brands, I’ll share not just how unicorns are leveraging the web and physical world to reach their audience but also the behind the scenes of what are driving those decisions.

Specifically, I’ll explore the answers to frequently asked questions like, how are campaign decisions being made? What are some of the most promising & new channels? and how is account-based marketing being used?

With that, let’s dive in.

How to prioritize campaigns

You would think most companies keep their campaign strategy a secret. One company turns that hypothesis on its head. In the spirit of openness, Gitlab, known as a leader in open-source tools for developers, has decided to share its marketing handbook online.

In the guide, the Gitlab team shares their multi-channel campaign approach. The team focuses on having two dimensions clearly defined:

Value — why a prospect would be interested in making a change in the first place.

Use case — the specific situation in which a product can be used.

The idea is simple. Create campaigns that clearly articulate why someone should care, and what it is he or she should care about. The company puts so much focus on creating customer-centric content that it has established a team dedicated to doing just that. The Strategic Marketing team comprises subject matter experts who deeply understand and can explain the value of the product to prospects.

This value-first approach makes a lot of sense when there’s an established market, but what if the market is still mostly unaware of what it is they need?

In an interview with product hunt, the founders of Brex, a fintech company that offers corporate banking solutions for startups, described how they use content loops to drive awareness. Here’s the basis: target your demographic based on what they are searching for, serve them content to draw them into the site, then retarget them with paid ads.

While that’s a good start, the magic lies in the strategy. The team discovered that keyword targeting of in-market buyers was less likely to turn into opportunities than running Youtube ads where potential customers were not yet in the research stage (or thinking about) corporate banking solutions but looking up related startup content.

Their ad is reminiscent of an Apple iPod commercial, complete with attractive visuals, clear value statements, and a catchy tune. No surprise, it also has been viewed over one million times. See for yourself.

Brex Youtube Ad

Some promising channels

UIPath, the leading robotic automation platform, found that one of the most successful lead gen channels involved retargeting visitors who had yet to convert into leads through LinkedIn InMail conversation ads. This feature lets you send personalized messages to the inboxes of specific target segments. With this approach, UIPath was able to convert 67% of the leads that engaged.

While investing in LinkedIn seems obvious, Facebook seems less so. However, Zenefits, an HR software leader, was able to cracked the code and make it work for them. The team ran 375 different experiments to eventually find a significantly lower cost per acquisition and a greater average order value than through other channels.

They were able to achieve these results by fine-tuning their target parameters leveraging third-party data from a provider called Clearbit. The results showed that educational content performed much better than product-driven messaging.

Marketing has long been a vehicle to get attention. Often, when you least expect it. New York subways and northern California interstates? It turns out several B2B tech companies — C3.AI, Monday.Com, Brex — to name a few, have all invested in billboards before they were a household name, hoping perhaps to become one.

Living in New York, I can pledge first-hand that it was impossible to miss the Monday.com takeover of the subways. Their messaging adorned the turnstiles, walls on the platform, and banners inside the subway.

Monday.com | C3.AI | Brex billboards (image put together by author)

While it’s difficult to measure the impact, out-of-home advertising does make sense for companies with a broad audience and a heavy concentration in one particular city or location (or did until the pandemic).

Plaid, a company that offers developer tools and APIs for consumer banks, has taken the opposite approach, by taking a narrow, targeted approach.

In a conversation with Saastr, the CEO shared that they have always been “developer focus.” As a result, they acquire most of their customers through forums, hacker news, and Reddit. The company understood well that to gain the trust of developers, you need to engage and embed yourself into the community, not just push ads. Visa just acquired them for $5.3B.

In a similar vein, UIPath has taken it upon itself to invest heavily in using its website to tackle “the industry’s most pressing issues and concerns, such as the current state and future of robotics automation across a variety of industries, while at the same time positioning UiPath as a thought-leader in this field.” By building up search result rankings for trending new concepts, the company was able to see a 4x growth in traffic in just a year.

Account-based marketing (ABM) best practices

The idea is simple: Proactively target the best-fit companies with your messages rather than hoping they stumble upon find you. To this day, I am still shocked that ABM requires countless hours of explanation to get organizational buy-in.

That said, the tides are starting to turn. Of the 25 unicorns I analyzed, I could validate that over 75% of them have hired an ABM leader and/or are actively engaging in ABM activities.

One of the first steps with any ABM strategy is picking which accounts to target and segmenting the list. Gitlab shares that they segment their accounts across 3 groups, one-to-one (completely personalized communication) 1 to a few (roughly 50 accounts), and 1 to many (75–100 accounts).

While nailing the segmentation approach is essential, The CMO of Thoughtspot would argue sales alignment is perhaps even more important, saying “Segmentation strategy is critical, but it just doesn’t’ fly at all if you don’t have sales bought in. If they’re not ready to segment accounts and narrow the focus to a reasonable level, then it won’t work.” In just 5 years, the company has grown to a valuation northwards of $2B, so perhaps they are onto something.

One common ABM strategy to get buyers’ attention involves direct mail. But does it work? Outreach.io, the leading Sales Enablement Platform, decided to run an A/B split test. They sent the clever direct mail package shown below to a list of 500 prospects in their target account list who had previously been contacted but never responded. They also had a control group of 500 accounts that did not receive the box.

The results were promising. The prospects who received the swag responded at a 26% higher rate, with three times as many meetings booked. Direct mail itself wasn’t the reason this worked. The company nailed the strategy. The value of their platform connects back nicely to the items included in the package. They nicely juxtapose a modern wireless charging pad with the old-school flip-phone. And equally important, they followed through, coordinating timely follow-up with the SDR team.

Direct mail happens to be just one ABM activation channel. Gitlab highlights two digital tools in their ABM tech stack. The team uses Demandbase for personalized 1:1 advertising to target accounts and Pathfactory to create personalized content web pages.

Tipalti, an accounts payable software solution valued at over $2B, brought in 6sense, an account-engaged platform, to help them identify buyers showing intent. Armed with additional data on how target accounts were engaging with their brand in real-time, Tipalti provided visibility and tools, such as a content suggestion matrix, to SDRs so they could reach out with more personalized messages based on searched keywords of target accounts.

The latest way to run ABM campaigns that have grown quickly in popularity is the conversational chatbot. Companies like Thoughtspot, Zenefits, and TripActions all mention that they use chat capabilities on their website to engage key target accounts in a dialogue in real-time. With better routing and tight integrations with Salesforce, solutions like Pardot and Drift have made this possible.

Marketing was never about one channel or one approach, but including an account-based targeting approach is just smart business. Now that we have the data-trails and knowledge to engage our best fit-prospects proactively, it should be a no-brainer.

Takeaways

This list is far from conclusive. With so many possibilities available at your disposal, marketing leaders and founders should get intentional about when and how they invest in adding new engagement avenues.

At a minimum, you must understand what stage in the buyer’s journey your ideal customer profile is, what problem they have that you can solve, what level of awareness they have of that problem, and what channels they use to seek out information. It’s okay to experiment, but it’s more important to have a reason beyond, “I heard it worked for someone else.”

In Steve Blank’s Startup Owner’s Manual, he shares a useful strategy to begin thinking about the buyer’s journey. He writes that people typically fall into one of the following 4 levels of awareness:

Aware of pain and actively searching for a solution

Aware of pain but not actively searching for a solution

Aware of pain, and have cobbled together a solution

Have pain but are unaware there’s a problem

Knowing the answer to this question will help build an engagement plan that is rooted in context. If you know the typical mindset of your ideal customer, then you at least have a starting point. And knowing where to begin, is half the battle.",10434
"Nowadays, Blockchain Smart Contracts is a hot topic; everyone just wants to know what it is. Day-by-day processes are digitizing, and this is why it is indispensable to get a path to make reliable and digital business agreements. Smart contracts are an excellent option for reinstating traditional contracts that are often complicated, slow, and costly.

So, let’s move further and first know what Blockchain smart contracts are…

Basic Things you should know about Blockchain Smart Contracts

Image Source: Research AIMultiple

A smart contract is a self-enforcing obligation with the terms of the contract within buyer and seller being straight written into code sequences.

Entering into Blockchain Smart Contract, the gatherings first need to adapt and accept the agreement terms before commemorating the terms (partially or fully) in Smart Contract code that are cached within the Blockchain.

Smart Contracts enable the production of credible transactions without the obligation of third parties. It is a decentralized path, which indicates mediators in confirming the ventures are not required.

Blockchain Smart contracts automatically execute when proposed terms and conditions match, based on the planned laws.

In addition; Smart contracts include a number of key parts that are:

Signatories: Two or more gatherings that utilize the smart contract and give their final ‘go forward’ concerning the terms via their digital signature.

Subject: In a limited manner, it is used in smart contracts.

Specific terms: These terms are defined in precise mathematical terms and executed in a programming language that fits with the Blockchain smart contracts.

If knowing basic things about Smart Contracts, you also want to make use of Blockchain Smart Contracts, then simply avail of Blockchain development services. This will help your business make effective use of it.

Blockchain and Smart Contracts

Smart contracts utilize Blockchain technology to verify, approve, apprehend and execute agreed-upon terms between various parties.

Smart contracts on the Blockchain acknowledge transactions and agreements to be executed among unknown parties without the involvement of a central entity, outside enforcement, or any legal system. Moreover, transactions done using Blockchain technology are translucent, unchangeable, and traceable.

Blockchain is the absolutely finest environment for smart contracts, as the entire data cached is immutable and protected. Furthermore, smart contract data is encrypted and subsists on a ledger, which means that the erudition entered in the blocks can never be lost, altered, or removed.

Where can Blockchain Smart Contracts be employed?

Various industries can use smart Contracts to perform multiple functions such as regulatory compliance, cross-border financial transactions, contractual enforceability, credit enforcement property purchase, legal processes, financial services home buying, supply management, document administration, and a lot more. Blockchain smart contracts can be used by:

Insurance

Healthcare

Supply Chain Management

Real Estate

eCommerce

Retail

Banking and Finance

Advantages of Blockchain Smart Contracts

Image Source: Kustard

Smart contracts grant several benefits over regular contracts. These contracts are more effective and reliable than traditional contract law and also proffers better security as all transactions are recorded and validated. Let’s move further and know the benefits of Blockchain Smart Contract.

Accuracy and Transparency

The codified terms are completely visible and open to all relevant parties, so there is no need to discuss them once the smart contract is set. These facilities can make transactions transparent and may exclude the possibility of supervision and error. Ultimately leads to reduced monitoring costs and risks.

Greater Effectiveness and Agility

Smart contracts can enhance the effectiveness and speed with which commercial systems are carried out. Smart contracts are automated, so less time and small paperwork are just required, and even manual error correction in documentation is also not required. In addition, if changes are required, then online changes can be done within a short time without the involvement of lawyers.

Security

Smart contracts offer better security as all actions are recorded and validated. The Blockchain transaction records are coded, which makes them remarkably tricky to mangle. Moreover, Security features can also be united into a smart contract to create backups and duplicates automatically if any mishaps, data loss events occur.

Trust

Smart contracts automatically execute transactions following planned laws, and the encrypted records of these transactions are shared with associates. Hence, the information and terms mentioned in the contract are straight. Additionally, specific validation by parties ensures that the smart contract can never be broken.

Challenges You May Observe In Adopting Smart Contracts

Well, if some new, creative and profitable thing is good for your business, then it doesn’t mean that it will not have any drawbacks as everything can’t be all alone perfect in all aspects. Same like this, the adoption of Smart Contracts can also bring a number of challenges stated below.

Human errors: Paper contracts can be easily read and rectified, but Smart Contacts are written by coders, so there may be a high chance of coding errors, and it may take time to correct them.

Confidentiality, security, and privacy: Smart contracts are scattered across all network nodes, so there may be a high chance of privacy concerns, especially when the parties’ accounts are associated with known entities.

Lack of engineering experience: In order to build Blockchain smart contracts, intelligent work and experience is required, and only cryptography experts can handle it. So lack of engineering skills can spoil smart contracts. Overcome this challenge, avail of Blockchain software development services, or hire Blockchain programmers from the best Blockchain app development company.

Legal and regulatory challenges: Smart contracts lack precise legal standing as no official government law applies to them.

The Best Blockchain Smart Contracts Platforms

Image Source: Research — AIMultiple

There are several smart contract platforms subdivided on the basis of technology, end-user means industry-type (banking, eCommerce, government, supply chain, healthcare, real estate, insurance, etc.), and region (North America, Europe, Asia, etc.).

Smart Contracts platforms are different in programming languages, blockchain consent, cost of maintaining, blockchain security, transaction verification speed, and a lot more. Considering these vital parameters here, I have mentioned the top 6 Blockchain smart contracts platforms names that you can trust blindly.

Ethereum Cardano Aeternity Qtum Waves Stellar

Wrapping Up

Smart contracts can actually improve the way of agreements that are made across diverse industries. However, it may take some time and require more development efforts before it reaches its mainstream strategy.

A non-experienced person will not be able to implement Smart Contract technology as it requires many changes and experiments at a certain time. At that moment, only a skilled Blockchain developer can perform such technical and innovative tasks, so it will be beneficial for you to hire Blockchain developers from the top-rated Blockchain development company in India.",7490
"With the quick acceptance and effect of new technological advancements, for example, IoT, AI, AR, VR, Robotics-a a significant number of us actually misconstrue the idea of Blockchain. Particularly in regards to the Mobile application development process.

Let me ask you, what do you mean by Blockchain? Digital currency, Ethereum, or cryptocurrency … all that? However, it worth searching the blockchain has an extension beyond reach.

First Understand the idea of Blockchain

A blockchain is fundamentally a result of unchangeable data which is governed by a decentralized structure i.e. informational data isn’t constrained by a solitary entity. The decentralization of informational data guarantees data security at all stages and at all platforms, and every data block is bound to one another by cryptographic standards.

“According to Statista reports, the blockchain market is required to climb a revenue development rate of over 23.3 billion US dollars in size constantly 2023”

How Blockchain can help the Mobile App Development Industry?

The fundamental fragments of the business are presently utilizing Blockchain Technology for Healthcare IT Solutions, Automotive, finance; for what reason should the mobile application development market fall behind?

In case you’re into the mobile app development industry, you would have an idea that stays updated with the most recent innovation trends to remain ahead of the competition.

Mobile app development is as yet in its beginning phase with the appropriation of blockchain, and industry influencers are reluctant about this idea. In any case, Blockchain innovation can lend some assistance by offering various secret advantages like performance, efficiency, and productivity, which also elevates the client experience.

Blockchain innovation would permit designers to make smoothed out eCommerce applications that empower purchasers and providers to trade online with no intervention or to pay outsider expenses.

Benefits of Blockchain Technology in App Development

This, but on the other hand there’s significantly more blockchain can accomplish for the mobile application development industry. We should jump further and get familiar with the top advantages:

Advantage 1: Increases Transparency

Security enhances transparency. Blockchain technology tracks each move of assets so that clients can just track them at whatever point they need. The blockchain avoided the chance of any deceitful exchange or fabricated data. The blockchain makes the application and the whole framework sealed and versatile to any false action.

Blockchain innovation can likewise help business visionaries acquire the trust of the buyer. The clients can securely execute through such applications and stay guaranteed the protection of their important information. Additionally, the idea is versatile to deal with different clients without a moment’s delay.

Advantage 2: Increases Reliability

As well as further developing data assurance, technology can extraordinarily upgrade the productivity of a mobile application. It is for the most part in light of the fact that the blockchain’s construction itself is vigorous and solid blockchain design delivers the organization complete against any disappointment or crash. Blockchain Technology likewise has a few squares that own the information in more than one spot, making blockchain safer.

The distributed plan of the blockchain servers and other related equipment forestalls any attempt at any unapproved modification of informational data. Millions are regularly accessible, because of various data centers at various areas, a tiny bit of warning. These highlights make the blockchain strategy viable and reliable for a mobile application development answer for the undertakings.

Advantage 3: Security is the foremost

The underlying advantage of progressive blockchain technology is that it makes the applications more secure. The technology can utilize the most exceptional cryptography. This innovative idea is one way that it can offer the most significant level of security with fortified encryption.

The technology gives a standard series succession or other interconnected block string. Each square has the information and timestamp for another square of exchanges. The information is put away and encoded in a cryptographic hash, which changes each inconceivable block. Presently, for every block, each area has a cryptographic route.

Advantage 4: Block Chain set you liberated from Passwords

Regardless of how amazing a secret key you’re setting for your PCs, there’s still some worry that it could get spilled. Blockchain innovation assists us with disposing of this dread. For Blockchain, the secret key to check any exchange or framework is not really required. The two parties included for bargains ordinarily make the installments through an SSL authentication. What’s more, the blockchain network audits everything while at the same time making it noticeable and available to all, consequently taking out the potential outcomes of being phony.

Advantage 5: Protection of Identity

Security is a fundamental prerequisite of blockchain technology. It is practically incomprehensible for any programmer or hacker to close down the whole framework in light of the fact that the information isn’t in a server, yet it is in each square. Blockchain offers secret key encryption with cryptography on the client’s work. The working of the blockchain guarantees that nobody can misuse any information.

Advantage 6: Blockchain has a digital laser framework

This is a clear method to see how Blockchain Development functions when you consider it. Blockchain is basically a digital ledger controlled by some broad corporate organization, supporting all parsing and information transmission. At the point when some data is changed, the change is communicated to the remainder of the machines in a similar machine and simultaneously to change the worth

Advantage 7: Provides Simplicity

Another significant benefit of blockchain innovation is its effortlessness i.e. simplicity. The blockchain has an extremely high ground over the overall models performing identical undertakings with regard to effortlessness.

On the off chance that the technology is refined, it needs more exertion, time, and money to incorporate, keep up with, and modify. In simple words, complex strategies add to expanding application development and maintenance-related expenses. The blockchain will help organizations free themselves of greater expenses by offering them an element-rich mobile application.

Advantage 8: Keep Blockchain Apps updated

Blockchain innovation is developing at an alarming speed. The cycle by which this day is developing, we can anticipate that it should bring an ever-increasing number of updates soon. The outcome will be a finished enterprise mobile app intended to address the issues of things to come, lastly, you can further develop client service with a particularly up-to-date mobile app.

The innovation behind the blockchain is additionally accessible as an open-source technological innovation. Blockchain application developers can undoubtedly share and utilize the advancement to make the application all the more impressive and secure. As it were, blockchain technology can assist with creating mobile application solutions while making application time and cost.

Advantage 9: Safeguard Digital Information That Is required by numerous clients

Encryption is a Blockchain Technology with extensive intricacy. It is perplexing to such an extent that it makes it outlandish for anybody to trick any decryption key. It fits any framework that should be introduced to various clients and yet requires verification of the data that is coordinated.

In the financial and legal circles, for instance, various marks on a single report or understanding are normal. Through blockchain, this data can be gotten to and altered all the while by various groups to speak with one another. You should simply check the Blockchain and arrange it.

Take away

Since Blockchain technology has grown tremendously in recent years, it has been explored the requirements of the actual market. We can expect a significant number of its advancements and applications to be introduced free of charge over the years as mobile application developers make it part of the worldwide application development cycle.

If in any case, you are seeking blockchain development solutions or a blockchain developer who have the art of incorporating the specialty of Blockchain. CDN Solutions Group is one of the award-winning organizations offering the best blockchain development services. We at CDN are bind to deliver quality solutions and help our customers in meeting their requirements on a pocket-friendly budget.",8812
"So far, the development of human society has experienced several information changes. From the invention of language to the emergence of the Internet, every change has undergone qualitative changes in the fields of social politics, economy, and culture.

The wheels of history are rolling in. Today, mankind has faced the seventh information revolution-the application of blockchain technology.

As a new information technology revolution, blockchain will, to a certain extent, like the Internet, change the existing production relations and business logic, stimulate global economic growth and promote the high-quality development of human social life.

From 1.0 currency to 2.0 finance, to 3.0 applications. Blockchain technology has gradually deviated from the encrypted digital currency scene and moved towards a wider range of application scenarios. These application scenarios will cover all aspects of social life.

As a new generation of blockchain system, TDOS trusted data operating system was jointly developed by the Financial Technology Research Institute of Shanghai University of Finance and Economics. And Changzhou Yongyang Information Technology Co., Ltd.

The system includes core nodes, multi-engine consensus, cryptographic components, P2P protocols, account models, virtual machine engines, multi-language SDKs, deployment tools, application development tools, browsers, operation and maintenance tools, operating system interfaces, and other components, which can be flexible Configuration. While improving production efficiency, it can better deliver value and change the production relationship of the Internet.

As the world’s first operating system level blockchain products, TDOS with the underlying technology framework to meet common industry key business requirements, not only with a full set of domestic technology, more encryption algorithms, consensus mechanisms, smart contracts, and distributed storage Independent innovation has been achieved in key technical areas such as computing, user privacy, and data security, and cross-chain interaction.

At the same time, TDOS is also equipped with a quick and visual operation interface, which greatly simplifies product deployment and improves deployment efficiency. Ensure that users can deploy a true and reliable data network in 10 minutes without the guidance of professional and technical personnel. After deployment, users can use various built-in development tools in the system to realize and implement upper-level applications.

In addition, TDOS trusted data operating system also has 5 major advantages:

Advantage 1: High performance, the flexible mechanism can deal with massive amounts of affairs;

Advantage 2: Strong security, Xinchuang compliance can guarantee information security;

Advantage 3: expandable, flexible structure can realize free configuration;

Advantage 4: Easy to supervise, directly on the chain can be connected to the supervising interface;

Advantage 5: Easy to deploy, fast deployment can be completed with standard solutions;

These advantages ensure that the TDOS trusted data operating system adapts to the needs of various scenarios, can better break the technical application barriers, and enhance the popularization of blockchain applications. Let the blockchain move from concept to actual deployment, transform technological achievements into industrial applications, empower all walks of life, and achieve a qualitative leap in productivity.

At present, the TDOS trusted data operating system is actively developing and expanding the application ecosystem. The system also plans to make new contributions in the fields of public welfare, medical care, education, social networking, government affairs, transportation, product authentication, and copyright certification.

With the landing of more application products, people’s production and lifestyle will be further changed. A programmable credit society will appear in the near future.

TDOS, link the world, make the future within reach.",4026
"Colorado lawmakers pass law to use blockchain for water management CryptoCurry Follow Jun 28 · 1 min read

Colorado lawmakers have passed a bill that serves as foundation for the study and use of blockchain technology for water management. The bill also encourages the exploration of other emerging technologies that may help improve water management in response to the worsening drought. The bill, dubbed House Bill 21–1268, passed by both chambers of the Colorado legislature is now awaiting Governor Jared Polis to sign it into law.

Image by Tumisu from Pixabay

Forty-five legislators sponsored the bill which permits and orders the University of Colorado and the Colorado Water Institute to explore blockchain and emerging technologies to improve monitoring and management of Colorado water systems. Specifically, the institutions are encouraged to use “blockchain-based documentation, communication, and authentication of data regarding water use; fulfillment of obligations under Colorado’s system of prior appropriation, including augmentation plans; and water conservation.”

Lawmakers hope that using such technologies would help reduce inefficiencies and waste in water management, as well as provide data that are more transparent and reliable. The University of Colorado is expected to provide both a written report and a live testimony on the results of their feasibility studies and pilot deployment on or before July 15, 2022.

Source: CoinGeek",1461
"Image Source: Bing Search Blockchain Images

Abstract

Blockchain systems are public electronic ledgers built around a peer-to-peer network that is publicly shared among different actors to create an unchangeable transaction record. Blockchain technology’s primary use is for distributed ledger for cryptocurrencies, notably bitcoin. Over the years, Blockchain has expanded its application area, such as smart contracts, financial services, video games, energy trading, supply chain, health care, etc. The concept of blockchain technology has been discussed extensively in different literature works, yet information regarding Blockchain-based application classification remains scarce.

This study provides a systematic review of previous studies on Blockchain technology and applications. I discussed an overview of Blockchain and the classification of blockchain-enabled applications across diverse sectors such as financial applications, education, integrity verification, governance, and data management. This research can be useful to blockchain-technology researchers and can help further studies and future development.

Keywords: Blockchain technology; Blockchain; Blockchain application; Blockchain classification

1 Introduction

Many years ago, Satoshi Nakamoto, the unknown person/group behind Bitcoin, described how blockchain technology, a distributed peer-to-peer linked-structure, could solve the arrangement of keeping transactions from having a double-spending problem (Nakamoto, 2008). Bitcoin arranges transactions and groups them in a constrained-size structure named blocks sharing the same time-stamp. The network (miners) nodes use blocks to link each other in chronological order, with every block having the hash of the previous block to create a blockchain (Crosby et al., 2016).

Blockchain technology brought about disruptions to the traditional way of doing business. The applications and transactions, which required centralized architectures and trusted third parties to verify them, can now function in a decentralized manner and with the same accuracy level. Blockchain architecture and design characteristics provide properties like transparency, robustness, audit, and security (Christidis and Devetsikiotis, 2016).

A blockchain is a distributed database organized as a list of ordered blocks, where the committed blocks are immutable. For instance, in the ideal banking sector, banks can cooperate under the same Blockchain and push their customers’ transactions. This way, beyond transparency, Blockchain enables transactions’ auditing. Organizations invest in this technology as they see the potential of making their architectures decentralized and minimizing their transaction costs as they become inherently safer, transparent, and in some cases, faster. For this reason, Blockchain technology is becoming increasingly important (Zhao et al., 2016). Almost 1000 (33%) of C-suite executives reveal that they are considering or have already been actively busy with blockchain technology. According to Christidis and Devetsikiotis, 2016 researchers and developers are already mindful of the new technology’s capabilities and investigate various applications over different divisions based on the intended readers. While there are several analyses regarding blockchain technology (Tama et al., 2017; Brand. o et al., 2018), they argue that state-of-the-art blockchain-enabled applications have received limited attention. Even in Zheng et al. (2016), blockchains’ applications have not fully covered their full extent or applicability. Some reviews focused on Blockchain’s particular role, including developing decentralized and data-driven applications for the IOT (Christidis and Devetsikiotis, 2016) and working with big data in a decentralized fashion. Other reviews concentrated on the security problems of the Blockchain and on its potential to enable trust and decentralization in in-service platforms (Seebacher et al., 2017) and Peer to peer systems (Hawlitschek et al., 2018).

However, even though there are several works of literature on Blockchain’s different aspects, there is still not enough research to show the classification of blockchain-enabled applications. Hence, by studying existing literature, I highlight the question: What are the types of Blockchain-based applications? In answering this question, I contribute towards understanding the Blockchain, its features, and taxonomy. The rest of the paper’s structure is in sections: Section 2 presents an overview of Blockchain. In section 3, I outline the method followed in conducting the systematic literature review. The findings was presented in Section 4, while in Section 5, I present the discussion on the articles reviewed. After that, I give my conclusion.",4779
"Image from devOps

Blockchain, blockchain, blockchain — everyone seems to be talking about it now. Just about two years ago, the term was meant for anyone in the crypto industry but modern blockchain has gone beyond that. A new term is “Enterprise blockchain” and companies are just looking for ways on exploring this new tech. But in case you don’t know, what really is Enterprise blockchain?

Image from freepik

What is Enterprise Blockchain?

An enterprise blockchain is more of blockchain for organizations. In a ‘normal’(public) blockchain, data and details of this data are recorded on a public, transparent ledger. Therefore, anyone with an internet connection can view this data. Just like the Bitcoin blockchain.

Seen the problem? Organizations do not always want people outside the organization to view their data and trade secrets. Therefore they adopt an ‘enterprise blockchain’ — a type of permissioned blockchain. Therefore, not just anybody can be part of the blockchain or view blockchain data, they need permission. Also, network users can choose who’s allowed to use it or not.

Types of Enterprise Blockchain

Private Blockchain: A private blockchain is one that is used usually within an organization. It is only open to members of an organization. Consortium Blockchain: This is used for organizational collaboration. Quick example. Suppose a group of organizations are operating a supply chain, but these organizations don’t trust one another. Each organization wants to monitor the progress of the goods. They don’t want it to be public but at the same time want to achieve organizational collaboration and transparency. A consortium blockchain is best for this. All organizations can track data of goods(current location, description, e. t. c) at real-time. Completely transparent, fraud free and secure!

So which major companies are currently using blockchain technology. Below is the list of the most popular

Supply chains are just one amazing use case of blockchain. Blockchain is not all about cryptocurrency. Bitcoin and Cryptocurrency is just a titbit of the world of blockchain technology. Blockchain has numerous use cases.

The World Economic Forum(WEF) estimated that by 2027, 10% of the world’s GDP will be stored on blockchains. This is a relatively new technology that will benefit organizations immensely. However, not all organizations will be benefited. Computer science researchers put forth a formal paper for determining which scenarios blockchains are best suited for. To know more about this technology beyond enterprise blockchain and even in art, music and entertainment, subscribe to my articles. Watch out for Part II!",2671
"7 Reasons Why You Need to Invest in Media Buying Technology for Your Agency ADvendio Jul 6·4 min read

The advertising industry is more competitive than it has ever been. Advertisers are always exploring methods to save money and stay up to date with industry developments. Investing in media buying technologies is one way they can accomplish just that. In this post, we’ll evaluate seven reasons why investing in media buying technology is essential. But first, let’s discuss what media buying technology is.

What is Media Buying Technology?

Media buying technology is an automated system that allows media buyers to purchase and manage digital ad campaigns. It enables automated bidding and real-time analytics, and it paved the way for programmatic advertising, which uses analytics and algorithms to deliver ads to the right target audience at the right time and at the right price. This technology has taken the manual work out of media buying for agencies and made it more strategic, allowing them to better utilize client budgets.

Why Invest in Media Buying Technology?

1. Inventory management

Unlike traditional media buying, programmatic media buying allows you to more efficiently manage your advertising inventory by purchasing media in real-time based on factors like keywords, publication location, current ratings, and audience demographics. Agencies have a full picture of the media space they’ve acquired for their clients and can handle the creation of each ad campaign efficiently with only one software package.

2. Maximum Control

With the adoption of programmatic advertising, media buying agencies have complete control of their clients’ advertising spending. Media buyers can bid on ad placements in real-time through instant auctions. When ad inventory is acquired, the media buying software automatically updates, allowing the buyer to quickly determine what other ad space they may buy for their client with their remaining advertising dollars. This allows the media buyer to more efficiently plan complete campaigns.

3. Specific Audience Targeting

With programmatic advertising, agencies may plan media buys in a more proactive manner by determining the best time and platforms for client campaigns. Agencies may acquire inventory across a range of platforms, such as mobile, desktop, or tablet, and formats, such as display and video, using programmatic media buying. They buy advertising space depending on where their target audience is most likely to be at a given time. As a result of such precise targeting, higher-quality leads are generated.

4. Real-time analytics

Media buying software provides agencies with a 360-degree view of all client campaigns in one place. This allows media buyers to adjust accordingly in real-time to increase performance success. Media buyers can reduce the risk of campaigns failing for clients by tracking performance in real-time. They can monitor performance changes and react promptly to negative ones. Clients save money as a result of this, and have more advertising dollars to give to media buyers for use in their campaigns.

5. Automated processes

Media buying agencies now have less manual labor thanks to programmatic media buying. Workflows have been simplified to give media buyers more time to develop relationships with their clients and truly understand campaign goals, allowing them to create campaigns that align with the client’s objectives. Automated processes are allowing media buyer’s to utilize their time more effectively, saving the agency money and allowing them to make smarter campaign selections with their client’s advertising dollars.

6. Enable Self-Service for Clients

Agencies now provide self-serve media buying to their client's thanks to media buying software. Clients have access to their own media campaigns and benefit from streamlined procedures, which helps agencies increase operational efficiency and streamline client communications. Clients can keep track of what their media buyers are working on and what campaigns are currently running. Along with having a clear picture of campaign expenses, budget use, and advertising design requirements.

7. Seamless Integrations

All popular ad servers and exchanges are fully integrated with media buying technologies, which helps to remove the fear of adopting a media buying technology platform for higher-level management. The media buying solution centralizes all data from many sources, making campaign data interpretation faster and easier. These integrations boost flexibility and improve the efficiency of various workflows, making it easier to acquire media from a single platform.

The Benefits of Media Buying Technology

The automation of media buying software makes it easier for media sellers and buyers to interact, allowing media buyers to purchase the media they want with less manual effort.

Programmatic media buying software is easy to set up and provides integrations with all the popular ad servers and exchanges. All you need is a computer and internet access and away you go. Programmatic media buying is a cheaper option compared to traditional media buying with real-time bidding used at open auctions.

Media buying software gives agencies more transparency, which means media buyers have a better understanding of campaigns and available inventory, and can work more effectively with all relevant data at their fingertips, resulting in more successful advertising campaigns.

Conclusion

Every day, more media buying agencies discover the benefits of media buying technology. As the digital world continues to transform how we conduct business, agencies that have yet to implement smart media buying technology will need to take the next step and begin integrating this cutting-edge technology into their operations. The benefits of media buying technology outweigh those of traditional media buying and as software companies continue to innovate, this technology is sure to advance even further in the coming years.",6013
"Well, it’s time already. We are quite opinionated about building future-ready solutions for business problems.

As a development agency, we have a unique view on technology. Our clients often come to us with a vision or an idea for a product, and they then rely on us to advise them on what’s possible. We’ve worked with hundreds of tech entrepreneurs over the years, co-creating solutions for intriguing and often challenging problems.

But for us, it’s not just about innovation in technology. We are also pretty proud of the social, environmental, and financial impact we helped create to make a world a better place.

For example, we created better automated systems for secure infrastructures with Siemplify. We also enabled expanded access to healthcare during Covid-19 pandemic with Lensbox. We even helped dramatically reduce waste in the fashion supply chain with Swatchbook.

So why join the Medium community now?

After using Medium to learn and read about our industry’s experience, we decided to join the conversation by sharing our thoughts on technology trends, collaboration and leadership for distributed global teams, and entrepreneurship.

We believe that our work with the global community of tech entrepreneurs, investors, and developers, keeps us at the forefront of technology trends. We have also seen many ideas come to fruition, succeed, or fail. We recognize that technology is just one piece of the recipe, alongside leadership, funding, team culture, market readiness, and feature roadmap. Over time, we learned a lot of related lessons that we bring in our relationships with our clients, and we want to share them with the world at large.

Finally, we believe that tech entrepreneurship has the capacity to solve some of the most urgent social and environmental issues facing our global community, and we want to enable as many entrepreneurs as possible. Even if we won’t get to build those products, we still want to see them built. That’s why we are happy to share what we learned along the way.

So join us, drop us a line at hello@fifth-llc.com, or comment below. Let’s change the world together!

About Fifth LLC.

FIFTH is an exceptional team of 35+ developers, designers and a new generation of scientists. The three co-founders with backgrounds in quantum physics, molecular physics, astrophysics, applied mathematics and artificial intelligence met at a university 9 years ago. They bonded over their shared love of technology and a keen sense of its business applications, starting a team that has built over 200 digital products to date.

We live and work in beautiful Armenia, and our clients hail from the Silicon Valley, Europe and Hong Kong. What we all have in common is the drive to elevate our clients’ business to the next level and create them loyal fans in the process. We build rich, interactive, fully integrated digital products that delight the users and are easy to update and maintain.",2943
"An On-Demand Stem Creation Platform for the Music Industry

We launched Audioshake earlier this year in the belief that the next wave in music innovation, re-imagination, and consumer engagement would come from stems — the different instrument and vocal parts of a song.

Stems are already an important part of today’s music industry — sync licensing, spatial audio, remixes, and adaptive music all rely on stems. But their value is set to grow exponentially as new experiences emerge in social apps, fitness, gaming, hardware, and other fields that allow fans to interact and engage with musical content in new ways.

But stems are not always easily available, which is where Audioshake comes in. Our best-in-class A.I. technology can take a song from any point in history and separate it into its stems, opening the song up for new uses as instrumentals, samples, and more. Since launching earlier this year, our work has been used by record companies, music publishers, music supervisors, audio engineers, and artists in commercials, documentaries, movies, podcasts, and remixes.

Today we take the next step by launching Audioshake Live, a platform that lets labels, publishers, and their partners bring our A.I. technology “in-house,” and create instrumentals and stems on demand.

Audioshake Live is easy to use. Platform subscribers simply upload their songs and choose the stems they want to create, choosing from six different stem types: bass, drums, guitar, instrumentals, vocals, and “other.” Audioshake quickly returns the selected instrument stems or a turnkey instrumental that is ready for pitching.

Subscribers can then listen to their stems in our player, or download immediately — for use in a sync pitch, to explore a creative partnership, or to explore re-mix possibilities. Here’s how it works:

This past spring, we gave a handful of record labels, music publishers, and distributors early access to Audioshake Live. First as demo testers, now as customers, we’ve been inspired to see the many ways our technology has been put to use by record labels, publishers, and music service companies like Warner Music Group, CODISCOS, Crush Music, Hipgnosis Songs Fund, Downtown Music Services, Spirit Music Group, and peermusic; distributors like CD Baby; production music companies like Audiosocket; and music supervisors like The Teenage Diplomat.

Sync Licensing — Record labels and music publishers use Audioshake Live to create turnkey instrumentals that reduce all the back-and-forth involved in preparing a sync pitch, and with older songs, open up new monetization possibilities that didn’t exist due to lack of instrumentals. Our instrumentals have been used in commercials, movies, and on podcasts like Switched on Pop and Song Exploder.

Remixes — Labels, songwriters, and producers use Audioshake Live to isolate vocals or remove drum beats from their songs in order to find new ways to breathe new life into their catalog.

Re-masters — Audio engineers at record labels include Audioshake Live as a part of their toolbox, leapfrogging some of the more tedious parts of their task, and allowing them to focus on the highly skilled parts of their work.

How to Sign Up

Audioshake Live is meant to serve the music industry, and is specifically aimed at record labels, music publishers, and others who regularly need to create or use stems for their songs. If you’re in that category, please sign up for a demo!

For those musicians, audio engineers, music supervisors, producers, and other authorized third parties who need to create stems on a one-off basis, we’ll continue to offer our services on a 1:1 basis.

Our aim is to put this technology in the hands of everyone who can benefit from it, but to do so respectfully, in partnership with the music industry, so that artists’ copyrights and creative wishes are front and center — not an afterthought, as has often been the case when technology has intersected with the music world.

If you’re in the music industry and are interested in the Audioshake Live platform, or Audioshake’s stem services more generally, please get in touch! You can find us at audioshake.ai or on Twitter @audioshakeAI.",4179
"There has never been more musical diversity than right now. We have enormous amounts of genres and sub-genres, and the combinations of those have no limit. Genre fluid bands and genre bending artists are becoming more and more expected in our music scenes. We are seeing what I consider to be the most interesting era of music history to date.

Music, as a creative endeavor and an industry, has never seen so much change in a relatively short amount of time. This generation of young music makers (the cusp of Millennials and Gen Z) are the first to grow up with democratized systems of global music distribution, an accessibility to music technology that is astounding compared to 30 years ago and the internet’s power of globalization has had profound effects on music genres and how any artist can achieve a career through music.

This is just the beginning of how advancements in digital technology and the development of new technologies (artificial intelligence, virtual reality, quantum computing, blockchain, etc…) will affect the creation process of music and new forms of music that will emerge in the near future.

Technological Accessibility

The advancement of technology has always pushed music, and the arts, forward in all eras of history. The inventions of keyboard instruments (harpsichord, piano, etc…), string instruments, concert halls, phonographs, radio, electronic instruments, the internet & DAWs — new technology placed in an artists hands will inevitably bring change to their medium.

This is no surprise to anyone but what we are seeing, within the past three decades, is technology, already developed, that allows just about anyone to write, produce, mix, distribute & market their music. Nowadays anyone with a laptop, that has some base level of sufficient software, can make a record and put it out for anyone to listen. While this is something that isn’t new to music professionals as it has been a point of discussion for over two decades, this phenomenon has never happened before and never to such an astronomical scale as we see today.

The accessibility of music software along with the development of online music distribution (CD Baby, Distrokid, Symphonic, etc…) has led to an overload of music being released today. So much music, in fact, that streaming platforms like Spotify, Apple Music and Deezer have to go through tens of thousands of music submissions a day. This is a complete democratization of the process to produce and release music that was once controlled by record labels & publishers through the barriers of high production costs and their sole ability to execute world-wide marketing and distribution.

There are pros and cons to having just about anyone to have the ability to make and distribute music. The documentary, “PressPausePlay”, dives into the possible pros and cons of technological accessibility, from a 2010’s perspective within the music & film industry (at the cusp of when streaming is going to make a profound influence in the industry).

One benefit is equal opportunity. Now, this isn’t an example of pure equal opportunity where we are all gifted a computer (or musical talent) with music software to produce music. What is real is that the financial threshold to make music today is exponentially lower than it was even 20 years ago. Apple laptops come with Garageband, you can get basic versions of Logic, Cubase & Ableton for relatively low costs. This low entry point allows more and more people to have the technological capacity to create music. Does that mean everyone is going to make amazing music? Of course not. What this does do is give talented individuals who will make amazing music an easier time to start creating at an early age. You are now seeing more and more young artists who had their start producing music within a DAW before they were even teenagers. Any aspiring artist, now, in their late teens have grown up with all this technology available to them. This means that any kid can begin the process of learning music production and developing their craft decades ahead compared to anyone who grew up in the 80's.

“When I was 7 years old I got musical piece of software called Cubase.” — Jacob Collier, 27 year old, 4 time Grammy Award Winning Multi-Instrumentalist, Composer & Producer.

The Internet and Globalization

With the development of the internet and creation of platforms like Youtube, iTunes, Spotify & Pandora; a new generation of artists, that also grew up with the development of accessible music technology, have also had the access to learn and be influenced by music and culture from any part of the world.

In my teenage years, when my music journey began, I would use Youtube to fulfill my obsession with music discovery. Where else could I find a live recording of Nina Simone from the 70’s, a taiko ensemble from Japan, Rachmaninoff’s 2nd Piano Concerto or a Led Zeppelin concert? You can argue that this was all accessible with TV and records stores or even iTunes and CDs — but the flexibility, both with the convenience of time and money, the internet provided has given people the power and liberty to discover as much music as their hearts desired. This has a profound effect in the upbringing of musical minds who want to create their own music.

The globalization power of the internet has brought up a breed of music creators that have been naturally influenced by music & cultures from different parts of the world. We have already seen the first generation of music producers influenced by the internet, and musical diversity has thrived in the 2010s because of it. The internet has also allowed these same musicians to grow an audience from anywhere. Especially artists within genres and styles that, 20–30 years ago, would have little chance at having a music career. Now, it almost doesn’t matter where you are as long as you make music, and a artist brand, that can captivate an audience.

A boy from England can be influenced by jazz harmony, RnB, African rhythms, pop music & choral music at a young age (Jacob Collier). A band that merges RnB, jazz, soul, hip hop, rock and indigenous rhythms can come from Melbourne, Australia (Hiatus Kaiyote). A band that fuses together folk, indie, rock and electronic music can become a sensation (Bon Iver). Some of the most compelling and modern, electronically infused, jazz music is coming out of London right now with the melting pot of cultures building that scene (Space Jazz Playlist). Genre fluidity is normal now and the examples are endless!

This globalization has resulted in a sort of “kaleidoscope” of musical styles to emerge in which multiple genres are directly influencing the sound of one artist. There are more styles of music being produced right now than ever before and the combinations of genres & cultures will only continue to expand. New intersectional cultures will also be created through the creation of new genres and styles. As someone who loves discovering music, this genre fluidity is a renaissance of musical creativity and an exciting time to be a musician.

Forward to the World of Tomorrow

This article may seem like a retrospective on “old news” but remember that everything mentioned so far has only happened in the past 20 years! The music industry, an industry that’s not even 100 years old, has changed faster than anyone could’ve anticipate. With the introduction of the internet, smart phones, social media & streaming this industry has seen a massive transformation since the early 2000’s. Partially due to the fact that computer technology has been developing at an exponential rate unlike any other industry in human history. There are not many industries where you can have a product’s efficiency, computing power, increase by 100% every few years. Yet, here we are with USB sticks that can hold 1 terabit of data when, over 20 years ago, I was inserting 1.44 megabit floppy disks at my elementary schools computer.

Now, with the development of artificial intelligence, virtual reality, augmented reality, blockchain technology & quantum computing we will most likely see the rate of progress and change keep growing. This technology will have profound effects on business infrastructures, consumption of media and arts, the way we create art and even how we will define work itself in the future. We are at the intersection of transferring into a “Digital Exponential” age, where the internet has long established itself as a normal aspect of everyday life (already is) and we will find new identity in a hybrid world of machine intelligence aiding our human limitations.

“One thing is for certain, though, in the Digital Exponential world: Any technology must be inextricably linked with human purpose — with a great story that uplifts the entire user experience — or it will be doomed to fast obsolescence.” — David Sable

How will music be consumed in 2050? What kind of new music will be created as a result of the integration of new technologies? How will we value the arts and the humans that make it?

If the our access to music technologies and the internet has brought about all the artistic creativity & diversity we are currently seeing now. How will all this new technology influence the trajectory of music within the rest of this century?

This is why, I believe, we are currently in the most interesting era of music history and it’s just getting started.",9414
"Photo by Jason Dent on Unsplash

Combining future tech and lessons from the past

History teaches us that nothing stands still. Technology evolves at an ever-increasing rate and Nationwide are constantly evaluating both the benefits and threats that evolution brings. But it’s not just technology that changes; Regimes change, geo-political alliances shift and this sometimes happens rapidly and with far-reaching consequences for individuals and organizations.

In the world of cloud storage, global infrastructures and the use of 3rd party companies to store our data, how do we face into this and ensure whatever the future holds, our members can be sure their information is safe & secure?

Data security in the age of cloud

Building Societies and Banks are not typically the organizations that come to mind when talking about new technologies and approaches. But look behind the scenes and there are large numbers of people, teams, projects and programmes tasked with understanding the opportunities and threats the future will bring. Nationwide is no exception and amongst these resources is the Engineering Proof of Concept Team; Software engineers dedicated to trialing new methods, architectures and technologies. These kinds of teams are crucial to ensuring customer experience is continuously improved and data security constantly evolves.

Recently, this team has applied its focus to the challenge of data security in the age of Cloud and how Nationwide could maintain a secure, robust vault of its data. The outcome sought by the project has been to prove that cloud-based storage can be implemented such that it is resistant to state actor level attack and does not rely on the stability of a single Cloud provider’s enterprise.

Photo by Alina Grubnyak on Unsplash

Embracing decentralization

One answer to how we embed resilience into data storage is to consider the “Don’t put your eggs in one basket” idiom. We have come to expect that the major players in Cloud Services offer incredibly resilient platforms in which to store our information. And indeed, they do; A well architected solution can leverage extraordinary availability and recovery capabilities from such vendors. We expect the technology to be resilient, we expect the platforms and the data within them to be available when we need them. We expect the data stored to be managed as per our implemented design and not used for any other purpose. And we expect this data never to be shared or utilized without our permission.

But expectations depend on a status quo — an understood and accepted set of circumstances and parameters. Nationwide wanted the Proof-of-Concept team to challenge those and combine that thinking with more traditional approaches to security. We wanted to think about if a regime shift impacted the availability or the trust of these services, or if a vendor was somehow compromised.

A logical area for focus when looking for more baskets in which to place our proverbial eggs, was to look at decentralization. To utilize multiple storage nodes that had no one dependency on any one particular vendor or platform appeared to fit the brief but came with its own set of questions and concerns, not least of which were around how it would be securely implemented.

Patented security design

The key to addressing security concerns for this kind of decentralized approach was a patented security model created within Nationwide’s Application Security team. By incorporating this model into the core of the prototype, the engineers were able to build capability that decomposed data into scrambled sub-block levels. This is then combined with powerful encryption and distributed using blockchain inspired techniques. As a result, the data payloads sent to the decentralized nodes are incredibly resilient to compromise or attack.

This amalgamation of techniques allowed secure distribution of data across multiple nodes that included major cloud service providers as well as community cloud assets such as employee laptops. This provided flexibility in where the data was hosted but the design also needed to take into account the challenges that come with a decentralized model.

To address these, innovative solutions were embedded into the application that monitored the health of each node and self-heal the network if one were identified as being retired or faulty. In addition, techniques are used whereby the node is identified as being at high confidence of compromise with actions being taken to remove it.

Crucially, with this type of design we must remember that nodes are outside of our physical control. So, what would happen if one such node was compromised? Yes, the application has the ability to detect this and remove it from the network but what if a bad actor were in possession of that node? They then have unlimited time to attempt to break the encryption in place.

With the prototype, this scenario was baked into the design from the outset. The method in which data is deconstructed, reconstructed and distributed means that no one node is capable of reconstructing any meaningful data even if the encryption layer is broken. Defence against the potential collusion of multiple “rogue” nodes is also addressed and further, sophisticated sampling techniques are employed to ensure regular verification of data stored within the nodes.

This combination of tools and techniques provides storage that is not just flexible, economical and resilient to technical failure but also resilient to geographical outages or compromise and able to withstand state actor grade attacks. Additionally, it allows for data storage across multiple cloud service providers, none of which ever hold enough information to reconstruct the original data.

Progress so far

The project is now in its second phase, with the first successfully demonstrating the core components were able to implement the design to distribute and reconstruct the encrypted blocks of data across multiple nodes made up of Windows10 devices. Performance testing showed overhead on the devices themselves was minimal, but overall performance for completing the data distribution was disappointing and not seen to be something that could scale in its current form to a production-grade capability. Whilst a number of variables affected this performance, the primary contributing factor was found to be the underlying Blockchain framework implemented. This opensource framework provided a good amount of the distribution “plumbing” and allowed for rapid progress and proving of core concepts, but it became obvious that performance at scale was not achievable.

Phase two is re-engineering the Blockchain elements and has shifted focus from community cloud assets (i.e., The Windows 10 devices) to storing data exclusively in Cloud platforms; Specifically, AWS and Azure. As a third provider would be utilized within a productionised version to protect against provider collusion, a Google Cloud Platform instance is mimicked as a separate instance on the AWS platform.

Go has been adopted as the primary development language with which an event-driven, microservice architecture is being implemented. MongoDB instances and Kafka queues are utilized for transient storage and processing of the source data being transformed, encrypted, and distributed. For the purposes of the proof-of-concept, processing for the data transformation and distribution is provisioned by EC2 instances within the AWS environment, with resulting data blocks being stored across resilient AWS S3 and Azure Blob instances.

Phase two distribution and retrieval is currently scheduled to complete around early December this year.

What does the future hold?

2020 was the busiest year on record for cyber-attacks against UK firms with 2021 looking likely to be even busier still. Hacking attempts have surged by twenty percent with bad actors taking advantage of factors such as COVID-19 and remote working. It is more important now than ever to ensure constant improvement and evolution of the tools and techniques to protect our society’s data.

The work the Proof-of-Concept team are doing is just one of many initiatives both ongoing and planned that will keep Nationwide’s members safe from an ever-increasing threat of cybercrime. Something that sadly, we can say with some conviction won’t be changing anytime soon.",8383
"Q: You have been at Spark for almost 5 years, which is a long time! What has kept you dedicated to staying with, and contributing, to the growth of Spark?

There are a few things that have kept me around for so long. Most importantly it’s the team around me. From less than 10 people when I first started to more than 20 now, the team dynamic hasn’t changed.

We really are a bunch of passionate real estate enthusiasts trying to advance our market! Next would be the product itself. Spark was good back in the day but it’s great now and only continues to get better with time!

Q: As the Director of the Customer Success Department, you and your team have a very important role in shaping our clients’ project launch strategies from behind the scenes — from your perspective, what is one thing that every project marketing and sales program needs to do in order to be successful?

Leverage the tools they have around them!

A lot of problems or frustrations can be solved with technology. Are you using digital displays? Is the availability list on your website live? Are you logging your damn interactions when they happen or waiting until the end of the day and forgetting about your wonderful meeting with Nicki? Are you using Digital Contracts or are you still having people sign 100+ pages with your crappy bic pen?

I have personally seen teams leveraging technology sell 200+ units in one weekend. I promise you, the drinks after a weekend like that have a different taste to them.

Q: In the last few years there has been a significant shift in client demand for technology enabled processes. Can you tell us about the biggest shift?

Absolutely there has been. The number of meetings I have had over the past couple years that have been titled “Company XYZ — Going Digital” is quite comedic.

People have been craving the ability to take any traditional process into the 21st century with technology. Digital Contracts have been a game changer, ID Verification is buttoning things up in terms of compliance — even Custom Reporting has been a huge focus for our partners.

Q: What’s the most valuable feature or work flow that clients need to be using to get the most out of Spark?

Digital Contracts, without a doubt. Everyone should be using them, full stop.

Q: You are one person on the team who has had significant impact in driving Spark’s new feature initiatives — Can you tell us about a new feature release that provides the largest value-add to Spark users and clients?

There have been a lot over the years.

ID Verification has been a big one this year though. Partnering with a trusted company like Jumio to offer our teams the ability to know with certainty who is buying from them has been a breakthrough for those who have used it to date.

Q: You were Spark’s first, and at one point, only Customer Success team member. What has it been like to see the company, and more specifically the Customer Success team grow to where it is today?

I sure was! What a rollercoaster that was… Thanks Sim. It’s been really fun. Challenging, but fun.

Starting out where it was just me in Customer Success to now having a team of smart, hardworking, dedicated people around me has allowed me to think bigger picture on how we can give our clients the best experience possible. It has also allowed me to hang up some hats and put others on (here’s to all you multiple hat wearers out there!). But now, instead of hounding people down for payments, I get to work with our Product & Development team to plan out what Spark is going to be in the future.

Q: What is a current or upcoming project that your team has onboarded that you’re excited about?

It’s hard not to be blown away by Central Park Tower!

At 1,550 feet tall not only is it the tallest residential tower in the world but it is the definition of elevated luxury, from the absolutely stunning floor plans — particularly the casual 3 storey penthouse — to the amenities and views, I certainly wouldn’t mind living there, it is definitely a special project.

In addition to this, besides being an engineering marvel amongst an incredible array of supertalls on Billionaires Row, the Stacking Air Rights process they had to go through is really cool. Essentially buying the air right from buildings beside them, and stacking them on top of each other is what allowed them to build as high as they did.

Q: Outside of Spark, what other trends in the Proptech industry are you seeing and how are they pushing the New Development industry forward?

No idea lol I work for Spark. Spark is life.

Q: At Spark, our staff’s dogs are considered team members. While most people seemed to pick up crocheting or sourdough making during lockdown, you adopted a dog! Can you tell us about Theo?

This question alone could take more than 5 min to answer… Let me try and keep it short!

Theo is a one year old rescue mutt from Bella Bella, BC. According to his DNA test, he is a mix of 20 breeds including Staffordshire Terrier (21%), Alaskan Malamute (18%) and even Mexican Street Dog (2%). He is sometimes loving, sometimes crazy, always a lot of work and I wouldn’t change a thing!

I didn’t know being a protective dog dad was a thing… I sure know now.",5212
"Image this scenario: you’re out on a road trip with friends, with plans to camp out and hike up some trails. Sounds fun! Everyone brings their own supplies, plus portable battery packs of varying capacity and design to keep things like lights, speakers, and other creature comforts charged. But, after about a day of happy hiking, just about everyone else’s batteries have already been depleted. What do you do now? Use a gas generator? I would hope not.

What you really should have had was a solar-powered portable generator. Or, as the claim for this product in question classifies it: “a lithium portable power station for outdoor enthusiasts.”

Yes, it is a hefty twenty-two pound battery, but the main selling point here is you can fully recharge it with two sets of solar panels in eight hours, under direct sunlight. Or, if it’s a cloudy day, over A/C power in seven hours or a car charge in 12 hours. Plus, and you can power a Dyson vacuum, TV, and laptop all at once, with ports to spare for smartphones and gadgets.

Price as Reviewed: $1499 for Explorer 1000W, x2 SolarSaga portable solar panels.

Specs

Capacity: 1002Wh (21.6V, 46.Ah) with 2000W surge protection

Weighs 22 lbs (10 kg)

Measures 13.1 x 9.2 x 11.1 in (33.3 x 23.3 x 28.3 cm)

Recharge Times: 7 hours on AC, 8 hours on Two SolarSaga 100W panels, or 14 hours on 12V car adaptor.

Has AC, DC, USB-C, USB-A, and Quick Charge outputs.

Operating Temp: 14–104F (-10–40℃)

24 month warranty

Using It",1473
"What’s the solution?

I don’t think we should accept this as the status quo. The good news is, it doesn’t have to be. By and large, I‘ve avoided almost all of these problems, and I believe that by following two proven principles you can too.

(1) Common Goals and (2) Communication

Yeah, it’s kind of that easy. But don’t think I’ve presented these here as empty platitudes, let me explain how you can actually leverage these.

But first, let’s get this out of the way: does UX or PM have the final say in product decisions? Simply: it doesn’t matter.

This is the wrong question. It’s a distraction. An immature inquiry to settle petty squabbles. It wrongly assumes that any one role is equipped with the definitively correct answers.

“The data should win!”, you say. Sure, I believe that is generally true, but data is too easily tainted by interpretation. Especially in discovery stages. Sometimes you just have to make a decision, ship it, and see how it performs.

So, it’s NOT about answering this question and more about having a common understanding of where we want to be as a team and then doing our best to get there together.

Common Goals

What should a product team’s common goal be? Easy: the business must win.

It’s almost that simple. All internal disagreements aside, the company for which you are building products must succeed. It’s futile to argue if UX or PM wins if the company loses.

A close second is: the team must deliver.

Only team’s that deliver value can justify their existence. If a team isn’t contributing value, it’s simply a liability.

Further, it’s not just one discipline that is responsible for delivering value, it rests on the shoulders of the whole team! Even if one role is performing perfectly, it still takes the rest of the team for that person to actually deliver value to the business.

So what do you do when two members of the team have strongly held opposing views? It may seem a simplistic answer, but there really is no other way: give and take.

“But what about the user?!”, a Designer cries. “What about our business needs?!”, a Product Manager wails.

Look, you’re both smart people. Your organization is trusting you to do what is best. So do your best. Together.

This is where the give and take comes in. The truth is, many products launch with less-than-ideal user experiences and the world hasn’t ended. Sometimes a product is behind schedule or may not have every feature you’d like and somehow the company goes on. It’s up to each role to make concessions where it can. Give here, take there. And if you are both doing it with the common goal of winning, everything will be fine.

Remember: a team is only as good as what it delivers together.

A product with great UX is worthless if it never makes it to market.

A product shipped on time might be full of bugs.

A product with elegant code may fail to generate revenue.

If you want to utilize the principle of Common Goals on your product team, keep these things in mind:

The best product is usually a released product!

product! Defer to the domain expert when feasible, but focus on team wins.

There’s more than one acceptable way to build a product, find one that helps the company win.

Communication

Having common goals points to where you want to go; communication is how you get there. We’ve already established that we have the same goal, this principle is about communicating in such a way that the team actually gets there.

I’m not going to wax-poetic on this one, let’s just get to it:

The Designer and Product Manager should be involved in all discovery efforts.

This is just good practice. Do it, you’ll thank me. People see the same things differently and having an extra set of eyes on the problem will pay dividends. You need more than one subject matter expert and nothing beats first-hand knowledge. As a PM or UX, don’t think that you can just explain things after the fact. It’s harder, takes more time, and is less reliable.

Even when you are in the same discovery sessions it’s helpful to have post-discovery debriefs to check for converging/diverging interpretations on what you just observed. Once you check for understanding you can decide on next steps and get to work.

This applies to face-to-face discovery sessions and even short one-liner chat questions (just make it a group chat). If one of you discovers something asynchronously, do your best to let the other know what you learned.

This is key: if this type of communication and collaboration is happening in discovery it’s very likely that you can (and you should) extend this to all other aspects of the product lifecycle: design, testing, grooming, and even development. PMs if you hear something about business strategy let your designer know. UXers, if you have a few design ideas you are considering, get them in front of your PM. If there is a clarifying question from development about requirements, discuss it together.

Learn to rely on your team-mate’s help and expertise. I’ve never regretted being fully integrated with a Product Manager. I’ve lost track of how many times I’ve had to rely on my PM’s memory to answer tough questions.",5162
"We’ve lacked words to name the profound change of the last decade. Creator Tattooing has nothing in common with my parents’ views or its bizarre reflection in most media. Even for enthusiasts like me, it’s been hard to define what category of tattooing hooked me six years ago.

Today, Tattoo Creators have gone from emergent to highly in-demand. I researched, met many of them, listened to their fans, and eventually started becoming one myself.

There’s still a lot to figure out. I write so we start creating a shared understanding. Only then can we get closer to a world where mutually rewarding tattooing has become the norm.

Tattoo Creators are taking over the tattooing category

TL;DR

Tattoo Creators empower their clients to get tattooed by someone they relate to. They offer mutually rewarding experiences end-to-end and develop unique styles.

They offer mutually rewarding experiences end-to-end and develop unique styles. While born offline, Tattoo Creators grow online . But they’re neither influencers nor digital entrepreneurs.

. But they’re neither influencers nor digital entrepreneurs. Tattooing isn’t a monolithic culture ; Creators each bring their own values. They turn competition into mutually beneficial support.

; Creators each bring their own values. They turn competition into mutually beneficial support. There aren’t nearly enough Tattoo Creators. But entry and growth are still hard. Only now do we realize growing imbalances and informal behaviors.

How Creators are redefining tattooing for good

By Cottonbro via Pexel

1. Tattoo Creators empower their clients to get tattooed by someone they relate to

Anyone can become a tattoo creator and resonate with a community that starts from their close circles first.

Tattoo creators have a specific combination of personality, values, professional and personal backgrounds that make them unique, beyond their style.

They may come from design, illustration, or simply they draw as a hobby. Newcomers to tattooing needn’t relate to conventional tattoo culture to delight their local clients.

The more tattoo creators emerge, the likelier we find one who we truly connect with.

2. Tattoo Creators develop unique styles

There’s room for client input but also recognizable as their work.

Tattoo creators develop their unique styles and the design load is mostly on them — whether they offer flashes or projects.

In the flash model, in particular, clients needn’t give any design input apart from placement and size. A creator will drop pre-designed motifs through their Instagram stories or posts. Then their client will choose the one they love most.

But creators may also work on projects, which are tattoos that incorporate their client’s input. They will then interpret the ideas in their own style.

3. Tattoo Creators offer mutually rewarding experiences end-to-end

When their clients request their work, tattoo creators know the entire experience matters.

As a client, I want to get tattooed by a creator so that I keep defining myself, make my own choices, or connect meaningfully. But really, who knows? By getting creator tattoos, I progress on important goals that have me do it again and again throughout my life.

Tattoo creators design defining moments from booking to aftercare. They connect with their clients, make them known, and do their utmost to foster a moment of truth. They know the value they create goes way beyond the tattoo itself.

4. While born offline, Tattoo Creators grow online

They have sharply different growth trajectories and unit economics than offline tattooers used to have.

Typically, their online incarnation is through prints and branded merchandise. Tattoo creators have as many revenue diversification options as other visual creators.

Meanwhile, in the offline world, tattoo creators can either set up a personalized studio or keep on traveling and meeting their local fans. Generating sustainable revenue in the tangible world eases the pressure to win the Internet superstar game.

The very nature of their work makes it scarce and all the more desirable. Just like live musicians, they tour. Conversely, fans may come a long way to get tattooed by them.

Now non-fungible tokens (NFTs) will enable clients to buy the exclusive right to a creator tattoo design. I’m a big believer in use cases beyond the category’s established superstars.

5. But Tattoo Creators are neither influencers nor digital entrepreneurs

Their core work happens offline, with tangible constraints.

It requires more than the adaptation of the general digital creator playbook for the new category to sustainably endure in the long run.

Communication and distribution once were small parts of a tattooer’s work, they’ve become essential, if not table stakes.

Meanwhile, time spent tattooing and meeting the offline world standards hasn’t decreased.

6. Tattooing isn’t a monolithic culture; Creators each bring their own values

Tattoo creators don’t necessarily relate to tattooing’s canonical culture.

They infuse their own values, personalities, and worldviews into their work.

Creators keep pushing the creativity frontier as soon as they hone their skills. They may revisit classical tattooing motifs too, but usually not without a pinch of irony.

Tattoo creators pay allegiance to what technic mastery allows them to create; not to technic mastery for itself.

7. Tattoo Creators turn competition into mutually beneficial support

Truly differentiated, they know they don’t take away clients from their peers.

Some established tattooers may believe newcomers are stealing clients from them. There is some zero-sum thinking to this notion.

Newcomers most likely appeal to people that would never have gotten a tattoo from offline-first tattooers. In fact, as they come from diverse backgrounds, creators’ personalities and styles have new clients relate to their work and to tattooing in general.

The total number of tattooing clients keeps growing, mainly driven by the growing number of creators one may end up relating to.

8. There aren’t nearly enough Tattoo Creators

Both superstars and long-tail creators attract more demand than they are able to physically serve.

Depending on whether they’re full or part-time, tattoo creators tend to tattoo between two and four clients a day.

When browsing Instagram, we get fooled into thinking there’s an explosion in tattoo creators. Well, that doesn’t say much about how available the creators are. Most often than not, it’s hard to get a booking.

9. But for many Creators, entry and growth are still hard

Youtube and Amazon removed important barriers to entry into tattooing.

Tutorials and access to tattoo supply enabled pioneer creators with diverse creative backgrounds to step into tattooing. Tattooing has grown more diverse and inclusive every day since.

However, mentorship from a traditional apprenticeship remains hard to get, and outcomes feel uncertain. For many creators, informal solo practical training is the only route. Newcomers face practical and ethical issues, besides longer feedback cycles.

Today, tattoo creators in their early days may replace mentor-feedback with peer feedback, or even solely client feedback. The situation ends up suboptimal for everyone.

10. Only now do we realize growing imbalances and informal behaviors

Imbalances between scarce supply and fast-growing demand result in individual opportunities and emergent informal behaviors like tattooing from home.

High transaction costs: as a client, finding an artist and ensuring mutual availability takes somewhere between two weeks and several months, with risks of dropping out in the middle of the process for both parties.

Congestion: difficult inbound management for creators and month-long waiting lines for clients due to too much demand

Participants’ incentives: in a context of high-demand and difficult access to practical education, creators tattoo at home in their early days

Given the number of tattoos done every day, it’s a problem for all of us that creators aren’t armed to get started with the right material conditions and people to get support from.

Up, and to the right

Let’s connect, join forces, and help each other thrive

Wanna connect with fellow passionate tattoo creators?

👉📩 Say Hi! pauline@flashh.co ✨

psst: Keep exploring creator tattooing

Twitter | Substack | Medium | (my tattoo project) @paghuo",8417
"Node.js vs .NET Core the winner?

This is a simple walk-through for anyone to get an idea about what's happening in these 2 leading technologies. I was wondering about the same issue and wanted to find out if there is a real winner

One thing I noted was that the .NET core is backed up by Microsoft in a very positive way. This is really good and with the Microsoft tag in them, it has created a professional-level programming environment with supporting tools like Visual Studio. Visual Studio has made it easy in building, debugging, and publishing the .NET C# code over multiple platforms and devices.

Introduction

.NET Core

.NET is an open-source software development framework that was initially owned by Microsoft and then released to the open-source community. Initially, it had 2 branches like the .net framework which runs only on Windows, and the .net core which is cross-platform. But from .net core version 3, Microsoft stopped the earlier .net framework support. All the future releases are only .net Core versions and they are rebranded as .Net. NET Core is famous for building modern cloud-enabled, Internet-connected apps.

Node.js

Node.js is an open-source runtime environment to develop fast and scalable network applications. Node js is built on chromes javascript runtime which is called the V8 engine. Node js is highly preferable in data-intensive real-time applications as it uses an event-driven, non-blocking I/O model. Node js is not much preferred in developing process efficient applications such as video and audio processing.

Processing Model Difference

Node.js

Node.js runs on a single thread so it uses fewer resources than traditional process request methods. What happens inside node.js request handling is it takes a request and if it can be processed immediately, sends the response. If it’s a time-consuming API call or long-running job pass it to the event loop and continue to the next incoming request. After the event loop has processed the request, the response is sent back using a callback function.

.Net core

Someone may think that the .NET core may be using only the old blocking thread pool method to handle requests, That's what I thought at the beginning. But .Net core provides excellent support for using asynchronous programming patterns.

An example DbContext.SaveChanges is a Synchronous(blocking) method of saving to the database while DbContext.SaveChangesAsync can be used to save data Asynchronously.

Net core also has an I/O completion method to handle asynchronous requests where the allocated threads are not waiting until the response arrives. NET core can also handle heavy resource-consuming requests like encoding and decoding videos and audio. .NET core gives a good competition when it comes to handling concurrent tasks. But as I said node.js loses out to .NET when requests become CPU intensive.

Performance

Photo by Luke Chesser on Unsplash

When it comes to performance we have to be really careful about the use cases which is used in measuring the performance. What we have to understand is that being Fast and handling Load is two different scenarios. Like that performance varies in each use case.

A Server can be fast when processing a single request per second while it gets slow when handling 500 requests per second(Load).

Node js is popular for handling load. Because .NET dedicates a single thread for each request from the thread pool. When it gets out of the threads, requests get qued up. But if the requests are non-blocking It won’t be a problem again. So you can see the type of requests also affects the performance.

Not only that the type of web page it renders also matter. For rendering static pages .Net core wins probably because of its in-built IIS server. IIS uses kernel-mode caching, which means that requests requesting the static page are not even going to get out of the kernel

from the .net core side, it can handle highly CPU-intensive tasks better than node.js. And .Net core is highly pushing its limits on each release. With its newest release .Net core gRPC server performance is above other leading languages like Java, Go, and C++.

.NET Core

I came across a kind of benchmarking website which was used by different YouTubers and even mentioned in the official .NET core presentations by Microsoft. So I got the below benchmarking charts from Techempower.com

Bench Marks from Techempower.com

Best plaintext responses per second, Dell R440 Xeon Gold + 10 GbE(8 tests)

These were simple requests to get Hello World text responses.

Latency

Remember that lower latency is better. Even though the graphs show a huge difference these are in milliseconds. And will not be much effective unless you are building projects with a really high volume of requests

Processing Single Queries

A single query request was sent to get the response.

Requirements summary

In this test, each request is processed by fetching a single row from a simple database table. That row is then serialized as a JSON response.

Example Response

Best database-access responses per second, single query, Dell R440 Xeon Gold + 10 GbE(26 tests)

Latency of database-access responses, Dell R440 Xeon Gold + 10 GbE

Here Node js has the lowest latency but it's in milliseconds which is negotiable.

Processing Multiple Queries

Requirements summary

In this test, each request is processed by fetching multiple rows from a simple database table and serializing these rows as a JSON response. The test is run multiple times: testing 1, 5, 10, 15, and 20 queries per request. All tests are run at 512 concurrency.

Example Response for 10 Queries

Responses per second at 20 queries per request, Dell R440 Xeon Gold + 10 GbE(25 tests)

Multiple Queries

Multiple Querries Latency

Multiple Querries Latency

With the techempower.com performance charts .Net core shows a clear high performance in various aspects. I did the same tests changing the physical DB to cloud DB and there was no difference. Net Core showed higher performance.

Above is a benchmark performance that was taken from the .Net 6 introductory video on the Microsoft Developer youtube channel. With new updates .Net is more optimized with increased performance.

Community support

Photo by Annie Spratt on Unsplash

When it comes to Community support both of them have active and strong community support. Node.js was on GitHub from the beginning so it has strong community support in GitHub. While .Net Core has huge community support in Stack Overflow.

dotnet Youtube Channel

It seems that .NET core is booming with its community support as well. Official Dotnet youtube channel uploads valuable content daily engaging the community in a positive way.

Platform Support

Photo by Amy Hirschi on Unsplash

Node.js runs on multiple platforms from the start. That was a reason it got more popular. It officially supports Windows, Linux, macOS, SmartOS, and IBM.

.NET core had a separate approach where it was initially built only to work on Windows platforms. But from 2018 it went through a massive revolution and from NET core 3.1 version runs on Windows, Linux, and macOS.

Node.js Use Cases

Node.js is better at handling multiple tasks simultaneously due to its single-threaded event loop. Node.js was designed as an alternative to Apache HTTP Server. Node.js is based on javascript and developers can use a single language to write both front end and backend.

Developers can use pre-written code from the node package manager(npm) and use the modules for quick and efficient programming.

Node.js can handle a high amount of traffic with stable requirements without needing to increase the server capacity. Node.js tends to handle all its requests in an asynchronous manner with its highspeed V8 engine and single-threaded asynchronous model.

Advantages Of Using Node.js

Can use a single language JavaScript to perform both front end backend development

Node is cross-platform and asynchronous by origin.

Support of NPM package manager to use external modules

Code modularity causes less dependency inside the project and exported modules can be used in multiple projects without any side effects on existing code.

.Net Core Use Cases

.NET core is based on object-oriented programming language C#. It also can be used to build projects with F# language. The .NET core ecosystem provides prebuilt class libraries including both simple and complex data structures. Net core also provides specific libraries for database access, security, encryption, and database access. These libraries can be used in graphic rendering projects database interactions and XML manipulations.

.NET core unlocked the possibility of cross-platform application development as well. Net core can be used within a vast variety of development requirements such as Desktop, Web, Cloud, Mobile, Gaming, IoT, and AI.

Due to its modular and scalable nature, .NET can be used to build microservices. so .NET can be used to build highly scalable systems with microservices to give an efficient user experience.

Office Add-in Development

Most of the companies and enterprises use Ms office or office 365 in their work. Developing office-addins to extend the ms office capability can be done using .Net.

Advantages of Using .Net Core

Possibility of separation of concerns

Reduced coding time with inbuilt libraries

Security

Supportive coding environment and tooling with Visual Studio

Cross-Platform support

Development and deployment across multiple platforms Cloud, IoT, Desktop, etc.

Popular companies that shifted to Node.js

PayPal, Netflix, Uber, LinkedIn, eBay

Popular companies that shifted to .NET Core

Chipotle, UPS, GoDaddy, Asgard Systems, Siemens Healthineers

Conclusion

Throughout the exploration even though most of the benchmarking results were towards .Net Core. Things may depend on the requirements of the project, hosting requirements as well. But I noted that .Net core has started to move forward passing node with the support of Microsoft and its enterprise-level influence. So I think .net core is taking the lead with all these benchmarks and with cloud support.

And Node.js is written on top of the V8 which again uses C++. So node has another layer below which includes extra work when compiling the code. But .NET core creates a .dll which is in a way closer to the machine-readable side. Because In the end, all that matters is how fast that the code gets compiled.

Please share your opinion as well!! Seems the comment section is going hot :D",10537
"I never had technology as my main focus of work, even though I have been working on it for almost a decade, I was never really aware of the great transformation that it implies for society. Only in the last couple of years, I have been able to see the bigger picture by putting all the energies to be an entrepreneur.

I think the exponential growth that technology has had in the last decade caught us all by surprise, and it has never been easy to adopt this changes since it implies restructuring the way you think; however, people that want to make a change in the world for good is embracing it and moving forward to a future where technology helps humankind to live better. Furthermore, this crazy idea that I started with a group of friends called Trascender Global is also surfing that wave.

Today, in a more-than-a-year of a pandemic and big changes in our lives, I want to talk about how a small idea can seize the technological opportunities to bring innovation in three big fronts: the redefinition of the work concept, the creation of opportunities in a region that desperately needs them and the empowering of people to entrepreneur and venture against all uncertainty.

Redefining work and teleworking

Teleworking has been in everyone’s mouth during this pandemic, and it has been one of the biggest challenges companies have had to endure. However, technology has once again come to the rescue: videoconferencing platforms, remote learning, online schedulers, task managers, automated to-do lists, and all of the tools we take for granted now had allowed us not just to survive, but to maintain steady growth.

However, for a Freelance, this is nothing new. On the contrary, this has been their day to day lifestyle for years: a freelance is used to do home-office, balance a productive work schedule with a healthy personal life, accommodate to tight deadlines and sudden work changes, constantly “reinvent” themselves with new courses that complement their skills and build a multidisciplinary contact network.

This type of philosophy and lifestyle is what Trascender is based on: even if we’re not physically close, or have never met beyond a screen, we can still communicate effectively, reach our targets, deliver projects, and build fruitful relationships with both our customers and our crew of Trascendentales.

Bringing innovative opportunities

Colombia and Latin America are full of splendid talented people that due to the economic and social difficulties never get to work in what they envision or dream of; also many of the tech roles are not equally paid when compared to the earnings in developed countries. Our situation is complex, and further than explaining or trying to justify that situation, we focus on how we can improve it and provide better opportunities to people in our region.

Trascender Global focuses on working with clients in a B2B model, creating solutions in the Data Science industry, which go from analytics and descriptive analysis in Business Intelligence, to Machine Learning and Artificial Intelligence innovation in different industries and economic sectors. We do that by transferring and applying state-of-the-art knowledge on data into need-tailored solutions for business in foreign countries. The beautiful thing is that those solutions are being implemented by people all around our country. We’re expanding now in different Latin American countries — with baby steps though — allowing those people to feel that they’re relevant by knowing that even if they’re in a small town, they can do artificial intelligence for a big multinational corporation.

We urge our people, our crew -which we call Trascendentales-, to strive, excel and realize that we can solve unsolved issues with our talent and capacity. And in the process, they learn not only about Data Science, Artificial Intelligence and other areas, but also about leadership, self-esteem and even other languages and cultures.

Encouraging the new era of entrepreneurs

Many believe that entrepreneurship is some rocket science: “It’s too hard”, “it’s too risky”, “why don’t you get a real job?” are some of the comments that discourage many people of pursuing this way of life, making them feel that they’re taking the wrong path.

In reality, to become an entrepreneur, only one thing is required: to have an idea. When the idea is nurtured, it becomes a vision, then an objective, a purpose, a hypothesis, and sometimes, a business model. All great companies started like that, with the seed of an idea, nurtured by passion and true self-belief.

However, every entrepreneur must also bear the humbling feeling of knowing that you can fail, that your idea might not be “a million-dollar idea”, but you must be willing to let experience and experimentation guide you, to fall and rise, to adapt and overcome, over and over, until you reach that turning point where everything falls into place.

In Trascender Global, we’re passionate about that process: that journey of self-discovery and evolution that every entrepreneur must go through is one of the most valuable growing experiences ever. We look forward to supporting all the ideas that come our way, to guide them in this path, to put our experience in the table and serve as a valuable ally, to grow together and make our crew ideas transcend. It is a huge shot where we create new staff groups within our crew to develop product or services, and soon you will be hearing from some of them.

We like to call this methodology “Monkeys” because just like an idea that moves around in your head, a monkey hangs in a tree and moves swiftly from branch to branch. Still, it has the capacity of evolving, use tools and language, to have its learning process, continually improving, until it can walk by its own. Oh, and also, because of Night Monkey, from Spiderman. Spiderman is cool 🤣.

What’s ahead for us?

Trascender Global dreams of following the steps of big tech companies and organizations such as Globant, PSL, Google, IBM, Accenture and others. We want to lead the technological path in Colombia and Latin America, continuing with the development of tailored solutions and new products in this huge world of data and technology, harnessing the power of Artificial Intelligence, Business Intelligence and Software Engineering.

Achieving this will allow us to continue innovating on those three fronts, bringing more and better opportunities for the people and the economy in our region.

Despite being a small company, we now see ourselves as a big and remarkable organization in the not so distant future. Trascender Global is not just me, a single person, but an idea made of all the dreams of people that somehow believe in us, and that force will continue to be developed even when we’re gone. Precisely, that’s what Trascender mean: to transcend. Feel free to share with us your thoughts, to share these words or even to join us in this pursuit.

A bid farewell

Andrés Martiliano, CEO of Trascender Global

Thanks for coming this far. You can find me, Andrés Martiliano-Martínez, in LinkedIn if you want to continue this conversation. I’m a mix between extrovert and introvert driven by strong passion of seeing dreams come true and I hope you take something valuable of our short yet beautiful story. See you soon!",7313
"Building The “Unsexy” of Healthcare — Interview with Redesign Health Venture Chair, Missy Krasner Michelle Davey Sep 1·8 min read

Photo credit: Insider

Earlier this year, I kicked off an interview series with leaders in the health tech space. Our industry is so vast and changing so quickly that it’s impossible to be an expert in every area. The goal of this series is to share insights, takeaways, and conversations with those that are raising the bar and building the next generation of healthcare.

I recently had the pleasure of sitting down with a dear friend and advisor Missy Krasner, who currently serves as Venture Chair for Redesign Health. Missy has one of the most impressive resumes in healthcare — including working for the U.S. Department of Health & Human Services (HHS) as Senior Advisor to the first Office of the National Coordinator (ONC) and serving as a founding member for both Google Health and Alexa Health & Wellness at Amazon. She also serves as a board member for four companies focused on delivering care both virtually and in person.

And that’s only a fraction of what she’s accomplished throughout her career.

In her current role at Redesign Health, Missy focuses on building transformative healthcare companies from the ground up. In our conversation, she shares her biggest takeaways about our healthcare system and what she’s been most proud of throughout her career.

Your career has touched almost every piece of the healthcare space, what initially inspired you to start working in healthcare?

During my childhood and undergrad at UCLA, my mom became sick and was diagnosed with multiple sclerosis. She also separated from my dad, who was a practicing physician, which meant she was no longer covered by his practice’s health insurance plan. This was before the Affordable Care Act, and because she had a pre-existing condition, she was rated out of the health insurance system and wasn’t able to get coverage. In other words, she was an MS patient with no health insurance.

Even though my dad is a successful physician (kidney specialist), I had never thought about working in healthcare. I was a Theater and English Literature major during my undergraduate studies. But this family experience completely changed my path, so I switched gears and decided to focus on patient advocacy. I went to graduate school at Stanford University and studied healthcare communications and administration. And from there, I began to learn the healthcare system from the inside out.

You’ve been able to see the healthcare industry through several lenses — first at Atena then at the Kaiser Family Foundation and eventually at Google and then Amazon. You’ve seen how healthcare works at digital health startups, large technology companies, foundations, the government, and in venture capital. How has that shaped your overall perspective on the digital health industry?

Looking back at where I started to where I am today, my take is that if you haven’t worked in the “bowels” of healthcare, you won’t be able to understand the industry as a whole. You won’t be able to understand change management, workflow, and the culture of healthcare.

As healthcare leaders, we tend to gravitate towards wanting to build something sexy. That makes sense, but when it comes down to it, those new shiny products aren’t always easily adopted. I’ve spent a big part of my career developing new solutions at big tech companies, and those innovations have failed. Why? Because healthcare has complicated and opaque buyers, it has regulatory hurdles, and it’s super hard to get those who deliver care to change their everyday work habits and workflows.

All of this has influenced my take on digital health adoption in healthcare. We need to focus on building the “unsexy” in healthcare. The back office stuff like — billing, credentialing, and better clinical documentation — to make our overall system more efficient. And that’s why I’m such a big fan of Wheel — they do just this — they help virtual health companies go faster and provide seamless workflow.

Looking back on your career, which campaign or product or initiative have you been most proud of?

I know that government isn’t the most glamorous industry, but working for the U.S. Department of Health and Human Services (HHS) was a seminal time in my career. It was a very different time in the industry. This was in 2005, pre-ACA, and we were charged with promoting the nationwide adoption of healthcare technology and Electronic Health Records (EHRs). At that time, EHR adoption by physicians in both the ambulatory and acute care environment was about 33 percent. Thanks in part to our work at the inaugural ONC office at HHS and the HITECH Act, which we helped author, the national EHR adoption rate is now at 97 percent.

It was also a very different time in politics, I was able to work on bipartisan legislation that brought real progress towards health IT for the first time. And, of course, the fangirl in me really enjoyed all the key political figures I got to meet and work directly for on both sides of the political aisle.

What is the one thing you wish the general public understood about digital health?

I don’t think people understand the complexity around what it takes to aggregate their personal medical history into a privacy-safe, consumer-centric, and easy-to-use format. Let’s face it, interoperability in healthcare is still a hurdle. We’ve had some incremental progress in the last decade, but we’re still not where we need to be. We still have perverse business incentives for why two competing health systems on two different EHRs would share data, all in the name of better care at the point of care and information arriving just in time to treat the patient.

We don’t think about our medical records until we have to interact with the healthcare system — like getting pregnant or ending up in the hospital due to an acute diagnosis or roadside accident. It’s like your credit report — you only pay attention to it when you have to make a big purchase, like buying a house. All of a sudden it matters and you’re scrambling to make sure you have all the right information in one place.

We haven’t yet answered the question of how to make health records easily accessible and portable. I spent a lot of time in government, at Google, and even at Box and Amazon, trying to work on this problem. Consumers want it but have resigned themselves to having a less than stellar experience interacting with their doctors, insurance companies, and hospitals. They simply anticipate poor service and legacy technology.

Can you tell us more about Redesign Health and how the program works?

When I left Amazon, I wanted to go back to early company creation and venture. I had worked in a traditional venture firm, Morgenthaler Ventures (now Canvas Ventures), before Amazon. While there, I helped make several early-stage investments in companies like: Practice Fusion, HealthLoop, Viewics, Doximity, and Vida Health. I loved my time in venture, but I ultimately wanted to be more operational and help our portfolio companies scale faster.

Redesign Health offers that model. We integrate a historically complex process of ideating, developing, funding, and launching healthcare companies into a streamlined platform — one that creates and scales companies without the friction traditionally found in the business-building process.

We launch these companies in collaboration with extraordinary entrepreneurs and corporate partners. We’ve assembled an industry-leading team of nearly 90 healthcare operators, technologists, and investors to redesign healthcare during a time of urgent need and unprecedented change. We have raised over $300MM in capital since inception and are backed by Declaration Partners, the family investment firm founded by David Rubenstein, and several prominent VCs.

We have built 20 ventures to date, 13 of which are publicly launched, across a diversified set of business models, including B2B2C, enterprise software, consumer, reimbursement models, and marketplaces/platforms. Our platform team provides deep expertise across every functional area relevant to launching and growing healthcare businesses.

Our portfolio includes companies like Calibrate, Vault Health, MedArrive, and a few that I am directly involved with: UpLift, Springtide, and Health Quarters.

What do you think makes a great healthcare founder?

It boils down to three things. First, you have to know how your company is going to make money over the long run. And if you don’t know that yet, you need to be willing to pivot to find it.

Second, invest in or hire a team around you that has healthcare expertise. You want to surround yourself with leaders who have played in the industry sandbox. I am all for innovation and tech leaders being in healthcare, but at least one Co-founder or someone from the management team has to be an industry veteran. If not, I highly suggest you get a deep bench of industry advisors.

Third, align around investors who have the stomach for investing in healthcare and digital health. They’ll know where to invest in relationships, how to get large incumbent brands to partner and pilot your solution, and they will understand the longer B2B sales cycles in payor and provider. They will also know how a digital health startup CEO leader will need to deploy capital and fundraise over time.

What is the best part of your job today?

I love the diversity and optionality of my everyday work at Redesign Health. I get to work on very early concepts and help bring them to fruition, recruit and build out founding teams, and coach stellar CEOs. It’s a nice blend of drawing on my past Operator, Government/Policy, and Investor experiences.

What’s the hardest part of your job today?

There is so much popping up in our industry right now due to the global pandemic, I just wish we all had more hours in the day to respond to demand and need. There are still many complicated problems in healthcare that need to be addressed — we at Redesign Health, strive to launch companies at an extremely high velocity, but it would be great to figure out a way to go even faster in order to tackle these immediate problems.

What’s the best or most impactful thing you’ve read lately?

I just re-read The Art of Possibility by Rosamund Stone Zander and Benjamin Zander. It’s a very quick and simple read, and it’s about changing your mindset to what is possible. It invites readers to learn how to be passionate communicators, leaders, and performers.

Name something that you can’t live without and tell us why.

My journal. Every morning I set a timer and write for ten minutes. Some days it may be a list of things I need to do, other days, it may be more creative or intentional. The point is that I’m writing and focusing on getting out of my head and really giving myself time to “think big” and “dream.” I found that the act of physically writing or scribbling on lined paper and not typing on a computer or phone opens my creativity and centers me to start my day.

Thank you, Missy for sharing your incredible journey with us. You’ve really done it all in healthcare and I’m sure you have inspired our readers to continue learning about healthcare from all angles.",11296
"We are excited to announce a new listing for KEX token. On June 17'th at 8 AM UTC the token will be available for trading at Gate.io

This is another important step in providing our token holders with liquidity and wider market access via secure and trusted listing partners.

Upon the launch of the KIRA Mainnet in the coming months, we’ll continue to support ERC20 KEX token along the staking program, while also introducing the native KEX Mainnet token. Both tokens will be able to be easily converted from ERC20 to native KEX at 1:1 rate. Further details regarding token swaps will be shared after the Mainnet launch.

About Gate.io

Gate.io was created to enable a new generation of global traders with the tools to access the revolutionary age of cryptocurrencies. It is an established exchange that holds integrity, transparency, and fairness to a very high standard. Charging zero listing fees and choosing promising projects, this exchange consists only of 100% real trading volume. Gate.io is operated by Gate Technology Inc.

For more information and updates, please visit:

Website: https://www.gate.io/

Twitter: https://twitter.com/gate_io

Telegram: https://t.me/gateio

About KIRA

KIRA is a decentralized network secured by both cryptocurrencies and real world assets at stake thanks to its proprietary Multi-Bonded-Proof-of-Stake (MBPoS) consensus mechanism. Positioned within the Interchain and Web3 ecosystems KIRA makes it safer and easier to gain access to decentralized financial applications and unlock the full economic potential of your capital to generate yield.

Website|Telegram Group|Telegram Announcement|Twitter |Medium",1651
"Why Progressives Must Adopt Bitcoin

Once unleashed, the Domino Effect is unstoppable

Bitcoin adoption is happening fast. Only 12 years into its existence, the digital currency has already been adopted as legal tender by the first nation-state. Shortly after, politicians from Brazil, Argentina, Paraguay and Panama signaled their support. Nigeria leads the world in Peer to Peer payments, using bitcoin’s 2nd layer, the Lightning Network, and this is true even after The Nigerian government tried to ban it. A group calling itself Project Mano is currently lobbying Ethiopia’s government to mind and hold bitcoin, with billionaire Jack Dorsey promoting their efforts.

In the US, bitcoin adopters tend to skew overwhelmingly white, male, and libertarian. It’s gaining traction around Republican politicians though. Texas Senator Greg Abbott and the Texas Department of Banking made Texas the 2nd state after Wyoming to empower state-chartered banks to custody bitcoin. Wyoming’s Senator Cynthia Lumis, a Bitcoiner herself, is evangelizing other US Senators, practically on the Senate floor. Meanwhile, my favorite Senator, Elizabeth Warren, has missed the forest for the trees, and is rampaging against bitcoin on the misinformed and plainly wrong assertion that bitcoin will destroy the environment.

On the contrary, Bitcoin mining will drive a global rush to adopt renewable energy. Speaking of billionaire Jack Dorsey, his company Square is developing 100% renewable Bitcoin mining operations right now. So is Norweigan billionare Kjell Inge Rokke. The billionaire class is adopting Bitcoin as an investment strategy, too. Famed investor Paul Tudor Jones allocates 5% of his wealth to bitcoin, with potential plans to add to his position.

Here’s the thing: There will only ever be 21 million Bitcoin. Unlike any fiat currency, or even gold, bitcoin’s supply is capped. Over the coming decades, the Bitcoin network will absorb the monetary energy of our entire global economy, transforming the geopolitical playing field beyond our imaginations. El Salvador was the first domino to fall, but the Domino Effect, once unleashed, is unstoppable.

Progressives have so many reasons to cheer the adoption of bitcoin. But if we don’t get our heads out of our proverbial asses, we will forfeit the opportunity to have a voice in the decentralized monetary system of the 21st century. Furthermore, if the communities Progressives purport to care about, namely women, people of color, LGBTQ communities, immigrants, and low income workers, are dissuaded from adopting bitcoin, they will see their purchasing power erode disproportionately over the coming decade, buying into a universally accepted, scarce resource at much higher prices. Bitcoin adoption is Game Theory in action, and Progressives are losing.

Bitcoin, like the telephone and Facebook, benefits from the Network Effect. Essentially, a phone is useless if there is no one to call, but it’s very useful when you can call everyone you know. Like the telephone, bitcoin becomes more valuable with increased adoption. Like all early adopters to any network, the earliest to adopt the network gain the largest increase in value from the network. Bitcoin is a monetary network, and money is the largest social network of all. The kind of power it may yield is boundless.

Facebook was just a contentious Harvard experiment when it first started in the Winkelveii Twins and/or Mark Zuckerberg’s dorm room.As use of its network spread across college campuses and beyond, Facebook quickly dominated the online social space, leaving Myspace, its predecessor, in the dust. Seventeen years into its existence, Facebook has the power to sway multiple democratic presidential elections.

The question I pose to my fellow Progressives is this: What happens if North Korea, Russia, China and Iran are next nation states to adopt Bitcoin? There is already speculation that their governments are secretly mining and holding it to bypass US sanctions. In China’s case, its government likely credibly views bitcoin as a more viable threat to China’s economic dominance than the US dollar. While the Chinese government is cracking down on mining operations, we do not know for certain that they’re not simultaneously mining their own bitcoin to monopolize power. We do know that despite several “bans” the Chinese government has never once, for even a second, successfully shut down the Bitcoin network.

Furthermore, what happens if, in the US, Bitcoin is viewed through a partisan lens, and only Republican led states adopt bitcoin friendly laws? El Salvador is already seeing an unprecedented surge in business and real estate interest, just one week after passing its bitcoin legislation. Meanwhile, Austin, Texas and Miami, Florida are competing to be the next Silicon Valley. Miami Mayor Francis Suarez has gone so far as to allocate city resources to explore the feasibility of paying city employees in bitcoin.

Do “blue states” want to lose out on attracting the best emerging talent, and all the job creation that comes with it? Speaking of job creation, the International Brotherhood Electrical workers Union was instrumental in blocking the passage of an anti-mining bill that passed in the New York senate. Progressives have been fighting to retain its Union support. Legislating the elimination of high wage jobs in an emerginc industry is not going to win them any votes.

While these questions may seem like hypotheticals for the distant future, the future is here. At the very least, it’s quickly approaching. The world is adopting Bitcoin at nearly twice the speed it adopted the internet. Ironically, New York Times’ Paul Krugman is lodging the same complaints against bitcoin that he once lodged against the internet. And we all know how the internet turned out.

Progressives can adopt Bitcoin now, collectively owning a larger stake in the world’s decentralized, censorship resistant, monetary network, or forfeit that stake to dictatorships, and opportunistic domestic politicians, some of whom never abandoned Trump. The clock’s ticking.",6103
"Welcome to the Greatest Video Game of Your Life

Imagine you turn on the news today, and the broadcaster says that valid evidence show we live in a simulation. Everything around us — the people, the sky above, the ground we walk on, and even our bodies and minds — are an elaborate illusion. Our reality is virtual, and we never truly existed. We are a series of zeros and ones.

Let’s suppose the whole world is a computer program, where we have to solve complex issues to move on a higher level — both individually and collectively.

If you were born in a developing country, your gamer chose a more challenging route. Or maybe our gamers abandoned us right after they created the playground.

Gods are only gamers or scientists in a more advanced society. The purpose of our world is yet to be discovered.

How Would This Affect Society?

Oliver will turn 16 in a few days. He’s an average teenage boy who enjoys spending his time with video games, friends, and cycling. Now Oliver is frustrated and slightly paranoid. He knows everything about computer games but could never imagine life could be one.

Oliver was playing video games when he heard the news on the TV. He spent the evening in the living room with his family on this sunny spring evening.

It’s already been 3 weeks since scientists first announced to the general public that reality as we perceive it is simulated. Now, the reporter talks about a new fear people collectively experience: They worry the gamer will unexpectedly turn off the simulation. Experts named it the ‘Game-Over Phobia.’

There is a significant rise in crimes. Many people feel they are entitled to do whatever they want; thefts, robberies, breaking-in houses, but smaller fights and even murders are increasing. The scientist in the TV show explained the discoveries “relieve us from being accountable for our actions,” he says. “There is nothing more damaging to our social order than this notion.”

The various religious groups also put some thoughts into the discussion. “Obviously, they cannot simply admit they were wrong for thousands of years,” snorted Oliver’s father, who is a researcher in a pharma company. “They always have to twist the arguments to their favor.”

The Catholic church took advantage of the discoveries. The pope claimed they were right all the time. Only one God exists; that is our “gamer.”

Hinduists and Buddhists argue if we’re living in a simulation, we can have multiple plays, not just one. Therefore, reincarnation is what is going to happen to us. “Even though you can scientifically explain something, it’s still magical. Nothing has changed, keep meditating, and you’ll have a life filled with purpose and calm.” said a Buddhist monk.

Another issue is that gamers around the world are trying hard to hack the system. Some of them solely want to find the money code; others seek ways out of the simulation.

“It’s enough for today; let’s have dinner” Rebbeca turned off the TV, Oliver’s mother. The sun was shining through the window, shedding light on the fresh flowers on the kitchen table. Everything felt so real.

“How was your day today, Oliver?” asked Rebecca. “Didn’t your class have that counseling session with the school psychologist today?” she added. Oliver is a bit more frustrated than usual, and her parents are worried. “Yes, we had that today,” said Oliver.

“And how was it?” Oliver’s dad showed some interest in the topic as well.

“It was cool.” shrugged Oliver.

“Okaaayy…A bit more in detail maybe?” went on Rebecca.

“She tried to calm us down about the whole fuss, and I guess she managed that quite well.”

When the family finished the tasty meal, they didn’t even think about what they actually consumed was only a few bits of information.",3749
"The Hardest Coding Interview Questions Ever

I looked at over 1000 coding interview questions and found the hardest questions as follows by tech giants.

Question 01

There exists a staircase with N steps, and you can climb up either 1 or 2 steps at a time. Given N, write a function that returns the number of unique ways you can climb the staircase. The order of the steps matters.

For example, if N is 4, then there are 5 unique ways: 1, 1, 1, 1 2, 1, 1 1, 2, 1 1, 1, 2 2, 2

What if, instead of being able to climb 1 or 2 steps at a time, you could climb any number from a set of positive integers X? For example, if X = {1, 3, 5}, you could climb 1, 3, or 5 steps at a time.

Question 02

Given an integer k and a string s, find the length of the longest substring that contains at most k distinct characters.

For example, given s = “abcba” and k = 2, the longest substring with k distinct characters is “bcb”.

Question 03

Given a list of integers, write a function that returns the largest sum of non-adjacent numbers. Numbers can be 0 or negative.

For example, [2, 4, 6, 2, 5] should return 13 , since we pick 2 , 6 , and 5 . [5, 1, 1, 5] should return 10 , since we pick 5 and 5 .

Follow-up: Can you do this in O(N) time and constant space?

Question 04

Given an array of integers, return a new array such that each element at index i of the new array is the product of all the numbers in the original array except the one at i .

For example, if our input was [1, 2, 3, 4, 5] , the expected output would be [120, 60, 40, 30, 24] . If our input was [3, 2, 1] , the expected output would be [2, 3, 6] .

Follow-up: what if you can’t use division?

Question 05

Given an array of integers, find the first missing positive integer in linear time and constant space. In other words, find the lowest positive integer that does not exist in the array. The array can contain duplicates and negative numbers as well.

For example, the input [3, 4, -1, 1] should give 2 . The input [1, 2, 0] should give 3 .

You can modify the input array in-place.

Question 06

Given an array of numbers, find the length of the longest increasing subsequence in the array. The subsequence does not necessarily have to be contiguous.

For example, given the array [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15], the longest increasing subsequence has length 6: it is 0, 2, 6, 9, 11, 15.

Question 07

We’re given a hashmap associating each courseId key with a list of courseIds values, which represents that the prerequisites of courseId are courseIds .

Return a sorted ordering of courses such that we can finish all courses.

Return null if there is no such ordering.

For example, given {‘CSC300’: [‘CSC100’, ‘CSC200’], ‘CSC200’: [‘CSC100’], ‘CSC100’: []}, should return [‘CSC100’, ‘CSC200’, ‘CSCS300’].

Question 08

A quack is a data structure combining properties of both stacks and queues. It can be viewed as a list of elements written left to right such that three operations are possible:

push(x) : add a new item x to the left end of the list

: add a new item to the left end of the list pop() : remove and return the item on the left end of the list

: remove and return the item on the left end of the list pull() : remove the item on the right end of the list.

Implement a quack using three stacks and O(1) additional memory, so that the amortized time for any push, pop, or pull operation is O(1) .

Question 09

Given an array of numbers of length N , find both the minimum and maximum using less than 2 * (N - 2) comparisons.

Question 10

Given a word W and a string S , find all starting indices in S which are anagrams of W .

For example, given that W is “ab”, and S is “abxaba”, return 0, 3, and 4.

Question 11

Given an array of integers and a number k, where 1 <= k <= length of the array, compute the maximum values of each subarray of length k.

For example, given array = [10, 5, 2, 7, 8, 7] and k = 3, we should get: [10, 7, 8, 8], since: 10 = max(10, 5, 2) 7 = max(5, 2, 7) 8 = max(2, 7, 8) 8 = max(7, 8, 7)

Do this in O(n) time and O(k) space. You can modify the input array in-place and you do not need to store the results. You can simply print them out as you compute them.

Question 12

Find an efficient algorithm to find the smallest distance (measured in number of words) between any two given words in a string.

For example, given words “hello”, and “world” and a text content of “dog cat hello cat dog dog hello cat world”, return 1 because there’s only one word “cat” in between the two words.

Question 13

Given a linked list, uniformly shuffle the nodes. What if we want to prioritize space over time?

Question 14

Typically, an implementation of in-order traversal of a binary tree has O(h) space complexity, where h is the height of the tree. Write a program to compute the in-order traversal of a binary tree using O(1) space.

Question 15

Given a string, find the longest palindromic contiguous substring. If there are more than one with the maximum length, return any one.

For example, the longest palindromic substring of “aabcdcb” is “bcdcb”. The longest palindromic substring of “bananas” is “anana”.

Question 16

Implement an LFU (Least Frequently Used) cache. It should be able to be initialized with a cache size n , and contain the following methods:

set(key, value) : sets key to value . If there are already n items in the cache and we are adding a new item, then it should also remove the least frequently used item. If there is a tie, then the least recently used key should be removed.

: sets to . If there are already items in the cache and we are adding a new item, then it should also remove the least frequently used item. If there is a tie, then the least recently used key should be removed. get(key) : gets the value at key . If no such key exists, return null.

Each operation should run in O(1) time.

Question 17

Given a string consisting of parentheses, single digits, and positive and negative signs, convert the string into a mathematical expression to obtain the answer.

Don’t use eval or a similar built-in parser.

For example, given ‘-1 + (2 + 3)’, you should return 4 .

Question 18

Connect 4 is a game where opponents take turns dropping red or black discs into a 7 x 6 vertically suspended grid. The game ends either when one player creates a line of four consecutive discs of their color (horizontally, vertically, or diagonally), or when there are no more spots left in the grid.

Design and implement Connect 4.

Question 19

There are N couples sitting in a row of length 2 * N . They are currently ordered randomly, but would like to rearrange themselves so that each couple's partners can sit side by side.

What is the minimum number of swaps necessary for this to happen?

Question 20

Recall that the minimum spanning tree is the subset of edges of a tree that connect all its vertices with the smallest possible total edge weight. Given an undirected graph with weighted edges, compute the maximum weight spanning tree.

Question 21

Sudoku is a puzzle where you’re given a partially-filled 9 by 9 grid with digits. The objective is to fill the grid with the constraint that every row, column, and box (3 by 3 subgrid) must contain all of the digits from 1 to 9.

Implement an efficient sudoku solver.

Question 22

A knight is placed on a given square on an 8 x 8 chessboard. It is then moved randomly several times, where each move is a standard knight move. If the knight jumps off the board at any point, however, it is not allowed to jump back on.

After k moves, what is the probability that the knight remains on the board?

Question 23

You are given an array of length 24 , where each element represents the number of new subscribers during the corresponding hour. Implement a data structure that efficiently supports the following:

update(hour: int, value: int) : Increment the element at index hour by value .

: Increment the element at index by . query(start: int, end: int) : Retrieve the number of subscribers that have signed up between start and end (inclusive).

You can assume that all values get cleared at the end of the day, and that you will not be asked for start and end values that wrap around midnight.

Question 24

Given an array of integers, find the first missing positive integer in linear time and constant space. In other words, find the lowest positive integer that does not exist in the array. The array can contain duplicates and negative numbers as well.

For example, the input [3, 4, -1, 1] should give 2 . The input [1, 2, 0] should give 3 .

You can modify the input array in-place.

Question 25

Given an array of numbers, find the length of the longest increasing subsequence in the array. The subsequence does not necessarily have to be contiguous.

For example, given the array [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15], the longest increasing subsequence has length 6: it is 0, 2, 6, 9, 11, 15.

Question 26

An 8-puzzle is a game played on a 3 x 3 board of tiles, with the ninth tile missing. The remaining tiles are labeled 1 through 8 but shuffled randomly. Tiles may slide horizontally or vertically into an empty space, but may not be removed from the board.

Design a class to represent the board, and find a series of steps to bring the board to the state [[1, 2, 3], [4, 5, 6], [7, 8, None]].

Question 27

Given a string and a set of delimiters, reverse the words in the string while maintaining the relative order of the delimiters.

For example, given “hello/world:here”, return “here/world:hello”

Follow-up: Does your solution work for the following cases: “hello/world:here/”, “hello//world:here”

Question 28

Given a list of points, a central point, and an integer k , find the nearest k points from the central point.

For example, given the list of points [(0, 0), (5, 4), (3, 1)] , the central point (1, 2) , and k = 2, return [(0, 0), (3, 1)] .

Question 29

Given an array of positive integers, divide the array into two subsets such that the difference between the sum of the subsets is as small as possible.

For example, given [5, 10, 15, 20, 25] , return the sets {10, 25} and {5, 15, 20} , which has a difference of 5, which is the smallest possible difference.

Question 30

Given a sorted list of integers of length N , determine if an element x is in the list without performing any multiplication, division, or bit-shift operations.

Do this in O(log N) time.

Question 31

You come across a dictionary of sorted words in a language you’ve never seen before. Write a program that returns the correct order of letters in this language.

For example, given ['xww', 'wxyz', 'wxyw', 'ywx', 'ywz'] , you should return ['x', 'z', 'w', 'y'] .

Question 32

Describe what happens when you type a URL into your browser and press Enter.

Question 33

You are going on a road trip, and would like to create a suitable music playlist. The trip will require N songs, though you only have M songs downloaded, where M < N . A valid playlist should select each song at least once, and guarantee a buffer of B songs between repeats.

Given N , M , and B , determine the number of valid playlists.

Question 34

Write a program that computes the length of the longest common subsequence of three given strings. For example, given “epidemiologist”, “refrigeration”, and “supercalifragilisticexpialodocious”, it should return 5 , since the longest common subsequence is ""eieio"".

Question 35

Given a list, sort it using this method: reverse(lst, i, j) , which reverses lst from i to j .

Question 36

You have an N by N board. Write a function that, given N, returns the number of possible arrangements of the board where N queens can be placed on the board without threatening each other, i.e. no two queens share the same row, column, or diagonal.

Question 37

Given an array of strictly the characters ‘R’, ‘G’, and ‘B’, segregate the values of the array so that all the Rs come first, the Gs come second, and the Bs come last. You can only swap elements of the array.

Do this in linear time and in-place.

For example, given the array [‘G’, ‘B’, ‘R’, ‘R’, ‘B’, ‘R’, ‘G’], it should become [‘R’, ‘R’, ‘R’, ‘G’, ‘G’, ‘B’, ‘B’].

Question 38

Suppose you are given a table of currency exchange rates, represented as a 2D array. Determine whether there is a possible arbitrage: that is, whether there is some sequence of trades you can make, starting with some amount A of any currency, so that you can end up with some amount greater than A of that currency.

There are no transaction costs and you can trade fractional quantities.

Question 39

Given an array of integers where every integer occurs three times except for one integer, which only occurs once, find and return the non-duplicated integer.

For example, given [6, 1, 3, 3, 3, 6, 6], return 1. Given [13, 19, 13, 13], return 19.

Do this in O(N) time and O(1) space.

Question 40

Given a list of integers S and a target number k, write a function that returns a subset of S that adds up to k. If such a subset cannot be made, then return null.

Integers can appear more than once in the list. You may assume all numbers in the list are positive.

For example, given S = [12, 1, 61, 5, 9, 2] and k = 24, return [12, 9, 2, 1] since it sums up to 24.

Question 41

Given a sorted array arr of distinct integers, return the lowest index i for which arr[i] == i . Return null if there is no such index.

For example, given the array [-5, -3, 2, 3] , return 2 since arr[2] == 2 . Even though arr[3] == 3 , we return 2 since it's the lowest index.

Question 42

Given an array of numbers arr and a window of size k , print out the median of each window of size k starting from the left and moving right by one position each time.

For example, given the following array and k = 3 :

[-1, 5, 13, 8, 2, 3, 3, 1]

Your function should print out the following:

5 <- median of [-1, 5, 13]

8 <- median of [5, 13, 8]

8 <- median of [13, 8, 2]

3 <- median of [8, 2, 3]

3 <- median of [2, 3, 3]

3 <- median of [3, 3, 1]

Recall that the median of an even-sized list is the average of the two middle numbers.

Question 43

You are given an array of integers representing coin denominations and a total amount of money. Write a function to compute the fewest number of coins needed to make up that amount. If it is not possible to make that amount, return null.

For example, given an array of [1, 5, 10] and an amount 56 , return 7 since we can use 5 dimes, 1 nickel, and 1 penny.

Given an array of [5, 8] and an amount 15 , return 3 since we can use 5 5-cent coins.

Question 44

Explain the difference between composition and inheritance. In which cases would you use each?

Question 45

You are given a 2D matrix of 1s and 0s where 1 represents land and 0 represents water.

Grid cells are connected horizontally orvertically (not diagonally). The grid is completely surrounded by water, and there is exactly one island (i.e., one or more connected land cells).

An island is a group is cells connected horizontally or vertically, but not diagonally. There is guaranteed to be exactly one island in this grid, and the island doesn’t have water inside that isn’t connected to the water around the island. Each cell has a side length of 1.

Determine the perimeter of this island.

For example, given the following matrix:

[[0, 1, 1, 0],

[1, 1, 1, 0],

[0, 1, 1, 0],

[0, 0, 1, 0]]

Return 14 .

Question 46

Given a string, return the length of the longest palindromic subsequence in the string.

For example, given the following string:

MAPTPTMTPA

Return 7, since the longest palindromic subsequence in the string is APTMTPA . Recall that a subsequence of a string does not have to be contiguous!

Your algorithm should run in O(n²) time and space.

Question 47

Given a list of strictly positive integers, partition the list into 3 contiguous partitions which each sum up to the same value. If not possible, return null.

For example, given the following list:

[3, 5, 8, 0, 8]

Return the following 3 partitions:

[[3, 5],

[8, 0],

[8]]

Which each add up to 8.

Question 48

Given a tree, find the largest tree/subtree that is a BST.

Given a tree, return the size of the largest tree/subtree that is a BST.

Question 49

A knight’s tour is a sequence of moves by a knight on a chessboard such that all squares are visited once.

Given N, write a function to return the number of knight’s tours on an N by N chessboard.

Question 50

Implement a file syncing algorithm for two computers over a low-bandwidth network. What if we know the files in the two computers are mostly the same?",16629
"Buying a Bitcoin emits 195x as much CO₂ as buying an iPhone Yann Eves May 2·3 min read

Today a discussion on Discord about NFTs and environmental impact respectfully erupted into a situation XKCD portrays well:

Well, it turns out I was the wrong someone on the internet.

My hypothesis was that CO₂ emissions of cryptoassets are much lower than a large US tech company like Apple when adjusted by market capitalisation¹.

To work this out I decided to compare Bitcoin with Apple and Microsoft. While Ethereum is more relevant to NFTs, reports on ETH carbon emissions are scarce and tended to be proportional to Bitcoin anyway.

At the end of 2020, Bitcoin (BTC) had a market cap of $539 billion², Apple (AAPL) $2.3 trillion³, Microsoft (MSFT) $1.9 trillion⁴.

And the estimated total CO₂ emissions (CO₂e) for 2020 for Bitcoin 35.89 Mt CO₂e⁵, Apple 22.6 Mt CO₂e⁶, Microsoft approx. 12 Mt CO₂e⁷.

While the carbon emissions for Bitcoin came up higher than I’d expected to find, my confirmation bias had convinced my limited ability in mental arithmetic that low emissions and high market cap meant I was about to nail this stat. I wanted to show crypto has a fraction of the emissions relative to tech giants. But the opposite is true.

Rounding to one decimal place, Bitcoin emits 6.7 times as much CO₂ as Apple, and 10.6 times as much as Microsoft. That is staggering.

And thus circling back to the headline you clicked on. An Apple iPhone 12 Pro late-2020 would set you back $999⁸ and one Bitcoin would cost $29,001.72⁹. On the basis that either choice supports the underlying institution and their overall carbon emissions, the decision to buy one Bitcoin versus an iPhone 12 Pro emits 195 times as much CO₂.

Does that mean cryptoassets are doomed technology? No, this spells the need for established institutions to adopt and support crypto. The demand is there. Meanwhile, we need to be more cautious of the actors we currently enable in crypto networks and the corners they’re willing to cut in the pursuit of profit.",2026
"The Galaxy Watch 4/44mm+Bluetooth:

I went to Best Buy to pick up the Watch 4, my first pre-order ever. I’ve only ever owned the Watch Active 2 almost 2 years ago, but then I stopped using it. I felt a little restricted with Samsung’s Tizen OS because it felt like I could “do” more with it, but the limitations of the OS didn’t allow me. However, Samsung’s recent adoption of Google’s Wear OS piqued my interest and I needed to try it out for myself. After a week of playing around with it with my Galaxy S21 Ultra and treating it as a digital watch with smart features, I have a lot to say about this wearable.

My wrist is 7 inches around, strap is on the 2nd to last hole. Fits well.

Day 1 — August 27th

This was the evening after picking the watch up. I was waiting for the screen protector to be ready so I can jumpstart my review that same day, but Best Buy messed that order up and I had to order a pack of some on Amazon. The only things I did that day with the watch was 10 minutes worth of exploring and updating the watch. It came with 57% battery, and charged it after I was done with it. Then, I turned it off for until I received my screen protector for it.

Day 2 — August 28th

Resisting my temptations to touch the watch, I finally succumbed towards the evening. I just needed to see what the Watch 4 could offer me that the Watch Active 2 couldn’t. My first impressions were:

Being able to use the Play Store

Smoother interface compared to what I remember from the Watch Active 2

Flat screen compared to curved on the Watch Active 2

In addition to the last bullet point, my own research led me to the conclusion that the Watch Active 2 screen protectors work just as well on the Watch 4 because of the flat screen.

I used Samsung’s Notes app to record the Watch and my usage with it. Only 3 things you need to take note of: Time, Battery Percentage, and Watch Activity.

I started the time recording late for this day. But starting the day after, I became more consistent.

Day 3 — August 30th

Took a day off to wait for the screen protector. But here’s what I did for this day.

This day was me really using it how I would expect to use it from that day forward. I permanently turned on location mainly for weather services and NFC for GPay. Media controller will be the main part of your watch if you’re constantly listening to music or watching something on YouTube on your phone.

Day 4 — August 31st

This was a slow day for activity. Mainly just let the watch operate on standby, seeing how the battery life is.

Forgot to add “turned off”, but almost 6 hours and only 4% lost? Not bad.

Day 5 — September 1st

This day, I was stuck on deciding between raise and tap to wake.

Day 6 — September 2nd

Slow day again.

Day 7 — September 3rd

Another slow day.

Day 8 — September 4th

Finally found a watchface that fits my style, permanently enabled Always On Display for now/to test how much it really drains battery. Might turn off in the future depending on how I’m feeling.

Day 9/10 — September 5-6th

September 5th

After using the watch more productively on the 5th and using it on standby for the rest of the day, I let it drain to 15% before I turned it off, to fully charge it the next day on the 6th.

I conveniently saved the best parts for last, which I noted on the 6th.

September 6th

I waited for the right moment to fully charge the watch.

A good hour will give the watch some juice for the rest of the day, especially if you use power saving mode.

In total, it took almost 2 hours to charge from 16% to 100%. Bigger battery, longer charge time.

Last, but not least, all the options I have turned on and the apps I downloaded from the Play Store.

September 8th discovery: turning off Wi-Fi in the watch settings, turns to “Auto Wi-Fi”, which turns it back on if Bluetooth is unavailable or if the watch needs it. The weather app only needs location on for refreshing.

This watch is works well. GPay was one of the things people were worried about not working, so I’m surprised to see that it works as it should.

Another thing people were worried about was the Always On Display feature, draining the battery. I haven’t seen any significant drain. I still see myself charging up almost every other day. However, there’s a weird bug/feature? where if you have AOD on and you’re doing something on the watch and let it go idle, a regular digital watch shows up and the background blurs, when it should just go straight to the AOD.

It stays there for a while before turning off and entering “Always On Display”. Doesn’t really bother me, but hope an update fixes it.

User Interface

Google Play Store

Looking forward to the lifetime of this device!",4702
"Make history and be one of the first people ever to run a Streamr node! Follow these instructions to join the Brubeck testnet and mine your share of the 2M DATA reward pool (worth around $300,000 at the moment).

Two ways to run your node

The software comes in two flavours of packaging: a Docker image and an npm package. Both approaches require you to run commands on the command line, so some technical skill or at least commendable bravery is required. :)

So which method should you choose? If you have either Docker or Node.js (14.x or newer) already installed, use the one you know. Otherwise, try the Docker approach first, which is less platform-dependent. If that doesn’t work for you, go for the npm approach.

Once you have either Docker or Node.js installed, the steps to download and start the node are very similar, regardless of whether you’re running Linux, macOS, or Windows (use PowerShell). You may need to adapt the commands for your platform or install OS-specific dependencies, if they are missing.

Matthew shows the steps needed to set up your node and join the testnet

The configuration wizard

As part of both approaches, we show how to run the configuration wizard to initialize your node’s config file, which will be saved on your disk. The wizard will let you either generate or import an Ethereum private key for your node, as well as ask which plugins you want to enable. If you only use your node for mining the testnet, you don’t need to enable any of them. If you plan to also use your node to connect applications to data streams in the testnet, see the section ‘Using your node for publishing and subscribing’ later in this blog post.

The Docker approach

If you don’t have Docker, get it here. Once installed, you can download, configure, and start the Streamr Broker.

Step 1: Set up a directory to be mounted into a running Docker container

You’ll need a place in the host OS where the Broker configuration file will be stored. This directory will be mounted into the running Docker container so that it persists and remains accessible outside of Docker.

Create the directory with the command:

mkdir ~/.streamrDocker

Step 2: Configure your node with Docker and Config Wizard

Start the config wizard with the below longish command. Docker will download the Broker image unless you have it already.

Linux / macOS

docker run -it -v $(cd ~/.streamrDocker; pwd):/root/.streamr streamr/broker-node:testnet bin/config-wizard

Windows PowerShell

cd ~/.streamrDocker docker run -it -v ${pwd}:/root/.streamr streamr/broker-node:testnet bin/config-wizard

Generate or Import Ethereum private key: generate one if you’re running this for the first time

Plugins to enable: press ‘a’ to select all and then ‘enter’

Select ports for plugins: press ‘enter’ for each one to use the defaults

Path to store the configuration: press ‘enter’ to use the default

Towards the end, the wizard asks if you would like it to display your Ethereum private key. From here you should copy-paste it to a safe place! You can also find it later in the configuration file, which is saved by default to .streamrDocker/broker-config.json under your home directory.

Step 3: Start the Broker Node using Docker

Start the node with the below command:

Linux / macOS

docker run -it -p 7170:7170 -p 7171:7171 -p 1883:1883 -v $(cd ~/.streamrDocker; pwd):/root/.streamr streamr/broker-node:testnet

Windows PowerShell

docker run -p 7170:7170 -p 7171:7171 -p 1883:1883 -it -v ${pwd}:/root/.streamr streamr/broker-node:testnet

You should start to see logging similar to this:

The npm approach

If you don’t have Node.js, install it using nvm or manually from the Node.js site. The Broker requires at least Node.js version 14.x. Once installed, you can download, configure, and start the Streamr Broker.

Step 1: Install the latest version using npm

Run npm install -g streamr-broker@testnet to download and install the package. You may need administrative access to run this command.

npm install -g streamr-broker@testnet

+ streamr-broker@29.0.0 updated 1 package in 10.623s

There can be plenty of output from npm. If the installation fails with an error, you should address it before continuing.

Step 2: Configure your node with streamr-broker-init

Run streamr-broker-init to generate a configuration file using a step-by-step wizard. Answer the questions by using arrow keys and ‘enter’ to navigate.

to generate a configuration file using a step-by-step wizard. Answer the questions by using arrow keys and ‘enter’ to navigate. Generate or Import Ethereum private key: generate one if you’re running this for the first time

Plugins to enable: press ‘a’ to select all and then ‘enter’

Select ports for plugins: press ‘enter’ for each one to use the defaults

Path to store the configuration: press ‘enter’ to use the default

Towards the end, the wizard asks if you would like it to display your Ethereum private key. From here, you should copy-paste it to a safe place! You can also find it later in the configuration file, which is saved by default to .streamr/broker-config.json under your home directory.

Step 3: Start the Broker node

Run streamr-broker to start the node!

Is my node connected to the network?

Once your node is running, it should appear in the Network Explorer. You can search for it using the address or mnemonic of your node, both of which are displayed when the node starts up.

Note that the locations shown on the map are guesstimates based on IP, and are not accurate! Nevertheless, if you are concerned about your privacy, you can edit your config file and input whatever coordinates you wish, and the Network Explorer will plot your node in that location.

Staying safe

The config file contains your node’s private key. If someone gets access to the private key, they can steal your rewards! If you lose your private key, you will lose your rewards!

Don’t share the private key with anyone! Streamr staff will never ask for your private key.

Don’t share the config file with anyone! Streamr staff will never ask you to send your config file or share its full contents.

Back up your private key. The config wizard will show you the private key, but you can also open the config file in a text editor and copy the private key to a safe place later on.

How do the rewards work?

The real tokenomics and incentivisation of the Streamr Network are only implemented in the next milestone (Tatum). To substitute for those at this stage, the Brubeck testnets use a centralized mining mockup that periodically checks which nodes are alive and participating in the preconfigured reward stream. For details on how this works, check out the previous two dev updates (here and here).

The amount of tokens you earn depends on your node’s uptime, the amount of other nodes in the network, and their uptime. To maximise your rewards, simply keep your node up and running during the reward periods of all the testnets. More uptime means more rewards!

Because the reward pool grows as new nodes join, and your earnings depend on how well you’re doing compared to others, the amount of tokens your node will receive can only be computed after all the testnets are concluded. However, during the testnets you will be able to see your node’s uptime and other stats. The reward tokens will be distributed at a later time to your node’s Ethereum address, so keep that private key safe!

There will be some additional bonus rewards and surprises available during the testnets, including a contest to find the most exotic, interesting, and best looking testnet nodes. We’ll have more to share on this soon.

Using your node for publishing and subscribing

You can do more with your node than just mine the testnet incentives! Now that you have a Streamr node up and running, why not also build applications that publish or subscribe to data streams in the network? You can create streams using the Core UI, then connect your application to your node over websocket, HTTP, or MQTT. Learn more about how to interface with the protocol plugins. Note that the data you publish in the testnet will only be available in the testnet, so the data won’t show up in the Core UI.

Troubleshooting

Everyone’s setup is a little bit different, so please understand that we can not test or even anticipate all the different combinations of OS, hardware, NAT type, and so on.

That being said, the software has been successfully run on at least Ubuntu 20, macOS Big Sur (x86 and M1), and Windows 10, using both Node.js and Docker. We also tried running on Raspberry Pi, and it worked. You can help by testing more platforms and letting us know how it went!

If you’re having trouble getting your node up and running, or it prints errors after starting, make sure to join the #testnet-troubleshooting channel on Streamr Discord. There will be team members and community members there to help, as well a #testnet-faq channel, which we will continue to update.

Happy hunting, and thanks for pioneering real-time data in Web 3.0 by helping us build and test the Streamr Network at a global scale!",9117
"Exceptional Blockchain Projects

For those of you just starting to pay attention to the different “altcoin” projects, I am going to attempt to summarize what I think are the more technically interesting ones out there. While fundamentals do matter, I am in no way insinuating any of these projects are “good investments” or not. So if you are a Crypto trader looking to get tips on what’s the next 100x altcoin in the next 2 months, you are probably reading the wrong post. However, if you are interested in getting the cliff notes of what I think is technically exciting out there in this space or looking for a post that will kick you off to further research for a long term investment with strong fundamentals, this post might just be for you.

Before I begin, I am going to first note that I am skipping the trending and technically interesting projects that are already super well known like Ethereum, Cardano, and Polkadot to keep my list under a controllable size. So without further ado, here are 12 of my favorites:

Cosmos Network

Cosmos is simply amazing for building your own proof-of-stake blockchain/dapp. To get started with a full blockchain template with tokens, accounts, and wallets ready for you to start developing with, all you need to do is execute 2 commands after installing Starport:

Voila! Everything is ready to go and you can checkout the localhost link it spits out and see what’s already there:

Pretty amazing right? It’s no wonder why some of the biggest projects out there are built with Cosmos. It scaffolds everything that would otherwise be reinventing the wheel and let you focus on the real innovations of your project. You will also notice that transactions are super fast and IBC will allow for robust cross-chain compatibility which is extremely important for building successful dapps.

For those who require “name dropping” to be convinced, Binance Chain and Binance Smart Chain were built with Cosmos. You can also see the rest of the projects built on Cosmos here.

There’s so much more to Cosmos that will turn this into too long of a post so I suggest that you start with their website and documentation and see what the hype is all about yourself.

Akash Network

Speaking of building on Cosmos, Akash Network, a Censorship-resistant, permissionless, and self-sovereign open source cloud was built on Cosmos too! If you are using Kubernetes or containers at work or for fun, you will love this project. It essentially allows for you to host your containers on some of the most robust data centers and compute resources around the world for a fraction of the cost of our existing cloud services like AWS, Google, and Azure. You can learn more about this project from one of my previous posts here or simply visit their website.

Skynet

Skynet AKA SiaSky is a really cool solution for building dapps but perhaps it’s most known for its storage capabilities and how fast the content delivery is. It even has a dapp store where you can monetize your app through them easily. It seems that there’s a lot of interest in the Akash community to use Akash for compute and Skynet for storage too.

Storj

Are you building a dapp coming from a lot of experience building on AWS with S3 or other cloud providers’ block storage services? Well, Storj is probably your answer. It’s a decentralized object storage solution that’s backwards compatible with S3.

IPFS + Filecoin

The IPFS and Filecoin combo has been one of the earliest leaders on building a decentralized internet. It’s also a great solution for storing/archiving files. Although many other similar solutions have popped up over time, this project with strong ties to Stanford University has been the one that remained strong and prevailing against all odds.

Solana

Solana has been one of the most talked about small cap gems lately. Some of my crypto investor contacts are all-in with Solana’s promise for being the blockchain that’s super fast and low cost on transaction. By concept, Solana can handle up to 65,000 transactions per second. It even has a live update on how many transactions has already occurred on chain which is constantly moving on their homepage. Akash also chose Solana as the tech to handle smart contracts since the CEO of Akash deemed Solana to be simply the best solution available.

The side effect of doing super fast transactions at low costs is that Solana’s blockchain is extremely huge in size. Understanding this downfall, Solana decided to partner with Arweave to handle the permanent data storage which seems like a great move on their part.

Theta

Theta is looking to disrupt the video delivery space with blockchain just like how Youtube and Twitch did in the web2.0 days. It already has a working product with a lot of things done right. The advisory team includes some heavy hitters too like Steve Chen (YouTube), Justin Kan (Twitch), and Fan Zhang (Sequoia Capital).

NuCypher

Security and encryption seems to always be the boring topic to the average person but yet, such a pertinent part of all technologies used in the real world. As proven in the past, while the concept of blockchains are more secure than its predecessors, it’s not invincible. NuCypher is looking to provide additional cryptography infrastructure (secrets management and dynamic access control) so that users are provided with more secure dapps that are also privacy friendly.

Brave Rewards

What can I say… Brave is hands down my favorite browser. Aside being a kick ass browser to begin with, it also comes with a privacy focused blockchain based advertising mechanism called Brave Rewards. You can literally earn money from just using your browser. There’s really nothing else out there that comes close to it.

Mysterium

Mysterium is not the only player out there in the dVPN space. However, it’s the only cross-platform solution that just worked when I tried it. The best part of it is that you get to try things out free before you start buying those myst coins to pay for their VPN service.

The Graph

The Graph provides a way for developers to create subgraphs (custom API) to access the open data indexed from Ethereum and IPFS. Need I say more? It definitely is a crucial piece of the dapp equation.

The Internet Computer

I have mixed feelings about this project due to the partial closed source nature and certain aspects of governance (permissioned and require standardized hardware) but nonetheless worth a mention. This project is launched by a non-profit out of Switzerland that has been around for quite some time so it has earned a lot of confidence as a legitimate project by both retail investors as well as institutions. Recently, it got listed on Coinbase and Binance and within just the matter of 24–48 hours, it went from US$0 to US$750. What a crazy bull run or what some of my other friends might call, “pump and dump”.

So what is Internet Computer? Think Ethereum but not just covering the application layer but it plans to replace the whole entire internet stack including networking to replace our existing internet all together. In the past, they have run a decentralized version of Linkedin that is just as fast as the centralized one.

They have been in the past criticized for their Network Nervous System (NNS) component. It has been said that this component has negative implications on privacy and that rather than solving the problem of the existing Internet that has a small number of tyrants ruling it all, it will have a new consolidated tyrant instead. Does this give one organization too much power? I leave that up to you to judge.

Needless to say, since the scope of the project is so large, I would suggest that you look at their website yourself to learn more as this post would turn into a book if I had to share all the underlying concepts of what the Internet Computer is all about.

That’s it!

Hope this list of 12 technically more interesting projects will be a good stepping stone towards your research on your next big dapp project or long term investment.",8037
"Do we need a college degree at all to succeed, be happy, or make a living as African Americans? My boyfriend Joe has no high school diploma yet fabricated a jet engine out of a juice can, to my astonishment. I watched as he 3D printed a BattleBot and used his Dremel, industrial hot glue, soldering iron, and table saw to make it a functioning robot. BattleBots is an American robot combat television series that only a few select niche of humans enjoy watching. I am one of the outliers. The robot specifications to submit to be on the program include, dressing spiffy and having a unique looking build that can be branded. That seems simple enough. The component of the robot hand builds he does include the radio controller, wheels, batteries, and more. He was self-taught in the operations of finding g-code (Geometric Code) and then using the software to 3D print the things he deemed worthy (phonographs and unidentified flying objects(?)).

One day as we discussed our son’s future within the public school system as a young African American child, we came to a realization that we both wanted him in a S.T.E.M. focused program before Kindergarten even begins (science, technology, engineering, and math). The realization came when we assessed Joe’s hobbies. Joe wants his son to mirror some of the interests that he has.

“You mean S.T.E.M. hobbies”, I asked. “What is S.T.E.M.?” he said.

The list of his hobbies ranges from:

· Building functioning scale model train tables

· Building BattleBots from scratch for the past 20 years

· Building RC (radio controlled) cars for the past 20 years

· 3D printing for the last six months

· Building a BattleBot Arena Table with Alexa controlled (Fairy App) LED (light-emitting diode) addressable lights

· Installing a very loud sound system in his vehicle with little assistance

· Installing LED (light-emitting diode) addressable lights on his vehicles

· Installing LED (light-emitting diode) addressable lights on his wheel rims alone

· Building Airsoft components (everything used on the field)

· Repairing CB (Citizens Band) radios

· Welding, sawing, and drilling all his own projects and inventions (fabrication)

· Building Cryptocurrency mining rigs

· Building Hydroponics and Aquaponics set ups

· Computerized Telescopes

· Astrophotography

· Raspberry Pi set up

· Building an arcade cabinet with functioning buttons and joysticks from scratch with no blueprints

· Music production and navigating music production software

· Building a fan for a grill (attachment)

· A Colin Furze electric bike build

· Extensive knowledge on any new technology

· General extensive knowledge within the S.T.E.M. field

· A souped-up cane for the elderly

I decided on my own to assist him with building a video catalog and let our son observe the process (from a safe distance) to pique our son’s interest in the S.T.E.M. field. I conducted research on similar people to Joe. Within each of the hobbies I could find no other uneducated African American participants or content creator. I felt very defeated and completely out of my league by even presenting his hobby content to the public. The deeper I dove into the market audience that participated in these types of hobbies the more I found PhD’s and rocket scientists. My heart dropped and I tried to understand why he was not like his peers.

“Why do you like doing this stuff?”, I asked. “I have liked it since I was a kid and watched Mr. Wizard”, he said.

I have never seen the show.

Aha moment! This could pay off for our son even though the tasks are only hobbies to Joe. I propose that early childhood observational learning combined with a mandatory college degree will result in a successful life for our black son. I immediately sat my son in a corner of the mancave as Joe searched the internet for free G Code and constructed BattleBots every weekend. After about three minutes our son gets antsy and runs off into the playroom, leaving me to inhale the smoke from the soldering iron hitting the filament and wires.

I have a General Education Diploma (G.E.D.) and a Master’s degree in Adult Education and was taught that learning styles do not exist. I have observed firsthand that YouTube does assist those who learn visually. I understand that focused educational observational learning can be important for young children to steer them in the right direction career wise. The age of the child does not matter as to when you begin to introduce S.T.E.M. terminology. Reading technology focused books and consistently discussing build components will build language and knowledge skills in your child. We want to be a positive example of the need for technical language building in children.

In high paying technology careers, it is always important to build fun activities into lessons. Early lessons in tool recognition, tool safety, fabrication, modification, modeling, and creation can open a world of opportunities. Local technical colleges offer basic Computer Science & Digital Media programs and Technical & Industrial programs that can all lead to successful career or business owner pathways. This is especially true dealing with inventions, innovations, and health improvement. Obtaining a degree within the S.T.E.M. field is advantageous for global unity and problem solving.

Let us circle back to the opening question. “Do we need a college degree at all to succeed, be happy, or make a living as African Americans?” Joe has no time to invest in his hobbies because he is stuck out on the road all day as a truck driver. He is an essential worker, but because he lacks a high school diploma and subsequent college education, he cannot fulfill his passion. This is evidenced by his first YouTube video “Homemade Jet Engine Afterburner” in 2009. Twelve years later he started over again by building an arcade cabinet with retro games for his son. Studies will always show that African Americans are disproportionally affected negatively in career areas. When you do not fulfill your passion in life you are not happy. He feels successful some of the time but not 100% of the time. He is interested in the technology field of employment, but as a man who is almost 50 years old and has no hope of obtaining a degree the only way, he can share his talent is through a video sharing platform. I am determined not to make the same mistakes with our son that he made by not finishing school.",6429
"It is easy to think of Uber primarily as a ride-sharing company, similar to how we thought of Amazon to be just an e-commerce company 10 years ago.

But as Roy Amara said, we overestimate the impact of technology in the short-term and underestimate the effect in the long run.

Ten years from now, Uber’s going to look completely different. The company is silently going through a massive expansion, across verticals, that will change the DNA of its business. Over the next decade, Uber will create unanticipated revenue channels:

1) Outdoor Ads

Image Source: Uber OOH

Billboard ads will be re-imagined as physical and digital worlds merge. We’ll see ads on Uber cartops, which will open up a whole new platform for local businesses to advertise in-time ads.

This may be the first time that physical ads, at scale, can be made dynamic, digital, and location-data-driven. Read more about Uber’s OOH cartop advertising.

Knowing the financial success and dominance of Facebook Ads, we can tell how lucrative selling advertising space is as a business model would be for Uber, especially when it offers advertisers dynamic rates based on location, time, and content.

The competitive advantage for Uber would be that it knows about your physical movement, your favorite eating & drinking spots, your time preferences, and your willingness to spend money.

2) Local Commerce

Image Source: Photo by Rowan Freeman on Unsplash

What could be better than a two-day delivery guarantee? You guessed it right — a two-hour delivery guarantee.

Amazon is an undisputed leader in supply chain and has built a network of 800 warehouses in the US that allows it to capitalize on supply chain optimization. However, Uber with its ubiquitous presence and partnerships with local small businesses as well as Target, Walmart, and other big retailers, could compete with Amazon with a much larger network of micro-warehouses, i.e. the stores themselves. These micro-warehouses not only make deliveries faster but also make returns easier & less wasteful. Just for context, there are ~2000 Target and ~4000 Walmart stores in the US. And, the penetration of small businesses and local stories in Tier 3 cities and small towns is going to be even greater.

Uber can compete with Amazon on fulfillment the same way Shopify competes with Amazon on creating a digital presence for sellers. This could help with the much-needed resurgence of small businesses and give them back the power to compete!

Uber Eats is just a gateway drug to a much bigger market — Uber Direct. From fresh flowers & toothpaste to iPhones & laundry, all our household items could eventually be delivered by Uber. Could you potentially, someday, order a plumber or home cleaner from Uber? I wouldn’t be surprised if Uber competes with Handy and Thumbtack, and dominates, the handy space because of its reach.

3) B2B Offering

While Amazon started its journey as an online retailer, its technology and engineering teams soon realized the complexity of managing and scaling the IT infrastructure needed to deliver their digital experience with performance, uptime, and quality. Compute and storage wasn’t easy. But, they innovated because they had to in order to operate at Amazon’s scale. Soon, Amazon realized that this challenge wasn’t unique to their business but they were uniquely positioned to solve this problem better than anyone else. As a result, AWS was born by abstracting the technology they built for themselves and offering that as a platform to others. This then allowed other big and small organizations to outsource the complexity of IT infrastructure to Amazon per a “pay-as-you-go” model.

The timing was perfect because more and more businesses went digital and generated exponentially more data than before.

The timing is perfect now for businesses to outsource the complexity of logistics to Uber because more and more businesses are adapting to the needs for the “work from home” and “shop from home” culture.

Logistics-as-a-service can play the same role for Uber as AWS did for Amazon. Uber’s ex-CTO, Thuan Pham said “Uber’s plan is to become the AWS of logistics. The fraud-detection capabilities and the mapping capabilities and routing capabilities — all those things, we don’t see why we can’t ultimately offer that to other people to build on top of it”.

Just like Netflix uses Amazon Web Services for its computing, we shouldn’t be surprised if Uber’s competitors use their logistics service at some point. Uber Freight is going to shape this story over the next decade.

Uber Freight is just the beginning of this trend (check out their YouTube channel). There are infinite possibilities for Uber to monetize logistics AND the technology assisting the logistics as B2B offerings.

If you like this, please give it some love 👏. Do you think I missed something? Did you find any holes in my arguments? Did you think these were pretty obvious? Let me know in the comments.

If you’re interested in reading more about the possibilities that lie ahead, subscribe to this publication: Predict. Also, check out reading my other article: The future of audiobooks.",5144
"No matter what database, programming language, or webpage you might be using, dates always seem to cause headaches. Different date formats require calculations between application date pickers in user-friendly formats and system dates in backend devices and data sources. Then, programming languages each have their own libraries and structures for dealing with dates, too.

This concept in the Neo4j ecosystem isn’t any less complex with Cypher (a graph query language) date formats, the APOC library date functions/procedures, and countless possible integration tools/APIs for data import and export. I feel like I’m always looking at documentation and dealing with lots of trial and error in order to format the date just right. You may have heard about “dependency whack-a-mole,” but dates are another aspect of programming that can feel like whack-a-mole, too.

In this post, I will do my best to provide you with the tools for less random whacking and more accurate decision making when it comes to formatting dates with Cypher. You can follow along by launching a blank sandbox (free) and copying the Cypher into the browser or tweaking and running the queries for your own data set. Let’s dive in!

Time Conundrum

The general concept of time is rather confusing, and one that I did not realize was quite so complex. There have been a number of humorous and eye-opening content pieces around time being the programmer’s nightmare. Why is that?

First, standard measures of time aren’t always true. The number of hours in a day can vary depending on daylight savings time (and geographies changing at different points during the year), days in a month can vary by month and leap years, and weeks in a year can vary depending on the day of the week Jan 1st falls on and leap years. Time zones are another matter entirely. Countries change time zones somewhat frequently and different eras in the past had entirely different calendars and time zone structures.

There is a humorous and sobering comprehensive list of one programmer’s experiences of time variance, as well as an entertaining video on time zones from a programmer’s point of view. It was very valuable and educational for me to see how much time can morph, making it exceptionally complicated to calculate and present a consistently accurate measure of time. Also, thank you to my colleagues @rotnroll666 and @mdavidallen for those links. :)

Cypher Dates

Let’s start at the base with Cypher date formats. For this, we can go to the official Cypher manual and take a look at the two different sections that cover dates. The first section is for the date and temporal data types themselves. The second section is for instant and duration calculations using functions. We’ll stick with just the instant today and worry about durations and other details in another post.

The date and temporal data types in Cypher are based on the ISO 8601 date format. It supports three different categories of time: date, time, and timezone. Within those three categories are the instant types Date , Time , Datetime , LocalTime , and LocalDatetime . There are also three ways to specify timezone — 1) with the number of hours offset from UTC (e.g. -06:00 ), 2) with a named timezone (e.g. [America/Chicago] ), 3) with the offset and name (e.g. -0600[America/Chicago] ).

For this blog post, we won’t explore the LocalTime and LocalDatetime types. These types are the exception to most rules and are very rarely required because they leave valuable timezone information out of the temporal value.

Alright, let’s stop discussing concepts and see Cypher temporal types in action. We will create a few different dates using the instant types, then handle some timezone examples.

Example 1: Setting a node property to current datetime.

MERGE (b:BlogPost)

SET b.publishedDatetime = datetime()

RETURN b.publishedDatetime;

NOTE: You might notice the literal T between the date and time values. This vital little connector is easily forgotten and something we’ll need to keep in mind when we start doing translations and conversions with other formats!

Example 2: Setting a relationship property where date value equals a specific string.

MERGE (e:Employee)-[rel:ASSIGNED]->(p:Project)

SET rel.startDate = date(‘2021–02–15’)

RETURN rel.startDate;

Example 3: Setting a node property to time with time zone.

MERGE (s:Speaker {username: ‘jmhreif’})-[rel:PRESENTS]->(p:Presentation)

SET p.time = time(‘09:30:00–06:00’)

RETURN p.time;

Example 4: Setting a node property to full date time (with time zone).

MERGE (c:Conference)

SET c.startDatetime = datetime(‘2021–03–01T08:00:00–05:00’)

RETURN c.startDatetime;

To round out our instant types section, you can specify the date as parameters to the instant, and you can also access individual pieces of the instant. I haven’t run across cases where the parameter-like definition of the date is required, but I’m sure it was built in for a reason!

Here are a couple of examples.

Example 5: Setting date property using parameter-style format.

MERGE (p:Project)

SET p.expectedEndDate = date({year: 2021, month: 9, day: 30})

RETURN p.expectedEndDate;

Example 6: Setting date using date component.

MERGE (c:Conference)

SET c.year = date().year

RETURN c.year

Example 7: Find blog posts published in March using date component.

MATCH (b:BlogPost)

WHERE b.publishedDatetime.month = 3

RETURN b.publishedDatetime;

Example 8: Return date component (dayOfWeek) of created node.

MERGE (b:BlogPost)

SET b.publishedDatetime = datetime()

RETURN b.publishedDatetime.dayOfWeek;

NOTE: dayOfWeek has Monday as the start of the week. Since I’m writing this on Tuesday, these results are accurate. :)

Getting to Neo4j-Supported Date Formats

Now, these are great if you have a date/time value that is already formatted for ISO 8601. But what happens when you don’t? How do you translate a date into something Cypher will understand and Neo4j will store?

In this post, we will stick to what is probably the common temporal measurements — i.e. using year, month, day, hour, minute, second. For weeks, quarters, milliseconds, and so on, check out the docs. Also, recall that a literal T character is required between date and time in a combined value, so we’ll have to keep that in mind.

We will look at the following scenarios to get the dates converted to values Neo4j and Cypher can read:

Epoch time (formatted in seconds or milliseconds) Other date string formats ( yyyy-MM-dd HH:mm:ss and similar) Multi-conversions (one conversion wrapped in another on one line)

Epoch Time

The website epochconverter.com defines epoch time as follows:

“… the Unix epoch (or Unix time or POSIX time or Unix timestamp) is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (in ISO 8601: 1970–01–01T00:00:00Z)”.

This website is really easy to use, and I visit it quite frequently for ad hoc conversions or example dates to use. As an example of epoch time and other date formats, here is the same date in three formats:

Human-readable: Monday, March 1, 2021 12:00:00 AM

ISO 8601: 2021–03–01T00:00:00Z

Epoch time (seconds): 1614556800

Cypher does have the capability to convert epoch values for certain cases, though the syntax is a bit different than the conventions we’ve seen thus far. For other types of formats, we will go to the APOC library, which is a very popular extension for Neo4j containing procedures and functions for many different utilities.

Ok, let’s see some examples of how to programmatically convert epoch time. We will use our example epoch time from above (1614556800, which is March 1, 2021 12:00:00 AM), just to keep things simple and consistent. We will show the results of the converted value, as well as the final converted Neo4j temporal value next to it.

Example 1: Epoch to datetime using Cypher

WITH 1614556800 as epochTime

RETURN datetime({epochSeconds: epochTime});

Example 2: Epoch to date string using apoc.date.format()

WITH apoc.date.format(1614556800, “s”, “yyyy-MM-dd”) as converted

RETURN converted, date(converted);

Now, because epoch time is a date and time in a seconds format (time-based), we are unable to convert straight from epoch time to a simple date (without time). However, we could either store as a datetime and return date portions for queries….or we could use APOC to get our date!

Example 3: Epoch to ISO 8601 format using apoc.date.toISO8601()

WITH apoc.date.toISO8601(1614556800,’s’) as converted

RETURN converted, datetime(converted);

Other Date String Formats

Now we know how to convert Unix-based epoch time, but what about strings in all different kinds of formats? How do we translate them to something Cypher will read? Cypher does accept strings and can convert strings in the ISO 8601 format to a temporal value, so we just need to convert a variety of string values to an ISO 8601 string format. We can do that using apoc.date.convertFormat() .

Note: all of the possible formats in the procedure’s third parameter below are listed here.

Example 4: Similar date format to ISO 8601 string

WITH apoc.date.convertFormat(‘2021–03–01 00:00:00’, ‘yyyy-MM-dd HH:mm:ss’, ‘iso_date_time’) as converted

RETURN converted, datetime(converted);

Example 5: American date format to ISO 8601 string

WITH apoc.date.convertFormat(‘03/01/2021’, ‘MM/dd/yyyy’, ‘iso_date’) as converted

RETURN converted, date(converted);

Finally, there are a few APOC procedures that deal directly with temporal values. Only one goes to a Neo4j date format, though, and it transforms a string to a temporal.

Example 6: Datetime string to Neo4j datetime

WITH apoc.temporal.toZonedTemporal(‘2021–03–01 00:00:00’, ‘yyyy-MM-dd HH:mm:ss’) as converted

RETURN converted, datetime(converted);

Notice that both the results are the same, showing that the apoc.temporal.toZonedTemporal() function transforms directly to the Cypher datetime() value.

Multi-Conversions

Okay, so we have done several conversions that translate strings or epoch times to strings, but that doesn’t always get us to the Neo4j date. In order to do that, we can wrap our converted value in another conversion function. This isn’t really different from what we’ve seen before, but they can get convoluted and you might think “you can do that?” Yes… yes, you can. :)

Let’s take a look!

Example 7 (from Example 1 above): Convert epoch time to string and then to datetime

RETURN datetime(apoc.date.format(1614556800, “s”, “yyyy-MM-dd’T’HH:mm:ss”));

Example 8: Convert date from Twitter API to ISO date time string, then to Neo4j datetime

RETURN datetime(apoc.date.convertFormat(‘Mon Mar 01 00:00:00 -0000 2021’, ‘EEE LLL dd HH:mm:ss Z yyyy’, ‘iso_date_time’));

For a reference to the letters in that date format, the documentation is here (under Patterns for formatting and parsing ).

Wrapping Up

In this post, we covered most of the Neo4j-supported temporal instant types — date() , datetime() , time() — for creating the values either from a current instant or from an ISO8601-formatted string. We then saw how to use the utility functions in the APOC library to transform epoch Unix time values and strings in non-ISO8601 formats into strings or temporal values Cypher can work with.

There is so much more to explore on the topic of Neo4j dates. Next time, we will discuss Cypher durations for calculating the time between two instants or for adding/subtracting dates and amounts from temporal values.

Until then, happy coding!

Resources",11526
"To view the updated DevOps course(101DaysofDevOps)

Course Registration link: https://www.101daysofdevops.com/register/

Course Link: https://www.101daysofdevops.com/courses/101-days-of-devops/

YouTube link: https://www.youtube.com/user/laprashant/videos

Welcome to Day 94 of 100 Days of DevOps, Focus for today is Introduction to Numpy for Data Analysis

NumPy is a Linear Algebra Library for Python and the reason it’s so important that all libraries in the PyData Ecosystem rely on NumPy as the main building block.

Installing Numpy

# pip2 install numpy Collecting numpy Using cached numpy-1.12.1-cp27-cp27mu-manylinux1_x86_64.whl Installing collected packages: numpy Successfully installed numpy-1.12.1

It’s highly recommended to install Python using Anaconda distribution to make sure all underlying dependencies(such as Linear Algebra libraries)all sync up with the use of a conda install.

In case you have conda install https://www.continuum.io/downloads

conda install numpy

Numpy arrays are the main reason we use Numpy and they come in two flavors

Vectors (1-d arrays)

Matrices (2-d arrays)

# 1-D Array >>> test = [1,2,3] >>> import numpy as np # We got the array

>>> np.array(test) array([1, 2, 3]) >>> arr = np.array(test)

Let’s take a look at 2-D array

>>> test1 = [[1,2,3],[4,5,6],[7,8,9]] >>> test1 [[1, 2, 3], [4, 5, 6], [7, 8, 9]] >>> np.array(test1) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

But the most common way to generate NumPy array is using arange function(similar to range in Python)

#Similar to range(start,stop,step),stop not included and indexing start with zero

>>> np.arange(0,10) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

But if we are looking for a specific type of arrays

>>> np.zeros(3) array([ 0., 0., 0.]) #We are passing Tuple where first value represent row and second represent column >>> np.zeros((3,2)) array([[ 0., 0.], [ 0., 0.], [ 0., 0.]])

Similarly for ones

>>> np.ones(2) array([ 1., 1.]) >>> np.ones((2,2)) array([[ 1., 1.], [ 1., 1.]])

Now let’s take a look at linspace

#It will give 9 evenly spaced point between 0 and 3(It return 1D vector)

>>> np.linspace(0,3,9) array([ 0. , 0.375, 0.75 , 1.125, 1.5 , 1.875, 2.25 , 2.625, 3. ]) >>> np.linspace(0,10,3) array([ 0., 5., 10.])

Let’s create an identity matrix(2-D square matrix where the number of rows is equal to the number of columns and diagonal of 1)

To create an array of random number

#1-D, it create random sample uniformly distributed between 0 to 1

>>> np.random.rand(3) array([ 0.87169008, 0.51446765, 0.65027072]) #2-D

>>> np.random.rand(3,3) array([[ 0.4217015 , 0.86314141, 0.14976093], [ 0.4348433 , 0.68860693, 0.88575823], [ 0.56613179, 0.56030069, 0.51783999]])

Now if I want random integer

#This will give random integer between 1 and 50

>>> np.random.randint(1,50) 27 #In case if we need 10 random integer

>>> np.random.randint(1,50,10) array([39, 34, 30, 21, 18, 30, 3, 6, 37, 11])

We can reshape our existing array

>>> np.arange(25) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) >>> arr = np.arange(25) >>> arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) >>> arr.reshape(5,5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]])

Let’s take a look at some other methods

>>> np.random.randint(0,50,10) array([10, 40, 18, 30, 6, 40, 49, 23, 3, 18]) >>> ranint = np.random.randint(0,50,10) >>> ranint array([18, 49, 6, 28, 30, 10, 46, 11, 40, 16]) #It will return max value of the array

>>> ranint.max() 49 #Minimum value

>>> ranint.min() 6 >>> ranint array([18, 49, 6, 28, 30, 10, 46, 11, 40, 16]) #To find out the position

>>> ranint.argmin() 2 >>> ranint.argmax() 1

To find out the shape of an array

>>> arr.shape (25,) >>> arr.reshape(5,5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) >>> arr = arr.reshape(5,5) >>> arr.shape (5, 5)

To find out the datatype

>>> arr.dtype dtype(‘int64’)

Indexing in case of NumPy

>>> import numpy as np >>> arr =np.arange(0,11) >>> arr array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) >>> arr[0] 0 >>> arr[0:4] array([0, 1, 2, 3])

How Numpy array is different from the Python list because of there ability to broadcast

>>> arr[:] = 20 >>> arr array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]) # Now let's try to slice this array >>> arr1 = arr[0:5] >>> arr1 array([0, 1, 2, 3, 4]) >>> arr1[:] = 50 >>> arr1 array([50, 50, 50, 50, 50]) #But as you can see the side effect it change the original array too(i.e data is not copied it's just the view of original array)

>>> arr array([50, 50, 50, 50, 50, 5, 6, 7, 8, 9, 10]) #If we want to avoid this feature, we can copy the array and then perform broadcast on the top of it >>> arr2 = arr.copy() >>> arr2 array([50, 50, 50, 50, 50, 5, 6, 7, 8, 9, 10]) >>> arr2[6:10] = 100 >>> arr2 array([ 50, 50, 50, 50, 50, 5, 100, 100, 100, 100, 10]) >>> arr array([50, 50, 50, 50, 50, 5, 6, 7, 8, 9, 10])

Indexing 2-D Array(Matrices)

>>> arr = ([1,2,3],[4,5,6],[7,8,9])

>>> arr ([1, 2, 3], [4, 5, 6], [7, 8, 9]) >>> arr1 = np.array(arr) >>> arr1 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> arr[1] [4, 5, 6] # To grab 5(Indexing Start with zero) >>> arr1[1][1] 5 #Much shortcut method

>>> arr1[1,1] 5

To grab elements from 2-D array

>>> arr array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) #This will grab everything from Row 1 except last element(2) and staring from element 1 upto the end from Row 2 >>> arr[:2,1:] array([[2, 3], [5, 6]])

Conditional Selection: This will return a boolean value

>>> arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) >>> arr > 5 array([False, False, False, False, False, True, True, True, True, True], dtype=bool) # We can save this value to an array and perform boolean selection >>> my_arr = arr > 5 >>> my_arr array([False, False, False, False, False, True, True, True, True, True], dtype=bool) >>> arr[my_arr] array([ 6, 7, 8, 9, 10]) #OR much easier way >>> arr[arr > 5] array([ 6, 7, 8, 9, 10]) >>> arr[arr < 5] array([1, 2, 3, 4])

Operations

# It's the same operation as we are doing with Normal Python >>> arr = np.arange(0,10) >>> arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) #Addition

>>> arr + arr array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) #Substraction

>>> arr — arr array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) #Multiplication

>>> arr * arr array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) #Broadcast(It add's/substract/multiply 100 to each element)

>>> arr + 100 array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]) >>> arr - 100 array([-100, -99, -98, -97, -96, -95, -94, -93, -92, -91]) >>> arr * 100 array([ 0, 100, 200, 300, 400, 500, 600, 700, 800, 900])

In case of Python if we try to divide one with zero we will get division by zero exception

>>> 0/0 Traceback (most recent call last): File ""<stdin>"", line 1, in <module> ZeroDivisionError: division by zero OR >>> 1/0 Traceback (most recent call last): File “<stdin>”, line 1, in <module> ZeroDivisionError: division by zero

In case of Numpy if we try to divide by zero we will not get any exception but it returns nan(not a number)

#Not giving you

>>> arr/arr __main__:1: RuntimeWarning: invalid value encountered in true_divide array([ nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.])

and in case of one divide by zero it will return infinity

>>> 1/arr array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111])

Universal Array Function

#Square root

>>> np.sqrt(arr) array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) #Exponential

>>> np.exp(arr) array([ 1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) #Maximum

>>> np.max(arr) 9 #Minimum

>>> np.min(arr) 0 #Logarithmic

>>> np.log(arr) __main__:1: RuntimeWarning: divide by zero encountered in log array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])

Looking forward from you guys to join this journey and spend a minimum an hour every day for the next 100 days on DevOps work and post your progress using any of the below medium.

Reference",8369
"By Ebrahim Moolla, November 16, 2020

Africa’s mushrooming FinTech enterprises hold the key to meaningful upliftment on the continent as they deliver far more than just improved banking services to its massive unbanked market.

Some 330 million adults , or 60 percent of the adult population, in Africa lack access to the most basic financial services. The sheer size of the unserved and underserved market in Africa means that Fintech firms can make a bigger difference on the continent than anywhere else.

African startups and established companies face numerous challenges like poor or absent infrastructure, low internet penetration, funding issues, political instability, and gender stereotyping, but the Fintech environment on the continent does hold advantages not seen in other parts of the world.

A space for collaboration

African Fintechs are not disrupting the financial services sector, as in many regions, traditional financial institutions have not found it viable to serve the market in any way at all. This means there is greater scope for collaboration, rather than direct competition.

And because the financial services market has such a low base on the continent, Africa is leading the world in sector convergence, the convergence of new technologies to solve logistical challenges and the potential impact of big data.

Based on Crunchbase data, the African Fintech sector comprises over 400 active companies, 80% of which are local companies, enabling payments, funds transfer, lending, and even wealth management. Nigeria, Kenya, and South Africa are the top Fintech hubs on the continent, accounting for the larger proportion of Fintech firms and attracting the lion share of investments.

In view of these environmental factors and developments, every year for the past 3 years, no other tech sector has attracted more funding in Africa than Fintech. In 2019 alone, African Fintech startups raised a combined $678.73 million in funding .

Government support needed

Fintech is spearheading the reshaping of the financial sector, with several key sectors benefitting from the development, and African governments are under pressure to maximise potential benefits by committing to infrastructure development, education and an equitable regulatory framework.

And the coronavirus pandemic may yet prove to be the catalyst for Africa becoming a digital economy, with resulting benefits for the Fintech sector. In a bid to curb the spread of the virus, the World Health Organization has been cautioning against the use of hard currency and encouraging the use of digital payments. This move has prompted African governments and regulators to enforce measures aimed at facilitating more cashless transactions.

Now more than ever before, Fintech is poised to become a crucial stepping stone that will propel Africa into becoming a truly digital economy. The sector remains a key driver for what many hope will be an African Renaissance marked by peace, prosperity, and cultural rejuvenation.

HANGAR49, as an outreach optimization business focused on building valuable human connections with the aid of digital technologies, is committed to supporting and partnering with Fintechs in Africa and around the world in their financial inclusion mission. Speak to us today to look at your options for uncovering revenue opportunities.",3351
"Researchers have traced dentistry back to 7500 B.C.! In the “olden days,” drills powered by foot pedals, wooden dentures, and stain-removing chew sticks were routine. Thank goodness we live in 2021, right? Today, we have access to advanced technology, which has enhanced the field of dentistry significantly. However, just because the newest dental tools are available, doesn’t mean your dentist is using them. Unfortunately, this can impact the quality of your care, affecting your oral and overall health for the worse. To prevent that from happening, make sure your dentist utilizes the latest dental technology, like the following!

Innovative Intraoral Cameras

Previously, dentists had one tool to detect cavities and other dental problems: their eyes. As a result, the only way to communicate their findings to the person in their treatment chair was by describing the issue or attempting to show them with a small mirror. Since neither of these options are ideal, there was naturally some distrust between patients and their dentists. Luckily, intraoral cameras have changed all that! With a tiny camera that produces crystal clear images, dentists like me can show you what we see in real-time, allowing you, the patient, to feel confident with the recommended treatment plan.

State-of-the-Art Soft-Tissue Laser Dentistry

From gum recontouring to gum disease therapy, many dental procedures required scalpels and sutures in the past. As a result, the risk of infection was high, and the recovery time was slow. Thanks to soft-tissue laser dentistry, that’s no longer the case! Instead, a precise, state-of-the-art laser can be used to treat various restorative and cosmetic concerns. The best part? This tool cauterizes the area instantly, eliminating the need for local anesthesia, resulting in quicker healing, and making the overall experience much more enjoyable.

All-Digital X-Rays

Without X-rays, issues brewing underneath the gumline could fly under the radar for months or years. Previously, checking for cavities between teeth and watching the development of wisdom teeth required traditional X-rays. While effective, the exposure to radiation left much to be desired. Luckily, patients today can eliminate this issue entirely with digital X-rays! Equally as effective and extremely precise, this state-of-the-art technology benefits your oral and overall health.

Comfortable iTero Digital Impressions

If you’ve had braces or a dental crown, then you likely remember the weird, putty-like substance that you had to bite down on when getting the impressions of your mouth. Unfortunately, this method left much to be desired in both taste and function, making patients feel claustrophobic and panicked. Thanks to advancements in dentistry, there is an alternative: iTero Digital Impression technology! In this case, a small, handheld wand is carefully run over your teeth, taking hundreds of pictures in the process. These images come together to create a 3D digital mold that can be used for everything from Invisalign to nightguards!

Big-Picture CBCT scanner

Monitoring your teeth and gums are certainly important, but checking on your bite, jaw, and overall facial structure is equally as crucial. Thanks to 3D cone beam imaging with a CT Scanner, dentists can get a big-picture perspective. As a result, there is no guesswork and problems are diagnosed sooner. This is especially helpful when it comes to advanced treatments like dental implants or braces.

With the above state-of-the-art equipment, you can achieve your smile goals with complete peace of mind. And you deserve nothing less! So, if you’re ready to experience comfortable, precise, reliable, enjoyable dentistry, then visit a dental office that utilizes the latest technology!",3771
"The global veterinary oncology market size was estimated at USD 199.9 million in 2019 and is anticipated to witness a CAGR of 10.7% during the forecast period. The increasing pervasiveness of cancer in the pet population and rising adoption of veterinary cancer therapy as a treatment method are propelling market growth. Growing expenditure on animal healthcare in the developed economies due to rising willingness to spend on pet healthcare by owners and initiatives by government for the veterinary cancer treatment are the major factors driving the market growth.

In 2016, the American Animal Hospital Association (AAHA) stated that neoplastic disease was the most common terminal pathological procedure in 73 out of 82 canines and also the reason for death in canines with 1 year of age. The disease often leads to traumatic injury. In addition, around 32% of the feline population was detected with Feline Leukemia Virus (FeLV). FeLV was also reported to be one of the foremost causes of lymphoma in feline. In addition, in October 2019, the American Veterinary Medical Association (AVMA) anticipated that approximately 1 in 4 dogs will develop neoplasia at some stage of their lives. Therefore, with the rising number of these cancer incidents, there is a necessity for effective veterinary oncology treatment in these animals.

𝐅𝐨𝐫 𝐌𝐨𝐫𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧,𝐫𝐞𝐪𝐮𝐞𝐬𝐭 𝑭𝒓𝒆𝒆 𝐬𝐚𝐦𝐩𝐥𝐞 𝐜𝐨𝐩𝐲 𝐇𝐞𝐫𝐞 ➦ https://straitsresearch.com/report/veterinary-oncology-market/request-sample

Veterinary oncology is a sub-segment of veterinary medicine that deals with cancer diagnosis and treatment in animals. The treatment is highly dependent on the cancer type, stage, location, and nature of cancer, which include targeted therapy, combination therapy, and chemotherapy.

Key Players

PetCure Oncology

Accuray Incorporated

Varian Medical Systems

BoehringerIngelheim

Elanco

Zoetis Karyopharm Therapeutics, Inc.

One Health

VetDC

PharmAust

Nippon Zenyaku Kogyo Co., Ltd.

By Region",1967
"The global ankylosing spondylitis market size stood at around USD 4.2 billion in 2019 and is projected to reach USD xx billion by 2028, exhibiting a CAGR of xx% during the forecast period.

The introduction of novel products, as well as the entry of biosimilar, is expected to drive market growth over the forecast period. The increasing prevalence of the disease is further anticipated to drive the market. According to a study published in Oxford University Press, the number of cases in Europe was 1.30 to 1.56 million and 4.63 to 4.98 million in the Asia Pacific. Commercialization of novel products and increasing collaborations between companies is expected to accelerate market growth.

Market Drivers

The increasing prevalence of ankylosing spondylitis itself is a major factor impelling the global ankylosing spondylitis treatment market. In addition, factors such as rising awareness, technological advancements, government initiatives and improvement in the reimbursement scenario augur well for the global ankylosing spondylitis treatment market. However, factors such as high treatment costs and a dearth of experienced professionals are restraining the global ankylosing spondylitis treatment market. In addition, a low adoption rate of ankylosing spondylitis treatment options is restraining the market.

Growing population and economies in the developing countries such as India and China are expected to offer lucrative opportunities for the global ankylosing spondylitis treatment market. Innovation of new, more efficient products is expected to offer growth opportunities for the global ankylosing spondylitis treatment market. A rising number of mergers and acquisitions, new product launches, and more collaborations and partnerships are some of the latest trends that have been observed in the global ankylosing spondylitis treatment market.

Visit https://insights10.com/ for more healthcare industry insights.",1935
"Connected Agriculture Market 2021: Explore Top Factors that Will Boost the Global Market by 2026 shubham k Dec 22, 2021·4 min read

Connected Agriculture Market 2021–2026

Straits Research has recently added a new report to its vast depository titled Global Connected Agriculture Market. The report studies vital factors about the Global Connected Agriculture Market that are essential to be understood by existing as well as new market players. The report highlights the essential elements such as market share, profitability, production, sales, manufacturing, advertising, technological advancements, key market players, regional segmentation, and many more crucial aspects related to the Global Connected Agriculture Market.

Connected Agriculture Market

The Major Players Covered in this Report:

Some of the notable players in the connected agriculture market are IBM Corporation, Microsoft Corporation, AT&T, Deere & Company, Oracle Corporation, Iteris, Trimble, Ag, SAP SE, Accenture, Cisco Systems Inc., Decisive Farming, Gamaya, and SatSure.

Get a Sample PDF Report: Connected Agriculture Market

Segmentation is as follows: -

By Component,

Solutions , Platforms , Services

By Application,

Pre-Production Planning and Management , In-Production Planning and Management , Post-Production Planning and Management

The report specifically highlights the Connected Agriculture market share, company profiles, regional outlook, product portfolio, a record of the recent developments, strategic analysis, key players in the market, sales, distribution chain, manufacturing, production, new market entrants as well as existing market players, advertising, brand value, popular products, demand and supply, and other important factors related to the Connected Agriculture market to help the new entrants understand the market scenario better.

Important factors like strategic developments, government regulations, Connected Agriculture market analysis, end-users, target audience, distribution network, branding, product portfolio, market share, threats and barriers, growth drivers, latest trends in the industry are also mentioned.

Regional Analysis For Connected Agriculture Market:

North America (United States, Canada, and Mexico)

Europe (Germany, France, UK, Russia, and Italy)

Asia-Pacific (China, Japan, Korea, India, and Southeast Asia)

South America (Brazil, Argentina, Colombia, etc.)

Middle East and Africa (Saudi Arabia, UAE, Egypt, Nigeria, and South Africa)

In this study, the years considered to estimate the market size of the Connected Agriculture are as follows:

• History Year: 2014–2019

• Base Year: 2019

• Estimated Year: 2020

• Forecast Year 2021 to 2026

The main steps in the investigation process are:

1) The first step in market research is to obtain raw market information from industry experts and direct research analysts using primary and secondary sources.

2) Extracts raw data from these sources to extract valuable insights and analyze them for research purposes.

3) Classify the knowledge gained by qualitative and quantitative data and place it accordingly to make final conclusions.

Key Questions Answered in the Report:

• What is the current scenario of the Global Connected Agriculture Market? How is the market going to prosper throughout the next 6 years?

• What is the impact of COVID-19 on the market? What are the major steps undertaken by the leading players to mitigate the damage caused by COVID-19?

• What are the emerging technologies that are going to profit the market?

• What are the historical and the current sizes of the Global Connected Agriculture Market?

• Which segments are the fastest growing and the largest in the market? What is their market potential?

• What are the driving factors contributing to the market growth during the short, medium, and long term? What are the major challenges and shortcomings that the market is likely to face? How can the market solve the challenges?

• What are the lucrative opportunities for the key players in the Connected Agriculture market?

• Which are the key geographies from the investment perspective?

• What are the major strategies adopted by the leading players to expand their market shares?

• Who are the distributors, traders, and dealers of the Global Connected Agriculture market?

• What are sales, revenue, and price analysis by types and applications of the market?

For More Details On this Report: Connected Agriculture Market

About Us:

Regardless of whether you’re looking at business sectors in the next town or crosswise over continents, we understand the significance of being acquainted with what customers purchase. We overcome the issues of our customers by recognizing and deciphering just the target group, while simultaneously generating leads with the highest precision. We seek to collaborate with our customers to deliver a broad spectrum of results through a blend of market and business research approaches. This approach of using various research and analysis strategies enables us to determine greater insights by eliminating the research costs. Moreover, we’re continually developing, not only with regards to where we measure, or who we measure but in how our visions can enable you to drive cost-effective growth.

Contact Us:

Company Name: Straits Research

Email: sales@straitsresearch.com

Phone:

+1 646 480 7505 (U.S.)

+91 8087085354 (India)

+44 208 068 9665 (U.K.)",5446
"Baudrillard’s work from1981, Simulacra and Simulation is premonitory in many ways. It is contemporaneous in the most insightful form — as it notices the first signs of our ever-increasing fictional world and describes it on its totality. Another work that is premonitory, without using any word, is the 2002 film Naqoyqatsi of Godfrey Reggio (with music from the giant Phillip Glass). Using only images, animations and music, this film shows the non-stop shift from a material world to one of bits and bytes, zeros and ones, immaterial matter — the real turned non-real — as the negatives of cities and nature transposed into data.

How to make sense of the times we live? If everything in 1981 already seemed a complete simulacrum to Baudrillard — (power, money, politics, TV, pornography, means of production and even the idea of nuclear annihilation), 2021 has elevated the simulacrum to new heights. Our means to escape it are ever more reduced. There is no space for dissidents — on the sense of a possible system that goes back to the real instead of continuing the illusions. Pointing out the fictional nature of our whole society is possible, but it changes nothing. Two of the biggest bestsellers of the 2010 decade, Sapiens and Homodeus from Yuval Noah Harari clearly point out all our historic collective fictions. It is as if we are slightly aware of our condition — but changing it is impossible. As many said, our current trajectory is much less of an Orwellian 1984 type of domination but more of a Brave New World Huxley one. People are ‘happy’ and not even aware of any domination — the system triumphs over the individual when the individual loses the capacity to imagine other possibilities out of it — nothing is worth fighting anymore.

2021 is opening up the new frontier of capitalism — why only market and sell things on the real/material world if many other virtual ones are possible to be exploited? This was already possible in a fringe (soon to be the main) sector of capitalism — gaming. A growing number of people, majority men — same as the metaverse proponents, already spend vast sums on in-game purchases. In 1971, money lost any left connection to the real (the end of Bretton Woods System). In the 2010 decade, the crypto currencies gained the world. When it seemed that money (and the ‘things’ it can buy) could not get any more fictional, the internet transformed it into a “simulation of a simulacrum”, if updating Baudrillard is even possible.

Nevertheless, money and gaming are only the first of being completely pushed into a new all encompassing ‘reality’ of its own. Our ever more complex societies seem doomed to a common collective future where the real and the fictional are one — from our joint consciousness to the most basic bodily sensations. 2021 also popularized the Non Fungible Tokens (NFTs) — private property transferred to virtual reality. If on a first level a photo or film keep certain relation to the real, a NFT is a second level of dematerialization of our world and perceived ‘reality’. It simulates, in a virtual world (per se already a simulacrum), the simulacrum of an image/photo/video — and, as if magically, it acquires ‘value’ and it is deemed proprietary.

So we get to the Metaverse — since a few months ago, it is not anymore a fiction or dystopian view presented in sci-fi books or films. It is openly THE next frontier of capitalism. Facebook (now naturally called Meta) was the first to be vocal about it, as it is desperate to keep its relevance. Besides Facebook/Meta, the whole system has fast enough noticed that the material world is too small for their ambitions. If capitalism main drive is continuous growth — how to keep growth happening in a material world — by definition, scarce? The internet, gig economy and never ending governmental quantitative easing’s have kept the system afloat since the financial crisis of 2008. Now, the technology is catching up to make the impossible possible — eternal growth.

From now on, our history might be separate into two — one of the virtual, where all possibilities are abundant, and one of the abandoned ‘real’, where pain and scarcity still exists. The future decade’s pressures are more and more on the deterioration of what is left of the real world — climate change, pandemics, automation of work, decreasing populations (first in the West, then in the rest of the world), and scarce resources. The work that produces material things were/are the first to be automated — first in agriculture, then manufacturing and now finally services. In such a decadent material world, continuous growth would not be possible anymore — but with virtual ones, duplicates — perfected forms of our real realm — eternal growth is not only possible, but in essence, the norm.

However, how would this metaverse be? We all had to laugh when we saw the ridiculous avatar of Mark Zuckerberg introducing the concept to the World. This is probably the version 0.1 of this next frontier. If we think a bit more, we already have the whole world mapped by satellite pictures, Google Street view, 3D maps of cities (over and underground), all human knowledge and trillions of pictures/videos willingly uploaded by the public. Naqoyqatsi poetically shows all this flow of data being transferred from the real to the virtual. The Metaverse would be the organizational platform of all this flow of data — the internet does it in some sense, but does not integrate it sufficiently. Today, there are multiple AIs doing some work with this data. In a fully developed metaverse, this virtual reality will become reality — or an ‘ameliorated’ problem free version of it.

It would not be surprising if still on this decade we have ways to evade our real bodies and live in a ‘simulated simulacrum of reality’. The logic of capital unequivocally leads to this virtual world, where growth is still possible. Soon enough, many of us will have a VR headset, with special glasses and headphones. It will permit us to go to the museum or cinema from our beds or sofas — in a 3D simulation. After it, new appliances will come by — a treadmill so you can safely walk in the metaverse from inside your home. Why would you go for a walk around the block if you can put your glasses on and walk with your friends on a simulated New York City, or Rio de Janeiro, if you want a tropical experience with a bit of a thrill? All while seeing some advertisements of products to buy in the virtual and ‘real’ worlds.

That is just the beginning… One that we can still imagine and deem as highly possible. But the more data there is, the more the virtual realm can grow and replace the material one. Data is the immaterial flow of this non-stop capitalistic drive — the end goal, in a sense that not even capital ever was. There is never enough data. More and more will be produced, processed and stored. The capabilities are more or less there — super fast internet (5G), increasing data centers, quantum computing, health trackers on human bodies, machine-brain interfaces, internet of things… The goal is for the AI to know how to reproduce material things in a virtual setting — it will replicate a fridge or a microwave by the ways of the data the ‘real’ appliance transmits. Moreover, this metaverse will be updated to know exactly our body sensations — our heart rates, blood pressure and breathing when walking or having sex. Then the texture, smell and taste of food. Perhaps even the feeling of a warm sunrise in a tropical beach. The more we transmit, the more it can render the virtual as real. In no time, it can reverse engineer those sensations. Soon many would not want all the burdens of dressing up to go outside in an ever more unpredictable climate. From the comfort of home, we can ‘enjoy’ all there is to enjoy. And even have the best orgasm of our lives — free of contact to another one and the risky air or bodily fluids contaminated with a new pandemic pathogen.

Pandemic — who would have imagined its consequences and the level of alienation it brought us? The COVID-19 pandemic probably accelerated this ‘metaversing’ of reality in many years. Overnight, being distant was a demonstration of love and communal fraternity. Office work, already immaterial and immeasurable, became ever more ethereal. The home became our offices — time and place for work and for anything else became one and only. We fear engaging with other human beings, so the communal aspects of our lives moved even more online. It is no surprise that the metaverse was announced in 2021 — it comes as a savior for all anxieties of an unsafe and contaminated world. It will simulate life, and it will market itself as the ‘happy and fun’ ‘life’ (Zuckerberg is already trying it) — if the outside world is dangerous and unclean, the metaverse is its opposite — the Promised Land: free of all adversity, viruses, bad feelings, pain or even death…

Since we humans started to organize labor outside the limits of mere survival (since we stopped being hunter-gatherers), we have been moving to an ever more ‘fictional’ world. It seems as if intelligence, language and an organized society natural path are toward living its own joint fictions. Any economic system is a fiction in this sense — it only exists because we humans imagine it. At some point, the fiction seems to have its own life — it lives and multiply not because of its actors — it is more than the sum of its parts, it is a total new entity on itself. Capitalism has been such for a couple of centuries. We now reach a level that all the incentives, a word that economists love, are towards perpetuating growth at all costs. It does not matter whether a big part of this growth is also a fiction — return on investments made with money created overnight by a central bank to avoid a new crisis. As long as we believe, we shall be redeemed by the duality of capital/data.

If not Facebook, any other would step in to build the metaverse — actually, it will be a multitude of actors doing so. The economy wants to make the metaverse possible — and it wants people to engage with it endlessly, making our social media feeds become entirely our social media lives. If all the mighty of human intelligence and the incipient artificial intelligent is to make growth a normality — more productivity, more resources, more people (it has been such for millennia until now) — duplicating our material world in the immaterial realm is the way forward. An immaterial realm that is monetized, in a similar way to our material one. Why one would own only real estate in our decadent material world if we can buy a perfected version of it in the Metaverse? Not to talk that real state in this world depreciates — in the Metaverse, it can only perfect itself with never ending updates. Rents can be charged, ads can be shown, consumption can continue unbothered on ever more ‘simulated simulacrum’ of experiences. Baudrillard’s Hyperreal giving place to a Hyper-unreal, a Hyper-unreal that only exists because we believe.

With work in the ‘real’ world being more automated, meaning shorter working days attainable in the next decades, one would turn to the metaverse for ‘producing’, for entertainment and to escape the ‘real’ — devoid of the material boundaries and its suffering. Baudrillard would say that we already do it for decades with our TVs. In the metaverse, it will be the complete triumph of the simulated — maybe many will even sleep plugged in the metaverse, so to have a minimum interaction with this ‘real’ realm and its problems.

This is the first of two essays on capitalism, metaverse and the idea of living in a simulation. On the next one, I will explore more the consequences of the metaverse on the very philosophical question of ‘are we a simulation’? If we are ever to produce conscience in a virtual setting (an AI that reaches intelligence levels comparable to a human) — this duplication of consciousness would be only possible through immense flows of data — something that only a metaverse constantly fed by billions of humans could assemble.",12101
"Ever since Facebook announced Meta, talk about the metaverse has been limited to two topics:

How to invest in it. Why it sucks

Both ways of thinking have major problems. In this article, I am going to touch on why both of these thinking patterns are wrong, and why the metaverse could be extremely beneficial (if we approach it correctly).

Note 1: Since there are many definitions, this article will be using Oxford’s definition of metaverse: “a virtual-reality space in which users can interact with a computer-generated environment and other users.” AR should be included in this definition. Even though there are many other aspects to the metaverse/web3.

Note 2: Unless otherwise stated, this article is not talking about Facebook/Meta’s metaverse. Instead, it will be about the concept in general.

Note 3: Not much is known about how the metaverse will turn out, or even what it will be. Because of this, I will use indefinite words like “could” and “may” quite often.

Build it, Don’t Buy it.

Most articles about the Metaverse on Medium (and other sites) are about which companies (Meta, Epic Games) or cryptocurrencies (Decentraland) are the most likely to make you money if you buy their stocks or tokens. This is very disturbing. A world where most people either think the metaverse is The Matrix or their next get-rich-quick strategy is bound to create a lot of problems and polarization.

This dynamic will likely cause the metaverse to repeat the same problems that the current iteration of the internet has. That is, making money at any cost and without thought to how a platform can negatively impact how society functions. We risk creating yet another, bigger, Facebook-like dilemma. Everyone knows Facebook is, in part, causing harmful societal shifts like skyrocketing suicide rates, political polarization, and misinformation. Even with this knowledge, very few people feel like they can delete Facebook or any of its counterparts like Insta or TikTok without missing out.

Right now, this type of FOMO, at least financially speaking, is the main thing driving people to metaverse projects.

Imagine what we could build if instead of saying, “I want money regardless of the consequences” we said, “Let’s build something really cool and that fixes problems”. The metaverse is inevitable and there is no hard line between it and the internet. The internet will, beyond any doubt, slowly morph into the metaverse. How it works and how it happens can either be up to us or, if we let it, up to Facebook/Meta’s dream dystopian virtual technocracy.

No, it Doesn’t Suck (or at least it won’t)

Well, actually right now it sort of does suck. But claiming it is going to stay that way is highly ignorant of how technology develops.

For example, let’s look at a picture of one of the first video games: Pong

And now, something a bit higher end:

Let's hope the metaverse isn’t anything like The Matrix though…

The point is virtual and augmented reality are relatively brand new. As an AR developer, I can attest to the fact that this tech, that will underpin the metaverse, is innovating much faster than most people realize.

So, does it suck? Yeah, right now most metaverse projects look underdeveloped. Can it transform as much as video games did between Pong and GTA? Yes. And that gives it the power to transform the world.

Problems with Technology:

Every new technology that has ever been invented has had two things:

Made up Problems:

All technology has had opponents to it that out of fear make up reasons why the technology is bad. When lightbulbs were first invented there were many people who claimed they caused blindness. When trains were first invented many claimed they caused back problems.

These problems seemed true at the time and only after years of the technology existing did they fade from public thought. Often focusing on wild ill-thought consequences of technology keep us from clearly thinking about what problems it will actually cause. And this just creates more problems because our focus is on the wrong problems, false fear-driven problems.

Real Problems:

New technology always causes real problems. Often these problems were unseen at the beginning. Trains did not cause back problems; they cause something much worse: climate change. I suspect the metaverse will have similar unexpected issues if we are not meticulous at thoughtfully building it into something that complements our lives and doesn’t replace them.

How the Metaverse Could Help.

The metaverse could fix (or at least not worsen) some of the problems people claim it will create. Namely these:

The inability for people to escape from technology and screens.

The metaverse could actually do the opposite. Here is a scenario of the world as it is right now:

You drive to the restaurant using a car with a GPS screen telling you where to go as you skip through songs on your phone (screen) You look out the window and see a giant screen advertising Horizon Worlds and soon arrive at your favourite restaurant to have a meal with your favourite people. You sit down to eat and talk with your friends. You order on a tablet that sits on the table. After a few minutes, half of your friends are watching NFL on one of like 20 TVs in the restaurant. The other half are casually scrolling through TikTok. Your phone buzzes to tell you that Karen posted saying that having a vaccine card makes you the equivalent of a Nazi and you just can’t let that slide.

Sound familiar? If not, tell me your secret. The fact is whether you like it or not nearly every situation you are in has screens trying to grab your attention. A metaverse would change this drastically. By simply taking off your glasses every augmented “screen” you see would disappear and the two realities are now completely distinguishable and being focused on the “real” world has never been easier. In a sense, reality is now augmented less than it was before metaverse.

Increased addiction/reliance on technology and social disconnect.

Honestly, it would be hard for people to spend any more time on screens when we already spend an average of over 7 hours on screens per day. Increasingly, our jobs are online, and most people spend a significant amount of time with people not actually “with” people. So, what difference will the metaverse make?

It could bring back face-to-face communication. Instead of texting endlessly with people thousands of miles away, you could talk in a sort of 3D facetime where it looks like the person talking to you is in the same room as you. It could bring back natural conversations. And this does not mean it is replacing real face to face conversations, it is replacing other less natural forms of digital communication. Can you really tell me texting is better? Think bigger, better version of Netflix Watch Party. Instead of endlessly sitting at your home office by yourself on your screen, you will be able to work with people as if they are there with you. And this applies to everything whether it is work, or binge-watching your favourite shows. While this may be bad in many ways it is better than the trajectory we are currently on where people are isolated in both the physical and digital world. This solves half the problem. Let’s work together to solve the other half. Remember, in-person work will always be necessary. The metaverse won’t replace physical in-person work as much as it will replace unhealthy existing remote work environments.

People will be able to make themselves appear however they like and get rid of any imperfections, creating unrealistic expectations of reality.

This is already true. People project their fake lives on social media anyway. On social media, users project a fake life and personality. The ability to fake good behaviour on social media may lessen with a metaverse. The reason for this is that many things will go from being pre-recorded to being live. You can’t re-record things that are live.

People who opt to not use the metaverse will have a hard time making money in this new economy.

Eventually, this will most likely happen. Not because of anything specific to the metaverse. This is true of nearly ALL technology. Imagine trying to make a living without using a: phone, car, lightbulb… Lightbulbs are a good thing and yet without them, it is difficult to participate in all modernized economies.

The Good Stuff

As we have seen the metaverse will most likely have the same problems the internet has (just to varying degrees). Many things may improve, and some things will worsen. Now let’s talk about how the metaverse could make things better.

Limitless Space.

Let’s say you are a non-profit looking to host a fundraiser. You want to invite 400 people. However, there is not enough space in the venue. In the metaverse, you could invite 100 people to the physical fundraiser and invite literally everyone to a photo-realistic 3D live-streamed version of it.

Accessible Technology.

Knowledge is power. Nowadays the main way to gain knowledge is through the internet. The current environment where the richer you are the more access you have to better technology must stop. To be on top of the digital economy and learning you need a: blazing fast PC, TV, mouse and phone. Slowly the metaverse could make it so that all you need is a set of AR glasses. Financially strapped people won’t need to worry about garnering an endless array of devices.

Censorship.

This is a double-edged sword. The metaverse could eliminate many forms of censorship. In many cases, this is a great thing. The problem becomes that it also becomes hard to censor illegal content. This may not be any worse than it is now. It is already nearly impossible to censor content on the internet as a whole.

And a lot more but this article is too long…

Conclusion:

Let’s not let companies like Facebook (sorry) Meta, who have over and over again displayed their irresponsible money-at-any-cost business model, create the technology we will depend on in the future. Instead, a collective effort among everyone building the metaverse in a thoughtful and at times self-sacrificial way (not all about money) is necessary. This transforms the metaverse from a threat to a marvelous opportunity to build a better world. Oh, and look we have the tools to build it ourselves: cryptocurrencies!",10350
"We are happy to announce our partnership with InfinityPad that will be supporting us in becoming one of the world’s first Blockchain backed gaming portal to help the eSports industry tackle major problems such as insecurity of funds and fraud, absence of a dedicated portal, and lack of secure financial compensation for players and teams.

InfinityPad is a revolutionary launchPad that helps in raising capital across multiple blockchains on a single platform in a fully transparent and decentralized way. It is a one-stop solution for both: Projects launching IDOs and Investors holding $INFP tokens. Taking the lead, InfinityPad is the first launchpad to introduce guaranteed time-weighted allocation. This means the longer you’re in the InfinityPad pool and the more INFPs you hold, the bigger the allocation you will get. This would in turn eliminate the need for gas wars which happens in most of the launchpads in the Blockchain space.

Lately, CLG has secured a seed investment and is working closely with its team to accelerate the development of its eSports-only NFT marketplace and Decentralized Application. The team has built an extensive community of developers and partners to work move forward with the project development.

In order to make our upcoming IDO a success, team CLG is here to clear each and every confusion our investors might have. As for now, the IDO date and the process of participation are to be announced. Make sure not to miss this golden opportunity and follow all our social channels to stay updated with the happenings around CLG.",1570
"Frequently Asked Questions

Purchasing a new property can be daunting and confusing with information from different sources. The same goes for digital Land Sale purchases. We’ve compiled the most asked questions from our community to help you better understand the land purchasing & lottery process at My Neighbor Alice Land Sale.

If your question isn’t here, feel free to reach out to us on telegram or email. We will be happy to chat about any questions you may have.

Q: When will I be able to make a deposit?

A: From the 26th of May.

Q: When does the staking begin?

A: From the 28th of May.

Q: For how long do I need to stake my tokens?

A: You need to stake minimum 1 day to get 1 ticket.

Q: Which tokens are eligible to stake?

A: ALICE and CHR.

Q: When is the last day to start staking?

A: 26th of May + 14 days = June 10th (before 10:00 UTC.)

Q: Since you can get several lottery tickets, can one user buy more than one plot?

A: Theoretically, yes.

Q: How can you stake 50 ALICE when you are actually staking LP tokens?

A: User has to send 20 ALICE tokens for plot collateral (which will be held should he win the plot, or can be withdrawn otherwise), We will use the sending address and check how many tokens of that addressare in liquidity pools. We can add locking contracts for people that don’t want to be liquidity providers should we see the need.

Q: You mentioned that we need to send 20 ALICE to register for the sale. Will details on that be announced later?

A: You’ll be able to transfer the 20 ALICE from May 26th when the land sale begins.

Q: Can we get bonus tickets for staking longer than 14 days?

A: No, you can get tickets for 14 days only.

Follow up Q: Is it a concern that liquidity pools will empty after the 14 days? Why not allow users to continue accruing tickets for subsequent sales?

We will continue to encourage people to participate in the liquidity pools and design yield farming programs in the near future. We will keep the land sales event separate from yield farming and for the follow-on land sales, we will inform the rules before the actual sales.

Q: Is this game only for rich people, what about the people who really want to play this game. It’s very expensive to buy land, even after a discount.

A: You don’t have to own land to be able to play and enjoy My Neighbor ALICE. 50% of the land will be reserved for rentals and public facilities. So even if you won’t be able to own a piece of land, you could still have fun in the game.

Land can be shared among friends (atm 10, but we plan to increase this limit). This means a cost per capita of less than 20€.

Q: It seems that after a user transfers the 20 ALICE to qualify for registration, they can use their tickets on said plot and the more tickets they use, the more likely they are to win the registration. Is this correct?

A: The tickets are fungible. The lottery gets one winning ticket per each plot. So the plot assignment will be casual. It is not possible to express preference atm.

Q: In terms of “For CHR”, you could stake the tokens in the official staking contract”, you mean the staking on Chromia´s official website?

A: Yes

Q: If I stake CHR and only hold CHR and win the lottery, will you convert my CHR to 20 ALICE to purchase the land or must I hold ALICE?

A: You need to deposit 20 ALICE in order to be able to participate in the lottery.

Follow up Q: Since you can only send the 20 ALICE on May 26, so just staking would be enough to generate tickets?

Yes, staking will be enough to generate tickets.

Q: Where do I buy the land if I win the lottery?

A: On the My Neighbor Alice marketplace website which will be available from May 26th.

Follow up Q: Will winners be granted access to the marketplace website at the same time and just race to buy the land they want?

A: They win a specific — random — land. In the future we plan other sales mechanisms.

Q: Why are you using this kind of model? Why can’t you offer land sales with a fixed amount for anyone to buy like other games have done.

A: We want to make the game accessible. If we had an auction only rich people would be able to buy the land. If we just have low prices there won’t be enough land available for everybody.

Q: When should they send the 20 ALICE to the address?

A: From 26th of May.

Q: What is the correct Uniswap link to the ALICE-ETH pair?

ALICE-USDT: https://info.uniswap.org/pair/0xde93684627d4e34d6ef96adf5bcabf10bbd8dd81

ALICE- ETH：

https://info.uniswap.org/pair/0x30bc873193466dc59fc08159460c49f26f609795

ALICE — BNB in Pancake Swap:

https://pancakeswap.info/pair/0xe022baa3E5E87658f789c9132B10d7425Fd3a389

Q: In what form would the ticket be?

A: Digital.

Q: What happens when my 50 ALICE in the liquidity pool fluctuates? Does that disqualify me for a ticket?

A: No, we calculate how many ALICE tokens you have at the time of the snapshot in the LP. So you can expect variation from one day to the other but your tokens will be counted.

Q: Do you have a fixed time in UTC on when you take the snapshots?

A: Done. It’s 10:00 AM UTC.

Every 24 hours we check if your tokens are still there for 14 days.

Q: As I am staking CHR for the lottery tickets, and ALICE for the possible land purchase, how do you guys know, I am the same person staking CHR and ALICE? Does it need to be from the same wallet?

A: We will issue tickets for the staking participants, and the receiving address will be the same as the address that participates in the land sales.

Q: If my CHR has just been staked in the site since last year, do I need to restake it on the 26th or do I automatically qualify once I send the 20 ALICE over to signify my intent in joining?

A: You will need to claim the tickets once the land sales are started.

Q: If I win twice in the lottery, do I need to send the 20 ALICE right away? When?

A: You will need to send in the 20 ALICE first before being qualified for the lottery.

Q: What happens to all of the ALICE bought from the land sale? Is it burned?

A: Proportion of the sales revenue from land sales will be contributed to the DAO in the future and that token holders could decide either a buy-back and burn design or to fund community development initiatives.

Q: Can I use LP tokens to farm? Will this affect their ability to get a lottery ticket? Since if LP tokens are in the farming pool, then their balance will not be displayed.

A: Yes

Thank you for your support and good luck in the land sale!",6467
"How to launch Uber on web 3?

A white paper is needed to answer this 7 word question, who has the time for that?

Here’s a try at the TL:DR version of it:

Where to start?

• start a story

• structure ideas

• leave some trails

How to start?

• find people

• create DAO

• create MVP

Remember priorities:

• scalable MVP

• road map

• hype building

How to find talent?

• join other DAOs

• tweet a lot of people

• join start-up incubators

How to collaborate?

• DAO monthly vote

• Discord/slack/clickup daily

• quarterly retreat

How to think of the structure?

• DAO ~ company (board)

• steering coin~ product (uber platform)

• wheel token~ services (uber credit)

How to fund?

• sell NFT early investor access

• gradual ICO (steering)

• DAO token treasury sale

How does the product look like?

• Give/Get a ride, earn crypto

• Drive more/Ride more, earn more

• wheels short term, steers long term

What does this mean?

• this could be applied in any industry

• we can change the world together

• this is a trail

How can I get involved?

• join discord/ mailing list

• join DAO to work

• buy steers NFTs as soon as you see them

.....?

•.

•.

•.

... Help us fill in the gaps, join our discord

Next article on 12/21

titled: ""how does Uber on web3 look like?""",1288
"Usage of non woven bags is increasing with the days as it passes. If you have been into some busy shopping centers, probably, you would have seen many people carrying colorful and stylish fabric bags.

You would have also noticed many shop keepers presenting the goods to their customers in the non woven bags. If you still not aware of the advantages and benefits of using this type of bag, just keep on reading this small piece of article.

As the names says, non woven bags are actually made from non woven fabric like material that are made long fibers held together through chemical and mechanical process. The incredible qualities of non woven fabric make it an ideal choice for making bags, than the regular plastic bag.

Plastic bags are highly non-decomposable and can emit poisonous or harmful gas on heating or firing. They also add to the high level of land pollution by land filling. But non woven fabric bags hardly does contribute to land pollution as they are decomposable and don’t emit any poisonous or harm gases on heating or decomposing. Hence these bags are very Eco friendly.

Non woven bags are strong and less reactive than plastic bags. It has the capacity to load more weights than the regular plastic bags. Therefore, shoppers using non woven bags China need not worry about the uncertainties.

These are so friendly and attractive to carry around while shopping. They come in different attractive sizes and shapes. They also come in attractive colors or have graphics printed on it. These bags are very light yet stronger than regular plastic bags.

Cost is another important factor that makes these non woven bags more advantageous. These bags are very cheaper when considering the quality and durability. A single non woven bag could be used for long and multiple times than a regular use-and-throw plastic bag.

The things that have been discussed so far are just some of the few advantages of using the non woven bags. Just browse the Internet thoroughly to find out the list of benefits of using non woven bags. You can find many China based non woven bag manufacturers offering these bags at good price.",2138
"Creator: MORENO SOPPELSA

Despite having captured the imaginations of technology enthusiasts and business strategists since the mid-1980s, 3D printing still isn’t the industry disrupting technology it has the promise to be. Much of the technology’s failure to launch has been due to slow printing speeds, lackluster quality, traceability issues, lack of talent, the high price of the most disruptive types of 3D printers, and the perception that it is little more than a hobby and prototyping tool. With the operating model transformation brought on by the Covid pandemic, things might be ready to change. Improvements to printing speeds and quality, the ascent of AI powered design tools, widespread post-pandemic digital adoption, and pressures to shorten and simplify the supply chain, more widespread adoption may be on the horizon. Now is a good time to start changing how manufacturing leaders view 3D printing and think about how it can transform their operating models.

Most western manufacturers have been dependent on low-cost overseas suppliers for items such as subcomponents, spare parts, jigs, tools, and fixtures for decades. The familiar low-cost country sourcing model comes with long lead times, little design flexibility and requires manufacturers to carry bulky inventory. On top of being a ridged and overly complicated model, companies are forced to absorb the cost of transportation and supplier markups. By adopting modern industrial 3D printing (in plastic and metals) many formerly sourced products can be produced in-house while still reducing costs and inventory. The value is a much simpler, cheaper, and agile supply chain.

There is already plenty of evidence of what the benefits might look like from industry users. Limited adoption and a number of recent proof-of-concepts have demonstrated its ability to reduce costs by up to 90% and deliver parts up to 48x faster for many tooling items, spare parts, and end-user components. Adoption also opens the door to a wide range of strategic benefits such as mass customization, on-demand manufacturing, rapid design iteration, and reduced administrative effort for sourcing, receiving and invoice handling.

Below we’ll look at a few ways industrial 3D printing can benefit manufacturers and list some potential use cases for consideration.

BENEFITS OF 3D PRINTING IN MANUFACTURING

Low-cost low volume production

3D printing is optimal for short production runs where a small number of specialized components or spare parts are needed. Manufacturers can produce the exact number of parts needed, with the exact amount of material required, from a multi-use inventory of raw material on site.

Faster delivery

Printing parts on-site can produce ready to use products in a matter of hours rather than days or longer when relying on a 3rd part supplier. Procurement action, supplier response, shipping, and receiving processes are all eliminated for all applicable parts.

Design flexibility

3D printed products require no special tooling. Designs can be iterated after each item is produced and the 3D printer will simply apply the changes to the next item. This allows improvements to be implemented almost immediately. AI powered ‘generative design’ software allows engineers to quickly produce new designs within whatever parameters they set, and even to combine subassemblies into single parts.

Mass customization

Design flexibility opens the possibility of offering customers highly customized products. This could be a differentiating capability for many customers. This also allows for much faster and customized production of some components for aftermarket sale, especially on low volume designs.

Lower inventory

Items that are appropriate for 3D printing are typically small but complex and take up a lot of space for their weight. ‘On-demand manufacturing’ with 3D printing allows users to only stock raw materials, while the product inventory exists as digital files. This can reduce warehouse space, loss due to damage, and handling needs.

Reduced administrative effort

Items produced in-house do not require administrative efforts such as sourcing capable suppliers, issuing purchase orders, warehouse receiving, and do not result in invoices which require additional back-office effort. These benefits will be most realized for highly customized items which typically require a lot of communication with suppliers.

CURRENT AND POTENTIAL USE CASES

3D printing is best suited for tooling needs, components in small to medium size production runs, and spare parts. Various printing technologies can print in metal or plastic, but the size of the machines means that the items must be relatively small. Common metals that are used include stainless steel, aluminum, titanium, cobalt chrome, and nickel alloys. You can consider some more details and constraints around metal printing here. Below are a few use cases for consideration:

· Tooling such as molds and inserts

· Rigid housings

· Ductwork

· Heat exchangers and heatsinks

· Spare parts

3D printing will become more and more commonplace as a manufacturing method in the coming years. As the technology gains wider adoption and improves, more products will fall into its purview. Users will benefit by shortening and simplifying their supply chains. Customers will benefit from much more rapid innovation and previously impossible levels of customization. At a higher level, 3D printing is part of a larger trend of AI fueled automated manufacturing, distributed production closer to the place of use, and mass customization. The future will be interesting.",5622
"What exactly is Silicone 3D printing?

Silicones are polymers made from siloxanes, which are polymerized. They are famous for the rubbery properties along with their superior thermal stability , chemical resistance as well as biocompatibility, watertightness as well as environmental hardness and electrical insulation. The list of properties resulted to silicones being utilized for many different applications including cooking utensils to seals for aircrafts, to electrical coatings to implantable medical devices.

Despite their widespread use with more traditional manufacturing techniques they are notoriously difficult material to 3D printer. This is the result to being an elastic material. In contrast to thermoplastics, which melt and return to an unformed state, elastomers are not able to melt down after they have solidified. This has kept many kinds of online 3D printing services, such as fused filament manufacturing (FFF) — from having the potential of 3D printing with silicones.

It is feasible to 3D print online with silicone-based materials and there are plenty of options in the marketplace today for silicon 3D printing. We’ll discuss the subject in greater detail later and discuss the way that silicone 3D printing operates, the various applications for this technology as well as the advantages and disadvantages of silicone 3D printing.

What is the process of silicone 3D printing function?

In the past few time, a variety of additive manufacturing technology that are that work with silicone materials have been created. It is interesting to note that since the material does not been compatible with any other 3D printing technique, companies and research organizations have required to create specific additive manufacturing systems that are suited to silicone.

There are many different methods to print silicone. The first one is based upon techniques for deposition. Contrary to FFF that uses the printing head heated to push layers of thermoplastics that have been melted on a print bed, which are then able to solidify when they cool deposition 3D printing does not require any kind of heating process. Instead, a printer is employed to deposit drops of silicone on the print bed, and a curing technique is employed to make the droplets harder.

The technology, dubbed the Liquid Additive Manufacturing (LAM) can be described in the context of “an additive manufacturing process in which liquids (or low-strength materials) can be additively processed, such as liquid silicone rubber (LSR).” The LSR material that can be used in this process is made by the chemical company Dow. The LAM method uses extrusion in volumetric form to precisely apply the material in liquid form. The curing takes place within the building area after each printing layer. This method of curing process is carried out by using a high-temperature halogen light.

Of obviously, these aren’t all the companies that offer the silicone 3D printing features. However, generally speaking silicone 3D printing needs some type or liquid dosing (whether droplets or an entire vat of resin) and vulcanizing or curing to make the printed layers more solid. In some instances support material removal as well as further curing to strengthen the silicone layer could be necessary, but the extensive post-processing process isn’t typically necessary due to the excellent surface quality.

Another method of 3D printing in india is utilized in the manufacture of silicone parts. Although we won’t go into the process in depth because it’s an indirect 3D printing method that requires printing masters for creating silicone molds. Molds themselves are also 3D printed.

Silicone 3D printing benefits and drawbacks

Like all materials There are pros and pros and to 3D printing silicone rubber. One of the major advantages of silicone 3D printing are due to the mechanical properties of the material that include its unique mix of flexibility and strength in addition to chemical resistance and temperature biocompatibility, electrical insulation. Naturally, these benefits can be obtained using conventional manufacturing techniques.

Online 3D printing india offers an different set of benefits for silicon manufacturing such as more design freedom, greater flexible production, and customization. Silicone 3D printing also helps in the development of products through rapid prototyping, and reducing costs for prototyping. This means that engineers and product designers can swiftly create silicone prototypes and test them, then make the necessary modifications, and repeat till the final design appears flawless.

Silicone’s biggest drawbacks when it comes to 3D printing are its processing capabilities and accessibility. Since the material is hard to print using conventional additive methods like FFF/FDM, it requires special hardware that could cost a lot. Additionally, since it is a niche market, and silicone 3D printing remains a niche part of the larger AM market, the silicone material choices are limited. This makes it difficult to access, in terms of availability as well as cost.

At the moment the present state of the current state of silicone 3D printing is also limited in regards to the types of parts it is able to create. Silicone 3D printing machines available currently available have limited volume of build and therefore are not suited for the production of large-scale components. This limits prototyping and production capabilities for the use of silicone 3D printing. Another drawback of it being an emerging technology it is the fact that silicon 3D printing isn’t regulated by guidelines and documentation that go with it. However, this issue isn’t as crucial for prototyping and design applications, but it is likely to be addressed when the technology advances to develop and the adoption rate increases.

Application of Silicone Additive Manufacturing

As there are a myriad of applications for silicone and other silicone materials, there are numerous opportunities for 3D printing with silicone. Since 3D printing is ideally suited for small or one-off production, mass production of silicone-based products is preferred to the traditional molding methods. The greatest potential for 3D printing applications for silicone will be in the development prototyping functional models, custom-designed parts or small production batches that are not economically feasible using injection molding.

Today, we can see silicone 3D printing in a wide range of industries including the medical field to aerospace. Let’s have a look.

The dental and medical industry are among the industries that are most fascinated by Silicone 3D printing capability. This is due to the fact that the silicone material is non-toxic and biocompatible material. 3D printing allows the creation of products that are specific to patients. For instance, silicone 3D printing could be utilized to produce customized Anatomical Models (based on the patient’s CT scans) that can enhance the pre-surgical preparation. Silicone is particularly appealing for these applications due to its transparent properties, which allow surgeons to observe internal anatomy. In the field of healthcare Silicone 3D printing is also able to be used for the creation of soft prosthetic like noses and ears because of to its soft, flexible texture. These characteristics are also helpful in dentistry, as silicone 3D printing is able to produce gingiva and soft gum models to be used in conjunction with dental models and devices that are made with hard materials using DLP and SLA platforms.

The bio-compatibility of silicon and its flexible properties also make it a great material for the market for consumer goods. For example, 3D printing is used to create custom-designed items that come in the contact of skin for instance adapters for earbuds for headphones as well as headphone pads and much more.

The last but not least this technology provides distinct advantages for industrial and robotic applications. For instance, the pliable yet sturdy material is ideal for soft robotic parts. 3D printing allows engineers to develop complex prototypes, or final-use components like pneumatic actuators or grippers, for soft robotics. Due to the electrical properties of silicone it is ideal for creating prototypes or designing custom electrical enclosures. In the industrial sector there is an increasing desire to use the silicone 3D printing for simplifying the design of gaskets and seals that are used in aerospace and automotive industries, as well as other.

Conclusion

While silicone AM is an emerging and highly specialized segment of the larger additive manufacturing sector There is a huge opportunity for this technology. Silicone’s properties are beneficial for many applications. 3D printing service is a great way for rapid prototyping, customizing and ever more intricate designs. With the increasing number of silicone 3D printing materials and solutions are created, and the process becomes more standardized, the number of applications and possibilities will remain to increase.",9085
"Formnext 2021 | One step towards better healthcare

As the foremost industry platform for additive manufacturing and industrial 3D printing, Formnext is the international conference point for the next generation of intelligent industrial production. This year, the highlight goes to healthcare and effective medical treatments. Khánh Vi Follow Nov 25, 2021 · 4 min read

3D-printed artificial limbs at formnext2021 (Source)

3D printing contributes so well to the healthcare industry. Medical devices, organs for studying purposes, and personalized prosthetics can be 3D-printed in medical and healthcare systems. It’s not only about lessening costs and time. It likewise means that we have some steps further to nourish and heal human lives. Let jump into some highlight healthcare innovations at formnext2021.

Artificial medical replacements

Deformable Acetabular Hip Cups

Built with Titan-Grade 1, the acetabular hip cups have a duration of up to 15 years. The long course helps minimize the replacement times and reduces surgical expense and risk, especially for the senior people. Hip cups manufactured by the Laser Powder Bed Fusion have high accuracy in micro detail, allowing bone cells to grow across the artificial cups.

Artificial ceramic bone parts

Since 2015, Particle3D has been developing disruptive bone-printing technology based on hard science from extensive pre-clinical work. Particle3D’s patented bio-ink allows the possibility of printing patient-specific, natural, and resorbable bone implants to rebuild the human’s frame according to the patient’s own CT or MRI scan.

With Create it REAL’s 3D print technology, the company enables companies like Particle3D to utilize the benefits of 3D Print, mass customization, speed, local production, and authenticity.

Breast cancer locator

Breast Cancer Locator

The Breast Cancer Locator™ (BCL™) is a proprietary, patient-customized, 3D-printed surgical guide that reduces positive margins during breast cancer tumor removal. The BCL™ provides data concerning tumor size, shape, and margin boundary to assist surgeons in removing cancer yet preserving normal breast tissue.

CairnSurgical™ collects patient MRI data, interprets it to mark tumor geometry and location, and uses the data to form the BCL™.

Single-arm and randomized, controlled clinical trials are underway to validate key aspects of the BCL™ technology.

Footwear optimization

3D-printed shoe soles

The orthopedic insole demand is significantly expanding these years as diabetes and heel problem patients rise. A peculiar personalized insole substantially increases the comfort and well-being of such patients.

The partnership between GeBioM and Create it REAL solves this issue by automating and digitalizing the workflow, allowing an orthopedic shoemaker to triple the production capacity and create even better products than possible today.

Pre-surgery model

3D-printed human spine for pre-surgeon purposes

The advanced technology in object scanning and real-time data transferring help to bring 3D modeling and printing to a higher level of efficiency. This provides surgeons better understandings and preparation, therefore raising the success of the surgery. Furthermore, medical training and education get vast benefits with more effective practical training and reasonable cost.

Artificial heart made by Create it REAL (Source)

The next level of 3D printing services is required for medical training and education to address the difficulty of learning material shortage: 3D-printed realistic, affordable organ models built to the closest accuracy with the patient’s scan. To improve students’ and medical professionals’ learning and training methods, Create it Real, the University of Heidelberg, and The University of Tübingen have started a project to produce realistic and accurate 3D models of a particular patient-funded within the framework of the Eurostars Programme.

Final thought

3D printing offers the medical industry great freedom of design, adaptability, and functional integration. For the manufacturers of dentures, medical and orthopedic technology products, orthoses, and prostheses, this creates many far-reaching opportunities. We, therefore, have the right to hope for a brighter future towards more effective and efficient healthcare practice and training.",4334
"In the previous blog, we started learning about the different 3D printing processes, and let's continue the same in this blog too!!

MATERIAL JETTING (POLY JET) 3D PRINTING:

Material Jetting (MJ) is a special additive manufacturing process. The feature that makes it so special is that operates also 2D printers. In material jetting, a printhead (these are analogous to the printheads used for standard inkjet printing) dispenses droplets of a photosensitive material that solidifies under ultraviolet (UV) light, thereby building the part layer by layer. The materials that are used in the Material jetting process are thermoset photopolymers ( paintings) that come in a liquid form.

Material Jetting 3D printing creates a corridor of high dimensional accuracy with a veritably smooth face finish. Multi-material printing and a wide range of accouterments ( similar to ABS-like, rubber-like, and completely transparent materials) are available in Material Jetting.

source: https://www.researchgate.net/publication/325876280/figure/fig3/AS:639591040155653@1529501795569/Schematic-representation-of-the-material-jetting-process.png

WORKING:

First, the liquid resin is hotted to 30–60 degrees centigrade to achieve optimal density for printing. Also, the printhead travels over the figure platform and hundreds of small tiny droplets of photopolymer are jetted/ deposited in the locations given as input in the form of G-code. A UV light source that’s attached to the printhead cures the deposited material, solidifying it and creating the first subcaste of the part. After the layer is complete, the build platform moves to the next layer position, and the process repeats until the whole part is complete.

These printers print in a line-wise fashion. Multiple inkjet printheads are attached to the same carrier side-by-side and they deposit material on the whole print face in a single pass. This allows different heads to deposit different materials, thereby allowing multi-material printing, full-color printing. Support structures are always needed in material jetting and need post-processing to be removed.

The liquid material in Material Jetting is solidified through a process called photopolymerization. This is the same medium that’s used in SLA. But unlike SLA they don’t bear fresh post-curing to achieve their optimal parcels, due to the very small heights that are used. The typical subcaste height used in material jetting is 16–32 microns. Indeed this material jetting also requires support while printing. There are two types of finishing in this process lustrous, matte.

Having learned some of the types of 3D printing, we shall now shift our aspect towards SLM and DMLS. Currently, SLM technology is extensively being used in the disciplines of aerospace, automotive, medical fields, etc. SLM has made it possible to manufacture colorful corridors without the demand for any supplementary coffers. Also, DMLS have made their stage in the manufacture of the high-performance corridor in the automotive and aerospace diligence, medical prosthetics, tools, and much further. We shall now explore each of these generalities extensively to understand their work.

SELECTIVE LASER MELTING (SLM) & DIRECT METAL LASER SINTERING (DMLS) 3D PRINTING:

Selective Laser Melting (SLM) and Direct Metal Laser Sintering (DMLS) are two metal additive manufacturing processes that come under the umbrella of the powder bed fusion 3D printing family. These two technologies are quite similar as they use a laser to scan and selectively fuse the powdered metal particles which enables bonding them together resulting in the development of a part.

Now, having said the similarities, we start thinking about their differences as I mentioned earlier, they are quite similar but not entirely. Well, Can you think of any difference between them?

The difference between SLM and DMLS is that SLM uses metal powders with a single melting temperature and fully melts the particles completely, while in DMLS the powder is composed of materials with variable melting points that get fused at elevated temperatures. Let me make things easier for understanding, SLM produces parts from a single metal, whereas DMLS produces parts from metal alloys.

Now let us begin understanding the working of DSLM printing.

Image of Direct Metal Laser Sintering process

source:https://www.researchgate.net/publication/330249826/figure/fig1/AS:870142959448064@1584469656348/Schematic-diagram-of-direct-metal-laser-sintering-process.jpg

WORKING:

1. Firstly, the build chamber is first filled with inert gas to minimize the oxidation of the metal powder, and then it is

heated to the optimal build temperature.

2. Then a thin layer of metal powder is spread over the build platform and a high-power laser scans the cross-section of

the component.

3. This melts (or fusing) the metal particles together, thereby creating the next layer. The entire area of the model is

scanned, so the part built is completely solid.

4. Once the scanning process is complete, the build platform moves downwards by one layer thickness and spreads another thin layer of metal powder.

5. This process is repeated until the whole part is complete.

6. When the build process is finished, the parts are fully encapsulated in the metal powder.

7. After some time the bin cools to room temperature, then the excess powder is removed manually and the parts are typically heated.

8. Now the components are detached from the build plate via cutting, machining, or wire EDM and are ready to use. [Note: The layer height used in metal 3D printing varies from 20–50 microns. The metal powder is recyclable.]

Now we understand the basic working of DMLS. We shall now move forward to learn about Binder Jetting 3D printing. Binder Jetting has a varied range of applications that include full-color prototypes, highly precise metal parts, sand casting cores, and much more.

BINDER JETTING 3D PRINTING:

Binder Jetting comes under the umbrella of additive manufacturing processes. A binder is selectively deposited onto the greasepaint bed, relating these areas together to form a solid part one layer at a time. The components generally used in Binder Jetting are metals, sand, and ceramics.

Image showing the process of Binder Jetting

source:https://www.engineersgarage.com/wp-content/uploads/2019/07/Binder-Jetting-3D-Printing-Process-and-machine-01.jpg

WORKING:

1. Originally, a recoating blade spreads a thin layer of powder over the build platform.

2. Up next, a carriage with an inkjet nozzle passes over the bed, depositing small droplets of a binding agent (glue) selectively that bond the powdered patches together.

3. In full-color Binder Jetting, the colored ink is also deposited during this step.

4. When the layer is complete, the build platform moves down, and the blade coats the face. This process is also repeated until the whole component is complete.

5. After printing, the part is reprised in the greasepaint and is left to cure and gain strength.

6. The part is also removed from the powder bin and the footloose, redundant powder is removed by passing pressurized air.

The main advantage of binder jetting is that cling occurs at room temperature. This means that dimensional deformations connected to thermal effects such as warping or curling aren’t a problem in binder jetting. Binder jetting requires no support structures.

In the next part let’s know about the DIRECT ENERGY DEPOSITION and SHEET LAMINATION process, with that we are going to wrap us the different types of 3D Printing processes and jump into the next topic.

Meet you in the next part of my 3D Printing-Series!!🙂",7678
"The primary form of additive manufacturing, 3D printing service in india, has revolutionized the way parts are produced and prototyping is done. Manufacturers and product designers have seen a significant improvement in the speed of prototyping, design and production since Vat Polymerization was introduced. There are many technologies available today that provide a wide range of options for both designers and producers. The central question of 3D printing service is ready for production as technology improves.

What is additive manufacturing?

Additive manufacturing or online 3D printing india is a method in which material can be selectively added to create physical parts. This is different from the traditional computer numeric controlled processes (CNC), which selectively remove material to create desired shapes. Although 3D printing has existed since the 1970s it was not popularized until the 2000s when the open-source movement and crowd funding allowed for affordable printers to be made. Some technologies have been made accessible to the masses. You can now buy a decent 3D printer for $200 that meets or exceeds the makers’ standards. Fuse Deposition Modeling, Vat Polymerization, SLA & DLP, Material Jetting and Powder Bed Fusion are the main 3D printing processes. Binder Jetting, Directed Energy Deposition and Sheet Lamination are also available. Each technology has its advantages and disadvantages. It is a good idea to prototype with a service bureau that offers a variety of printing technologies in order to get the best part possible for your application.

Technology Fuse Deposition Modeling: Heated Extruder that melts and disperses plastic filament in a layer-by-layer bed.

Heated Extruder that melts and disperses plastic filament in a layer-by-layer bed. Vat polymerization (SLA/DLP): UV lights selectively cure liquid resin in a tank. Resin is a photosensitive plastic that crosseslinks and hardens under UV exposure.

UV lights selectively cure liquid resin in a tank. Resin is a photosensitive plastic that crosseslinks and hardens under UV exposure. Material Jetting is Material is selectively jitting from a printer head, and then UV light cures all of the bed. The part is moved down one layer, and then additional material is added.

Material is selectively jitting from a printer head, and then UV light cures all of the bed. The part is moved down one layer, and then additional material is added. Powder Bed Fusion A layer of metal or plastic powder is laid on a flat surface in a heated oven. The powder is then sintered by a laser to create a solid part. After the part is cooled, a layer of powder is added to it.

A layer of metal or plastic powder is laid on a flat surface in a heated oven. The powder is then sintered by a laser to create a solid part. After the part is cooled, a layer of powder is added to it. Binder jetting: This is another powder-based system. It uses a similar inkjet printer’s printer head to deposit a binding solution on a powder bed. As in SLS, the part lowers a layer while more powder is added on top.

This is another powder-based system. It uses a similar inkjet printer’s printer head to deposit a binding solution on a powder bed. As in SLS, the part lowers a layer while more powder is added on top. Directed energy deposition: Like the SLS, the plastic powder is heated in a chamber. However, instead of using a laser for fusion, the inkjet array uses an inkjet array that selectively dispenses a fusing ingredient. This heats the powder to its melting point with heat lamps.

Like the SLS, the plastic powder is heated in a chamber. However, instead of using a laser for fusion, the inkjet array uses an inkjet array that selectively dispenses a fusing ingredient. This heats the powder to its melting point with heat lamps. Sheet Laminate:Compressed sheets of sheet material are bonded together in order to form a 3d part.

Are 3D printers ready for production?

3D printing in india has revolutionized prototyping and reduced the time required for product development for decades. Rapid advances in additive manufacturing have led to new materials and processes that are more efficient, reliable, and more durable. The question remains, however, if online 3D printing services are ready to be used in production. Although it will never be able to match the speed and cost-effectiveness of injection-molded parts, the technology can unlock the constraints of traditional manufacturing processes and allow for new products. Today additive manufacturing is happening and there are many examples of 3D-manufactured parts on scale.

Design

This has changed the way we think about design. Engineering students were able to access 3D printers during their education. Now, graduates incorporate additive manufacturing processes into part designs. Metal sheet forming and injection molding are not the only options. This new approach allows multiple machine and molded parts to be combined into a single, more complex, 3D-printed part. It is also cheaper. GE engineers have reduced 20 parts of a fuel injection nozzle to one part, and since then they have mass produced 30,000 units using metal powder bed fusion machines. Software advancements have made it possible to create complex structures that are both efficient in material usage and provide precise performance characteristics.

Operational software

Operational software has also seen significant improvements that have reduced the cost and improved the performance of additive-manufactured parts. A lot of the work involved in creating a buildfile can now be automated. It used to take a lot of skill to pack parts into a 3-dimensional build tray. The task can be done in a few clicks. The slicing algorithms that break down 3-dimensional parts into 2D layers, then create tool paths or instructions to the printer/laser, have been improved to produce reliable and predictable parts. Service bureaus can now charge more for additive manufacturing by using cost analysis as a standard feature.

Quality Control

Up until recently, quality manufacturing and consistency in additive manufacturing were barriers to mass production. Quality management is essential in the automotive and aerospace industries, where lives are at risk. A new set of tools is available to manage quality. Some of these tools are currently being developed by makenica.",6386
"NOZZLE:

Now we’re getting to mention the nozzle component, both the inside and the outside. Nozzles are mostly made from brass because it’s easier to machine and excellent at transmitting heat. Usage of brass also has its limitations. The qualities that make them easy to produce mean that they’re soft enough when processing abrasive materials. The materials will then decline the nozzles over time. The material exiting widens the opening because it moves out of the nozzle. The function of the nozzle is two-fold. It mounts into the thermal block and extends the melt chamber, and typically most of the melted material is within the reservoir inside the nozzle’s tip. It also limits the amount of material that comes out and mechanically influences the placement of plastic on the bill pipe. Nozzles and thermal blocks are generally suited to specific filament diameters. But the method works better if you’re using the proper internal hot in diameter to match your chosen filament. The closer it holds to the filament, the more precision you’ll have in controlling the amount of extruded plastic. The most important factor while selecting a nozzle is the size of the opening diameter. The most common size used is 0.4 millimeters. 0.4 millimeter may be a good compromise between the dimensions of the 2 commonest diameters of filament, in terms of pressing down without producing an excessive amount of backpressure, and the width of the little bit of the plastic that’s extruded from a 0.4-millimeter nozzle is somewhere pretty on the brink of 0.4 millimeters, maybe a touch bit less counting on the material, and offers enough X, Y resolution to satisfy most prototyping needs. As there are more materials and more applications for 3D printing than ever before, there’s now a variety of nozzles available. Some machines ship standard with a 0.5-millimeter nozzle or wider, typically for printing large objects, and nozzles as small in diameter as 0.25 and 0.15 millimeters are available for very fine detailing. A wider or smaller nozzle doesn’t change the accuracy of placement in the X, Y plane, but changes the diameter of the bit of plastic leaving the nozzle. Certain materials are better processed with particular nozzle diameters. For example, filaments containing wood or metal powders can clog more easily with really small openings. The powder content might pile up onto the tip of the nozzle and block extrusion. Other materials are more viscous and need an excessive amount of force to exit a little opening effectively in any case. There is a variety of various terms for features of internal geometry within the nozzle. I will use the terms external tip, cone, shoulder, and throat to describe the space while the material travels from the melt chamber to the outside. As the material moves forward, it shrinks down for matching the width of the filament to the width of the nozzle diameter. This area is called the cone. It’s essential for guiding the forward pressure while extruding the fabric, ensuring that the plastic exits the basketball shot a line and not at an angle. The external tip establishes both, the outer diameter of the extruded material and also features a shape of its own. There’s a flat ring surrounding the opening and it has an additional mechanical function. It irons down any material that has risen upon the top surface of the print. The shoulder may be a flat internal ring round the throat. The throat is that the tiny tube that extends from the sting of the cone and bends the external tip of the nozzle. A throat isn’t typically very long. But with some materials, it can be helpful while evening out the pressure as the material emerges. In the early desktop printers, the nozzle was a hard and fast part of a hot end.

source: https://www.researchgate.net/publication/339153639/figure/fig1/AS:857192693497856@1581382072553/Schematic-and-nozzle-size-of-the-PORIMY-3D-printer-The-outline-dimension-is-380-390_Q640.jpg

BUILD PLATE:

Now we shall see about the build plate which is in fact one of the most important parts of a 3D printer.

After the fabric leaves the nozzle, it lands on your build plate. The build plate may be a surface on which you build your part layer-by-layer. The build plate is that the final step within the extrusion system. The aspect of the build plate that factors into the extrusion system, maybe a build plane. This is the layer on which extruded material lands. When a print starts, the build plane is that the same because of the surface of the physical build plate, but because the build progresses, the build plate rises moving up higher and higher, becoming the topmost layer of the pair part. The important things to debate once we will mention the difference between the build plate and therefore the build plane, are material adhesion, cohesion, warping, elephant feet, and heated build plates and surfaces. For this whole process to succeed, the fabric must stay where you set it. This means that it has a stick to adhere to something. In order to possess an honest first layer adhesion and accurate parts, you would like to form sure that the build plane is correctly calibrated in order that where your nozzle is driven across the XY plane by the motion mechanical subsystem, it remains the same vertical distance from the build plane, everywhere and also the build plate, ie, the top surface of the plate itself, where you deposit the primary layers of the part. When a print starts, these two flat planes should be precisely parallel to each other, with just the first layer of plastic keeping them apart. When you successfully bring these two planes into alignment, the machine makes an accurate model of where the platform and gear head are located. We always get confused by the term bed leveling. We don’t care if the machine itself is on a level with the world. We actually only care about the relationship of that nozzle, its range of motion, and its distance from the build plate just below. Another term for the CNC milling and machining world that’s an accurate metaphor, is tramming. When a CNC mill trams the highest surface of a piece, it’s cutting away the fabric across the plane to ensure that the worth understood by the CNC about the work plane is perfectly reflected within the physical world, also because of the digital world. Sorting out this element first is really more valuable than most of the opposite adhesion strategies because when the build plate is badly calibrated, the probabilities rise considerably, that your part fails, and fall off. Cohesion describes materials sticking to materials, through chemical or mechanical processes. Let’s talk about the adhesion strategies you can implement in your design. The most aggressive solution is named a raft. In your design, you’ll account for one or more layers of material set down flat on the plate, and you’ll then use this as the base for the rest of your part. It finishes up looking sort of a platform you would possibly see beneath a sculpture. Think of a sculpture of a horse. You only have the hooves of the horse touching down on the bottom. If it doesn’t stick well, then the whole horse is going to be lost. The material from your print will stick with itself more easily than other materials. So by employing a raft, you’re likely ensuring a pleasant, sturdy object is fixed down. While it serves an identical function to a raft, providing a bigger base for sitting the printed part, it is also solving the extra problem of cooling and warping. As the fringe of the part is exposed to the air and cools, sooner than the remainder of the part, it shrinks slightly moving far away from the print. By using a brim, you’ve delayed the cooling and warping of the important parts of your object. Some operators will model a raft into their designs, but we will use our 3D control software to automatically generate a raft. That’s the most common. When you produce a raft during this fashion, the way the layers are printed is going to be unique to every raft, guaranteeing that it sticks to the plate and may also be pulled far away from the bottom of the thing. After you’ve finished printing the part, you ought to be ready to easily tear off the brim. A skirt is produced in an equivalent way as a brim, but the layers are offset far away from the bottom of the thing and don’t inherit contact with the final printed part. While this does not necessarily appear to be it might help with adhesion, by printing the skirt first. Without the skirt, the first layers might not adhere as properly. A skirt also can help visually guarantee that the build plate has been properly calibrated to the nozzle. If the skirt is not of even height, getting thinner and thicker in different places, then you should stop the print and re-calibrate your build plate. It’s sort of a test drive that you simply take around the block if the block is that the build plate. Mouse ears were an early strategy in 3D printing, almost like a brim. A series of flat disks were incorporated into the planning around the base of the model. After the part was printed, you’ll easily tear the discs far away from the finished product. While less frequently used than other solutions, having this feature available when designing are often helpful, especially if you encounter just one tricky edge that you simply got to fix. We will now move on to the physical measures you can take during the actual print job to ensure proper adhesion. Let’s start with the heated beds.

While using a heated bed, you have a heated surface that is not hot enough to melt the printed part, but it’s warm enough to keep the base material a bit flexible and prevent it from cooling and pulling away from the plate. Common materials for heated bill plates include aluminum and borosilicate glass. For a heated build plate strategy to work, you need to have a heating element in your 3D printer and a temperature monitoring solution to maintain the perfect temperature for your part. One of the setbacks from heating the plate too aggressively is named an elephant’s foot. An elephant foot error is when the temperature is high enough, that the rock bottom layer of the print has melted and expanded around the object. It looks a lot like an elephant’s foot, wide at the base and tapering off at the edges. Not all machines have a heated bed, and it’s possible to have a good adhesion by providing a better base of material that sticks down regardless of temperature. The simplest way is to offer your prints something to grip. If your surface doesn’t offer quite enough grip on its own, then you can apply adhesives. Some people like better to spray adhesives down on the plate rather than rolling it on with a glue stick or printing down PVA material. You need little or no PVA glue to carry the part down because PVA tends to stay pretty much to most build surfaces, and most printing materials. When you use this together with a heated bed, PVA glue offers another advantage. It helps evenly transmit the heat to the base of the part, and a twisting or levering action causes it to lose its grip, making it easier to remove your part at the end of printing. There are other glues in use, either specifically created for the sector of 3D printing or for other industrial applications. One such which is used extensively is 3M blue painter’s tape. The reason that the 3M Blue painter’s tape works so well is two-fold. The adhesive is often removed without leaving a residue, and therefore the top surface of the tape has particularly good micro fishers, making it a perfect grippy material for decent plastic.

Image of a build plate

source: https://hackster.imgix.net/uploads/attachments/1014924/_i2VLLbHVpp.blob?auto=compress&w=900&h=675&fit=min&fm=jpg",11790
"3D printing starts with a digital model stored in the 3D CAD (Computer Aided Design) file, and then producing the physical 3D object. A model is scanned as well as an old scan the object is used. The scan can be processed using a specific piece software referred to as”slicer. “slicer.” Slicer transforms the model into layers of thin, 2-dimensional layers . It then creates an instruction file (G-code) specific to the particular type of 3D printer.

The kind of 3D printer commonly used in schools is the FDM (Fused Deposition Modeling) machine.

The 3D printer makes the necessary mix of raw materials (plastic rubber, metal, and similar) and then creates the model by adding layers one at a time, two-dimensional layer after 2D layer, until the object is fully constructed and finished in accordance with the design guidelines from the initial CAD document.

In the case of 3D printing service in education it’s about taking objects off the computer screen into the physical world. It’s also in the hands of the students to be examined, analysis, and various other activities which benefit by physical manipulating.

Here are a few examples of how teachers and students could benefit from 3D printers for classroom use:

History students can print out historic artifacts for them to study

Graphic Design students can make 3D prints of their work

Geography students can print topography or demographic, as well as populations maps

Students studying Chemistry can print three-dimensional models of molecules

Biology students can print out viruses, cells organs, and various biological artifacts

Math students can print 3D models of the problems they need to be able to solve

Here are a few methods 3D printing technology can bridge that gap in between real and digital worlds. Look up the information you require on the screen, and then print it to life.

It’s mind-boggling It’s not it?

With the price of 3D printers getting cheaper, they’re not just another tech tool for students to play with, but they are now a vital and effective tool for education. They help make teaching and learning easier.

3D printing in india is one of the tools that aids students in conceptualizing and visualize their ideas as they design their projects starting from the initial stages of sketching to the final result.

The 5 major benefits of 3D printing in education

From the point of view of growth and development future engineers, designers and artists will all have been students who were directly impacted through online 3D printing india

Take a look at these five advantages from 3D printing’s influence on the education system…

Brings Joy - 3D printing in india gives students the chance to experience their designs from the initial stage right through to the creating the model. This brings excitement and an understanding of the process of design when they experience of the design process from beginning to end. The various features are viewed more easily as students build the design layer by layer. It also gives students the possibility of exploring aspects in the real world rather than just on a computer screen or in the text. 3D printing brings the realm of theoretical thinking into the real world, which students can view and feel, opening new opportunities for learning and engaging in activities.

3D printing in india gives students the chance to experience their designs from the initial stage right through to the creating the model. This brings excitement and an understanding of the process of design when they experience of the design process from beginning to end. The various features are viewed more easily as students build the design layer by layer. It also gives students the possibility of exploring aspects in the real world rather than just on a computer screen or in the text. 3D printing brings the realm of theoretical thinking into the real world, which students can view and feel, opening new opportunities for learning and engaging in activities. Enhances the Curriculum — No whatever curriculum is being utilized 3D printing could assist teachers and students to work more effectively. 3D printing quote can help students move away from being inactive consumers of information on a display with little thought to productivity. Contrary to conventional classrooms where students can easily become bored, they are now engaged and active participants in the creation, design and implementation of their ideas and interaction through the printer as well as the instructor.

— No whatever curriculum is being utilized 3D printing could assist teachers and students to work more effectively. 3D printing quote can help students move away from being inactive consumers of information on a display with little thought to productivity. Contrary to conventional classrooms where students can easily become bored, they are now engaged and active participants in the creation, design and implementation of their ideas and interaction through the printer as well as the instructor. Provides Access to Knowledge previously unavailable - Because the majority of 3D printers come pre-assembled and can be used with plug-and-play, it’s an enjoyable cutting-edge technology that allows students to understand. Students discover that it’s completely acceptable to not succeed on the first attempt and to attempt again to get better. When students realize that failing is part of the learning process and they are less hesitant to try and implement innovative and new ideas in everyday life. This boosts confidence of students and teachers appreciate the benefits of self-motivated, confident students.

Because the majority of 3D printers come pre-assembled and can be used with plug-and-play, it’s an enjoyable cutting-edge technology that allows students to understand. Students discover that it’s completely acceptable to not succeed on the first attempt and to attempt again to get better. When students realize that failing is part of the learning process and they are less hesitant to try and implement innovative and new ideas in everyday life. This boosts confidence of students and teachers appreciate the benefits of self-motivated, confident students. opens new possibilities to Learn — A 3D printer that is affordable provides endless possibilities for learning for students. 3D printing gives students opportunities to play with concepts, while also expanding their creative abilities. It’s difficult for young children to think things through without the aid of visualization. Visual learning environments enhance their comprehension of the world and is capable of touching and seeing their creations. 3D printers provide new avenues for teaching information to students efficiently and cost-effective way.

— A 3D printer that is affordable provides endless possibilities for learning for students. 3D printing gives students opportunities to play with concepts, while also expanding their creative abilities. It’s difficult for young children to think things through without the aid of visualization. Visual learning environments enhance their comprehension of the world and is capable of touching and seeing their creations. 3D printers provide new avenues for teaching information to students efficiently and cost-effective way. Improves problem-solving abilities — A 3D printer can provide numerous learning experiences for students. They must learn the different ways 3D printers function and how they operate as well as how to solve and troubleshoot issues. This is a subject which a lot of students don’t encounter in the course of their regular study. When they learn to identify and resolve 3D printer issues students develop determination and endurance when it comes to overcoming challenges. This will help students tackle their own issues in everyday life too.

Instilling in students the ability to be creative can encourage a desire for innovation and creativity that could later be utilized in business. 3D printing helps students achieve their goals and prepares them for college. It helps them build confidence that will allow them to take on challenging courses like those in STEAM-related fields. When students discover and expand their imagination, they are able to develop imagination and creativity. Students create their own unique 3D designs which can assist in training others and also solve problems.",8364
"It’s Christmas time — the season of giving! It’s time to celebrate the year by preparing gifts and spending time with your loved ones. In celebration of the holiday season, why not decorate your house and set up your Christmas tree? This time, let’s use resin 3D printing to create all sorts of Christmas ornaments, hangings, and decoration items.

We here at Phrozen have come up with a couple of resin 3D printing ideas for you. We recommend using the Phrozen Aqua-Gray 8K Resin and ABS Like Creamy White Resin to create strong, sturdy, and festive-looking 3D printed designs that are perfect for Christmas.

Let’s find out:

3D Printed Christmas Tree Ornaments

No Christmas is complete without setting up a Christmas tree. After setting up your Christmas tree, it’s time to decorate it with Christmas ornaments! Look at Gyroscopic Christmas Ornaments created with resin 3D printing. This Christmas 3D printed model definitely stands out from the rest due to its unconventional structure and eye-catching movements. Using Aqua-Gray 8K resin and the Phrozen Sonic Mighty 4K 3D printer, you can create 3D printed models of detailed and movable Christmas ornaments that add to the Christmas spirit.

After decorating the tree, it’s time to put the star on top of the tree. Look at this pretty white star we printed with Phrozen ABS Like Creamy White Resin. Doesn’t it steal the show? You can also paint it over with other colors if you choose to do so.

You can also 3D print tiny models of Christmas trees to be placed along with the main Christmas tree or decorate it in other parts of your house.

3D Printed Christmas Reindeer Decor

After decorating your perfect Christmas tree, it’s time to create other decorations for the house! Take a look at these resin 3D printed reindeer. Doesn’t it look lifelike? Santa’s reindeer are all geared up for the holiday season. You can use resin 3D printing to create these resin 3D printed models and place them indoors or outdoors to add to the festive spirit.

These were also printed with Phrozen’s ABS Like Creamy White Resin. Feel free to be creative and decorate the reindeers in celebration of Christmas!

3D Printed Christmas Caged Ornament

Last on our list, here’s a customized caged Christmas ornament that you can gift to your friends or family. Just look at the delicate Christmas tree and snowman sitting inside. We used the Phrozen Aqua-Gray 8K resin to print out this detailed 3D printed model. You can also color it with festive colors such as red and green in celebration of the holiday season.

Don’t worry, there are a ton of free Christmas 3D print files available online, just click here or here.

Conclusion

These 3D printed models and designs will truly help your Christmas decor to stand out from the rest. You can use resin 3D printers such as the Sonic Mighty 4K to work on your resin 3D printing skills while creating useful decoration items for the holiday season. Isn’t that great?

Want to experience the joy of resin 3D printing? Then click here to find out which Phrozen 3D printer is best for your needs.",3082
"Aviral Singh has been working with multinational banking corporations and the software industry. He has been with great organizations like Credit Suisse, Barclays and Citi.

MUDIT JAIN: So, you are not quite a new entrant and you’ve been trying different things, experimenting 3d printers. What makes you interested in, how did you start it with this?

AVIRAL SINGH: From an industry 4.0 perspective, I got interested in it about four years ago when I read about what 3d printing was and what 3d printing was doing and Augmented Reality and Virtual Reality have been growing on their own and including IoT. And I’ve been trying to follow what’s happening in the space for a while. And December last year was when just fortuitously in a conversation with a friend, we started talking about 3d printing and his wife happened to be a printer engineer. And the conversation started from there. And then I just got a 3d printer, earlier this year and started to play around with it. And, and it’s been very exciting because reading about 3d printing and doing 3d printing, as I’m sure, are two very different things.

So, getting a printer and then starting to experiment with various things, it has really caught my attention and I have absolutely no intention of stopping. So, I hope that gave you kind of a background of what is driving me to the side.

MUDIT JAIN: I’m sure there are multiple applications 3d printers connected with IoT, multiple apps. That’s also going to go, have you tried to experiment in that area as well?

AVIRAL SINGH: I’m starting to think about that, but what’s very interesting is if you look at the market today and what’s happening, 3d printing is an area where things are changing. Sometimes it feels like on an hourly basis, but if you look at the materials coming out, if you look at the techniques coming out, post-production techniques coming out, and one of the things that has happened, is, somebody released a full suite of software to manage, I think up to 40 printers at a time. So those things are already out there. And if you look at, how 3d printing will really work in the future, if you look at least from an industrial perspective, you have to integrate it with the floor management software of the firms. Because if you can’t do 3d printing on demand, which automatically starts things, monitors things, finishes things, takes it off the better place, so you can do your next a bill. That’s going to be very difficult to manage. So, I think the automation piece that you’re talking about, which will include everything from, IoT to monitoring, to control systems around 3d printer are going to be very important. And at the same time, if we don’t look at the AR and VR technologies from a variety of perspectives, in my opinion, AR, is going to be critical in terms of making sure that the models are appropriate for whatever we are trying to do. And VR, is going to be critical in terms of doing the modelling, right? Making sure the models look fine by themselves. And if you’re going to take something which is in the virtual world, which are our models and make it real right, then we have to go for those steps.

MUDIT JAIN: We have virtual reality augmenting, which are capable of creating a material product just from the view of it and directly into 3d printer.

So, you touched upon that, that doing 3d printing and knowing and reading about 3d printing is completely different.

Especially when we start, we started FDM printing That’s very tough, or very critical because we often fail and patience plays a critical role. Do you want to share a few of the learnings in the early days?

AVIRAL SINGH: FGM is awesome, but one of the things is it’s also slow.

A few other things, I think especially the hoppy printers. When you first get them, a hobby printer out of the box and an upgraded hobby printer are two completely different printers. So, for me, just that do-it-yourself thing has allowed me to understand how this thing actually works and you realize it’s not magic, but it’s absolutely brilliant in terms of everything that has been done over the past 20 or so years with an FDM world to make it happen. And to me, even more than just the printing has been the understanding of what printing can do for us and some of the conversations that I’ve been having.

There’s another conversation I was having with somebody who is starting, a company to make devices for people with disabilities and the first product they were thinking of doing an advanced wheelchair. So how did they prototype it? 3d printing with carbon fiber. I’ve had conversations with people and the people trying to work on the reduction of carbon emissions and net-zero. Without decentralized manufacturing that isn’t happening. If you look at the transportation industry, I think that’s going to be severely affected by this. After the supply chain disruption of last year in 2020, the US did a week-long exercise of what they would do if that happened again. And the whole basis of that was the existing 3d printing infrastructure in the US and while they haven’t made the results completely public, the part that was public while they found two or three things, which they need to improve on the whole, it seems to be working and they’re pretty comfortable.

MUDIT JAIN: Exactly. And, you touched about the medical, so that’s a massive field. Surgery planning is a very good upcoming field in India. And in India, we have been quite behind the US and other European countries, but, there’s the company, which is doing surgery planning on demand for if a hospital has to do a surgery. They just give them a call and they do on-demand printing for them and there are many services.

And also, we didn’t know that most of the jewellery and exotic designs are mostly 3d printed first and then it’s casted. So, coming back to, this 3d printing and the contribution to the industry, how are you seeing that you will be contributing to the 3d printing industry because you have garnered a lot of experience and knowledge in 3d printing.

AVIRAL SINGH: I’ve been doing this for about six to seven months. So, my knowledge is very limited. And I don’t know how much I’ll actually end up contributing, but there’s a couple of things which I am going to do. One of the beliefs that I have is that we need to make this much more natural and spread the knowledge. So as an example, before I started 3d printing, I realized I need to learn 3d design.

One of the things that I want to start doing and fairly soon actually is start holding classes for school students around 3d design and 3d printing, right. Prepare some light course material and handle that on a regular basis, get people excited about it. And the second objective that I have at this point is just talk to as many people as possible, because when you talk about jewellery, I talked to a couple of jewellers, right? I talked to dentists, in fact, one of the areas which blew my mind was hearing aids, right? The number of hearing aids, pretty much all of them are made because they need to be customized to the shape of your ear. Right? And I think just talking to people, gives people ideas as to what can be done and that’s something I intend to do. The one thing we really need to figure out is how we can do get 3d scanning to become more egalitarian and accessible. The moment we can do that, I think 3d printing will follow in suite.

MUDIT JAIN: And you touched a very good point of, because I know the person I was talking about, my friend who runs the startup, he also is the similar kind of guy who do not know design, but he’s having a full-fledged, 3D printers, army over them. We get designs, obviously custom designs have to be made, but we get so many ideas. So, we have to share some of the products or some of the articles feature of 3d printed, which are some cool stuffs just to know what we are printing.

So started with an idea to have this community or the 3d culture part. The name culture comes from that idea. Then, I was speaking to one of our associates to Vivek that 3d printing and its application. So, my idea was that I have a vacuum cleaner at home, and it’s an actual story. But the nozzles of that vacuum cleaner, they’re all gone. I do not have the nozzles, but I have a convenience, but can I 3d print the nozzles? Yes, I did. I just took the sizes and I have vacuum cleaner nozzles without spending money outside. And as, just to say that you handled one part and you have a part for your light fixtures, that’s the real 2d printing utility. And that has to come to the masses. I think we are not there yet.

We need to create that culture. And that’s like 3d printing has so much of a scope. And you talked about that printing place. So, removing or reducing the carbon footprint. So obviously like automobile workshops, and those are the requirement. That’s the real case we see quite often.

I have a last question for you and, quite a common one, like what do you advise to the people who are starting in 3d printing, each painting or designing, or just for hobbying, doing both or any one of them. And also, the small entrepreneurs who wants to start something into 3d printing.

AVIRAL SINGH: The advice I’m going to give is my learning. So, in my opinion, if you start it up as a business, you’re going to have issues. You’ve got to first start it up as a hobby, because if you don’t enjoy it, it’s going to be a problem. Because 3d printing is not really plug and play today. Even the most expensive machines, right? While they advertise plugin plan may be much more plug and play than the hobby machines. They’re still not plug and play. So, until you enjoy it and understand what actually goes into it and how it happens, if you’re not going to have fun, then you’re not going to progress very easily. So, the one thing is that don’t get disheartened. The market doesn’t exist today, but it will.

Five years from today. I think there’s going to be a huge market. So, riding that wave from the technology being there and no market to the technology, being there on a huge market is I think going to be incredible for anybody who really joins the bandwagon.

MUDIT JAIN: Absolutely. If you look at the technology, it’s nothing. I mean, I talk about how one of the person by whom I got introduced to 3d printing, and he talked about it, he started with saying that I am a robotic engineer and I wanted to build some parts for my robots, but I was not able to customize them. And that’s where he bought the 3d printers. Eventually, he reverse-engineered the 3d printer and he said that this is nothing but a robotic arm, which creates up a particle on a product layer by layer. So, he created his own 3d printer, and he’s now having a good business in India, manufacturing 3d printers. And he has been into the industry for the last eight years. And still, he says that the market is nothing today, just as it does now. So, we do believe the market is going to explode. And I am talking about the COVID. You also talked about the COVID days. There have been many talks about 3d printing. This guy actually created the face shields, 3d printed on order from multiple government organizations. And that was a great use case in this and a very quick turnaround. So definitely 3d printing is going to be a key player in manufacturing and service. And also, the bespoke industry, I think, as you were saying that the customization will be key. The Paralympics had 3d printed, which are products for the athletes. I think it’s going to be a great journey.

Thank you very much. And definitely, we’ll be talking much more in the future course and definitely looking forward to having your more talks and more products out into 3d. Thanks a lot for your time today.",11795
"“Iron man is cool”, and nobody will ignore that statement.

Photo by Massimo Virgilio on Unsplash

If you ask me who is the coolest and best hero in marvel universe, it will be IRON MAN and Tony Stark. I’m sure that nobody will against my ideas that IRON MAN is the coolest. How about best hero ? maybe, some people will dislike that statement. But let me explain first.

Tony stark define himself as Billionaire, Playboy, and Philanthropist. But in my mind, Tony stark is selfless, innovator, and genius of all heroes. We can see how he die in End Game just to save universe by sacrifice himself. 2021, You can be iron man too. You can start learn, and do something to world using technology.

Iron man technology is possible and let me tell you why.

1. Artificial Intelligence is developed so fast today

As you notice, artificial intelligence is everywhere now a days. They monitor our health, browsing data, and give us best recommendation to live better life. Tony Stark also have his own robot and artificial intelligence friday and jarvis (that turn to be Vision in age of ultron). Tony Stark is powerful because he created so many AI for himself, they help him to invent and fight in combat. As we know, Tony Stark do not go to millitary school (Missed shoot in IRON MAN 3).

So we can started this project by creating “our own jarvis”

2. Speed of sound waves < Speed of electromagnetic waves

The next step is we need to apply supersonic appliance to our suit or a strong engine to help our job. We can apply this to travel all around the world, or do a hardest work in space station.

As we know with better engine we will achieve supersonic speed that compare with speed of sound, and how about our communication with jarvis ? Don’t worry, speed of electromagnet is faster than speed of sound, so we can still have our connection to jarvis even if we on flight.

3. We can create that suit using 3D Printer with metal

After all, build 1 suit is so hard if manually crafted by hands so we can use 3D Printing to boost this crazy job. We can add stronger metal additives and let our AI do the job. We can see the result in shortest time than crafted by human. This is already achieve by car manufacturer like tesla or toyota. Using machine to build machine is better and less mistake than human.

4. Connectivity and Networking is faster now

5G is sure fast enough for general purpose right now, but we need to be sure that fast enough and have wide coverage to connect to our suit. Also, it have to take so many effort if we build the infrastructure by ourself, so just take AWS Cloud or other cloud provider that already have infrastructure globally. They have great network bandwidth too, so we will not worry so much anymore about networking.

5. So many company will support this wonderful invention (Avoid millitary company)

Last but not least, you need funding to create this wonderful idea become real. If you’re already rich like billionaire, start by your own. But if you don’t, be humble and start your fund raising. Capital is not always bad, if we build this tools to help people to work better we can get our fund faster. So many company that provide funding for idea like Venture Capital. But keep in mind that millitary use is prohibited by Tony Stark either do us. Just keep world in piece, we hate war.

Conclusion

So many invention is already describe in science fiction movie years before it becomes real. Maybe the next great invention will be iron man suit in real life. So people will not working too hard and suffer accident anymore. We can prevent it by using this wonderful tools. But keep in mind that every tools is Weaponizable, even GPS is creat for millitary purpose from start, and today every card has one GPS tools inside. Keep world in piece.

Thank you for read my ideas and leave comment when you get something wrong in my article.",3883
"One of the most fascinating advances in technology is the three-dimensional (3D) technology that has revolutionized the world with its widespread applications. The aim of the technology is simple — apply three geometrical parameters such as height, length and depth to make an object life-like.

Even though 3D technology was applied in motion pictures in the early 1920’s, it peaked and became popular amongst the masses only after the movie ‘Avatar’ in 2009. Nowadays, it is a fairly common sight to see people wearing the anaglyph also known as 3D glasses and watching 3D motion pictures. Apart from films, 3D tech has already made its presence felt in different industries solely because of its huge market potential, especially in the luxury industry.

It is now possible to design digital 3D models of products for visualization and companies have integrated this tech on their platform to improve customer experience.

Another massive application of 3D technology is 3D printing which enables creating a physical object from a digital design. Recently, architects built sustainable homes using 3D printing as a response to the climate emergency.

But have you wondered about the influence of 3D in medicine? Well, here are some important examples -

3D cell culture — This is an emerging tool in mainstream research which offers a great substitute for animal models. Using this technique, it is possible to grow tissues in the lab exactly like how they grow inside the body. As the complexity and architecture of the tissues is preserved, these models are reliable and predictive. Thus they serve as an accurate model to study mechanisms of a disease or for drug based investigations especially in cancer research.

3D bioprinting — 3D bioprinting is like a sibling of 3D printing, which is used to create a tissue or an organ. Integrating the foundations of 3D cell culture, 3D bioprinting involves producing an organ or a tissue made from using biomaterials and cells in a well specified design. In 2014, Organovo recreated a liver tissue which is extensively used to study chronic liver diseases. Although nascent, research and advancements in 3D bioprinting has the potential to overcome the burden of organ donation as well.

There are limitless possibilities of what can be achieved with such disruptive innovations in the future. They can be used to develop accurate prediction models of diseases or to generate brand new organs from scratch. They are also a great tool to effectively test the pharmaceuticals and could most certainly eliminate dependency on animal models for the same.",2600
"Taking Inspiration From Nature

In 2019, Bao came in first place in NTU’s Singapore Global Digital Art Prize with his final year project, “Museum of Marine Life 2119”. The project highlighted the dangers of human impact on the planet through a dystopian narrative: a future in which aquatic creatures have all gone extinct, replaced by robotic sculptures created in tribute by a marine biologist.

The robotic marine life sculptures were constructed using 3D printing, where various segments were produced and assembled together with a motor that enabled some parts to move realistically.

One of Bao’s intricate 3D-printed, marine-life-inspired sculptures. [Image source: Bao Song Yu]

In the same year, Bao also collaborated with DBS at their DBS Asia X workspace to showcase one of his pieces, “In-Motion”:

Taking inspiration from the golden ratio found in nature, the installation represents an artistic showcase of constant innovation, captured by the movement and interactivity of the piece. Similar to “Museum of Marine Life 2119”, this installation was constructed and assembled through 3D printing. Bao created the models through modelling software (such as Tinkercad, Fusion 360, or Rhinoceros 3D), which allowed him to construct objects on his computer before 3D printing them.

Getting into 3D Printing

Bao’s journey as a maker began during his school years through his desire to turn the ideas he had into physical works.

As with anyone getting into 3D printing, it took some time before Bao was able to create items to his satisfaction. He had to overcome challenges on his own when he first started out on his 3D printing journey, such as learning through trial and error to understand the appropriate print settings for the different print designs.

“A lot of times people have the misconception of, ‘Oh, if I design this print, I can just press the print button to get it.’ But it’s not like that. So I took a very long time just to know what settings to use for which design.”

It also helps to inspect your print every now and then to ensure that things are going smoothly.

Bao suggests beginners approach someone with 3D printing knowledge when they are first starting out. “You have to learn quite a bit before you can start to even design a print that is printable on a 3D printer.”

It’s probably why the spaces at MakeIT at Libraries left such a deep impression on him when he first visited them.

Bao thinking up even more ideas to bring to life. Check out his Instagram account for some of his latest creations.

Getting Help at MakeIT at Libraries

MakeIT at Libraries are free-to-use makerspaces located within selected NLB libraries where people can learn about 3D printing, robotics, coding and other crafting tools of the future. To help beginners learn the ropes of being a maker, programmes are also held regularly at the venue, conducted by experienced coaches.

When Bao was just starting out, he had to figure things out on his own through lengthy trial-and-error experiments with the printer settings. Because of this, he now has a healthy appreciation for the guidance that MakeIT at Libraries provides to newcomers. With the various workshops, facilities, and maker coaches at MakeIT, such makerspaces are now better equipped to cater to aspiring creators.

“It’s better to have someone advising you on how you can create your prints. Having other people coaching you on the basics is the most efficient way to go about learning something. After perhaps a few sessions, you will start to understand the thought process that goes behind each setting or print,” says Bao.

Bao assembling the intricate mechanism he had printed.

While Bao does his own 3D printing at home, MakeIT at Libraries provides an appealing alternative as the venue is also a common space for others with similar interests. It’s where he’s able to interact and exchange ideas.

With enough equipment to go around, the space provides an opportunity for makers to foster a community. “You can learn from each other instead of only one person — the maker coach.”",4074
"Ocean Orb lights suspended above the square outside Whites Tavern in Belfast. Photo via Ultimaker.

Creative design studio Urban Scale Interventions (USI) has created an immersive lighting piece and soundscape in Belfast’s historic ‘Entries’ using 3D printing and recycled filament.

Made up of 43 suspended ‘ocean orbs’, the lighting piece is the result of a contract won by USI in 2020 to reimagine the city’s central vision to lighting. The orbs were created using Ultimaker S5 3D printers and recycled PETG filament from Filamentive, and were designed to improve safety in the area and reflect the historic significance of the site.

Lighting up Belfast’s ‘Entries’

Steeped in history, the ‘Entries’ in Belfast are a series of narrow alleyways that connect many of the main city center streets and districts. The Entries are still used today as cut-throughs and gathering spaces for outdoor dining by bars and restaurants, although over the years they have experienced antisocial issues and irregular lighting.

Segment of the 3D printed Ocean Orbs printed on an Ultimaker S5. Photo via Ultimaker.

USI turned to 3D printing to bring their immersive lighting installation to life due to the increased flexibility allowed by the technology to design the bespoke orbs and test prototypes. The team leveraged the capabilities of Ultimaker’s S5 3D printer to manufacture their orbs at scale in-house at a reduced cost than they could have hoped using alternative methods.

USI also took advantage of Ultimaker’s Material Alliance Program, which allows filament companies to offer their products and print profiles in the Ultimaker Marketplace, in order to gain greater material efficiency. The firm wanted to produce the orbs from a translucent recycled material in order to make a design statement regarding the potential of more sustainable and recycled materials within the built environment.

Source: 3D printing industry",1926
"3d printing services — Inoventive 3D printing brings 3D Printing technology at your door step. With our latest 3D Printing facility in UAE, we can now cater any of your 3D Printing needs. Whether it is a large scale 3D Printing project or Industrial 3D Printing project or a small object to be 3D Printed. At Inoventive 3D Printing Dubai, you can find the solution for every of your 3D Printing need which suits your budget. Just WhatsApp or email 3D Printable file and we will get it done and deliver it to your doorstep.

We are specialized in 3D Printing of Large scale objects like art installations, wall panels, wall fixtures, home decors, 3D printed customized furniture, giant sculptures, 3D printed boat hulls, automobile parts like car spoilers and bumpers etc. In our ultra-modern facility, we use latest technology for 3D Printing which is environment friendly and superfast in printing.

3D printed items are light-weight, environment friendly, cost-effective and the best advantage of additive manufacturing is that, it will reduce wastage of raw-material considerably on its production. 3D Printing improves lives with its customizability. Automobile and aerospace companies are always keeps an eye on this new technology for its capacity to save weight on their products.

Specially in aerospace, every ounce saved on aerospace spares and materials means more weight is available for passengers or cargo. Reducing weight never means, 3D Printing will compromise on its quality. At Inoventive 3D, we make sure that we maintain the strength and integrity of the product and its quality. If we speak about the complex designs and matrix designs, with 3D Printing it is easy to achieve. Some parts with complex designs are difficult to produce with the traditional manufacturing methods and 3D Printing suits well in such situations to produce parts with complex designs.

3D Printing Dubai, has changed manufacturing concept, with its unparalleled ability of 3D Printing, lower cost and customizability of speciality parts, Inoventive 3D Printing UAE is leading the market with the innovative ideas and providing affordable 3D Printing services to the region.

Please feel free to contact us for any further assistance. Call/WhatsApp: +971 52 598 8448 | Email: info@3dprintingdubai.ae | https://3dprintingdubai.ae/

https://3dprintingdubai.ae/3d-printing-services-near-me/",2385
"let's discuss the last two 3D printing processes and understand them.

DIRECT ENERGY DEPOSITION 3D PRINTING:

Next is the direct energy deposition (DED) 3d printing technology, which constructs parts by directly melting materials and deposits them on the workpiece, layer by layer. This additive manufacturing technique is mostly used with metal powders or wire source materials. While this process can be definitely used to manufacture new parts, it is typically used for repairing and rebuilding damaged components. Other popular terms of DED like laser engineered net shaping, directed light fabrication, direct metal deposition, Laser Deposition Welding (LDW), and 3D laser cladding.

A typical DED 3D printer consists of a nozzle mounted on a multi-axis arm inside a closed frame. This deposits melted material onto the workpiece surface, where the material solidifies. The process is similar to the metal extrusion 3D printing technique, but the nozzle in DED can move in multiple directions, with up to five different axes compared to only three for most FFF machines.

SOURCE: https://www.lboro.ac.uk/media/wwwlboroacuk/external/content/research/amrg/vat-process.jpg



DED’s ability to control the grain structure of a part makes it a good solution for the repair of metal parts. And also, typically, DED machines have high material deposition rates. Some DED processes can achieve a speed of up to 11 kg of metal per hour (Sounds pretty fast right!). With DED, powders or wires can be changed or mixed to create custom alloys for better parts. But on the downside, the parts product tends to have low resolution and poor surface finish. This will need some secondary machining that will add time and cost to the overall process.

SHEET LAMINATION 3D PRINTING:

The Sheet Lamination (SL) 3D printing manufacturing technique, also known as Laminated Object Manufacturing (LOM) consists of superpositioning several layers of material composed of foil in order to manufacture an object. Each foil is cut to shape with a knife or laser in order to fit the object’s cross-section. Layers are coated with adhesive and successively glued together layer by layer, similar to other 3D printing processes. These 3D printers are based on LOM (laminated object manufacturing) technology and produce parts made with a combination of PVC (PolyVinyl Chloride) which produces models which are inexpensive at a very low cost. This technique is mainly used to produce objects in a highly detailed resolution.

SOURCE: https://www.lboro.ac.uk/media/wwwlboroacuk/external/content/research/amrg/Direct%20Energy%20Deposition%20-%20process.jpg

Yes! Now you know the basics of major core technologies involved in 3D printing. In the next one, we will see about various 3D printing materials, their properties, and their applications in detail.

Having learned the basics and foundation, let us now dive deeper into the 3D printing process. As mentioned earlier, there are different kinds of materials that can be used, but, how do we choose the best for our project? What properties to look for? — These are the quick questions that come to us when we want to work with 3D printing. This section helps you to find the answers!

SELECTION OF MATERIALS FOR 3D PRINTING AND THEIR PROPERTIES:

Materials are usually graded into 3 categories- mechanical performance, visual quality, and process. We further break down these categories into the following to paint a clear picture:

1. Ease of printing

2. Visual quality

3. Max stress

4. Elongation at break

5. Impact resistance

6. Layer adhesion(isotropy)

7. Heat resistance.

POLYLACTIC ACID (PLA):

It is the most widely used polymer for 3D printing. It provides very good visual quality. It is a rigid and fairly strong material, but is brittle and has less toughness due to which it snaps and does not bend. It also has a low thermal resistance and tends to warp in hot environments. This makes the material well suited for non-mechanical applications that need low toughness and low heat resistance. But it’s a fantastic choice for general printing applications or prototyping. ABS ( ABS is an abbreviation for Acrylonitrile Butadiene Styrene, it is an impact-resistant thermoplastic and amorphous polymer.) is being used for a long time and is in use even now because of its advantages over PLA.

Advantages:

1. Biodegradable

2. Odorless

3. Good UV resistance.

Disadvantages:

1. Low humidity resistance

2. Can’t be glued easily

ACRYLONITRILE BUTADIENE STYRENE (ABS):

ABS is another majorly used polymer and is usually picked over PLA when higher temperature resistance and higher toughness are required.ABS is not significantly stronger when compared to PLA, however, it is tougher so it will flux more before it fractures and thus performs better under impact. ABS is a little less glossy than PLA and can also be easily centered and painted. You can give it an acetone vapor bath and completely smooth the printout. This material tends to warp when it’s not evenly heated. So a closed chamber is strongly encouraged. Not all printers can print with this material at these higher temperatures. So, we need to check on the printer before we choose this material for higher temperatures.

Advantages:

Can be post-processed with acetone vapors for a glossy finish

2. Can be post-processed by using sandpaper and painted with acrylics

3. Acetone can also be used as a strong glue

4. Good abrasion resistance

Disadvantages:

1. UV sensitive

2. Odor while printing

3. High fume emissions

POLYETHYLENE TEREPHTHALATE (PET):

PET is a form of polyester that’s molded into water bottles and other packing material. PET is not nearly used as widely as PLA or ABS but provides some very unique opportunities for 3D printing. It is a tougher material than PLA, similar to ABS. So it provides some flexibility for fracture, It has a higher heat resistance than PLA but not as high as ABS. Visually, it’s glossy. There are a wide variety of opaque and transparent PET filaments that allows us to give a unique touch to our model. PET is not that difficult to print with, but compared to PLA, it leaves some stringing on the print. PET does not warp as ABS does, so it does not need an enclosure to operate. You can print PET reliably on any printer that has a higher temperature range and a heated bed.

Advantages:

1. Can come in contact with foods

2. High humidity resistance

3. Recyclable

4. Good abrasion and chemical resistance

Disadvantages:

This material is that it is heavier than PLA and ABS.

NYLON:

Nylon is widely used as an engineering polymer and has various functional applications. Nylon is significantly stronger than ABS, PETG, and PLA. It’s also very tough, has high impact resistance, and has a good thermal resistance. It also has a very low coefficient of friction. There is a wide range of colors to select the color of nylon filament from. While painting may not work well for nylon, Its challenges are in printing it. Nylon is very hygroscopic, tending to absorb moisture from the air. All polymers absorb moisture, but nylons are high absorbents. Nylon must be kept dry or be dry before printing. To achieve this, many people use a food dehydrator for multiple hours before and during the print. Well, printed adhesion can be difficult also as nylon warps much like ABS. To print nylon, your printer will need to maintain higher print temperatures, have a heated bed, an enclosure, for desired results with nylon.

Advantages:

1. Good chemical resistance

2. High strength

Disadvantages:

1. Absorbs moisture

2. High fume emissions

THERMOPLASTIC POLYURETHANE (TPU):

TPU and TPE filaments are more commonly known as flexible materials. They are not commonly used but have significant advantages. They have very high impact resistance and will not fracture like most other polymers. These materials are extremely soft. These materials are often used in parts that need to be flexible. One important thing about TPUs and TPEs is that we need to use less infill in places where we want more flux.

Advantages:

1. Good abrasion resistance

2. Good resistance to oil and grease

Disadvantages:

1. Difficult to post-process

2. Can’t be glued easily

POLYCARBONATES:

Polycarbonate is naturally clear but can be made opaque. Polycarbonate is the strongest of the materials that we’ve discussed. It Not only is somewhat ductile, but it also has good impact resistance. Polycarbonate is essentially a much stronger version of ABS. Printing polycarbonate is very difficult. It does stick to anything easily and will readily warp. Much like nylon, polycarbonate is extremely hygroscopic and needs to be dried before and during printing to ensure the part comes out correctly.

Advantages:

1. Can be sterilized

2. Easy to post-processing processes like sanding

Disadvantages:

1. It is UV sensitive.

Based on all this data we can choose the best material for our 3D printing based on our requirements and the properties of the materials.

And summing up all the information, here is a picture to help us visualize better!

An easy representation of the 3D printing materials and their characteristics

SOURCE: https://s3-eu-west-1.amazonaws.com/3dhubs-knowledgebase/Which-FDM-3D-printing-plastics-is-best-for-my-application/chemistry-comparison.png

Hoping that you got an idea of the parameters on which the printing material should be selected. And in the next blog, we are going to discuss the printing conditions to be followed before and after 3D Printing.

Meet you in the next part of my 3D Printing-Series!!🙂",9592
"Blockchain in Energy Market 2021–2026

Straits Research has recently added a new report to its vast depository titled Global Blockchain in Energy Market. The report studies vital factors about the Global Blockchain in Energy Market that are essential to be understood by existing as well as new market players. The report highlights the essential elements such as market share, profitability, production, sales, manufacturing, advertising, technological advancements, key market players, regional segmentation, and many more crucial aspects related to the Global Blockchain in Energy Market.

Blockchain in Energy Market

The Major Players Covered in this Report:

Microsoft (US), Accenture (Ireland), IBM (US), Infosys (India), SAP (Germany), Accenture (Dublin), Electron (Ukraine), Btl Group Ltd. (Canada), and Kaleido (USA).

Get a Sample PDF Report: Blockchain in Energy Market

Segmentation is as follows: -

By Type,

Private, Public

By Component,

Platform, Service

By Application,

Energy Trading, Grid Management, Payment Schemes, Supply Chain Management

By End User,

Power, Oil and Gas

The report specifically highlights the Blockchain in Energy market share, company profiles, regional outlook, product portfolio, a record of the recent developments, strategic analysis, key players in the market, sales, distribution chain, manufacturing, production, new market entrants as well as existing market players, advertising, brand value, popular products, demand and supply, and other important factors related to the Blockchain in Energy market to help the new entrants understand the market scenario better.

Important factors like strategic developments, government regulations, Blockchain in Energy market analysis, end-users, target audience, distribution network, branding, product portfolio, market share, threats and barriers, growth drivers, latest trends in the industry are also mentioned.

Regional Analysis For Blockchain in Energy Market:

North America (United States, Canada, and Mexico)

Europe (Germany, France, UK, Russia, and Italy)

Asia-Pacific (China, Japan, Korea, India, and Southeast Asia)

South America (Brazil, Argentina, Colombia, etc.)

Middle East and Africa (Saudi Arabia, UAE, Egypt, Nigeria, and South Africa)

In this study, the years considered to estimate the market size of the Blockchain in Energy are as follows:

• History Year: 2014–2019

• Base Year: 2019

• Estimated Year: 2020

• Forecast Year 2021 to 2026

The main steps in the investigation process are:

1) The first step in market research is to obtain raw market information from industry experts and direct research analysts using primary and secondary sources.

2) Extracts raw data from these sources to extract valuable insights and analyze them for research purposes.

3) Classify the knowledge gained by qualitative and quantitative data and place it accordingly to make final conclusions.

Key Questions Answered in the Report:

• What is the current scenario of the Global Blockchain in Energy Market? How is the market going to prosper throughout the next 6 years?

• What is the impact of COVID-19 on the market? What are the major steps undertaken by the leading players to mitigate the damage caused by COVID-19?

• What are the emerging technologies that are going to profit the market?

• What are the historical and the current sizes of the Global Blockchain in Energy Market?

• Which segments are the fastest growing and the largest in the market? What is their market potential?

• What are the driving factors contributing to the market growth during the short, medium, and long term? What are the major challenges and shortcomings that the market is likely to face? How can the market solve the challenges?

• What are the lucrative opportunities for the key players in the Blockchain in Energy market?

• Which are the key geographies from the investment perspective?

• What are the major strategies adopted by the leading players to expand their market shares?

• Who are the distributors, traders, and dealers of the Global Blockchain in Energy market?

• What are sales, revenue, and price analysis by types and applications of the market?

For More Details On this Report: Blockchain in Energy Market

About Us:

Regardless of whether you’re looking at business sectors in the next town or crosswise over continents, we understand the significance of being acquainted with what customers purchase. We overcome the issues of our customers by recognizing and deciphering just the target group, while simultaneously generating leads with the highest precision. We seek to collaborate with our customers to deliver a broad spectrum of results through a blend of market and business research approaches. This approach of using various research and analysis strategies enables us to determine greater insights by eliminating the research costs. Moreover, we’re continually developing, not only with regards to where we measure, or who we measure but in how our visions can enable you to drive cost-effective growth.

Contact Us:

Company Name: Straits Research

Email: sales@straitsresearch.com

Phone:

+1 646 480 7505 (U.S.)

+91 8087085354 (India)

+44 208 068 9665 (U.K.)

Blockchain in Energy Market Growth, Blockchain in Energy Forecast, Blockchain in Energy Industry Trends, Blockchain in Energy Market Share, Blockchain in Energy Industry Size, Blockchain in Energy Market Data Analysis, Blockchain in Energy Market Overview”",5468
"From building flying cars to cultivating life on Mars, we clearly love imagining a future that is simpler, better, more interesting, and something way beyond our limited understanding of the world.

It’s something that gives us hope, it’s something that reminds us of how advanced we really are and what we’re truly capable of as a society full of high (more low than high) functioning apes.

For instance, in 1958, ‘America’s Independent Electric Light and Power Companies’ launched an advertisement that showed two women and a dog (presumably a beagle) just casually flying around in a UFO-like vehicle.

The advertisement stated, “tomorrow’s higher standard of living will put electricity to work for you in ways still unheard of!”.

AIELPC further pitched to the readers that electricity is the way forward, that you would require a “lot more” of it, and promised that they would lead the world in that direction.

And, are we there yet?

Well, today, we have Tesla and other leading automobile behemoths along with the power & energy sectors making all of that a reality.

But what happened to the flying car dream?

Well, we’re not there yet. Honestly, I’m not sure if we ever will.

Can I imagine a world full of electric vehicles? Yes.

But do I envision a world where there will be cars flying around along with airplanes, helicopters, and spaceships? A big fat NO.

One could make the argument that the AIELPC used something like a ‘flying car’ to sell the idea of a better tomorrow or perhaps just to ensure stakeholders that they are in it for the long run.",1569
"Time flies when you are having fun, also when you’re changing the world too. As far as good years go, 2021 has been an awesome one for Forward Protocol. We were busy building and distributing the framework to solve the blockchain adoption problem that we barely had time to savor all our progress. So much has happened in 12 months, and some of it can fade into memory easily. Let’s see how much you remember.

Ready? Here we go!

Community

The community is at the center of Forward Protocol’s vision, and we make sure they understand how important they are to us. Our community is spread over Twitter, Discord, Telegram, Medium, Facebook, Instagram, and YouTube, and we do our best to always keep them in the loop. Social media is where it all happens around here!

We crossed the 100k community members milestone in 2021; that was a highlight! We also hosted over 20 AMAs throughout the year. Our AMAs are where you’ll catch our co-founders talking about their vision for Forward Protocol, sometimes in the company of partners and other team members. You should watch out for those and in case you want to see what all the fuss is all about you can read up on our AMAs’ recap in our Medium blog!

Partners and Investors

“Success breeds success.” The Forward vision caught on pretty fast in 2021, and it is rightly reflected in our partnerships and collaborations. We made powerful friends in high places who have supported us in achieving a great deal so far.

We are not one to name names for the sake of it. Instead, here is a challenge to find a more exclusive list — Polygon, Reef, Unilend, Metis DAO, Unvest, Coreto, AcknoLedger, ArGO app, Leyline, Shavo Odadjian, KCC, Supra Oracles, HSC, Unified Council, BlockPad and more. We look forward to growing together in the next year.

Forward Protocol also closed investment deals to get our dreams off the ground. Master Ventures, Polygon, CV VC, X21, AU 21, MarketAcross, Magnus capital, Tokenova, NFT Technologies, Ardura, Octopus, Bitcoin.com and more, all contributed to our growth throughout 2021.

Forward Protocol is immensely grateful to our investors who helped us raise $1.25 million and everyone who contributed to a wonderful year.

Impressive Numbers in IDO Rounds

Forward Protocol posted some impressive stats this year too. We got listed on CoinGecko and CoinMarketCap, with more than 778,000 of you adding us to your watchlist.

We also raised $1.25 million in our private round. 20,000 of you also got whitelisted to participate in our IBO event on MahaStarter’s Launchpad, helping us raise $100,000. Our SHO on DAO Maker also reached 29.2 million DAO locked, just 400k short of the all-time record. Way to go Forward!

Kucoin BurningDrop and Gate.io StartUp and Notable Mentions

Forward Protocol also received some love from names that you already know. KuCoin’s BurningDrop knocked it out of the park. Users staked USDT, KCS and ETH assets to mine FORWARD tokens. Gate.io also shared some forward cheer to platform users, giving out 6,000,000 FORWARD tokens during their Free Airdrop Program.

Top7 ICO and ICODrops also shone the spotlight on our work. It felt nice to be recognized by blockchain heavyweights. We also managed to catch the eye of several top media houses such as Cointelegraph, Bitcoin.com, Entrepreneur, Yahoo Finance, CryptoSlate, CryptoDaily, Investing.com, Cryptopolitan and more.

For all of this support and opportunities, Forward Protocol is grateful. We look forward to exceeding expectations once again in 2022. We hope to have all of you here by our side then too!

And to those still on the fence, we have seats for you on the train to a blockchain-inspired future. Looking for a place to start, you can join our social media channels. That’s where most of the magic happens anyway.

| Telegram || Twitter || Facebook || Instagram || YouTube | |Discord|",3859
"Image credit: Shutterstock

University of New South Wales (Sydney): Engineers have demonstrated a way to help 3d printed plastic heal itself at room temperature using only standard Led lights for around one hour which causes a chemical reaction and fusion of the two broken pieces.

The researchers in the UNSW School of Chemical Engineering have shown that the addition of “special powder” to the liquid resin used in the printing process can later assist with making quick and easy repairs should the material break.

The entire process makes the repaired plastic even stronger than it was before it was damaged, and it is hoped that further development and commercialisation of the technique will help to reduce chemical waste in the future.

That is because broken plastic parts would not need to be discarded, or even recycled, and could be mended simply even when remaining embedded in a component including many other materials.

Researchers says that the new technology could potentially be used in a range of applications where advanced 3D printed materials are currently used in high-tech specialized components. These include wearable electronics, sensors, and even some shoe manufacturing.",1201
"❂ Artist Event : Tokyo International Film Festival

❂ Venue : Tokyo Midtown Hibiya, Tokyo, Japan

❂ Live Streaming Tokyo International Film Festival 2021

Conversation Series at Asia Lounge

The Japan Foundation Asia Center & Tokyo International Film Festival

Marking its second installment since 2020, this year’s Conversation Series will again be advised by the committee members led by filmmaker Kore-eda Hirokazu. Directors and actors from various countries and regions including Asia will gather at the Asia Lounge to engage in discussion with their Japanese counterparts.

This year’s theme will be “Crossing Borders”. Guests will share their thoughts and sentiments about film and filmmaking in terms of efforts and attempts to transcend borders. The festival will strive to invite as many international guests as possible to Japan so that they can engage in physical conversation and interaction at the Asia Lounge.

The sessions will be broadcast live from the festival venue in Tokyo Midtown Hibiya every day for eight days from October 31st to November 7th. Stay tuned!",1081
"❖ ALL CATEGORY WATCHTED ❖

An action story is similar to adventure, and the protagonist usually takes a risky turn, which leads to desperate scenarios (including explosions, fight scenes, daring escapes, etc.). Action and adventure usually are categorized together (sometimes even while “action-adventure”) because they have much in common, and many stories are categorized as both genres simultaneously (for instance, the James Bond series can be classified as both).

Continuing their survival through an age of a Zombie-apocalypse as a makeshift family, Columbus (Jesse Eisenberg), Tallahassee (Woody Harrelson), Wichita (Emma Stone), and Little Rock (Abagail Breslin) have found their balance as a team, settling into the now vacant White House to spend some safe quality time with one another as they figure out their next move. However, spend time at the Presidential residents raise some uncertainty as Columbus proposes to Wichita, which freaks out the independent, lone warrior out, while Little Rock starts to feel the need to be on her own. The women suddenly decide to escape in the middle of the night, leaving the men concerned about Little Rock, who’s quickly joined by Berkley (Avan Jogia), a hitchhiking hippie on his way to place called Babylon, a fortified commune that’s supposed to be safe haven against the zombies of the land. Hitting the road to retrieved their loved one, Tallahassee and Columbus meet Madison (Zoey Deutch), a dim-witted survivor who takes an immediate liking to Columbus, complicating his relationship with Wichita.

✅ ANALYZER GOOD / BAD ✅

To be honest, I didn’t catch Zombieland when it first got released (in theaters) back in 2009. Of course, the movie pre-dated a lot of the pop culture phenomenon of the usage of zombies-esque as the main antagonist (i.e Game of Thrones, The Maze Runner trilogy, The Walking Dead, World War Z, The Last of Us, etc.), but I’ve never been keen on the whole “Zombie” craze as others are. So, despite the comedy talents on the project, I didn’t see Zombieland….until it came to TV a year or so later. Surprisingly, however, I did like it. Naturally, the zombie apocalypse thing was fine (just wasn’t my thing), but I really enjoyed the film’s humor-based comedy throughout much of the feature. With the exception of 2008’s Shaun of the Dead, majority of the past (and future) endeavors of this narrative have always been serious, so it was kind of refreshing to see comedic levity being brought into the mix. Plus, the film’s cast was great, with the four main leads being one of the film’s greatest assets. As mentioned above, Zombieland didn’t make much of a huge splash at the box office, but certainly gained a strong cult following, including myself, in the following years.

Flash forward a decade after its release and Zombieland finally got a sequel with Zombieland: Double Tap, the central focus of this review post. Given how the original film ended, it was clear that a sequel to the 2009 movie was indeed possible, but it seemed like it was in no rush as the years kept passing by. So, I was quite surprised to hear that Zombieland was getting a sequel, but also a bit not surprised as well as Hollywood’s recent endeavors have been of the “belated sequels” variety; finding mixed results on each of these projects. I did see the film’s movie trailer, which definitely was what I was looking for in this Zombieland 2 movie, with Eisenberg, Harrelson, Stone, Breslin returning to reprise their respective characters again. I knew I wasn’t expecting anything drastically different from the 2009 movie, so I entered Double Tap with good frame of my mind and somewhat eagerly expecting to catch up with this dysfunctional zombie killing family. Unfortunately, while I did see the movie a week after its release, my review for it fell to the wayside as my life in retail got a hold of me during the holidays as well as being sick for a good week and half after seeing the movie. So, with me still playing “catch up” I finally have the time to share my opinions on Zombieland: Double Tap. And what are they? Well, to be honest, my opinions on the film was good. Despite some problems here and there, Zombieland: Double Tap is definitely a fun sequel that’s worth the decade long wait. It doesn’t “redefine” the Zombie genre interest or outmatch its predecessor, but this next chapter of Zombieland still provides an entertaining entry….and that’s all that matters.

Returning to the director’s chair is director Ruben Fleischer, who helmed the first Zombieland movie as well as other film projects such as 30 Minutes or Less, Gangster Squad, and Venom. Thus, given his previous knowledge of shaping the first film, it seems quite suitable (and obvious) for Fleischer to direct this movie and (to that affect), Double Tap succeeds. Of course, with the first film being a “cult classic” of sorts, Fleischer probably knew that it wasn’t going to be easy to replicate the same formula in this sequel, especially since the 10-year gap between the films. Luckily, Fleischer certainly excels in bringing the same type of comedic nuances and cinematic aspects that made the first Zombieland enjoyable to Double Tap; creating a second installment that has plenty of fun and entertainment throughout. A lot of the familiar / likeable aspects of the first film, including the witty banter between four main lead characters, continues to be at the forefront of this sequel; touching upon each character in a amusing way, with plenty of nods and winks to the original 2009 film that’s done skillfully and not so much unnecessarily ham-fisted. Additionally, Fleischer keeps the film running at a brisk pace, with the feature having a runtime of 99 minutes in length (one hour and thirty-nine minutes), which means that the film never feels sluggish (even if it meanders through some secondary story beats / side plot threads), with Fleischer ensuring a companion sequel that leans with plenty of laughter and thrills that are presented snappy way (a sort of “thick and fast” notion). Speaking of which, the comedic aspect of the first Zombieland movie is well-represented in Double Tap, with Fleischer still utilizing its cast (more on that below) in a smart and hilarious by mixing comedic personalities / personas with something as serious / gravitas as fighting endless hordes of zombies every where they go. Basically, if you were a fan of the first Zombieland flick, you’ll definitely find Double Tap to your liking.

In terms of production quality, Double Tap is a good feature. Granted, much like the last film, I knew that the overall setting and background layouts weren’t going to be something elaborate and / or expansive. Thus, my opinion of this subject of the movie’s technical presentation isn’t that critical. Taking that into account, Double Tap does (at least) does have that standard “post-apocalyptic” setting of an abandoned building, cityscapes, and roads throughout the feature; littered with unmanned vehicles and rubbish. It certainly has that “look and feel” of the post-zombie world, so Double Tap’s visual aesthetics gets a solid industry standard in my book. Thus, a lot of the other areas that I usually mentioned (i.e set decorations, costumes, cinematography, etc.) fit into that same category as meeting the standards for a 202 movie. Thus, as a whole, the movie’s background nuances and presentation is good, but nothing grand as I didn’t expect to be “wowed” over it. So, it sort of breaks even. This also extends to the film’s score, which was done by David Sardy, which provides a good musical composition for the feature’s various scenes as well as a musical song selection thrown into the mix; interjecting the various zombie and humor bits equally well.

There are some problems that are bit glaring that Double Tap, while effectively fun and entertaining, can’t overcome, which hinders the film from overtaking its predecessor. Perhaps one of the most notable criticism that the movie can’t get right is the narrative being told. Of course, the narrative in the first Zombieland wasn’t exactly the best, but still combined zombie-killing action with its combination of group dynamics between its lead characters. Double Tap, however, is fun, but messy at the same time; creating a frustrating narrative that sounds good on paper, but thinly written when executed. Thus, problem lies within the movie’s script, which was penned by Dave Callaham, Rhett Reese, and Paul Wernick, which is a bit thinly sketched in certain areas of the story, including a side-story involving Tallahassee wanting to head to Graceland, which involves some of the movie’s new supporting characters. It’s fun sequence of events that follows, but adds little to the main narrative and ultimately could’ve been cut completely. Thus, I kind of wanted see Double Tap have more a substance within its narrative. Heck, they even had a decade long gap to come up with a new yarn to spin for this sequel…and it looks like they came up a bit shorter than expected.

Another point of criticism that I have about this is that there aren’t enough zombie action bits as there were in the first Zombieland movie. Much like the Walking Dead series as become, Double Tap seems more focused on its characters (and the dynamics that they share with each other) rather than the group facing the sparse groupings of mindless zombies. However, that was some of the fun of the first movie and Double Tap takes away that element. Yes, there are zombies in the movie and the gang is ready to take care of them (in gruesome fashion), but these mindless beings sort take a back seat for much of the film, with the script and Fleischer seemed more focused on showcasing witty banter between Columbus, Tallahassee, Wichita, and Little Rock. Of course, the ending climatic piece in the third act gives us the best zombie action scenes of the feature, but it feels a bit “too little, too late” in my opinion. To be honest, this big sequence is a little manufactured and not as fun and unique as the final battle scene in the first film. I know that sounds a bit contrive and weird, but, while the third act big fight seems more polished and staged well, it sort of feels more restricted and doesn’t flow cohesively with the rest of the film’s flow (in matter of speaking).

What’s certainly elevates these points of criticism is the film’s cast, with the main quartet lead acting talents returning to reprise their roles in Double Tap, which is absolutely the “hands down” best part of this sequel. Naturally, I’m talking about the talents of Jessie Eisenberg, Woody Harrelson, Emma Stone and Abigail Breslin in their respective roles Zombieland character roles of Columbus, Tallahassee, Wichita, and Little Rock. Of the four, Harrelson, known for his roles in Cheers, True Detective, and War for the Planet of the Apes, shines as the brightest in the movie, with dialogue lines of Tallahassee proving to be the most hilarious comedy stuff on the sequel. Harrelson certainly knows how to lay it on “thick and fast” with the character and the s**t he says in the movie is definitely funny (regardless if the joke is slightly or dated). Behind him, Eisenberg, known for his roles in The Art of Self-Defense, The Social Network, and Batman v Superman: Dawn of Justice, is somewhere in the middle of pack, but still continues to act as the somewhat main protagonist of the feature, including being a narrator for us (the viewers) in this post-zombie apocalypse world. Of course, Eisenberg’s nervous voice and twitchy body movements certainly help the character of Columbus to be likeable and does have a few comedic timing / bits with each of co-stars. Stone, known for her roles in The Help, Superbad, and La La Land, and Breslin, known for her roles in Signs, Little Miss Sunshine, and Definitely, Maybe, round out the quartet; providing some more grown-up / mature character of the group, with Wichita and Little Rock trying to find their place in the world and how they must deal with some of the party members on a personal level. Collectively, these four are what certainly the first movie fun and hilarious and their overall camaraderie / screen-presence with each other hasn’t diminished in the decade long absence. To be it simply, these four are simply riot in the Zombieland and are again in Double Tap.

With the movie keeping the focus on the main quartet of lead Zombieland characters, the one newcomer that certainly takes the spotlight is actress Zoey Deutch, who plays the character of Madison, a dim-witted blonde who joins the group and takes a liking to Columbus. Known for her roles in Before I Fall, The Politician, and Set It Up, Deutch is a somewhat “breath of fresh air” by acting as the tagalong team member to the quartet in a humorous way. Though there isn’t much insight or depth to the character of Madison, Deutch’s ditzy / air-head portrayal of her is quite hilarious and is fun when she’s making comments to Harrelson’s Tallahassee (again, he’s just a riot in the movie).

The rest of the cast, including actor Avan Jogia (Now Apocalypse and Shaft) as Berkeley, a pacifist hippie that quickly befriends Little Rock on her journey, actress Rosario Dawson (Rent and Sin City) as Nevada, the owner of a Elvis-themed motel who Tallahassee quickly takes a shine to, and actors Luke Wilson (Legally Blonde and Old School) and Thomas Middleditch (Silicon Valley and Captain Underpants: The First Epic Movie) as Albuquerque and Flagstaff, two traveling zombie-killing partners that are mimic reflections of Tallahassee and Columbus, are in minor supporting roles in Double Tap. While all of these acting talents are good and definitely bring a certain humorous quality to their characters, the characters themselves could’ve been easily expanded upon, with many just being thinly written caricatures. Of course, the movie focuses heavily on the Zombieland quartet (and newcomer Madison), but I wished that these characters could’ve been fleshed out a bit.

Lastly, be sure to still around for the film’s ending credits, with Double Tap offering up two Easter Eggs scenes (one mid-credits and one post-credit scenes). While I won’t spoil them, I do have mention that they are pretty hilarious.

✅ FINAL THOUGHTS ✅

It’s been awhile, but the Zombieland gang is back and are ready to hit the road once again in the movie Zombieland: Double Tap. Director Reuben Fleischer’s latest film sees the return the dysfunctional zombie-killing makeshift family of survivors for another round of bickering, banting, and trying to find their way in a post-apocalyptic world. While the movie’s narrative is a bit messy and could’ve been refined in the storyboarding process as well as having a bit more zombie action, the rest of the feature provides to be a fun endeavor, especially with Fleischer returning to direct the project, the snappy / witty banter amongst its characters, a breezy runtime, and the four lead returning acting talents. Personally, I liked this movie. I definitely found it to my liking as I laugh many times throughout the movie, with the main principal cast lending their screen presence in this post-apocalyptic zombie movie. Thus, my recommendation for this movie is favorable “recommended” as I’m sure it will please many fans of the first movie as well as to the uninitiated (the film is quite easy to follow for newcomers). While the movie doesn’t redefine what was previous done back in 2009, Zombieland: Double Tap still provides a riot of laughs with this make-shift quartet of zombie survivors; giving us give us (the viewers) fun and entertaining companion sequel to the original feature.",15687
"Announcing the 2022 CODAME ART+TECH Festival with a Mantra of honest connection & interaction. We call it 『 RAWTHENTIC 』

RAWTHENTIC: Mantra of honest connection & interaction.

As facades fall away we have an opportunity to let shine the true colors of our inner flames. Our open call invites artists to present their purest sense of expression in any medium.

By building up each others’ vulnerable, natural desires we forge sustainable, RAWTHENTIC bonds. Create deep wells of understanding and possibility with a sensitive unmasking of experiences. Join us for a gnarly ride of unfiltered vibes!

Curated by Alissa Christine, CODAME is excited for your transparent exploration of physical and metaphysical realities. What inspiration can your deepest mysteries reveal?

/// CALL FOR ARTISTS

Artists of all disciplines and artist levels are encouraged to apply.

Send us a proposal to be part of CODAME ART+TECH Festival 2022『 RAWTHENTIC 』

Submission Deadline: Tuesday, February, 8th, 2022

/// THE PROGRAM

The CODAME ART+TECH Festival 2022 is a multidisciplinary group exhibition and year-long mantra of honest connection & interaction, codenamed this year RAWTHENTIC.

Held all year round with various events and locations. It will include online, physical experiences, workshops, art+tech hackathons, and performances. The artwork will be showcased in art galleries around the world, online in gallery.codame.com. Optionally also available for sale on shop.codame.com and as NFTs on nft.codame.com

/// STAY IN TOUCH

Sign-up to our newsletter to be notified about our new events or initiatives, call for artists, project proposals.

Contact us for any inquiry or just to say hi!

/// VOLUNTEERS

It’s a fun and extremely rewarding experience as you learn new skills and pass your knowledge on to others. You’ll meet like-minded people, make friends and build a network of people from all backgrounds who share a common passion for ART+TECH.

Apply to be part of the magic! We’re looking for outstanding volunteers to support our ART+TECH initiatives.

/// SPONSORS

Join our list of amazing Sponsors. Be recognized as an innovative and visionary organization, Contact us

Special thanks to @Twilio, our most recent CODAME supporter

/// DONATIONS

Thank you for your support for ART ♥️ TECH

CODAME is a member of Intersection for the Arts a non profit 501(c)(3) organization.

All proceeds will be split with the participating artists. Donations are always welcome.

Contact us if interested to make an in-kind donation.

/// CODA

Credits Flyer background art courtesy Char Stiles",2591
"Businesses use CRM software to stay on top of their sales and make informed decisions by analyzing the sales pipeline at different stages. In fact, one of the matters most concerning for such businesses is how to make the most of their CRM to improve their performance. CRM is a great tool for organization of client data if you want to be in touch with different aspects of a potential sale or a lead, but it does not offer mobile accessibility by default, which would enable you to analyze a potential sale and follow leads more conveniently. That is where the idea of cloud storage comes in.

Cloud Storage and Backup

Cloud storage is the way of storing documents online such that they are accessible via any device that can connect to the internet, be it a desktop or mobile. An essential practice is to set up a backup of all data over cloud to keep it safe outside of the CRM as well. Mobile access is a secondary benefit, but it comes in handy for sales teams who want to have easy access to a contact or account’s information.

Box Cloud Storage

Box is a popular choice for businesses looking at cloud backup solutions, not only because of its added security features which give the owner close control of which files are accessible to whom, but also because it offers integration with different workflow applications to make it easier to implement.

SugarBox makes sure the organizational benefit of CRM is not lost in cloud storage.

Introducing RT SugarBox: SugarCRM and Box Integration

One of the applications that Box can be integrated with is CRM, and the most popular CRM, Sugar, can now be set up to backup documents to Box with a simple plugin. With RT SugarBox, you can have the organizational benefits of the CRM with the accessibility and security features of Box, to help your business get the best of both worlds.

Each uploaded document is automatically backed up to the cloud, and if you link it to a module such as a lead ot contact, SugarBox will create a special folder in cloud to store it in. This way, even on the go, you can easily find what you’re looking for.

Easy Integration

Simply install the SugarBox plugin on your SugarCRM instance, and the scheduler takes care of backing up all the documents without needing your intervention. At the set-up, you are asked to authorize access to your Box account, and that’s it. One-click, and your work is done.

Bidirectional Sync

Documents are synced both ways.

Not only does SugarBox backup all documents from Sugar to Box, but adding a document to your folder on Box will also sync it to appear on Sugar. What’s more, any updates to the documents are also synced both ways. You can stay on top of your work from the platform of your choice.

Organized Sync

SugarBox creates a separate folder for each module.

SugarBox makes sure the organizational benefit of CRM is not lost in cloud storage. When you link a document to a record, be it an account, contact, opportunity, or any other module, it is stored in the specific folder in the appropriate hierarchy for you to access easily. Each record will have its own folder inside the main folder of the whole module. You never lose track of the information you need even on the go.

Get RT SugarBox for your SugarCRM Instance

To get the most out of your CRM with the help of Box cloud storage, download the RT SugarBox plugin here. We also offer a similar solution for SuiteCRM, the free and open-source alternative to Sugar, on Suite Store.

For any further enquiries, and for CRM solutions tailor made for your needs, reach out to Rolustech at support@rolustech.com or visit our website www.rolustech.com.",3643
"As a business, the customers’ data is the most important thing to manage. Be it a critical sale that needs to be pushed over the line, or a lead that needs to be qualified, the time critical nature of these processes makes it important that the sales team has easy access to all the material they may need related to the potential clients.

We know that CRM is a great tool to manage customer data in an organised manner, providing us the ability to link records with particular documents and vice versa, providing important context for each case. How do we make it so the documents are more accessible as well as easy to manage in the structure the CRM creates for us with modules? Luckily, there’s an answer.

Cloud Storage

The most popular solution to the conundrum of accessibility is cloud storage. Cloud storage enables any data to be saved in the cloud, so that you can access it online whenever you like.

Cloud backup is a popular practice among businesses where the documents are accessed and edited in different platforms, but in the end they are saved onto the cloud as well for added convenience.

Box Cloud Storage

Box is a cloud storage solution that offers all the portability and accessibility of the typical cloud solutions with enhanced security features, putting you in control of what is accessible to whom. It has emerged as a popular choice among businesses because it offers integrations for several different workflow applications.

Importantly, one of the applications that benefits from integration with Box is SugarCRM. What is critical is to keep the data as organized as it is within a CRM hierarchy.

Introducing RT SugarBox

RT SugarBox is a one-of-a-kind solution that combines the portability of cloud storage with the organisation of CRM. With a simple set-up, you can synchronise your CRM documents with cloud storage such that linked records are organised by folder hierarchies, meaning you can find a document in its specific folder if it’s been associated with a contact, account, or opportunity etc. The hierarchical nature of storage enables easy management and control of documents. The CRM scheduler works to periodically back up all data, including any changes that may have been made.

Bidirectional Sync

SugarBox syncs documents both ways.

One of the best features of SugarBox is that not only can you back up documents from Sugar to Box, but also the other way round. Add a document to Box and it gets synced to your CRM without any complications, providing a flexible solution for organization on the go.

Easy Integration

SugarBox is a simple plugin that simplifies Box integration with CRM. Immediately upon installing the plugin, you are asked for permission to access your Box account, and with a single click, the synchronization is started.

Organization

SugarBox organizes documents by modules.

The biggest advantage of SugarBox is its ability to organize your documents for you. If your documents are linked to a specific module, say a contact, SugarBox will create a folder for that contact name inside a folder named Contacts, and place the document there. The same is done for other modules, such as Accounts, Opportunities and Leads. You can easily find any documents by looking for the related module folder.

Get RT SugarBox for SugarCRM

To get the most out of your CRM with the help of Box cloud storage, download the RT SugarBox plugin here. We also offer a similar solution for SuiteCRM, the free and open-source alternative to Sugar, on Suite Store.

For any further enquiries, and for CRM solutions tailor made for your needs, reach out to Rolustech at support@rolustech.com or visit our website www.rolustech.com.",3689
"A large number of brands consider their warranty management process to be merely a cost and drain on revenue. And not many companies have invested in the warranty to improve customer satisfaction and cut costs.

You will be surprised to know the functions a sound digital warranty management system can perform for your business. What’s even better? You pay for one solution, i.e., warranty management, and get a whole bundle of solutions for free. Let us take you over the benefits!

Saves Cost -

According to a recent survey by IBM -On average, warranty costs for electronics companies are close to 3 per cent of revenue. Of that 3 per cent, less than 1 per cent is spent on repair or replacement.

For a one-billion-dollar company, that’s USD 20 million in processing and administration costs across the repair cycle. A digital warranty system can help you reduce Money spent on customer communications, processing costs and identifying duplicate claims.

Improve Customer Experience -

In 2022, a study from Gartner shows that 81% of companies will be competing completely on customer experience. Hence it becomes vital for brands to provide an unparallel customer experience. A smooth warranty claims process is one of the most important post-sales services you can offer your customers. A well-executed Warranty process reinforces customers’ faith in your brand because they feel taken care of even after the purchase.

Leverage Data-Driven Decision Making -

A digital warranty solution can bring you inbuilt reports and analytics, enabling you to track customer behaviour and make insightful business decisions. Take better business decisions with relevant insights from Smart Dashboards to manage product sales across geographies, markets and demographics.

Bring Customers Online -

In 2020, goods bought online globally grew by 24%, while at the same time, store-based sales declined by 7%. Online is the next normal in retail sales, and your ability to convert your offline customers into online customers will define the future of your business. NeuroWarranty is a digital warranty solution that brings all your existing offline customers online. Assuring a smooth transition, NeruroWarranty also gives you access to valuable customer data, enabling you to retain more customers and increase revenues.

Get Customer Data -

If customer data is a goldmine, a digital warranty solution is your pickaxe.

No matter how good your remarketing and upselling messages are, without knowing who your end customers are, all those remarketing efforts will not bring in any results. In-depth Customer Data such as Names, contact details, time and place of purchase and other details help your marketing teams personalize and segment your marketing messages.

It also allows you to track product life cycle and remarket effectively in a hyper-personalized manner, creating an overall experience tailored specifically to match your customer’s needs.

The way your brand handle warranty claims sends a clear message to your customers about the importance you give to make sure they have a satisfying experience. Manual warranty processes leave customers wanting, and can even lead to serious data security breaches or fraudulent claims.

NeuroWarranty is a digital warranty solution that offers several solutions at no additional cost, along with a super-efficient tool to manage your warrant system. Built-in CRM, customer data, cross-selling opportunities, independent identity, direct communication with the customers, and the pathway to grow and increase turnover are just some of the tangible benefits.",3606
"Twilio is a customer engagement platform used by businesses to build unique, personalized experiences for their customers. It powers all manners of platforms on both web and mobile environments.

CRM is a powerful sales automation tool that gives a bird’s eye view of sales to business managers.

You might wonder if it is possible to harness the powerful communication capabilities of Twilio in CRM software. Maybe you want to streamline your sales process by having salespeople within calling distance of any interested customers, or you want to be able to contact your leads directly from the CRM to be able to close sales more efficiently.

Good news is, now you can. With our remarkable RT Telephony plugin, you can make use of Twilio’s excellent APIs for both voice calls and messaging, right inside the CRM. Reach out to potential customers, or have your sales team accessible to any customers by using telephony and messaging system built for use within the CRM.

What can RT Telephony do?

With RT Telephony, not only can you use a Twilio number as a single access point for callers, but also, you can set up an IVR (Interactive Voice Response) system complete with extension numbers and phone directory that can guide customers in reaching their desired representative. You can even set up an operator who can forward calls to available team members. The highlight is that the administrator can set up access privileges for each team member individually so that you can control which of them can take calls, which can make calls, and if they are limited to SMS etc.

If your sales team is not up to par in taking customers’ calls, not only is there an option to leave voicemail messages, but also you can set up individual external numbers that the calls can be redirected to so that the customer doesn’t get ignored.

When it comes to SMS, not only can team members message a contact or lead in a familiar chatbox environment, but those with appropriate access privileges can set up SMS marketing campaigns that can send messages at scheduled times to target lists of contacts. This is a way to keep ahead of the curve by covering multiple potential sales at once.

How do I set up RT Telephony?

If RT Telephony sounds like something you can use in your day to day sales activities, and you think it will enhance your CRM experience, you need to do the following:

Set up a Twilio account with at least one phone number. You will need to buy a number from within Twilio. Purchase a monthly or yearly subscription for RT Telephony on Suite Store. Log in to your CRM as an administrator, and install the RT Telephony plugin. Follow all the steps carefully. In the RT Telephony Configuration settings, set up the IVR and User Management settings according to your preference. You are ready to go. If you are having issues, or if the installation doesn’t go as planned, you are welcome to contact support@rolustech.com, or create a case on Suite Store itself.

Other CRM Solutions

If you would like to know about Rolustech, or find out more about our work, including our customized CRM solutions for all your business needs, please visit our website www.rolustech.com.",3177
"It costs five times as much to acquire a new customer than to keep an existing one. And whether or not a customer retains and decides to shop with your brand again is determined by the quality of post-sale service.

The post-sales or after-sales experience is crucial to ensuring customers come back. It’s not enough to focus your marketing efforts just on getting the customer to purchase but getting them to go through a cycle that encourages them to interact with you again.

How can you create a great post-sales experience?

Communicate

It is important to communicate with your customers after they have purchased with you. Make sure you keep your customers in the loop with the latest updates regarding their order delivery, product specifications, services offered. It does not have to be overly complicated.

An effective post-sales experience could be as simple as letting your customers know when their order is expected to be delivered.

Email should be the most preferred medium of communication as 80% of retail professionals said email drives retention, to improve your post-sales experience, you should look into the different emails you could send.

Struggling to get email ids of your end customers to click here to know how you can get customer data of your end customers.

Cross-sell

In a market full of options, customers like suggestions, especially when they are not very salesy. Personalize your communications to show your existing customers similar products you offer or products they would need with their existing purchase.

Manage refunds and returns

Reduce post-purchase anxiety by ensuring your customer is fully aware of your refund policy and returns process. Being transparent will build trust with your customer as it shows your brand isn’t all about sales and that you understand your customers’ needs.

Make sure you have a clear returns and refund policy in place and a helpful customer support team.

Manage warranties

A well-executed warranty process reinforces customer faith in your brand because they feel taken care of even after the purchase. Customers often have to go through the hassle of finding old purchase bills and going through endless calls with your client servicing teams to access some essential post-sales services like warranty claims and product replacement. Leading to an overall bad customer experience and leaving the customer dissatisfied with your brand.

NeuroWarranty’s digital warranty solution allows your customers to claim a warranty with just a few clicks, avoiding the hassle of finding old purchase bills and going through endless calls with your customer service teams. This creates an unparalleled customer experience and results in customer satisfaction.

Acknowledge feedback and reviews

When a business replies to at least 25% of their online customer reviews, on average, they earn 35% more revenue, says Womply. Acknowledging customer feedback and solving customer complaints is of paramount importance. Because in doing so, you are letting them know that their opinion matters, their complaints are being resolved and that your business will adapt to their changing needs.

SproutSocial research shows 47% of people use social media to complain about a business. While social media is an effective tool for you to connect with your customers, negative reviews and customer complaints on social media tend to hurt your brand image online, and as a result, discourage other people from being your customers, thus impacting your revenues.

While buying separate CRM tools can be expensive and difficult to manage, you can save tons of money spent on buying additional CRM tools. The NeuroWarranty Inbuilt CRM allows you to make your customer service highly efficient with minimum human intervention in addition to optimizing your warranty management.",3834
"Nov. 24, 2021

MEMBER IN THE NEWS

(Leonard Wright quoted.)

3 times you need money advice from a human

You can now manage most aspects of your money without ever consulting another human being. You can budget, borrow, save, invest, buy insurance, prepare your tax return and create a will — among many other tasks — by using apps, websites and software.

AP

https://apnews.com/article/technology-business-lifestyle-5ca1fe815851d1d6210398163fd6933d

BUSINESS

Not Worried About Employee Burnout. Here’s Why They Should Be.

A sure-fire way to help bring about a crisis is to ignore the problems that can create it.

Forbes

https://www.forbes.com/sites/edwardsegal/2021/11/24/report-90-of-managers-are-not-worried-about-employee-burnout-heres-why-they-should-be/?sh=2056690128c2

— — -

Employers look to retirement and other benefits to retain workers

Spurred by state mandates and the so-called “Great Resignation,” some small and mid-size businesses are changing their benefits offerings, including adding and enhancing retirement plans.

MarketWatch

https://www.marketwatch.com/video/sectorwatch/employers-look-to-retirement-and-other-benefits-to-retain-workers/47A0F50A-1FFA-4FFA-9BC8-DAE7904ECD4C.html

PERSONAL FINANCE

Some consumers still have debt from last holiday season

Some 29% of shoppers who charged their holiday gifts to a credit card last year are still paying off the debt, according to NerdWallet. This holiday season, consumers could spend more, as prices are rising and coveted gifts are in short supply.

CNBC

https://www.cnbc.com/2021/11/23/ahead-of-black-friday-some-shoppers-still-have-holiday-debt-from-2020.html

— — —

Retirees need to watch for this misstep with RMDs

One mistake retirees often make is pulling more money than necessary out of retirement accounts when taking required minimum distributions, thus triggering higher taxes. Many retirees have access to other accounts that could be a better source of funds if they need the money.

Kiplinger

https://www.kiplinger.com/retirement/retirement-plans/required-minimum-distributions-rmds/603823/a-simple-rmd-mistake-that

POLITICS & POLICY

About 1 million Californians could have to repay pandemic unemployment money to EDD

About 1 million Californians who got unemployment payments from the pandemic-related federal benefit program now have to prove to the state they had a prior work history — or face paying back benefits.

Sacramento Bee

https://www.sacbee.com/news/politics-government/article256063367.html

PROFESSION

6 lessons from audit experts who adopted AI early

Machine learning is within reach for many audit firms, but using AI platforms is not a plug-and-play operation. Hear from one firm applying AI to its audit engagements.

Journal of Accountancy

https://www.journalofaccountancy.com/news/2021/nov/6-lessons-audit-experts-adopted-ai-early.html

— — -

Do You Ask the Right Questions to Deepen Client Relationships?

If you’re having the right conversations with your clients, new opportunities naturally come up where you can help them solve their issues beyond compliance.

CPA Trendlines

https://cpatrendlines.com/2021/11/24/do-you-ask-the-right-questions-to-deepen-client-relationships/

— — -

Accountants press IRS for faster refunds and responses

The National Society of Accountants has sent a letter asking the IRS to speed up the refunds for 2019 and 2020 tax returns and communicate better.

AccountingToday

https://www.accountingtoday.com/news/accountants-press-irs-for-faster-refunds-and-responses

— — -

The future of tax talent

Tax departments have reached a tipping point and are actively rethinking their tax operating models.

AccountingToday

https://www.accountingtoday.com/opinion/the-future-of-tax-talent

— — -

Accounting Practitioner Trends Picture is Bleak

Several surveys and data-backed reports have demonstrated that many accounting practitioners have left the profession over the past few years, this is especially true in public accounting. In addition, fewer people are enrolling in college-level accounting programs in the first place.

AccountingWeb

https://www.accountingweb.com/practice/growth/accounting-practitioner-trends-picture-is-bleak

REGULATORY

What can be done to manage inflation?

This article compares and contrasts today’s elevated inflation levels to conditions in the 1970s and discusses potential strategies for organizations. Optimizing the balance of funds held in cash and short-term investments can help.

Treasury & Risk

https://www.treasuryandrisk.com/2021/11/18/navigating-the-inflation-headache/?slreturn=20211024101939

— — -

US bank regulators drafting rules for cryptoassets

The three main US federal bank regulators have said they will publish new guidance next year on how banks can be involved with cryptocurrencies. The Federal Reserve, the Office of the Comptroller of the Currency and the Federal Deposit Insurance Corp. have said they will provide guidance on what types of cryptoassets banks can handle, as well as how to protect consumers and comply with existing laws.

BNN Bloomberg

https://www.bnnbloomberg.ca/crypto-oversight-road-map-is-set-by-u-s-banking-regulators-1.1686146

TECH

New State Cyber Bureau to Combat Hack Attacks

Secretary of State Tony Blinken announced plans for the State Department to create a new bureau of cyberspace and digital policy earlier this month.

CPA Trendlines

https://cpatrendlines.com/2021/11/23/new-state-cyber-bureau-to-combat-hack-attacks/

— — -

Are Financial Planners Ready to Replace Excel?

Excel has long played a major role in the accounting and finance profession, but is it time for a change?

AccountingWeb

https://www.accountingweb.com/technology/trends/are-financial-planners-ready-to-replace-excel

— — -

All Versions of Windows Are Vulnerable to a New Zero-Day Exploit

Malware writers are already trying to take advantage of this privilege escalation vulnerability.

PC Mag

https://www.pcmag.com/news/all-versions-of-windows-are-vulnerable-to-a-new-zero-day-exploit",6035
"Businesses of all types rely on good accounting practices in order to run smoothly and ensure that their operation is making a profit. The methodologies may differ from company to company, but they all fall into one of two general categories: cash basis or accrual basis. Most individuals and smaller businesses use the cash method for ease of use, but larger and more complex businesses use the accrual method of accounting. One aspect of accrual accounting that can be somewhat confusing is related to accrued expenses.

Basis of Accounting: Accrual vs Cash

To really make sense of how accrued expenses fit into a business’s financial reporting, it’s crucial to understand the difference between the two methods. The fundamental difference between the two is related to when revenue and expenses are recognized. This is an important distinction, both in terms of keeping accurate books and staying in compliance with SEC regulations. Public companies, for example, are required by the Generally Accepted Accounting Principles (GAAP) to use the accrual method in large part because of revenue and expense recognition.

Cash basis accounting, as noted earlier, is typically used by small businesses or for tracking personal finances. As the name implies, this method is all about cash flow and tracking money going in and money going out. Revenue is recognized in the cash basis method when the money is actually received, either in currency or a credit to a deposit account. Likewise, expenses are only recognized when money is paid out. The advantage of cash accounting is its simplicity, but the disadvantage is that the focus on cash flow may overstate the company’s financial health by omitting payable accounts.

Under the accrual basis of accounting, by contrast, revenue is recognized after it is earned; in other words, it is recognized after a good or service is delivered to a customer with the expectation of being paid at a later date. Likewise, expenses are recognized on financial statements after they are incurred: after a transaction but often before any money is paid out. Understandably, this method of accounting is more complicated, but it also paints a more accurate picture of a company’s finances because it incorporates accounts payable and accounts receivable.

What Are Accrued Expenses?

Accrued expenses are a component of accrual basis accounting, and they represent expenses that are recognized before they have actually been paid. It is because accrued expenses are considered current liabilities on the balance sheet (because they are an obligation to make cash payments in the future) that they are sometimes also referred to as accrued liabilities. Accrued expenses can sometimes be estimates of what will eventually be paid out as well. The following are some examples of accrued expenses:

supplies have been purchased but no invoice has been received

accrued interest expense

product or service warranties

taxes

employee bonuses, salaries, or wages

utilities

A somewhat related term is a prepaid expense. Unlike an accrued expense, a prepaid expense is paid in advance for goods or services that will eventually be received in the future. Prepaid expenses are actually recorded as assets on the balance sheet at first; over time, the value of these assets is expensed and is noted on the income statement. Whereas accrued expenses are always recognized in the period in which they are incurred, the value of prepaid expenses can be measured over multiple accounting periods.

How Are Accrued Expenses Recorded?

When accountants and bookkeepers reconcile the general ledger, they usually treat an accrued expense journal entry as a debit to an expense account and a credit to an accrued liabilities account; this is how both expenses and liabilities are increased. An accrued liability is considered a reversing entry (a kind of adjusting journal entry) that is temporarily used to adjust the books between different accounting periods.

Find the Best Data Package

At Intrinio, our team of financial data experts is passionate about providing clients with the best data from established, reputable sources. If you’re ready to integrate new, trustworthy data into your systems, contact us to request a consultation or review our financial data packages.",4299
"description about this product Innovative Multi-Surface vacuum: Deep clean with the whirlwind canister vacuum. This vacuum features an integrated airflow control on the handle that can be easily switched at your fingertips. It includes three settings: carpet, upholstery & hard floors.

2.5L dust container, no maintenance costs: No bags or filters required. The whirlwind has a Bagless design and uses washable filters. Rinse the filters as needed and you’re ready to go.

Lightweight and easy to maneuver: vacuum anywhere around your home with ease with this lightweight vacuum. The Eureka whirlwind vacuum cleaner weighs less than 8 pounds and can easily maneuver under and around furniture and stairs thanks to its swivel steering and telescoping metal Wand.

2-In-1 integrated crevice tool: The Eureka whirlwind canister vacuum features a crevice tool integrated into the hose handle so it’s at your fingertips whenever you need and will never get lost. Easily disconnect the handle from the hose and then you can switch between a crevice tool and a dusting brush.

Easy-to-use: Spend more time cleaning and less time struggling with your vacuum cleaner. The Eureka whirlwind has automatic cord Rewind, saving your valuable time. The whirlwind has a one-button release for easy dust cup emptying.

What’s the difference between NEN110A and NEN110B. There is an Extra washable Filter in NEN110B.

Capacity: 2.5 liters for order click this link https://amzn.to/3cU0N1x blogger id link https://abouthealthinformation50.blogspot.com/ youtube channel link https://www.youtube.com/channel/UCczFPd47JMYjEC2qzD_C0sw second youtube channel link https://www.youtube.com/channel/UCgez83Hqpa822GsMBazEcog facebook group link https://www.facebook.com/groups/387680909727762/media/videos affiliate link https://dashboard.teespring.com/overview?from=login&from=login&auth_method=email&from_spring=",1887
"let’s start with the born of Artificial intelligence (AI). The time was 1956 and Ai means the theory and development of computer system able to perform tasks normally requiring human intelligence such as visual perception, speech recognition decision making, translation between language. after founding ai and research several years they thought this Ai is a loss project and that time is called(Ai winter) but today in 2021 AI technology hit the revolution and this revolution touch every important sector of innovation and science. From space to earth everywhere you can see the impact of Artificial intelligence (AI).

Open AI is the one of companies which working on developing Artificial intelligence (AI) there are many companies nowadays. we must talk about the use sectors of AI.

The fields of Artificial intelligence (AI) such as web search engines (Google), Netflix. YouTube. Amazon on internet sites there is a huge use of ai technology. understanding human speech such as Siri, Alexa, self-driving cars automate decision making than, some strategic games like chess. there is many life easier impact of AI technology.

By that discussion, we understand that in which way AI helps humans so much but there is another question also dose Artificial intelligence (AI). have bad impacts? Now I want you to jump on the movie’s world to give you some idea maybe that will be worse or maybe more help full anything will happen .so let’s move to the movie’s world.

Remember? Avengers the Ultron in this marvel movie they show that Ultron has smart AI although he is a program. He can make his own dissensions.On the movie, they make this for humans but at a certain time, Ultron can make his decision because he has Artificial intelligence (AI).and that became a threat to humans as he has Artificial intelligence (AI) it means he has every human’s personal information so he can do whatever he want to peoples.

Now just think ai is Ultron and he is gathering everyone’s information like bio-metric, face, eye scan, password when he gets everything he will act like Ultron maybe. The ai has lots of data of humans if he wants he can do anything. now the point is should we need to make the boundary for Artificial intelligence (AI). Otherwise, if Artificial intelligence (AI) doesn't control by humans and ai became limitless then maybe it can be the Ultron in real life.",2378
"An FBI document lays out the information various secure messaging apps can share with law enforcement.

By Nathaniel Mott

It can be hard to decide which secure messaging app to use. Luckily, a newly leaked document that was reportedly prepared by the FBI’s Science and Technology Branch and Operational Technology Division makes it easy to see what kinds of information various services can provide in response to requests for user data.

Rolling Stone reports that the leaked document was prepared on Jan. 7. It’s titled “Lawful Access,” and according to its header, it describes the “FBl’s Ability to Legally Access Secure Messaging App Content and Metadata.” The document is unclassified, but it’s alternately designated as “For Official Use Only” and “Law Enforcement Sensitive.”

“As of November 2020, the FBI’s ability to legally access secure content on leading messaging applications is depicted below, including details on accessible information based on the applicable legal process,” it says. “Return data provided by the companies listed below, with the exception of WhatsApp, are actually logs of latent data that are provided to law enforcement in a non-real-time manner and may impact investigations due to delivery delays.”",1240
"Photo by Andrew Neel on Unsplash

Hey guys, I’m here again! This is a sequel article to my last article but you can actually enjoy it without having to read the last article.

So, in this article, I will be discussing how I’ve been able to leverage technology in doing what I do and you know, how I make an income and still increase my income options with it, and how it makes my job easier.

Let's start from the basics. The basics of my carriers as a writer, public speaker, and life coach, revolve around the use of my phone, laptop, and the apps and software I download on them, technology-wise. I do a lot of other things asides from these three but those roles are majorly volunteered roles. So I won’t be discussing them on here unless I have to.

Everything I do, I do it on my phone and Laptop. I don’t do any physical meetings, asides from some of the demands in my public speaking career. Because even that space has chosen to leverage online platforms. So, technology is an integral part of what I do and it would be very hard for me to progress without technology.

In my career as a writer, I use Medium, Substack, Microsoft word, LinkedIn, Instagram, WhatsApp, Google Chrome, and Grammarly. I have used WordPress, Surfer SEO, Google sheet, Freelancer, Poetizer, Fiverr, e.t.c in the past. Now let’s talk about what I use each of them for and how I use them.

The platforms where I publish my articles are Medium and Substack. Medium is my personal blog and Substack is for my brand's webpage, rmwca.substack.com. I use Microsoft word to most times outline and plan my article ahead.

Sometimes, I pre-write the article on Microsoft Word and then paste it on Medium or Substack. Other times, I use it to write my short stories that I deliver to clients on Freelancing platforms or to work on personal projects.

I use Google Chrome as my browser on my laptop. I access Medium, Substack, Grammarly, Gmail, basically, all the websites I work with are accessed on Google Chrome. You know being able to access multiple tabs conveniently and also access my mail right from my browser is it for me.

I use Grammarly for spelling and grammatical errors and it is installed on my Chrome browser. Freelancer and Fiverr are my freelancing pages. I use Surfer SEO, word press, while I was interning as a technical writer. I also used google sheets to access topics then. I used Poetizer when I was actively writing poems.

My day-to-day apps are WhatsApp, Instagram, and LinkedIn. These are the platforms where I market myself, publicize my articles once they are published, and continuously bombard with value. I use them all for my career in writing, public speaking, and as a life coach.

How? In my career as a writer, immediately after I publish an article, I share it on my WhatsApp, LinkedIn, and Instagram. On Instagram, I have an account for my brand @rubb_ingmindswithcoachAliyyah(please follow) and I have a personal blog under my name @aliyyah_kalejaye. On my brand’s account, I post daily quotes, weekend specials, I leverage on insta stories and the likes, the same with my personal blog, to promote my career as a life coach.

On LinkedIn, I have a professional account in my name also, where I encourage my connections to engage with my articles and what I do. My social media pages depict who I am and what I do, all day every day and it has attracted a series of offers, connections, and opportunities to me.

These are my biggest ways of leveraging technology to do what I do and make what I do easier and seamless for me. You could also adopt some of these ways if you haven’t. Also, tell me ways you are Leveraging Technology in your career in the comment section. I’d love to hear ittt!

You can find me on all social media platforms as Aliyyah Kalejaye.

Like, share, comment, and SUBSCRIBE! if you enjoyed reading this article.",3854
"3 innovation and digital news in 1 minute. Every Monday. Episode 275

News 1. Pizza but made by a robot

Pazzi is an autonomous pizzeria in Paris with another location in Serris in the East of Paris. They call themselves the world’s first autonomous restaurant. Customers order on a self order terminal which allows them to customise the exact toppings of the pizza. The robot arms then prepare the dough, add the sauce and toppings, put the pizza in the oven and cut it in front of your eyes. If you also ordered a drink you’ll get that from a vending machine. Opinion: Pazzi is an excitable example of how a human task can be automated. There is a large team behind it, so we expect to see more of these restaurants in the near future.

A not even that simple task is automated by a robot arm, fun to watch Pazzi make Pizza!

News 2.Click & Collect but cool

Delipop launched a pick up location for groceries in cooperation with Carrefour in October. To use its service, customers have to order groceries in Carrefour Drive and set the Delipop store as the pick up location. Customers can pick up their orders the same day if ordered in the morning or next day if ordered later during the day. Customers get an access code to enter the colourful Delipop pick-up store. Opinion: Delipop aggregates orders and thus operates at significant lower cost compared to ultra-fast deliveries. At the same time the fresh and modern designed pick-up stores can keep up with the coolness-factor of Gorillas.

A hip and cool pick-up store for groceries, we hope to see more of these around Europe’s neighbourhoods.

News 3. “Almost Just Walk Out” but more practical

Carrefour launched their new, first of its kind, autonomous store “Carrefour Flash 10/10” in Paris at the end of November. Utilising cameras and scales the store tracks the products taken and customers just have to pay — no scanning needed. Opinion: Compared to “Just Walk Out” technology, customers still have to pay manually. However they don’t need to download an app and register before before shopping. This makes this store more practical to use for a larger audience.

Convenient, no download of an app required.",2174
"There are 3 basic video marketing technologies you may choose for effective engagement through your business explainer videos. They include video advertising via Google AdWords, advertising with YouTube bloggers or influencers, and promotion of your YouTube channel. Find which video marketing technologies are the best for your tasks.

What are videos marketing?

Video marketing is a generalized concept. Behind it, there may be hidden various technologies for attracting new customers with the help of videos. If we talk about digital business, video ads shown through Google AdWords, placing ads on YouTube bloggers’ channels, and creating and promoting your own YouTube channel with unique content will be the best examples.

Each technology is suitable in some cases, has its own advantages and disadvantages. And it’s up to you to pick when and what to apply either of them, be it a video of marketing mix or either of them used separately.

How to choose video marketing technology

So, before you pick technologies you like or need to apply for your video campaign, you’d better familiarize yourself with all the nuances of each. Also, it will help you understand which way is the best in your particular case.

1. Content on your YouTube Channel

Explainer videos or other commercials are usually self-created. But if you advertise your product or brand, then you may shift this work to professionals. Therefore, you will need some production budget to cover costs for a professional promo video.

Using self-made branded content will help you develop your business faster and engage the audience in various ways. The technology allows regularly filling in your channel with helpful content and attracting the target audience.

Create brand awareness;

loyal community around the brand;

communication with subscribers through videos;

train potential customers to use the product;

inform about new updates;

attract potential customers to the channel from YouTube, Google, and other search engines;

attract to your website and blog.

How to launch:

Define video format; make a content plan; purchase equipment for filming, or hire a reliable vendor for shoots, create videos, edit, optimize for search and placement on the channel; promote video using your own website, social networks, thematic channels on YouTube or advertise through AdWords; work with video comments (answers, tips, thanks); analyze the audience’s reaction to different videos, adjusting the style and format of the video. prepare new material.

forming an expert image and gaining trust;

a full-fledged communication channel with the audience;

keeping the audience’s attention with the help of subscription forms;

videos engage much better than social media posts.

serious resources are needed to shoot high-quality videos;

regularity in video posting is required;

promotion of channel and attracting first customers is a long-playing process;

It is not easy to assess the contribution of a channel to conversion, especially if it is launched simultaneously with the site, groups in social networks, and other advertising tools.

2. Working with YouTube bloggers

There are dozens of bloggers on YouTube with a loyal audience and high rates. Placing ads in their videos attracts potential customers because they are loyal to bloggers and tend to trust them in everything.

There are several types of advertising with bloggers:

direct advertising (your videos 30–60 seconds long are inserted at the beginning, middle, or end of the video);

promo (blogger’s video is completely dedicated to your product);

mentions (in this case, the blogger organically integrates information about your brand or product into their video, and viewers do not think it an advertisement);

links in the description (a link to your page with a short explanatory text is added to the description for the video).

Expand the audience coverage through new channels;

Attract new target audience to the site;

Increase brand awareness and build trust;

Increase the number of targeted actions (subscriptions, sales, etc.).

How to launch:

First of all for this type of marketing with videos, you

Search for channels on YouTube according to your specific criteria (coverage, topic, number of subscribers, placement price, channel age, etc.).

Clarify on available ad formats (link in the description, promo video, mentions).

Agree on the placement of advertisements in detail (who makes the video, what data is needed, the budget, etc.).

Do the execution control and traffic flow tracking.

attracting a loyal audience and expanding coverage;

fixed price of advertising;

when ordering promo videos and mentions in the form of integration, you do not need to spend money on content creation;

the cost of one click is lower than when placing ads through AdWords;

there is no need to negotiate directly with bloggers and attract agencies with high commissions;

there are not many TOP bloggers and they usually work with agencies;

you need to find and pitch them on your own and order placements;

it may be expensive if you blog with a real influencer.

Influencers from Dribbble

3. Video advertising via Google AdWords

Google AdWords is one of the video marketing technologies used to advertise on YouTube.

There are 3 types of advertising there:

TrueView In-Stream (video advertising at the beginning, middle or end of the video, which the user may skip after 5 seconds of showing. The fee is charged if the user has watched 30 seconds of the video or watched it to the end if the video is less than 30 seconds long).

(video advertising at the beginning, middle or end of the video, which the user may skip after 5 seconds of showing. The fee is charged if the user has watched 30 seconds of the video or watched it to the end if the video is less than 30 seconds long). TrueView Video Discovery (advertising is displayed in the list of related videos. Clicking starts watching the video. Money is debited after the user clicks on the advertising icon).

(advertising is displayed in the list of related videos. Clicking starts watching the video. Money is debited after the user clicks on the advertising icon). Splash ads (this is a 6-second video at the beginning of YouTube videos. You cannot miss them. You will be charged for 1000 impressions).

(this is a 6-second video at the beginning of YouTube videos. You cannot miss them. You will be charged for 1000 impressions). Increase brand awareness;

warm-up the audience;

get fast traffic to the site or attract YouTube subscribers;

attract traffic and boost sales, etc.

How to launch:

Create advertising videos;

register in Google AdWords and launch a new campaign (type — “Video”);

select the campaign goal (“Brand and product awareness, etc.”);

do the campaign setup (daily budget, start and end dates, networks, languages, display locations, bid strategy, targeting demographics, interests, etc.);

specify the key phrases and create ads in the selected format;

launch ads moderated by Google AdWords;

monitor display parameters and adjust settings.

Measure success and analyze results.

the quick attraction of traffic and increase in coverage;

accuracy of performance assessment thanks to the built-in analytics system;

the ability to make adjustments or stop the campaign at any time;

payment for specific actions — views or clicks from links;

the ability to accurately target your audience thanks to the wide targeting options;

the ability to bring back the audience thanks to retargeting.

high cost of click-through advertising, especially in expensive niches with high competition (development and promotion of websites, Fintech, insurance, etc.);

direct dependence of the flow of clients on the advertising budget costs (you stop the ad — there will be no new clients);

restrictions on advertising in certain niches.

the need to involve an AdWords specialist to properly set up a campaign if you are a no-skilled one.

Video designing art from Explain Ninja

Basically, to measure the success of your campaign, in either case, you will need to pay attention to the following performance indicators:

the number of clicks on the link under the video;

time spent on the website;

bounce rate and viewing depth;

the cost of one conversion;

the number of subscriptions received (registrations, purchases);

the cost of a new subscription or other targeted action;

the average check of the user who clicked from the video and made a purchase.

Also, do not forget to read out the AdWords instructions carefully and bloggers’ agreements and other materials at hand. They will allow you to get better at what you’re doing and open up some more potential as a professional video content marketer.

Animation design

Conclusion

Of course, the best marketing video game technology or video business technology or whatever the video you use to promote your business will be a mix. To get the most out of your video marketing, it’s worth using all three tools for sure.

To begin with, you may start creating and promoting your own channel, which is a minimum budget case. Next, enable the placement of advertisements for YouTube bloggers or partner one to place something there. And to back up this network, launch a video ads campaign via Google AdWords for good.

Such a tremendous balanced approach will allow you to reach a broad audience, optimize costs and take full advantage of video marketing opportunities with incredible explainer videos. Have a try!

Have anything to add? — Welcome!",9558
"Software Outsourcing Process And Models for Successful Project Completion

Software development outsourcing offers many positive outcomes and helps you save money and time. Let’s see the models and processes for outsourcing your software development project before completion. Vijay Khatri Follow Dec 13, 2021 · 6 min read

The software outsourcing process commences when a business is facing certain limitations and decides to employ a third party to carry out the work on its behalf. Outsourcing software development projects to a trusted development partner enables you to get acquainted with top-notch web, mobile, and other software products.

Not only that, outsourcing offers several benefits and other positives that enterprises and individual software developers can experience. So, if employing trusted third-party software developers resonates with you, here’s a step by step procedure for outsourcing software development projects:

Software Development Outsourcing Process

The following guide helps you navigate through your outsourcing journey for hiring a reliable outsourcing partner.

1. Defining Your Expectations and Goals

Without clear and concise goals and expectations, the outsourcing team would find it difficult to comprehend the software development project. Thus, the first step before you do anything is to prepare a detailed project outline, what features to include, technology to use, and a timeline for completion.

Include your internal team or even the people you know from your business relations if possible for a brainstorming session. This would help you ensure your software development projects sound interesting and achievable.

2. Devise Scope of Work

Preparing the scope of work is the next step in the process of outsourcing software development. It does not make sense to reach out to a software outsourcing partner without any documentation that clearly outlines the objective and scope of work.

Devising such documents would require things such as product specification documents, plans, budget estimates, system admin documents, and other reports. These are just the tip of the iceberg, there’s a whole bunch of things needed depending on the type of the project.

3. Research & Find Reliable Software Development Partner

Finding a trusted and reliable software outsourcing partner is a tough nut to crack. The right software development agency is decided based on the project requirements, goals, and scope of work.

The first thing you should do is get references from your team, colleagues, business partners, or just google it. Also, there are various company comparisons and review sites such as clutch to find a reliable outsourcing company.

Shortlist some of the agencies and filter out to find the best and reliable one to do the job. To shortlist companies, you’d have to do a bit of research first, which can be a tedious task but have huge implications on your software development project.

There are certain ways to research and find the best outsourcing agencies such as checking out their portfolio, reviews/testimonials from past clients, years of experience, etc.

4. Contacting the Outsourcing Partner

The first interaction with the agency of your choice is going to be crucial. You can approach more than one agency with similar interests and conduct an interview with them to get an idea if they are experts in their field or not.

Schedule a video interview or a phone call to establish a personal relationship. Here are a few things you should discuss while on a call with the outsourcing agency:

Ask about their process, past experience, and technical skills for completing the project.

Set out your goals and expectations from the start.

Discuss about their prior projects, what problems they had, and how they resolved them.

Find out the team size and who’ll work on your project, who will be the point of contact, etc.

Discuss the budget, time scale, and other important aspects of the project.

Invite the outsourcing partners of your choice and get to know them for better understanding and relationship.

Models for Software Outsourcing Project

Most sophisticated software development agencies have their own standards which they follow for efficient work. Knowing such standards and models of work during the interview process would be imperative.

Also, it’s a subtle way to let others know and look more professional while explaining the outsourcing models. Here are the software outsourcing models mostly in use:

1. Staff Augmentation Model

One of the simplest models of outsourcing, the staff augmentation model is where the software development tasks are carried out by the outsourcing team. In simple terms, it’s like leasing a team of experts from other agencies, be it onshore or offshore, and providing them tasks to work upon and complete the project.

It sounds similar to an offshore development center where you set up a whole team of experts to carry out your software development project.

Also known as team augmentation, this outsourcing model enables your in-house team to maximize development efficiency while retaining control over the project. Aspects like defining the work process, managing the project to work on, etc. are controlled by the client.

2. Dedicated/Managed Team Model

A dedicated or managed team outsourcing model is where you outsource your software development project to a dedicated team of experts. Here. you’ll get a team of developers to work and complete certain tasks and project delivery pipeline.

Also, you’ll have direct access to the team leaders and project managers that take care of the software delivery schedule to ensure the project stays on track.

The client is still in control of making software development decisions and can control projects individually. However, they can pass on a great deal of decision-making to the outsourcing providers in cases where a software product is required to be maintained.

3. Project-Based Outsourcing Model

In the project-based outsourcing model, your only concern is the result which is the software product and not the means behind it. Here, your development partner looks after and manages the entire software development process as per the specifications and requirements provided prior to project commencement.

The client hands off the requirements to the outsourcing partner, who is responsible for developing and delivering the final product. Although the client has the least amount of control, they still can have some oversight to ensure that the product quality remains intact.

Software Outsourcing for Successful Project Completion

Software outsourcing models help you get started with your journey to software development. Once you have established who’ll have more control over project decisions, you must tick off the following things for successful completion.

Straighten out who’s gonna provide the technical support when and if there arise any out-of-scope issues.

Know yours as well as the outsourcing provider’s limitations of doing certain things. And since you have mutually understood who’s in control, distribute and assign tasks to particular team members. Figure out what they are best at and what needs improvement.

The last thing is to have some faith and trust in the outsourcing team of your choice. You don’t always have to lean on their shoulders considering they know what they are doing and that’s why they have been recruited.

Final Words

Outsourcing software development has countless positive outcomes for your business. It saves money and time and also enables you to get high-quality solutions from a team of experts. Now the only thing between you and an outsourcing software project is choosing the model that suits your requirements best.

Also, through the steps listed in the outsourcing process above, you must first define your goals, requirements, the scope of work, etc. before finding and reaching out to a reliable outsourcing partner.

Speaking of the same, Ashutec Solutions Pvt Ltd. is a reliable outsourcing services provider trusted by many small to large enterprises. Our experienced and adept team of professionals is ready to serve you and offer you unique, scalable, and maintainable software and product development solutions.

Contact us today or write to us at connect@ashutec.com for more discussion on the topic. Also, follow ashutec for reading more such articles.",8430
"Freelancers Vs Agencies: What’s Best for Outsourcing Software Development?

Deciding to outsource software development projects to Freelancers or Agencies is sometimes a difficult choice. Let’s find out the pros and cons of outsourcing software development to each. Vijay Khatri Follow Nov 15, 2021 · 5 min read

Finally, you have decided to outsource your software development projects to a reliable outsourcing partner. However, finding the right and trustworthy outsourcing partner is like finding a needle in a haystack. Since the freelancers have taken over, it has become more difficult for organizations to choose between freelancers and agencies.

Deciding over which one’s best and right for you without understanding what each approach offers is like sitting in a rowboat without oars. There are certain considerations while outsourcing your software development projects that you’d have to take care of.

To help you decide what’s the best approach for your organization, the team at ashutec presents the pros and cons of outsourcing software development to freelancers and agencies.

Pros of Outsourcing to Freelancers

1. Freelancers are Cheaper

Economical reasons play the biggest factor while outsourcing software development. We all know that outsourcing is cost-effective and affordable than in-house development. As freelancers work independently, it is even more affordable and cheaper in comparison to the agencies offering similar development services.

2. Skill Specialists

Freelancers are individuals with specific skillsets. Oftentimes, small-scale projects require only professionals that hone the expertise and skills in one of the areas. In such situations, you can handpick the best out of all freelance professionals for the task. Agencies also do have experts in specific skillsets too. But it’s no guarantee that all the agencies will have the expert professionals your project demands.

3. They are Flexibility to Work With

You can hire freelancers whenever you want. They provide you with the flexibility to hire them for whatever number of hours you need on your project. Often they don’t have requirements such as minimum engagement level or long-term contracts to hire them. Further, you can hire multiple freelancers with different skillsets to carry out large software development projects.

Cons of Outsourcing to Freelancers

1. Unreliability

Freelancers work with multiple clients and on multiple projects at times. Oftentimes, you’ll find them juggling between different projects, which may delay your project’s deadline. This unreliability of project completion is why most organizations don’t consider outsourcing to freelancers.

Further, the overworking nature of the freelancers makes them rush project delivery with poor performance. Thus, you’ll often find yourself compromising on the performance front of your software project.

2. No Senior or Supervisor

As freelancers work independently, there’s no senior developer or supervisor to inspect the work or quality of the project. Additionally, it’s hard for you to make them understand your brand identity, expectations, and project deadlines. Thus, it becomes vital for you to have some level of oversight over your freelancers.

What this means is you’ll have to take time to communicate, review the work and quality standards, then offer your feedback and decide on project deadlines, which may increase your workload.

3. Too Tight Work Schedules

Freelance developers sometimes work on too tight schedules, thus managing them often feels like climbing a mountain. It takes a lot of effort from your side especially when you choose to work with multiple freelancers simultaneously. Many times, freelancers may not be available to work on your projects when you need them the most.

Pros of Software Outsourcing to Agencies

The major factor why software companies outsource to agencies is the reliability they get in return for their project’s completion. Thanks to this reliability, companies prefer to work with a well-established business rather than individuals. Let’s see other benefits of outsourcing software development to an agency:

1. Accountability and Less Oversight

Outsourcing to agencies is a more trustworthy choice and requires less oversight. They would never do such things like compromising on quality, performance, or not responding to your queries or even quitting projects midway that tarnishes their image and reputation. Furthermore, you won’t be as much invested in project management.

Agencies have professional project managers tasked to attend to each client’s requests and report development progress. They are responsible for streamlining the project communication and completing project milestones within deadlines.

2. They have a Good Track Record

Well-established agencies have experience and a great track record of working with many clients in past. They may have completed projects similar to your niche and have built an extensive portfolio, case studies, testimonials, and reviews.

Further, agencies want to retain their clients and form long-term relationships to sustain their workload rather than keep finding new clients as in the case of freelancers. To form a long-term relationship, they’ll work to prove their worth over and over, again and again.

3. Streamlined Project Management

As opposed to freelancers, agencies have specialized teams and departments to take care of your project requirements and keep it on track. They are better at project management, especially the larger ones because of the large team size than an individual freelancer.

Moreover, the specialized team of agencies can organize the workflow for you and offer access to enterprise resources. Thus, large projects with high complexity levels are better handled by agencies.

Cons of Outsourcing to Agencies

Hiring agencies have certain drawbacks as well. Let’s have a look at them:

1. Expensive Than Freelancers

As mentioned, freelancers are cheaper than outsourcing to agencies because of the overhead fees and taxes agencies charge. Also, salary payouts to professionals and non-billable people to account for factor-in to make outsourcing to agencies more expensive than freelancers. Such factors create a vast difference in the per-project and hourly rates of agencies and freelancers.

Even though agencies are expensive than freelancers, outsourcing to agencies in countries like India is still a cost-effective option and saves a significant amount on your software development.

2. Communication Delays

You are not only outsourcing to an agency but the whole corporate administration. They have multiple people involved in a project, which is oftentimes great but leads to communication delays. Many agencies allocate a project manager or account manager to handle all the communication with the clients.

But sometimes such a single point of contact may lead to communication delays as passing on the client requests depends only on them. To overcome this communication gap, many agencies allow direct access to resources working and delivering the project.

Freelancers Vs Agencies: Who’s the Winner?

While freelancers are the cheapest option to outsource your software development project, they are not as reliable as outsourcing to a well-established agency like ashutec. Freelancers are more trusted for small-scale, short-term projects to outsource, which many big agencies wouldn’t consider and be interested in.

However, large, complex, and long-term projects require multiple hands of experts such as designers, developers, QAs, writers, etc., which agencies can easily fulfill. They can deploy multiple skillsets required on a project for which freelancers are incompatible. Thus, agencies are the right choice to outsource your large-scale, complex software and product development projects.

Ashutec Solutions Pvt. Ltd. is one such agency with multiple experts having various skillsets ready to be deployed on your software project with high complexity level. Further, outsourcing to the ashutec team in India has many advantages over other geographies like Ukraine. We have a large portfolio and a good track record of working with small to large enterprises.

Moreover, we offer unique, scalable, and maintainable software and product development services at cost-effective rates. Write to us at connect@ashutec.com to discuss your software project outsourcing needs further in detail.",8454
"Top Tech Skills to Master in 2022: Coursera Report, Pt. 1

Global digitalization is here: the tech industry grows so fast that new solutions, which make every sphere of our life much easier, are released weekly, and nearly all jobs have digital elements now. No job is exempt from digitalization and the COVID-19 pandemic accelerated this transformation in nearly every industry. SimbirSoft Nov 28, 2021·4 min read

Coursera, an online course provider, published the Global Skills Index 2021, which analyzed the proficiency of its learners across 10 industries in a set of 26 skills. We would like to highlight the impact on the technology domain and how outsourcing can help companies find the people who possess the skills needed for a project.

According to Coursera’s research, each industry reported an acceleration in the need for technology skills to account for the lack of physical interaction induced by the pandemic. These are changes that appear unlikely to recede as the pandemic does; rather, companies will continue to move forward with digitalization at an accelerated pace.

To maintain the transformation velocity, specific technology and data skills are needed across industries — namely cloud computing, cybersecurity, data analysis, and software development. Currently, Coursera’s analysis of trending skills in the Technology and Data Science domains shows that people are highly interested in learning software engineering and data analysis skills — look at the picture below.

Source: coursera.org

Technology Industry Not Immune From COVID-19

This year, the overall skills proficiency ranking of the technology industry fell from number one in 2020 to number six. Last year, Coursera predicted that tech companies would need to remain deeply committed to addressing the continuously shrinking half-life of skills within their talent pool. That idea, along with the stress of a global pandemic and increased learning efforts in other industries, affected the skills ranking of workers in technology companies. In a pandemic that saw significant unemployment growth, skills shortages persist. It’s increasingly difficult for employers to find qualified candidates even though there is a growing number of applicants for many of the most-needed positions.

Technology industry learners do excel in technology skills. Though the number-one ranking from last year slipped to third this year, skills proficiencies remain cutting-edge overall with cloud computing (100 percentile), software engineering (89 percentile), and computer networking (78 percentile) topping the list. Technology skills that over-index for the technology industry are distributed computing architecture, software testing, network architecture, software architecture, computer architecture, software engineering, and operating systems. These are the same skills that software developers, quality assurance analysts, and testers will need. According to the U.S. Bureau of Labor Statistics, employment for those jobs will grow 22% by 2029.

Source: coursera.org

Outsource Everything

The answer is on the surface: outsourcing is a good option that can help businesses find the people who possess the skills needed for a project. Outsourcing the project or bringing the outsourced engineers to work on-site allows in-house employees to learn from the contractors while ensuring that the job is done with the necessary level of skill.

In addition to giving access to the top talent, outsourcing can benefit business in many other ways which explain why companies of all sizes look outside their company for their staffing needs. Here are some of them:

Rapid growth. Having more staff on hand may seem promising, but it results in expenses that go into hiring more full-time employees. In the end, it may limit the potential for growth.

Having more staff on hand may seem promising, but it results in expenses that go into hiring more full-time employees. In the end, it may limit the potential for growth. Flexibility. Outsourcing can help the business make it through a busy season without hiring in-house employees: after a big project is done, you can easily switch back to an entirely in-house team, and a good contractor can help you with scaling your outsourcing needs.

Outsourcing can help the business make it through a busy season without hiring in-house employees: after a big project is done, you can easily switch back to an entirely in-house team, and a good contractor can help you with scaling your outsourcing needs. Maintaining focus. Outsourcing secondary tasks allow your in-house team members to focus on internal tasks, helping your business run more efficiently.

There are several formats of outsourcing that can help businesses get access to the best specialists in the market. A company can strengthen its development team by inviting experienced outsourced specialists to take part in the project. The other option — a dedicated team, formed by a contractor to work under the client’s management. Thirdly — end-to-end software development: the outsourcer organizes a full cycle of work, including development, management, testing, and implementation.

In Conclusion

To sum it up, Coursera’s report reviews the state of digital transformation in each industry and how the pandemic has affected it. This information can help companies determine how they can best take advantage of opportunities to reskill and upskill workers to increase innovation and gain a competitive advantage. And if you are looking for a reliable IT outsourcing contractor or want to strengthen your in-house development team — contact us!

The full research was originally published on coursera.org",5693
"HomeTechnology Blog WhatsApp Launched 10 Features This Year And Learn How to Use All These Features

WhatsApp launched 10 features this year

Learn how to use all these features

WhatsApp, the leader in social media, is constantly adapting to the tastes of its customers, with more than 250 million users worldwide. With WhatsApp, users can also enjoy services like SMS videos and photos as well as voice and video calls as well as locations. WhatsApp is always changing considering the preferences of the customers. WhatsApp has seen a lot of changes this year as well. Today we are going to tell you what is special about WhatsApp.

1. WhatsApp Flash Call :

This feature protects the user experience. You can verify WhatsApp via Flash Automated Call instead of SMS for verification. When you register for WhatsApp, turn on the WhatsApp Flash Call SMS Verification option.

2. Multiple Device Support :

WhatsApp has recently launched a multi-device feature for its users. This option allows one WhatsApp to connect up to four devices. Users can enjoy it on laptop or computer.

3. Missed Group Calls Feature :

This feature allows users to participate in missed group calls. With this help, there is no need to restart the entire group call to add participants. Those user who want to join an ongoing call, it can go to their WhatsApp call log to join the call. To do this, you need to open the Call Details screen and then tap on ‘Join’ to join.

4. Disappearing Mode :

This is a new privacy setting, currently available to beta users. All users’ chats disappear as soon as this setting is started.

5. WhatsApp Payment :

WhatsApp Payments WhatsApp has also launched an online payment system through which money can be exchanged. Many users are now using the WhatsApp payment system as well.

6. WhatsApp Advanced Search :

WhatsApp Advanced Search feature gives users the option to filter search with text, photo, Gif, video, audio, document and link which can be done by clicking on the search button. Which is given above.

7. WhatsApp Dark Mode :

Dark mode option is offered in WhatsApp. What is special is that this dark mode can work for all types of displays. This feature gives some protection to the eye of the customer and also lowers the battery of the mobile. To start Dark Mode go to WhatsApp Settings and select Chat Options then go to Themes where you will find Light, Dark and System Default options. Choose the one you want.

8. Group voice-video call limit increased :

WhatsApp has increased the number of group voice-video calls to eight. Previously, only four people could enjoy voice-video calling. Now that the number has increased, Android and iOS customers will be able to enjoy it.

9. Storage Management Tools :

WhatsApp has launched a new feature called Storage Management Tools for its customers. This allows customers to check and delete photos, videos or other files using Management Storage. Users can delete their personal chats or media themselves anytime.

10. Sticker Feature :

Now you can also make a sticker of your photo on WhatsApp. Customers will have to use WhatsApp web to enjoy it. For this, you can select a photo, crop it and edit it. You can also send a sticker of your photo to the person in front of you.",3262
"Effective communication is the tool that can enhance the power of understanding. To make it clear to others, we use various forms of communication. It all started with sending letters and birds to convey the messages. After the advent of digitization, telephones came into existence where people used to book trump calls for making calls. Now, after decades, we have messenger apps through which we can send and receive messages instantly.

The users can participate in instant chatting with others in the app. This has proved to be an effective way of communication as there is no delay or interruption in between. Data security is the biggest concern of people when it comes to a messaging app. If you could fulfill that aspect, then your messenger app will be a surefire hit.

How Does An Instant Messaging App Like WhatsApp Work?

Talking about the earlier stages of WhatsApp, it was primarily launched for iPhone users exclusively. During the initial stages, the users were asked to pay for sending messages to the others. When compared to the normal text messages, the charges collected by WhatsApp were quite less. This made several individuals use this platform for texting messages.

WhatsApp understood that people are so deliberate in using the app and came up with new advanced features. Every time its updates, it unveils different features for the users. This is one of the reasons why people are so comfortable with using the application. The app completely relies on an internet connection to engage in chat.

Initially, the app was meant only for chatting, and there were no other choices for the users. Then they introduced various features like voice calling, video calling, group calling, and group video calling. They did not have any restrictions for borders or time so that anyone could contact anyone from any time zone.

Architecture For Developing An App Like WhatsApp

Messaging app development is not an easy task as it has too many complexities. Let us discuss the architecture of developing a WhatsApp like app,

Simplicity is the core

Developing an app like WhatsApp involves complexities and procedures. But when it comes as an end product, it should be simple and easy for the users to access and understand. If not, the purpose of your app will remain a mystery. Following a simple architecture for your app development is highly recommended. The data structure should be simple and should only store what is essential. This will result in fast calls, fast responses, and overall perception of a fast and responsive app.

Make sure that the messages are stored in the elementary class. However, you can later be derived to complex design to support image files, video files, audio files, geographic locations, and more. In the process, you can avoid over-queueing, which is a process of retrieving data more than the required level. You can optimize the queries with the support of GraphQL, which can enhance the operations over a time period.

Flexibility and scalability

Scalability is the prime factor of an app. In the future, the app might undergo various changes and transformations. In such a situation, the app should be scalable enough to support its functionalities. In the same way, the flexibility of the app should also be governed. By designing the system architecture into layers, you make sure the scalability and the flexibility of your application.

However, you can use Backend as a Service(Baas) platforms can be a highlight in terms of scalability. You can outsource the server-side architecture to a third party to simplify the app development process. This also removes the hassles of running and maintaining the servers.

App performance

Before finalizing the program languages for WhatsApp clone development, evaluate them on the basis of your performance. Because in some cases, the language, platforms, and services do not go hand-in-hand with the demands. To evaluate the app and find the pros and cons before implementing the same in the application.

Focusing on the major app functionalities

As mentioned earlier, data encryption is the core ideology that boosts users to use your application. Messaging apps involve sending information from one user to another. So let’s see how messenger apps work,

Secret chats — To provide an extra level of security to the users, you can implement chats that can disappear after a certain period of texting.

Delivery status — Once the sender initiates or sends a text message, he gets notified about the delivery status.

Unsend messages — This is the latest feature found in recent messenger apps. This allows the users to unsend the messages after a certain period of time.

Group chats — The messenger app should support the users to send group messages to communities and teams.

Moderation — When creating group chats is enabled, you should also focus on moderation functionality. This will enable the admins to remove participants, delete messages, and create admins.

Note-Worthy Feature To Ensure In Your WhatsApp Like App Development

The efficiency of a messenger app is determined by its features. Some of the noteworthy features to consider for the messaging app is listed down below,

Profile creation

Upon downloading your messenger app, the users will get directed to the registration phase of the app. The user should allow access to the app for syncing the contacts from the mobile devices. They can enter their mobile number and enter a code to verify their device and number. However, adding profile pictures to the contact is of their choice.

Notifications

The users will get notified about their incoming calls, messages, and media. They also have the flexibility to either turn on or off their notifications. This will also alert the users when someone tries to register an account using their phone number. Enabling notifications in the app is the first step towards establishing a network between the users and the app.

Messaging

The chatting application should allow the users to send or receive messages with the support of the internet. The app will provide encrypted messages to the users. The chat messages of the users should be secure so that the messages shared among them remain confidential.

Multimedia transmission

Along with text messages, the users should also be facilitated to share multimedia files like images, videos, audio files, stickers, and Gifs. The app should support transmitting heavy multimedia documents to the users.

Enabling web and desktop connections

They are nothing but the reflection of the mobile version. The users can scan the QR code present in the app and open the web version of the app. Thus the application should support both the app and the web.

Voice and video calls

The WhatsApp clone must have voice, and video call features so that the users make video calls and voice calls deliberately. This will make your messaging app more interactive and interesting for the users to access. However, initiating a group video call and voice call will be a feather in the cap for your app’s success.

Status and posting of stories

Apps like WhatsApp have a special feature where the users can post status and stories in the form of images and videos. The status of the stories will disappear after 24 hours of uploading. This is another attractive feature you should never miss out on.

The Final Thoughts

How ready are you to taste success in the messenger app market? Brace yourself to witness the storm in the messaging app market. Meet the best team of developers who can flawlessly craft your instant messaging app like WhatsApp.

More content at plainenglish.io. Sign up for our free weekly newsletter here.",7696
"Image Source by Canva

Block or disable Whatsapp unknown number calls|Quit obtaining unknown get in touch with whatsapp (No Root) Many WhatsApp individuals are wondering if it’s feasible to immediately obstruct unidentified numbers. Expect you don’t want to obtain inappropriate web content or spam messages from details calls or numbers. Because case, you can tweak your WhatsApp personal privacy setups. Spammers often utilize arbitrary numbers to send out their messages. However, things are a little bit much more made complex when it pertains to instantly obstructing unidentified numbers.

How Do I Block Unknown Numbers Automatically on WhatsApp?

WhatsApp and whatsapp group does not immediately obstruct unknown numbers. There’s no option to instantly block messages from unknown numbers. The solution does not provide any kind of function similar to this.

When you get a message from an unidentified number, Whatsapp will certainly inform you the sender is not in your list of contacts. You have three options at hand: Block, Record, and also Block, or Contribute to contacts.

How to Block Unknown Numbers on WhatsApp

When you get a message from an unidentified number, you can simply select the corresponding chat as well as faucet Block.

Initially, open the chat, and then select the particular telephone number. Select Block Call, as well as tap Block or Report Call. Select Record and also Block to report as well as obstruct that number.

By the way, if you wish to obstruct a particular get in touch with, most likely to WhatsApp Settings, choose Account, and also faucet Privacy. Then select Blocked, faucet Include New, and also pick the contact you intend to obstruct.

What Happens When You Block a Number on WhatsApp?

Obstructed numbers and calls can no more contact you. They won’t be able to send you messages, include you to groups, or call you. At the same time, they won’t be able to access profile-related info such as status updates or adjustments to your account pictures.

Bear in mind that obstructing a number or contact on WhatsApp does not immediately eliminate them from your get in touches with. You require to visit your phone’s calls list and manually delete the respective get in touch with.",2235
"WhatsApp Bug Is Inflicting The iOS Model Of The App To Preserve Crashing Jordanbertha Dec 16, 2021·2 min read

https://technoticsguide.com/whatsapp-bug-is-inflicting-the-ios-model-of-the-app-to-preserve-crashing/

In the event you’re utilizing WhatsApp in your iPhone and also you’ve found that the app retains crashing for no cause, you’re not alone. It’s because many customers have taken to numerous social media platforms to report that they’re discovering that the iOS model of the app has been crashing moderately continuously.

Nonetheless, what’s unusual about this concern is that WhatsApp hasn’t issued any main updates just lately to counsel that it might be a bug with the app, and it appears that evidently the beta model present in TestFlight is crashing as effectively. This has led to hypothesis that the explanation behind this crash is because of points with Meta’s servers that might be the explanation why the app is crashing.

WhatsApp has but to formally remark or acknowledge the problem, and there doesn’t appear to be any explicit workaround for the issue which implies that in case you rely closely on WhatsApp on your chats, then you definitely could be barely inconvenienced till WhatsApp lastly addresses the problem, which hopefully received’t take the corporate too lengthy to repair.

It’s unclear how widespread this downside is as a result of on my finish, at the moment of writing, there are not any points for me so we’re unsure if that is affecting all WhatsApp customers or simply some. Both approach hopefully a repair will come quickly.

https://technoticsguide.com/whatsapp-bug-is-inflicting-the-ios-model-of-the-app-to-preserve-crashing/",1679
"The day Facebook, Instagram & Whatsapp were wiped off the face of the world (How did Meta’s global outage actually took place ? ) Aayush Shukla Dec 11, 2021·5 min read

Remember Facebook, Whatsapp & Instagram outage that occurred on 4th October 2021 ? As a developer what surprised me was how can a company as big as Facebook ( Or as we call it now “Meta” ) goes down and that too for more than 6 hours! I tried looking down for the reason behind it on the internet but only answer i found was it happened due to a DNS issue. Now do you feel it is plausible that one of the biggest tech company in the world faces 6 hour global outage because of “some” DNS issue ? I didn’t. I decided to dig up further & found this!

Prerequisites:

Before i go on further with the chain of events that happened, I wanted to list down some networking abbreviations which even i forgot the next day after my computer networking final exam.

D NS — Domain Name System ( The only one i remembered out of these ) ANS — Authoritative Name Server / Authoritative DNS / Name Server RNS — Root Name Server BGP — Border Gateway Protocol

Domain Name System (DNS) — Think of DNS like a phone directory or like contacts app on your smartphone ( if you are born after 90’s ). It basically eliminates the need to memorise an IP address, all you need to know is the domain name like facebook.com. DNS will translate domain name to IP address, from which browsers can load the requested resources. The 4 main DNS servers are DNS Recursor, RNS, TLD Name server & ANS.

Root Name Server ( RNS ) — The job of RNS is to narrow down the search of IP addresses using the given domain name by user. It does that by using the “.com” in the “facebook.com”. Basically all the domain name & IP are stored in an hierarchy based on “.com” , “.co.in”, “.org” and so on.

Authoritative Name Server (ANS) — ANS is that one guy who actually has the working knowledge of the project in the entire team. ANS is the final holder of domain name requested by the user. It returns the IP address to the DNS Recursor , which is the starting point of a DNS.

Border Gateway Protocol (BGP) — It’s a mechanism to exchange routing information between autonomous systems (AS) on the Internet. Confused right ? Think of BGP like an Indian aunty who knows what all is happening in each and every house hold in her locality. All the ANS are real time listed over the BGP so that anyone from anywhere can access it.

If you wish to know more about DNS do comment down below.

What is Cloud ?

So for someone not familiar with how cloud works, it is a bunch of servers located in different part of world which are connected with each other with ultra high bandwidth connection enabling them to be connected and transfer data between each other at high speed.

Image from AWS

The chain of events that took place on 4th Oct 2021

1. The Human Blunder

“Humans make mistakes, so let computer take care of such things while you hold its beer” — Aayush

On 4th October 2021 someone from Facebook was given the task to update a single node ( server in single location of entire cloud cluster ) but the person accidentally took down all the nodes in the entire infrastructure. How it happened ? maybe he was sleep deprived or playing COD while working. Who knows. Anyway this was the first event that took place which let to the following domino effect.

2. The Human Blunder 2.0

Question: What more worse can happen if your parachute doesn't open while you jump off a place ? Answer: Your backup parachute also fails to open.

For a company as big as Facebook its sounds odd right that someone can take down the entire network with a single command ? Yes. So Facebook has a auditing program in place to prevent such things to happen at first place but the auditing program itself had a bug in it which let the command take down all nodes at once.

3. The only right thing that happened that day

Once a node goes down in the Facebook’s cloud infrastructure, it automatically stops broadcasting its ANS to the BGP, disabling the users to reach out to Facebook.com. Now you might think why is it right to take off the ANS from BGP right ? If the ANS is not able to give a response because it has been taken down, it doesn't really make sense to broadcast it over to the BGP. That is what happened here, since all nodes were taken down Facebook.com took out its domain broadcasting to BGP. This is the point from when Facebook, Instagram and Whatsapp were taken off the face of the world.

4. Checkmate ( DNS failure & physical security )

The failure of Cloud Infrastructure resulted in disconnection between all nodes and failure of intranet, eventually which led to DNS failure. Now since all nodes were disconnected and were on their own, it was impossible to SSH in between them to restart other nodes. So now comes the final hurdle, because the servers can’t be reached remotely, only option left was to access to in person. All the Meta’s data centre's are highly secure and require a person to go through multiple checkpoints which made this who process even more time consuming.

This is my first medium post. Please comment down below if you have any doubts or suggestions for me. The failure of DNS itself is a lengthy part, if you would like to more about it, hit me up. Let’s discuss!",5325
"Meta now makes it possible

WhatsApp has implemented cryptocurrency transaction technology directly into the app via Novi Wallet, one of the biggest messaging apps. At least, that’s how Reuters reported it today. The article in question states:

Meta Platforms Inc’s cryptocurrency wallet Novi will allow users to send and receive money through the social media giant’s messaging app WhatsApp.

According to screenshots posted by Novi CEO Stephane Kasriel, the application supports transactions within the app via a new interface for sending money. Users can select the amount of money they send and confirm the transaction directly with the recipient. On Twitter, the Novi CEO wrote:

There is a new opportunity to test the Novi digital wallet. Starting today, a limited number of people in the U.S. can send and receive money with Novi through the following channels WhatsApp This makes sending money to family and friends as easy as sending a message.

Cryptocurrencies on WhatsApp — It doesn’t get much more mainstream than this.

So the program is already available to some users in the United States and is currently being tested. According to the wallet's CEO, Novi does not change WhatsApp’s current privacy policy.

Meta, formerly known as Facebook, has long been working on implementing cryptocurrencies within its apps, especially since the company abandoned plans to create its own digital currency due to regulatory concerns.

Now it seems the time has come. WhatsApp users will soon send cryptocurrencies to contacts via the app. Which cryptocurrencies Meta will implement is still unknown to us so far. However, more news will surely follow very soon.

I share more intimate thoughts in a monthly newsletter that you can check out here. Please let me know in a comment, and let’s build your crypto universe via Patreon. Join me on various social media platforms:

Twitter ● Podcast● Patreon ● Homepage● LinkedIn ● Telegram

JOIN MY CHAINEXPLAINED ALERTS CHANNEL ON TELEGRAM FOR COIN TIPS, FORECASTS, AND UPDATES!

1:1 Coachings you will find on Patreon! Write me about how I can help you, and we will find a solution!

Register and get $25 with Crypto.com use Referral code LUKAS",2194
"New disruptive technologies are on the rise, and the most famous names among them are blockchain technology, non-fungible tokens, and decentralized finance. All of these are user-oriented and have the capacity to reshape traditional industrial processing. However, all three have their equal share of deniers and advocates. Earlier, only techies were familiar with the concept of these technologies, but now, regular people realize the advantages of leveraging them due to their ease of use.

Moreover, all three of them are moving beyond their initial applications affecting industries like healthcare, supply chain, finance, funding's, the art industry, and many more.

So here we bring you the details on the fundamentals of blockchain technology, NFT, and decentralized finance. So if you are not familiar with their basics, then keep reading further.

Table Of Contents

Fundamentals of blockchain technology

Understanding blockchain technology

Three aspects of blockchain technology

Decentralization Transparency Immutability

Fundamentals of decentralized finance

Understanding decentralized finance

Lending and borrowing processes and Defi

Fundamentals of NFT

Fundamentals Of Blockchain Technology

Understanding Blockchain Technology

Blockchain is a system that records digital information so that it becomes impossible for anyone to fraud, modify, or hack the data.

It maintains a digital ledger that records transactions in chronological order. Further, the system duplicates and distributes copies of transaction information across the whole network of computers within the blockchain platform.

Every block in the digital ledger contains a certain number of transactions. Further, when the system adds a new transaction to the ledger, it ensures that every participant of the network gets a copy.

We can call it a decentralized database where numerous members manage its operations.

Additionally, blockchain is a form of distributed ledger technology where it records transactions with an unchangeable cryptographic signature known as a hash.

Three Aspects Of Blockchain Technology

Decentralization

Decentralization refers to a system where no central unit controls the network or processing. A blockchain is independent of third parties, the government, or any powerful company controlling it.

Transparency

As we know, that system makes copies of transaction data and distributes it over a global network of computers. So, there is no privacy of information. Hence it maintains incredible transparency between members of the network.

Immutability

Blockchain systems contain such digital data that no one can modify or rewrite.

In case someone tries to make changes or fraud the system, members can easily detect it as everything on the blockchain is transparent to every user.

Fundamentals Of Decentralized Finance

Understanding Decentralized Finance

Decentralized finance is a collective term for numerous projects and applications over a public blockchain. These applications can cause disruptions in the traditional finance sector.

Defi represents financial apps that operate under blockchain technology with the help of smart contracts.

Smart contracts are a form of the agreement containing predefined rules and regulations that involved parties must follow to perform a transaction. The best thing about smart contracts is it eliminates the need for middlemen or any kind of third-party involvement.

Furthermore, the system comprises a peer-to-peer protocol that decentralized blockchain networks develop. Every member can access financial services, including trading, borrowing, lending, etc., without any censorship.

The majority of decentralized finance applications operate on the Ethereum blockchain. But, we are also seeing the rise of alternative public networks offering better security, speed, scalability, and lower transactional prices.

Lending And Borrowing Processes And Defi

Decentralized finance enables permission less lending and borrowing between members. Therefore, it represents itself as an Open Finance platform offering multiple lending opportunities to cryptocurrency holders on the network, helping them to earn remarkable yearly yields.

Moreover, borrowers can get the desired loan amount at a specific interest rate using the decentralized borrowing service. Additionally, the platform aims to satisfy the requirements of the crypto community by offering real-world use cases in financial assistance.

Fundamentals Of NFT

Non-fungible tokens are digital assets that run on blockchain technology; furthermore, every non-fungible token comprises unique data that represents the originality of digital assets. It is a digital certificate of authenticity of objects. And objects that people trade on NFT marketplaces have their own NFT certificate.

NFT can be anything ranging from a piece of digital art, virtual property to in-game accessories. The majority of NFT operate over Ethereum blockchain. However, other blockchain networks are also emerging that support NFT trading, for example, FLOW.

Uniqueness is the key characteristic of NFT; in contrast to digital money like cryptocurrencies, users can not interchange two similar-looking NFT as it contains unique values. Furthermore, the real-world value of a digital asset defines the worth of its NFT token. For example, the worth of a real world painting will depend on the taste and preference of people and its demand among buyers. Similarly, members bid on NFT arts on virtual auctions that drive its worth. Also, the scarcity of an object plays an important role in driving its value.

Further, blockchain technology secures the immutability of NFT assets. Also, it plays a vital role in protecting the ownership of non-fungible tokens. Blockchain makes it easier to track and check the originality of an NFT token so that buyers can be sure that they are investing in an authentic object and not the fake one. If you want to get better insights into the workings of NFT, then it would be helpful if you take the NFT course available online.

Closing Thoughts

Hopefully, you gained valuable insights into these three promising technologies. Furthermore, if you want to have an in-depth understanding of such disruptive technologies, it would be feasible to take relevant courses and enroll in certification programs. However, ensure that you are using reliable resources.",6409
"Free gifts from OPPO and Shiseido worth up to RM450 and more are up for grabs upon purchase of the portrait expert limited-edition gift set

This Christmas, OPPO is simplifying your to-do list with the launch of its one-of-a-kind OPPO Reno6 Z 5G Shiseido Gift Box! With pre-orders of the limited-edition gift box going kicking off this Thursday, 2 Dec 2021, consumers can stay comfortably prepared ahead of their Christmas schedule with the perfect year-end gift for their special someone

Priced at its regular retail price of RM1,699, this OPPO Reno6 Z Gift Box is something you do not want to miss out on, as it comes with special free gifts from OPPO and Shiseido worth up to RM 450, such as:

Limited-edition OPPO x Shiseido gift box Limited-edition OPPO x Shiseido Reno6 Z phone case 1x selected Shiseido cosmetic product

In the spirit of Christmas, OPPO is also looking to give a little bit more to a few lucky buyers of the gift set.

Selected customers who purchase the OPPO Reno6 Z Shiseido Gift Box during the campaign period will also receive a free Shiseido Synchro Skin Radiant Lifting Foundation worth RM210. They will also receive a comprehensive professional in-store consultation from Shiseido experts on how to find their perfect foundation shade. With this new knowledge, you will be as prepared as you can be for Christmas and year-end celebrations.

The OPPO Reno6 Z 5G Shiseido Gift Box is limited to the 1st 300 customers only, so be sure to snag one for your loved one as soon as you can!

Finally, OPPO is also running a campaign to reward both OPPO and Shiseido members this year-end and new year season. From today till 31st January 2022, registered OPPO members can receive the following from Shiseido:

RM50 instant rebate with a minimum purchase of RM300 at Shiseido’s Online Store FREE Shiseido Ultimune Sampling Kit at Shiseido outlets in Sunway Pyramid, Mid Valley, and Suria KLCC (no purchase needed!) 4-piece Shiseido gift set upon purchase of any Shiseido products with a minimum spend of RM380 at Sunway Pyramid, Mid Valley, and Suria KLCC

To redeem these freebies from Shiseido, simply refer to your My OPPO App!

On the other hand, registered Shiseido members will also be entitled to a RM50 instant rebate upon the purchase of a new smartphone in the OPPO Reno6 Series and selected IoT products such as the OPPO Enco Buds, OPPPO Enco Air, and OPPO Band. To redeem, register as a Shiseido member at camellia.shiseido.com.my and flash your EDM to show your RM50 rebate code, and use it at any participating OPPO Brand Store.

Stay ahead of your Christmas schedule and pre-order the gift box on 2 Dec 2021 at https://www.oppo.com/my/store/contents/event/OPPOReno6ZxShiseido/",2714
"Buyers of Mid-Sized CTV/OTT Inventory Twice As Likely to Beat Key Campaign Goals

PubMatic , a sell-side platform that delivers superior outcomes for digital advertising, released the findings from a custom commissioned Forrester Consulting study entitled “New Opportunities for Scale and Value in OTT/CTV Advertising.” The study uncovers two significant findings. First, brands are unsatisfied with the limited number of media companies that they currently leverage for buying CTV/OTT inventory. Second, mid-sized CTV/OTT publishers present a major opportunity for brands to get more from CTV/OTT, including access to key audiences and higher campaign performance. The study, available for download here, surveyed 307 brand and agency buyers of OTT/CTV inventory in the U.S. and U.K.

The study finds that, “respondents who currently buy mid-sized CTV/OTT inventory were twice as likely to say they overachieved against their key video advertising goals last year compared to those who do not.”

Marketing Technology News: Aims Community College to Deploy YuJa Enterprise Video Platform to Store, Manage and Deliver…

“This study’s findings are consistent with what PubMatic has also seen. Today’s CTV marketplace has combined the high-quality aspects of linear TV with the data and scale of digital media. Brands that diversify their media buying across diverse CTV/OTT inventory are often more bullish on biddable environments, allowing them to make real-time optimizations that drive better campaign performance,” said Nicole Scaglione, VP of OTT & CTV at PubMatic. “Too many brands have limited themselves to inflexible guarantees with high-cost CTV/OTT publishers due to unfounded fears about scale and complexity, and are missing a major opportunity.”

CTV/OTT is a major growth channel for brands looking to reach consumers who have migrated from linear TV to streaming content. Forrester’s research found that the benefits of CTV/OTT include gaining incremental reach and the ability to address media fragmentation.

However, too often, brands stick to familiar relationships within a very limited subset of CTV/OTT inventory. The study found that 40% of brands buy from OTT suppliers while 34% buy from broadcast networks compared to only 28% that buy from small- and medium-sized publishers.

Marketing Technology News: MarTech Interview with Sara Spivey, CMO at Braze

Buyers agree that a more diverse media buying approach would improve audience reach and performance, exposing the opportunity that awaits those who increase their spend with small and medium publishers. The top reasons for buying more inventory from small and medium publishers includes “ability to reach audiences in a relevant context,” “higher quality” and “greater efficiency” all cited by nearly a third of buyers.

The study notes that, “buyers who diversified their CTV/OTT media strategies across small and medium-sized publishers (mid-sized CTV/OTT) were more successful at their media objectives over the prior year than those who did not.”

To tap into the opportunity provided by small and medium CTV/OTT publishers, media buyers need to lean into partners that can provide comprehensive solutions to reduce complexity and leverage lessons and partner experience with more mature ad formats in order to seize the opportunity.

“The promise of CTV/OTT is enormous for the media buyers who are motivated to diversify their approach. The consumer trend toward streaming media across a growing number of apps and channels proves that investing now will set media buyers up for dramatically better returns,” said Scaglione.",3611
"DSP Technology for Middle-market Brands

DSP Technology for Middle-market Brands

Brittany Wray shares insights on programmatic advertising and its influence on marketing. She highlights the outcomes derived from the said technique creating a win-win situation for all. If you look at most leading indicators for programmatic digital advertising, the trends point steadily upwards with DSP Technology for Middle-market Brands. The latest forecast from eMarketer projects that marketers will spend more dollars on programmatic ($133B), that it will account for a greater share of the media mix (91%), and that a growing and increasingly premium share of available inventory will be transacted programmatically.

It’s Only Middlemen for The Middle Market — Major brands and agencies have long mastered the ins and outs of programmatic tech. Reading the industry trades, one could easily assume that the vast majority of advertisers have followed suit. The simple reason is that they don’t spend enough money on their own to meet the minimums for leading DSPs like MediaMath and The Trade Desk. These arbitrary minimums have excluded the majority of advertisers from the programmatic advertising technology.

It’s a lost opportunity for everyone. These budget minimums have limited the middle market’s autonomy and curtailed its ability to bring advertising operations under their own control. They have disincentivized these companies from developing the talent and budgeting the resources necessary to master this technology and get the most value from it. And they have kept the major DSPs from accessing the largest, most diversified, and therefore most durable segment of advertisers. Instead, middle market companies rely on media agencies (like my company, AudienceX) to manage their digital budgets, essentially acting as surrogate operators of this DSP technology. On the surface, that would seem to benefit digital agencies like us, but in the long run I’m not sure it benefits anyone.

For more such updates, check Out recent Martech Cube guest blogs.",2059
"TIPS & INSIGHTS

➜ Venture capital firms set aside $1 billion for women-led startups. Here’s how it’s going

➜ The pandemic highlights the importance of investing in women

➜ Gordon Ramsay moves restaurant HQ to Texas from California as his huge brand expansion plans start to take shape

➜ Securing the metaverse — What’s needed for the next chapter of the internet

➜ 3 keys to operationalizing startup sales from day one

➜ Venture funding to booze startups boomed this year, but will the buzz linger?

➜ 7 innovative brands speaking at NRF 2022: Retail’s Big Show

➜ $1B+ market map: The world’s 936 unicorn companies in one infographic

➜ ‘Play-to-Earn’ gaming and how work is evolving in Web3

MACROECONOMY & INDUSTRY

➜ How do you feel about inflation? The answer will help determine its longevity: Supply-chain disruptions, labor shortages and climbing oil prices have pushed inflation to a 39-year-high. But attention is now focused on another variable: Do people think inflation is here for a while? Because people’s expectations can factor into inflation, the answer plays a critical role in determining how the Federal Reserve and the administration manage the rising numbers — and how soon and how much the Fed will raise interest rates.

TOP RETAIL TECH STORIES

➜ Headless Commerce: Part 1 — The Evolving State of E-Commerce Infrastructure: Coresight Research and Nogin’s three-part report series, Headless Commerce, explores the emerging commerce-as-a-service (CaaS) space in the US and the key trends in the market. We leverage findings from an October 2021 Coresight Research survey of e-commerce leaders in the US to identify the key challenges and benefits in building and using e-commerce infrastructure and tools.

➜ DoorDash introduces ultra-fast grocery delivery, new approach to labor: Boasting delivery times in as little as 10 minutes, DoorDash has launched a grocery delivery service from its DashMart location in Manhattan’s Chelsea neighborhood. The offering of items available for delivery is limited to 2,000 fresh and frozen grocery staples, snacks, household goods, and local products. Initially, the speedy deliveries will be offered from a new DashMart location in Chelsea, with more locations and partners coming over the next few months. DoorDash introduced DashMarts in eight markets in 2020. New Yorkers also turn to a rapidly expanding set of so-called “instant needs” companies such as Jokr, Buyk and GoPuff, who promise rapid deliveries on a limited range of items, typically in markets with high population densities.

➜ Walmart adds science-based targets to supply chain financing option: Walmart announced Wednesday it has added science-based targets to its supply chain finance program, with the aim of helping smaller and mid-size companies make their operations more sustainable. Walmart worked with HSBC and CDP, a nonprofit that helps companies and cities with their environmental impact disclosures, on changes that provide smaller suppliers access to special financing for their sustainability efforts. Walmart said the new finance program for suppliers marks a “key” step in its efforts, called Project Gigaton, to cut 1 gigaton of greenhouse gas emissions from its global supply chain by 2030.

➜ Zippin brings checkout-free tech to JFK airport store: As more retail tech companies roll out cashierless tech tools, JFKIAT, which runs Terminal 4 at John F. Kennedy International Airport, has partnered with Zippin and SSP America to integrate Zippin’s A.I. technology into Camden Food Express at Gate B 42, according to information emailed to Retail Dive. When shoppers pass through a turnstile, they can tap their credit card, and Zippin’s A.I. technology will identify and track the items they pick. Once they leave, their credit card will be charged for the cost of the goods they chose. About 100 Zippin-powered stores are expected in train stations and airports by the end of next year, according to the company.

➜ Buy now pay later boom shows no signs of slowing this holiday season: 7% of shoppers said they will be using buy now, pay later as a payment method for holiday purchases this year, according to a CNBC/Momentive Small Business Survey. The credit card alternative has exploded in popularity as online shopping has boomed during the pandemic and more retailers and payment providers have adopted it. The use of BNPL globally during Cyber Week — from Nov. 23 to Monday — jumped 29% year over year, according to Salesforce data.

➜ Walmart’s former e-commerce head is now CEO of food startup Wonder: Marc Lore, who is credited with transforming Walmart’s e-commerce operations, is going “all in” on his latest tech-driven venture, which features an on-demand food truck business called Wonder. Wonder trucks deliver made-to-order meals ordered online through the Wonder app directly to the customer’s home. Each truck is equipped with a mobile kitchen, with a chef who completes the meals once the vehicle arrives at the customer’s home. The chef places the meal on a foldable tray table at their door. The entire process is contactless.

➜ Alibaba rolls its marketplace businesses into a single organization: Alibaba Group, operator of global marketplace platform Alibaba.com, is shaking up the ranks of its senior executives. And the shuffling does include the realignment of B2B marketplace operations. In a letter to employees today, Alibaba Group Chairman and CEO Daniel Zhang announced that all its global business-to-consumer and B2B marketplace operations will be folded into a new business unit: International Digital Commerce.

➜ Zara lands in the metaverse with South Korean label Ader Error: Zara has taken the leap into the metaverse with South Korean label Ader Error to launch its first collaborative project which explores the metaverse and virtual reality. According to Ader in a release, the AZ Collection is ‘inspired by the identities and uniqueness of all generations from A to Z, blurring the boundaries between the virtual and real worlds, and features the lives of young people who pioneer their own Persona.’ Alongside its physical apparel range, the collaborative project is available through the Zepeto platform, which is already being used by the likes of Ralph Lauren, Disney and Nike, where users can purchase digital versions of the clothes and makeup for their avatars.

TOP RETAIL STORIES

➜ Holiday shopping? Check out some of the Dallas area’s most interesting new stores: The long holiday weekend proved that stores are still important to shoppers, at least this time of year. Optimistic retailers banked on consumers heading back to stores as the restrictions of the pandemic eased. From pop-ups to permanent locations and relocations, local entrepreneurs and national chains have committed to new stores in the Dallas area. Some concepts are driven by people who quit or were forced from their day jobs and started their own businesses. Creative sorts have filled new collaborative spaces, such as MKT Dallas in West Village and Mosaic Makers in the Bishop Arts District. Both have opened second stores in Galleria Dallas.

➜ Activist firm Engine Capital pressures Kohl’s to consider sale or online separation: The New York-based hedge fund Engine Capital wants Kohl’s to explore the two alternatives to try to boost its stock price, it said in a letter sent to Kohl’s board that was published publicly on Monday. Engine Capital owns a roughly 1% stake in Kohl’s. Engine Capital said in its letter that assuming Kohl’s brings in online sales revenue of about $6.2 billion, Kohl’s digital business alone would be worth $12.4 billion. Engine Capital also said it believes that there are private equity firms that would pay at least $75 per share for the retailer

➜ Target’s Lego collaboration is what technicolor dreams are made of: Big-box retailer Target has a long history of high-low designer collaborations that offer fashion-forward product mashups at reasonable retail prices. In the past several decades, they’ve brought their cheap-chic magic to Marimekko, Missoni, and Rodarte, and more recently, Christopher John Rogers and Rachel Comey. The ever-famous Red Dot Boutique’s latest partnership is a little more youthful but no less stylish — this time, they’ve teamed up with the legacy toy brand Lego for a 300-piece collection of gifts, home goods, toys, and pet accessories inspired by the Danish toymaker’s signature primary-hued building blocks. The assortment also includes a wide range of apparel for kids and adults, with sizes extending up to 4X.

➜ Oatly plans 3 new facilities by 2023 to combat supply shortages: Oatly is building out its global manufacturing footprint “at an unprecedented pace” to address supply shortages and meet the growing demand for plant-based milk, CEO Toni Petersson said on the company’s Q3 earnings call. Three plants are set to open in the U.S., U.K. and China in 2023, which are expected to produce 450 million liters of product, according to COO Peter Bergh. The company already opened three facilities in Utah, Singapore and Ma’anshan, China, earlier this year. Oatly had to rely more on contract manufacturers this year after struggling to build inventory and meet global demand. Supply issues also pushed the company to prioritize production of oatmilk over other dairy-alternative food products.

➜ That big office building? It’s an e-Commerce warehouse now.: Allstate Corp.’s suburban campus outside Chicago, with its interconnected buildings, manicured grounds and acres of parking, represented a new vision for the U.S. office when it opened in 1967. That vision is now dead. The insurer reached a deal last month to sell most of the campus. The new owner plans to demolish the office buildings and convert the Northbrook, Ill., site into more than 3 million square feet of e-commerce warehouses and other logistics facilities. In New York and San Francisco, more than 80% of all office space is more than 30 years old, and Chicago isn’t far behind, according to Phil Ryan, director of U.S. office research at Jones Lang LaSalle Inc. These three cities also have some of the lowest office occupancy rates in the country

➜ Retail shopping center trends to watch in ’22: Real estate investment trust Phillips Edison and Company (PECO) is out with its predictions for the retail industry heading into 2022. PECO is literally and figuratively close to the ground when it comes to retail insights through its management of nearly 300 shopping centers that include Kroger, Public, Ahold Delhaize and Albertsons anchor stores. As retailers compete with restaurants for the share of the consumer food dollar, they can also heed foodservice trends likely to impact shoppers’ behavior. PECO highlighted the launch of digital-only restaurants and ghost kitchens in the quick-service and fast-casual spaces, noting that those trends reflect the restaurant industry’s drive to accommodate demand for off-premise dining, which is likely to continue beyond the pandemic.

➜ How money shapes retail: In early 2020, Retail Dive launched what was supposed to be an extended series of stories that looked at how financial interests and players had shaped the industry as well as the fates of individual companies. That was also the last story we formally published in the series. In March 2020, as everyone knows all too well, the world changed suddenly and dramatically. With retailers temporarily closing their entire chains, and the industry grappling with how to operate in or even just survive the pandemic, we shelved the “money in retail” series to focus on the ramifications of COVID-19.

TOP EXITS

➜ Chobani S-1: Sum of the parts here points to a $6B valuation for @chobani, but we all know @chobani is going to trade a lot higher than that. Why? (1) This is a beacon for every other ESG company and will attract capital commensurately; and (2) value will be put on their innovation platform + distribution capabilities.",11978
"the first event that brought together European companies’ leaders

to provide global equal access to actionable knowledge

Hosted by Edain Technologies AG, the first edition of the “Global Knowledge Revolution” was held in Dubai, the UAE, on December 11th, 2021. European companies’ leaders came together to raise awareness about a groundbreaking technology committed to accelerating human progress by providing equal access to actionable knowledge.

In the age of technology that is now governing the creation of information, its distribution, and, most importantly, access to it, Edain Technologies AG brought together European business leaders, decision-makers and fintech innovators that understand the significance of using accurate information in the decision-making process to share with the world the importance of having equal access to reliable data from which actionable knowledge can be extracted.

The event featured nine speakers that ranged from scientists, authors, and tech architects to fintech innovators, founders and company executives. Speakers from major European companies included founder and CEO of UNGSII Foundation — Roland Schatz, founder and CEO of Patentpool Group Germany — Dr. Heiner Pollert, FX & MM Specialist at Refinitiv (LSEG) — Konstantinos Pogiatzis, CEO of Prisma Analytics GmbH — Sebastian Poetzsch, creator of the C+8 Technology® — Dr. Hardy F. Schloer, CEO of CryptoDATA Tech — Ovidiu Toma, Lead Social Scientist at Prisma Analytics GmbH — Dr. Adela Fofiu, Technical Solution Architect at CryptoDATA Tech — Andrei Stănescu, and Head of Artificial Intelligence at CryptoDATA Tech — Felix Negoiță.

The event’s eagerly awaited agenda included sessions on the challenge of providing access to reliable data at a global scale, the development of both the revolutionary central data repository called the Knowledge Vault and the cutting-edge big data analytics engine called C+8 Technology®, together with the commitment objectives set by the Edain project.

Dr. Heiner Pollert — CEO Patentpool Group Gemany

“Today’s world very much reflects what we are seeking to achieve with Edain. In fact, it’s our endeavor to help them and every other country, every other city or community, and every individual to become the juggernauts of their own disciplines and interests. The Edain project has shared values with this amazing country hence the reason why we are here this weekend in Dubai”, said Dr. Heiner Pollert, founder and CEO of Patentpool Group, in his opening speech at the event.

Presenting the innovation brought by the ambitious Edain project, and its unlimited applicability that varies from its contribution to the provision of actionable knowledge for individual users to the environmental, social and governance (ESG) analysis tools provided to entities and organizations that require complex data processing in the financial sector, Roland Schatz, the founder and CEO of United Nations Global Sustainability Index Institute Foundation (UNGSII), said:

Roland Schatz — Founder and CEO of United Nations Global Sustainability Index Institute Foundation (UNGSII)

“Beforehand, we already had these great computers called Watson, and the Sherlock software, pretending that they could be better than mankind, but what was missing was interaction. If you want to get smart information, what is relevant and what is not relevant, you need to understand the details in order to make everything work.

You need to get the full picture, as Dr. Schloer explained, and that is the reason why I’m here. That is the reason why I am not at Sherlock or in any of the other places.

The Knowledge Vault, for the first time ever, has a fair chance to capture the best of both worlds: science, honest science and us, the human beings. That is what convinced me with the Knowledge Vault: that is not just data that is thrown at you, and then take it or leave it, draw your own conclusions from the different resulting bubbles. The Knowledge Vault will not let the user alone in this, the Knowledge Vault will mentor and will teach the user how to use its data and will help the world understand what needs to be understood. The Knowledge Vault is not just about data, it’s about relevant data, about understanding ahead of time what will come so we can prepare”.

Felix Negoiță — Head of Artificial Intelligence, CryptoDATA Tech

The major difference between the Knowledge Vault and other data repositories that software applications utilize was explained by Felix Negoiță, CryptoDATA’s Head of Artificial Intelligence, who emphasized the fact that Edain is a transformative AI environment based on big-data analytics tools that will allow any desiring human access to the most complete repository of knowledge for the purpose of making decisions of any complexity more efficiently and based on facts.

Ovidiu Toma — CEO CryptoDATA Tech

Ovidiu Toma, CEO of CryptoDATA Tech, spoke about Blockchain technology and CryptoDATA’s active involvement in ensuring the cybersecurity of the Edain project:

“We understood back in 2015 that Blockchain technology is more than meets the eye, more than just a technology with single field applicability. We started researching and managed to discover ways to implement it in cybersecurity. For us, one of the most ambitious projects that we are involved in is represented by the partnership with Prisma Analytics GmbH for building the Edain Ecosystem as specialists in Blockchain technology and cybersecurity”.

During the event, Andrei Stănescu, Technical Solution Architect at CryptoDATA Tech, presented the first C+8-based commercially available application for the financial industry called Decision Point.

Decision Point is a highly configurable big data research and analytics framework designed to support strategy building and decision making in today’s fast-paced digital Financial Industry global environment.

Adela Fofiu — Lead Social Scientist at Prisma Analytics GmbH (Left) & Andrei Stănescu — Technical Solution Architect at CryptoDATA Tech (Right)

The C+8-based app is hosted on Refinitiv’s Eikon platform, now part of the London Stock Exchange Group. It eliminates the long and tedious processes associated with synthesizing large, unstructured, complex and fast-moving streams of information, providing actionable knowledge.

Konstantinos Pogiatzis, FX & MM Specialist at Refinitiv, London Stock Exchange Group

Commenting on the importance of financial analytics tools for sustainable business growth, Konstantinos Pogiatzis, FX & MM Specialist at Refinitiv, London Stock Exchange Group, said:

“Prisma Analytics is a partner with us at Refinitv, and we provide the world with financial data. The speakers talked earlier today about the Decision Point, and in our history and with the legacy that we have, we help Edain to put the basis for the development of a better future. The London Stock Exchange is putting one billion dollars of investments in the next three years in order to commit to long-term infrastructure. We help businesses that require data to have access to the best available data. We are here to make sure that the users have the best available data and they can perform their best decisions through our applications. Three years ago we discussed with Prisma Analytics just about an idea and now we have a fully developed application with a scientific workspace where users from the financial industry management, wealth management or advisors can use this data in a form that permits them to extract valuable insights. Thank you very much Prisma Analytics for partnering with us and we are here to support you”.

Sebastian Poetzsch - CEO of Prisma Analytics GmbH

The concluding speech, delivered by Sebastian Poetzsch, CEO of Prisma Analytics GmbH, focused on both the progress that the Edain project will bring for global development and the impact that it will have on accelerating human evolution:

“This event marks our progress towards our ultimate goal: delivery of knowledge as a utility to every connected human on Earth. This is what we are doing with Edain and the Knowledge Vault. We are building the first, truly global, centralized knowledge repository that will empower all of us with the same ability to solve problems by providing oceans of data, processed and available to provide the answers we seek in near real-time”.

Additional information about the event is available on the Edain official website: https://edain.ai/

Highlights from the Global Knowledge Revolution event held in Dubai are available on the official Edain YouTube channel: https://bit.ly/3oWd24g",8599
"The Problem:

In today’s world many students are willing to go to abroad for higher studies and to get better job opportunities and lifestyle but most of the consultancies in present time are not loyal to their work and are doing fraud with their customers and misguiding them.

The Solution:

The idea is to create an online consultancy that would not cheat with its customers and provide them the best support and help they could get and go abroad for a better future they deserve.

What is UpBroad:

Upbroad is an online study abroad consultancy that guides the student in the right direction and provides the student a complete roadmap to his/her favourite university and in the desired country they want. As we all know every country has different visa processes as well as different requirements for their universities and in that too many universities also have different admission criteria, our company will help the student in each and every step be it visa, be it admissions or be it accommodation.

Market Demand:

According to Times of India India sends 71000 students abroad in first two months of 2021, and according to India Today 94% Indian students say they are more eager to study abroad in 2021 than in 2020, and in addition to that according to Careers360 in 2019, a total of 10.9 lakh Indian students had travelled overseas for higher education. These numbers are not few they are a lot and they are increasing at a high rate. While doing research I have found out that majority of the off line consultants are not doing their work honestly, so it’s a great opportunity for a company like ours in this digital era.

Relevant Products in The Market:

There are some companies that are providing similar services as we are like Shiksha, LeapScholar but the difference is that that their services are not affordable for everyone as on the other hand we are going to provide basic services for free and a custom roadmap for every country according to their requirements and in the premium services we will provide a personal counsellor, Coaching for the Exams like IELTS, TOEFL, Aufnahmeprüfung custom designed practice exams that are required for the admissions in the desired university. And also, many of these companies had been started not so long ago, like LeapScholar was started in 2020 so it will not be very difficult to overtake them.

Target Audience:

Our target audience are those students who are willing to go abroad after their high school or those students’ wo have recently completed their graduation and are willing to grab better opportunities and lifestyle for them abroad, as countries like USA, Canada, Germany, Australia, etc. are developed and pays better and provides better work life balance than India. In addition to that we will organize large scale counselling sessions for parents as well to connect with them and make more familiar with our counsellors.

Required Expertise:

1. Hardware — The strategy is not to buy our own servers to provide servers, but rather to hire servers that will reduce operating costs per ton. The maintenance of these servers will also be handled by rental companies which also saves us the cost of repairs. Therefore, there are no hardware requirements like those of the idea to be implemented.

2. Software — As our company is basically a based online, we would need employees who have expertise in programming languages like Java, Kotlin, C++, C#, Python, HTML, CSS, Java Script, dart, for a very optimised app which would provide the user great experience.

3. Human Resource: We would need counsellors with great past experience so they could guide student in the right direction to their desired university and country, and send the customer happy back with a good experience.

Estimated Budget:

This project would require the estimate budget of ₹ 4,00,000, which would cover the development and maintenance of our app (estimated budget for developing and maintaining a great app is around 1 lakh INR) and would also cover rented servers (estimated budget for renting server is 1 to 1.5 lakh INR) in which we would host our app and also it will cover the salary of our start-up team members(In the initial phase of our company the salary will range from 15 to 30 thousand INR per month), and beyond that it is also sufficient to cover the rent of our working space.

Product Pricing:

To keep it affordable we will keep different segments in our premium membership so that everyone can get those services which they want despite of getting services they don’t want.

First Segment — This segment would include a counsellor that would guide the student in the right way till the end and this would cost 10,000 INR consultancy fees only.

Second Segment — This segment would include a personal counsellor and custom set test papers which would prepare the children for the challenges they would face abroad, and this would cost 50,000 INR.

Third Segment — This segment would include a personal counsellor and a full course of the exam the student will be giving with practice test papers which would prepare the student for his best so the student could increase his/her chance of getting into his/her favourite university, and this segment would cost 70,000 INR.

Marketing Plan:

In this current world marketing does not cost a lot thanks to digital marketing in this digital era. We could do the marketing of our product by running ads on YouTube videos, by running ads on Instagram and on Facebook, which would cost us around 8 thousand to 10 thousand, and also, we could hire and sponsor influencer so that they could influence their audience to trust us and we could also give special discounts to those people who are trying us after watching our promotion in their favourite creator’s video, by this we will be able to build a sense of trust among the customers and this would nearly cost around 50 thousand to 60 thousand INR. After this with just a little effort word of mouth will do its work for us.

Risk Analysis:

If anyone starts something there is always a chance of failing down but in our case the chance of failing is very less because all the facts and figures are in our favour, the rate of India in sending students abroad is increasing at a high rate and it does not feel like that this rate is going to slow down in near future. For now, this field is not that competitive in online mode so it’s a good chance for us to step into this now because many new start-ups are stepping into this field and getting success by being a well-established company.

Milestone and Timeline:

December 24th, 2021 — January 24th, 2022

This will the time period of the research, during which all the research work will take place, these 30 days of planning would enable us to think ahead of time and would allow a smart implementation.

January 25th,2022 — March 25th,2022

This will be the time when the development of our well optimised app will take place to serve our customers.

March 26th,2022 — April 10th,2022

This phase will include setting up of the servers which we would rent to run our app.

April 10th,2022 — April 15th,2022

During this time period the testing of the app will be conducted, if we will find any bug, we will fix tat bug in this time period.

April 16th,2022: The Launch of UpBroad

After April 16th,2022, The app will be available for Public and the marketing of our product will be started.

Milestone:

After sometime when our company will be well established our next aim will be to expand in different nations in all the 7 continents.

Conclusion:

This app will bring a change in the market as this will completely end the scams and frauds done by offline counselling companies, and we will be a symbol of trust.",7744
"Galaxy Samsung Galaxy S21 FE Full Phone Specification: Everything You Need to Know

in this blog you can get information related to the general specifications of the product Samsung Galaxy S21 FE Full Phone Specifications, Galaxy S21 FE by Samsung and also mention the expected price and launch date. Remember the thing that we are not responsible for the whole information in this blog, there is might be few changes.

Source: whatmobile.com.pk

Samsung Galaxy S21 FE Full Phone Specification

Samsung Galaxy S21 FE uses built in Android 12 OS with OneUI 3.1 UI.

This device Galaxy S21 FE by Samsung is supported 2G, 3G, 4G LTE, and 5G band as well.

It contains Octa Core CPU, with Exynos 2100 Chipset and Adreno 660 GPU.

This device named S21 FE has display size of 6.4 inches with 1080 x 2400 Pixels, Dynamic AMOLED 2X Capacitive Touchscreen, and Multitouch with Gorilla Glass Protection and 120Hz of refresh rate.

Source: whatmobile.com.pk

Galaxy S21 FE Pro has built in 128/256GB memory and 6/8GB of RAM with dedicated SD slot available (up to 1TB).

The device contains Triple Rear Camera of 12MP + 12MP + 8MP with LED Flash, and containing 32MP of front camera.

Source: whatmobile.com.pk

S21 FE has fingerprint sensor (under display) for security, Audio, Browser, Games, Torch, Messaging, and lot of extra features.

The battery that is used in S21 FE Galaxy is 4500mAh with 25W fast battery charging and 15W of wireless charging.

Connectivity: The connectivity of the device Samsung’s Galaxy S21 FE is WLAN, Bluetooth, GPS, USB, NFC (Yes), Radio (Yes), and Data.

Samsung Galaxy S21 FE Price

Source: whatmobile.com.pk",1634
"Samsung is working on its first “Ultra” tablet, and it appears that it isn’t the only manufacturer interested in the large-screen tablet format. Apple is rumored to be doing the same, developing a gigantic 15-inch iPad to compete in the burgeoning premium tablet market.

Apple’s 15-inch Tablet

According to Bloomberg, Apple will be a little late to the party, as its 15-inch iPad may not be ready for delivery until 2022. On the other hand, Apple isn’t known for being the first to market with fresh concepts, so it’s normal to expect Samsung to take the lead before Apple.

The Samsung Galaxy Tab S8 Ultra is expected to pave the way for larger-screen tablets, and Apple is already planning a response. Granted, the Galaxy Tab S8 Ultra may be obsolete by the time the 15-inch iPad is introduced, but the Ultra tablet series is likely to continue, thus newer variants should be produced every year.

More than an iPad

More than the iPad Pro series, Apple wants the rumored 15-inch iPad to be a mobile productivity powerhouse. It’s a multimedia tablet that compromises comfort for performance, but when propped up on a desk and connected to the right accessories, it can function as a desktop computer. The device will easily carry out complex tasks such as iPhone transfers.

In terms of multimedia, Apple’s 15-inch iPad could have a bigger profile to accommodate a more powerful speaker system. This demonstrates yet again a preference for multimedia consumption and productivity over mobility and comfort.

Wrapping Up

In 2023, the 15-inch iPad could be available for purchase. This should allow Samsung plenty of time to test the large-screen form factor with the Tab S8 Ultra as a launchpad, as well as figure out how to improve the user experience for future versions.",1777
"Samsung presently has two foldable phone models on the market. The Galaxy Z Flip series is recognized for its vertical, compact fold, and the Galaxy Z Fold series is noted for its book-like mechanism. Samsung, on the other hand, might include a third mechanism in a future foldable phone, according to a recent patent.

Samsung’s Double-Folding Phone

Samsung was recently granted a patent for a double-folding smartphone with three unique portions that fold in a Z form, according to a reliable source. This design also allows a portion of the screen to serve as a primary display with an embedded fingerprint sensor. The phone resembles a little tablet when fully unfurled. A triple rear camera array and a USB charging and data port are located on the phone’s back.

Surprisingly, the patent illustrations also show how Samsung may divide the phone’s internals across the three halves. The phone’s batteries and circuit boards are only housed in the back and front foldable sections. The device’s structural rigidity and strength are provided by the central part, which appears to be devoid of any important internals. We are not sure of the specifics but considering the company’s reputation, the device will be able to easily carry out SMS backup.

Expected Availability Date

It’s worth noting that this isn’t the first time we’ve heard or seen Samsung’s intentions for a double-folding smartphone. A somewhat different patent design surfaced in April 2021, showing an option for the dual-folding method. Samsung released a working prototype of its “multi-foldable” structure a few months later, offering us a firsthand look at how the gadget would work.

While the recent patent implies Samsung’s concept is well advanced, don’t bet your home on a gadget being released in 2022. Patents aren’t necessarily certainties or direct foreshadowings of consumer electronics. A double-folding technology, on the other hand, appears to be the inevitable progression of the foldable phone, especially if additional screen real estate is desired.",2042
"Samsung Galaxy Z Fold 3 and Z Flip 3 can now access the latest One UI 4.0 beta Bianca Patrick Nov 23, 2021·2 min read

Samsung has been testing the One UI 4.0 beta for the Galaxy Z Fold 3 and Galaxy Z Flip 3 in select areas for a few weeks now. As a result, owners of Samsung’s latest foldables can now enjoy Android 12 One UI 4.0.

For the devices, the beta program is going well. The latest One UI 4.0 beta for the Galaxy Z Fold 3 and Galaxy Z Flip 3 has been released by Samsung. This indicates that the final version of the update may be available soon for these devices.

One UI 4.0 Beta

The ZUKG build is the Galaxy Z Fold 3 and Galaxy Z Flip 3’s third One UI 4.0 beta upgrade. It is currently being rolled out to users in South Korea. This release provides several enhancements, according to the changelog. A few bugs have also been addressed. Users will be able to easily carry out tasks such as remove contacts with this new update.

It’s worth mentioning that South Korea isn’t the only nation where Samsung’s new foldable phones can participate in the beta. It was recently made available to users in the United States by the corporation. However, the procedure for joining the beta remains the same.

Samsung has already issued the One UI 4.0 Android 12 upgrade for the Galaxy S21 series to the general public. The first lineup to receive the upgrade is this one. Other high-end Samsung handsets should receive the final One UI 4.0 update before the year’s conclusion.",1481
"Samsung To Construct $17 Billion Chip Manufacturing facility In Texas, Marking Largest Ever International Funding Into The State Bradley William Nov 24, 2021·3 min read

Samsung will construct a $17 billion semiconductor manufacturing facility in Taylor, Texas — marking the most important overseas direct funding within the state’s historical past.

The funding comes as a world pc chip scarcity threatens American companies, particularly within the vehicle business.

A from the workplace of Gov. Greg Abbott (R-TX) says that Samsung selected the Lone Star State on account of its business-friendly local weather:

The brand new manufacturing facility will produce superior logic chips that may energy next-generation gadgets for purposes resembling cellular, 5G, high-performance computing (HPC), and synthetic intelligence (AI). The challenge will create over 2,000 high-tech jobs, hundreds of oblique jobs, and a minimal of 6,500 development jobs. Development will start in early 2022 with a goal of manufacturing begin within the second half of 2024. The $17 billion in capital investments consists of buildings, property enhancements, equipment, and tools. A Texas Enterprise Fund (TEF) grant of $27,000,000 has been prolonged to Samsung for his or her job creation. As well as, Samsung has been provided a $20,000 Veteran Created Job Bonus.

“Firms like Samsung proceed to put money into Texas due to our world-class enterprise local weather and distinctive workforce,” stated Governor Abbott. “Samsung’s new semiconductor manufacturing facility in Taylor will carry numerous alternatives for hardworking Central Texans and their households and can play a serious position in our state’s continued exceptionalism within the semiconductor business.”

“As one of many largest overseas funding financial growth tasks in United States historical past, to say Samsung’s dedication to this challenge is monumental can be an enormous understatement,” added Taylor Financial Improvement Company President and CEO Mark Thomas.

As The Every day Wire reported final week, provide chain bottlenecks are limiting the supply of semiconductors and different key inputs for American corporations. Ford struck a strategic settlement with GlobalFoundries to develop chips, whereas Common Motors is likewise pursuing relationships with producers.

Certainly, The Wall Street Journal experiences that corporations throughout the globe are racing to supply extra semiconductors:

Samsung’s doubling down on Texas the place it already has a footprint comes amid a 12 months of historic spending for the semiconductor business, spurred by authorities incentives searching for to draw native manufacturing. A world chip scarcity has undercut many industries from smartphones and residential home equipment to automobiles. Samsung, the world’s largest semiconductor maker by income, plans to speculate greater than $205 billion over the following three years, with chip-making a precedence. Taiwan Semiconductor Manufacturing Co. has earmarked greater than $100 billion over the following three years to construct new chip factories. Intel Corp. has additionally unveiled greater than $100 billion price of semiconductor manufacturing facility investments plans within the U.S. and Europe over the approaching decade.

Tesla CEO Elon Musk stated earlier this 12 months that he has “by no means seen something like” the semiconductor scarcity.

“Our largest problem is provide chain, particularly microcontroller chips,” Musk stated on social media, evaluating his firm’s determination to “overorder” varied merchandise to the “rest room paper scarcity” that occurred on the outbreak of COVID-19 in the US.

The Every day Wire is certainly one of America’s fastest-growing conservative media firms and counter-cultural shops for information, opinion, and leisure. Get inside entry to The Every day Wire by changing into a",3904
"In each nice historic rivalry — from army commanders and sporting legends to fizzy drinks and R&B divas — the will for the rival’s destruction normally conceals a horrible throb of awe.

And so, for a lot of hotly-contested years, it was with Sony and Samsung — two of the world’s biggest tech firms, bruisingly and brilliantly entwined of their battle for dominance. Since then, their methods and abilities (one presently has its eye on Indian Premier League cricket, the opposite on a $17bn Texan chip plant) have diverged. However the pair, say buyers, additionally look destined to fulfill once more — within the metaverse.

The transformation of Samsung, South Korea’s Most worthy firm, from producer of humdrum merchandise within the early Nineties to a peerless client electronics titan in reminiscence chips, batteries, cell phones and TVs a decade later was a decided emulation of the rise of its Japanese nemesis.

When the worth of Samsung’s model (as calculated by Interbrand) overtook Sony’s in 2005, the symbolism may hardly have been extra seismic. Even Sony’s prime executives later acknowledged in non-public {that a} essential TV panel three way partnership between the 2 had supplied the stage on which it had been expertly outmanoeuvred.

However, because it turned out, that humiliation was additionally the second that the rivalry started to lose its which means — not simply because the remainder of the tech universe was quickly transferring past that sort of one-on-one rivalry, however as a result of the 2 firms themselves have been in flux.

Samsung, whose experiments with content material and software program had by no means actually labored, was realising that its future lay in producing the {hardware} that might each underpin and host successive generations of the patron tech revolution. Sony, after some exceptionally painful years as a company abattoir for sacred cows, started to see that its co-founder Akio Morita’s imaginative and prescient of convergence and management of content material was now extra attainable than ever in its historical past.

In current weeks and months, the strategic gulf between Samsung and Sony has been much more crisply outlined — even when the execution in each instances has left some puzzled about what may occur subsequent. Final week, when Samsung introduced the US metropolis of Taylor as the situation for its most superior semiconductor facility, the funding (Samsung’s largest ever within the US) represented one other decisive wager by the corporate on tech {hardware} and a product for which the world’s urge for food appears insatiable. The information adopted the corporate’s announcement of a $206bn, three-year funding plan.

But, huge although that dedication is, Samsung stays an organization with an ideal deal extra to spend and, as but, little indication of what it has in thoughts. Its third era inheritor, Lee Jae-yong, is freshly emerged from jail, and anticipated to unleash among the M&A starvation that grew throughout his absence. The most certainly goal, say analysts, will probably be in non-memory chips and additional dedication to {hardware}.

Sony, in the meantime, has since 2018 launched into what Jefferies analyst Atul Goyal describes as an “eyebrow-raising” world splurge of 40 acquisitions, partnerships and stakebuilding workout routines. These have collectively expanded its portfolio of online game studios, streaming firms, movie, animation, TV and music producers — and, on the {hardware} facet, in specialist sensors. Over the previous week, it has made progress on its deliberate merger with India’s greatest listed leisure group, Zee.

The place some buyers have chosen to grouse that this spree is haphazard and a return to the dangerous outdated Sony days of dismal capital allocation, mentioned Goyal, it ought to in reality be seen as a coherent, transformational resolution to go on the offence. Sony, which has lengthy owned a Hollywood studio, a big music enterprise and the large PlayStation video games empire, is clearly bidding for extra dominant management.

Whereas the methods of the 2 Asian tech teams now look completely completely different, they’ve one thing doubtlessly essential in frequent. To the extent that anybody is aware of what the metaverse means, and no matter the shape or supply mechanism it takes, buyers are already trying to place their bets on who may dominate it.

Sony and Samsung, for now, appear like strong winners. For all of the vagueness round visions of digital worlds, augmented actuality workplaces and every part else that has been crammed into the fledgling metaverse narrative, two parts appear reliable. One is relentless incremental {hardware} demand for extra reminiscence, extra non-memory chips, extra sensors and extra shows. The opposite is ever-greater convergence of leisure. If the metaverse does nothing else, it could recast an embittered zero-sum rivalry as an epic pincer motion.

leo.lewis@ft.com",4981
"Samsung is one of the largest phone companies in the world, its major competitor being Apple. Based in Korea, they are also a mass producer of chips, an industry that has had a rough ride in the past year due to supply chain struggles. They are prioritizing this part of the business, making heavy investments to compete with players like TSMC, Intel, and AMD, including a unique one in the United States.

Samsung announced on Monday that they are going to build a 17-billion-dollar chip making plant in Taylor, Texas, a small town located in the outskirts of Austin. It’ll produce up to 2,000 jobs starting in 2024, and even though the location isn’t confirmed, most people related to the situation say that it will be Taylor. Taylor offered Samsung property tax breaks of 92.5 percent for the next 10 years, with the number gradually decreasing in the coming decades, which sealed the deal for Samsung. The move will help bolster US chip production to compete with leaders like Korea, Taiwan, and China, and they have support from the US government as there are plans to provide funding to the industry. It also marks a major economic gain for Texas, who has taken one step forward towards becoming a tech hub in the United States. Earlier this year, Tesla moved their factory to Austin, Texas, and tax laws in Texas make it an easy decision for companies to move there. Governor Abbott plans to make the announcement today, and it will help the local and national economy, although there is risk for increased pollution. Do you think this was a smart decision by Samsung?

I am not a financial advisor and my comments should never be taken as financial advice. Investments come with risk, so always do your research and analysis beforehand.",1744
"The festive season is well and truly upon us and blockchain isn’t missing out on the fun gifting madness this year! Following the Polkadot announcement that you can gift DOT or KSM to friends or family, many have taken advantage of this and given the gift of a digital asset this year.

What’s great about this feature is that you can send the gifts to anyone, even if they don’t already have an account or wallet. This allows you to overcome one of the biggest challenges for any blockchain network which is the onboarding process. Thus, you may not only be gifting a digital asset but also an invitation into the wonderful decentralised world of cryptocurrency.

As Christmas is merely days away, we’re having a look into the gifting data to see just how generous people have been this time of year.

All in all there was a total of 1,364.3 DOT (US$37,791.10) and 18.4 KSM (US$5,100.30) gifted. The luckiest single gift recipient received 5.1 KSM which is over $1,400 USD and contributes to over a quarter of the total amount of KSM gifted. The average gift amount was nothing to scoff at with 3.4 DOT (US$94.20) and 0.3 KSM (US$83.20). As of now, there are still gifts worth over 178 DOT and 0.77 KSM out there unclaimed, maybe Santa got a little lost?

Across Polkadot and Kusama there were a total of 457 gifts sent, with one popular claimer having received 13 KSM gifts. It’s clear from the graph that the gifting ramped up following the initial announcement from Polkadot on the 13th of October, with the peak number of gifts being sent on the 20th of October where 85 gifts were sent that day. In fact, more than a third of the total number of gifts that have been sent thus far were sent during the week-ending the 24th of October where 172 gifts were sent.

What’s interesting to see is the number of gifts not yet claimed is sitting at 84. Perhaps these will be ‘opened’ come Christmas day and we will see this number drop rapidly. Of the 373 gifts which have been claimed, the average amount of time taken to claim was just over 1 day (26.1 hours). Most gift recipients have claimed much faster, but one DOT recipient is still waiting to claim their gift of 12 DOT back from the 15th of October (over 67 days ago!).

So if you’re still in the market for a last minute gift for your loved one, why not send them the gift of Polkadot? Follow the steps listed on the Polkadot Gifting article and get your Christmas presents sorted so you can focus on what’s most important — the food!

About SubQuery Network

SubQuery is Polkadot’s leading data provider, supporting an indexing & querying layer between Layer-1 blockchains (Polkadot) and decentralized applications. SubQuery’s data service is being used by most of the Polkadot and Kusama crowdloan and parachain auction websites live today.

SubQuery’s protocol abstracts away blockchain data idiosyncrasies with the SubQuery SDK, allowing developers to focus on deploying their core product without needlessly wasting efforts on custom backend technologies.

​​​​Linktree | Website | Discord | Telegram | Twitter | Matrix | LinkedIn | YouTube

Appendix",3114
"The end of the year has been a very remarkable period for Gear. We finally announced that we had raised $12 million in a private investment round led by Blockchange Ventures. In addition to Blockchange, other top venture capital funds who participated in this round include: Three Arrows Capital, Lemniscap, Distributed Global, LAO, Mechanism Capital, Bitscale, Spartan Group LLC, HashKey, DI Ventures, Elysium Venture Capital, Signum Capital, and P2P Economy lead by Konstantin Lomashuk, along with several top executives of Web3 Foundation and Parity Technologies, including its founder Gavin Wood.

Other important milestones reached in December were mainly technical improvements to the Gear platform. The changes are as follows:

We added a program_id() function to gstd, which returns the program’s identifier. It can be used where the program wants to store funds, like fungible-tokens for itself.

New message processing logic with core-processor has been implemented.

This migrates current processing to the new logic of processing messages with a functional approach, and includes message journaling. This allows us to use multiple methods to execute core logic in different environments (collator, validator, and cumulus setups).

We have enabled a mechanism for the network maintainers (a.k.a. validators) to charge fees for the network’s resource usage. In particular, having a message stuck in the Wait List will cost the original sender a certain per-block fee. Furthermore, external users can participate in this game by keeping track of the WaitList state and suggesting those messages which have stayed there longest, thereby increasing the overall efficiency in terms of collected rent per message, in exchange for a portion of the total fee.

We added a submit_code call. This allows committing actors to the chain to be instantiated later from other actors.

In December we saw strong growth of the Gear community around the world. Following our series of educational events in both the US and Russia, we held another workshop at the Bauman Moscow State Technical University and two online workshops for the Chinese community. We outlined the benefits of our technology and demonstrated how to deploy smart contracts on the Gear platform.

According to tradition, before New Years is a good time to reflect on the achievements of the year, so we also would like to share a small summary of what we accomplished during 2021.

Since our GitHub became public in August, we reduced block time and the process queue at the end of a block. We replaced the procedure of handling the message queue to something more similar to what will be used in production mode. Messages are handled immediately; in other words in the same block, and they are submitted to the message queue (if the block gas limit is sufficient). Block time was also reduced to one second. As a result the latency of the network theoretically goes down (improves) by a factor of 18x.

In September, we made changes to the process of obtaining Metadata. In October, we wrote our custom Asynchronous Mutex, which allows programs to exclusively lock specific data and ensure it is not mutated by other messages while locked. Along with Asynchronous Mutex, we wrote Asynchronous RwLock — a reader-writer lock that allows more fine-grained async data locks.

Tree structures have been valued for the (future) gas spending algorithm, which isa step towards a self-consistent gas economy where gas associated with the message is always preserved. This was one of the milestones from our November report. ​

The growth of the community has played an essential role in 2021. In October, we launched our website with a new design and user-friendly interface. We held seven workshops and various MeetUps. Three workshops took place in Russia and one in the USA. The other three events were held online: the first was for students from the Computer Science and Engineering Society of the University of California San Diego. Another two were held for the Chinese community. So far, we have held five successful AMAs with our CEO and Founder, Nikolay Volf, hosted by PolkaWarriors, PolkaWorld, the famous Turkish influencer OrientusPrime, and Russian YouTube channels Cryptovo and ProBlockchain.

If you would like to learn more about Gear’s development in 2021, you can keep up with our monthly reports on Medium.

We would like to thank our fantastic audience who participated in all our events during the year, and we hope to see you again in 2022! More great things are coming, and we cannot wait to share them with you! We wish you all a Happy New Year!

Sincerely,

The Gear Team",4655
"Photo by Laura Ockel on Unsplash

Recently Spin Servers has put together a new build using the Intel Xeon 2696v3 to create a server that provides excellent computing power for encoding & rendering, shared hosting servers, virtualization, and many other web projects that require a large amount of computing power.

Let’s talk about the benchmarks and a few comparison CPUs. Using cpubenchmark.net, the 2696v3 scored an average CPU mark of 24701 with a single thread rating of 2232.

Comparing the 2696v3 to the AMD EPYC 7763, who wouldn’t love to have a few 7763s thrown into their build? Well, the people who don’t want to spend upwards of $14 to $15,000 on a dual chipset. Unless that is what your project requires, then more power to you!

The 7763 CPU clock speed and turbo are on par with 2696v3 in both base and turbo speed. This makes this CPU great for very large processing needs, but again, at a large cost. This is where the 2696v3 comes in. With a dual 2696v3, you will be utilizing 36 Cores with 72 Threads. Which brings the cost to performance really close.

In short, the 2696v3 is on the top 100 list of high-end CPUs. This CPU is sure to support you and any of your web project adventures.

We are proud to offer this CPU in our new build and hope to have many more setups in the future. If you are in the market for a high compute CPU build and need bulk quantities, contact us so we can talk about getting you set up.

You can get this build with a hefty discount for our Medium Blog users, with the discount code below.

Dual Intel Xeon E5–2696 v3

36 x 2.3GHz

512GB DDR4

4x 1.6TB SSD

20TB @ 10Gbps Monthly Bandwidth

A+B Power

ILO 4 with HTML5 Console

WAS $599 NOW $399 USE CODE: E2696$200OFF

This promotion is a recurring discount for the life of the service. Valid for new customers only.",1817
"Photo by Kieran White on Unsplash

Happy holidays from the team at Spin Servers! With the pandemic, shipping issues, global shortages, there is one thing that hasn’t changed since… the valuable time spent with family and friends. We hope everyone across the globe makes time to spend with their loved ones and push through these tough times.

With this in mind, we would like to spread from holiday cheer of our own with some insane discounts on some of our best dedicated bare-metal builds.

From now until January 2nd, 2022, take advantage of these deals while they last. There is no order maximum! Just hope you can snag a deal before someone else does.

Dallas, TX Location:



12x 2.40 GHz

64GB DDR3

1.6TB SSD

10 TB / 10Gbps

In Stock

$79.00/month

PROMO CODE: 2630LV2XMAS21

Deploy Now Dual Intel Xeon E5–2630L v212x 2.40 GHz64GB DDR31.6TB SSD10 TB / 10GbpsIn Stock/month

16x 1.80GHz

64GB DDR4

1.6TB NVMe

10 TB / 10Gbps

In Stock

$99.00/month

PROMO CODE: 2630LV3XMAS21

Deploy Now Dual Intel Xeon E5–2630L v316x 1.80GHz64GB DDR41.6TB NVMe10 TB / 10GbpsIn Stock/month

San Jose, CA Location:",1106
"Being as versatile as it is popular, the best Python hosting in UK matches the programming language’s balance of simplicity and scalability.

Choosing the Best Python Hosting service can be slightly tricky for beginners with a pool of hosting servers flooding across the internet.

Python is an easy and adaptable language making it an attractive choice for web hosting services. so let’s start to know.

What is Python Hosting?

Web Hosting that supports Python programming language and has Python installed on the user’s web server is called as Python Hosting.

Web hosts offer exclusive plans to facilitate developers with flexibility and freedom of coding in the language.

This calls for use of a higher category of hosting plans such as a VPS or dedicated hosting plan. Python extensions have the ability of coding in C or C++ and can be operated from Java-based applications.

Python Features

👉Very clear, readable syntax

👉 Strong introspection capabilities

👉 Intuitive object orientation

👉 Natural expression of procedural code

👉 Full modularity, supporting hierarchical packages

👉 Exception-based error handling

👉 Very high-level dynamic data types

👉 Extensive standard libraries and third-party modules for virtually every task

👉 Extensions and modules are easily written in C, C++ (or Java for Jython, or .NET languages for IronPython)

👉 Embeddable within applications as a scripting interface.

Let Start know the Best Python Hosting in UK 2022.

YouStable is one of the best web hosting providers that offer the most affordable Python Hosting service with all the features required. The latest python version, SSH access, and multiple framework support.

Including features:

👉 SSH Access

👉 Time Saving 1-click Installation of Python.

👉 Multiple Framework Support

👉 Good Support

👉 24*7 Guaranteed Uptime.

👉 Full-stack server protection.

I hope this article is helpful for you. If yes, please like & share with your friends and your family.",1968
"If MarTech makes you want to hide behind a sofa, the first thing to realize is that you aren’t alone. For the traditional marketers among us, you probably yearn for the old environment when you could just print a leaflet for the local community and be done with everything. Now, you’re worried about event tracking, metrics, and building stacks.

Today, we’re going to address these three very topics. Which user events are best for tracking? Which metrics are most valuable when making decisions? What is the best way to build a stack for this purpose?

In this guide, it’s important to note that the information applies to e-commerce brands. For example, we’re focusing on a responsive stack that attracts prospects and encourages them through the marketing funnel. With an effective system, people will mostly stay within the funnel rather than dropping out (a nightmare for any marketer!).

Defining Success as an E-Commerce Business

Before offering advice, you first need to define what success means to you. By doing this, you’ll know what we’re trying to achieve in the rest of the article. Primarily, success should mean helping marketing and sales team members to identify the leads with the most value. Then, the infrastructure should be in place to gently nudge these leads from one stage of the funnel to the next.

Of course, the goal is to get all leads to the bottom of the funnel and encourage them to spend money with your brand. To do this, you need to know what B2C events to track within your stack and the most valuable metrics to cover. Without further ado, let’s learn more!

Important Events for Tracking

Stacks don’t inherently offer value because you’ll need to implement them correctly first. When your stack is optimized, it’s possible to also optimize the customer experience with increased capabilities and tools. Soon enough, revenue increases, and the whole business benefits. Here are some valuable user events for 2021:

Feature Used — What is the relationship between customer retention and feature usage? For instance, is a certain feature keeping people engaged in the long term? Soon enough, you’ll identify relationships between long-term customer value and key features. Once you identify these trends, you can optimize your offering and encourage people to return time and time again.

Depending on the nature of your service, this event could be when the user completes a lesson, creates a note, or watches a video.

Lead Created — Essentially, this event is triggered whenever somebody goes from being a stranger to a lead for the business. Perhaps the most common example of this is when they provide contact information.

Order Completed — When a customer reaches this stage, they’ve traveled through the funnel and are now at the end. Since the individual has become a customer, you can use the event to learn more about ROAS, revenue, retention, conversion rates, and more.

User Created — Finally, another important event for e-commerce businesses is when visitors/leads create an account on your website. As they log into your platform, this event is triggered. With a closer relationship to your brand, they’re now more likely to spend money (if they haven’t already!).

Valuable Metrics

As an e-commerce business, you need to learn as much information as you can about the purchase cycle for customers. Where does the journey begin? Through which channels do they progress? What eventually encourages conversions?

Thankfully, it’s possible to analyze the audience through metrics. From here, you’ll learn about the most effective channels and how you can encourage conversions in the future.

Monthly Recurring Revenue (MRR)

If you use a subscription model for your business, this is an especially valuable metric because you learn both total recurring revenue from one month to the next as well as new recurring revenue. Soon enough, you’ll learn more about the product features and channels that contribute most to your strategy. Of course, you can also calculate annually recurring revenue (ARR) to assess how it changes from one year to the next.

Visitor-to-Purchase Conversion Rate

Next, we recommend calculating the number of people visiting your e-commerce pages and converting. Of all the people visiting your store, how many are spending money on products? As before, those who run a subscription e-commerce service will need to determine signups compared to website visits instead.

Once again, the goal is to find what makes people trigger the User Created event that we saw previously. Alternatively, you might want to learn what triggers the Order Completed event. Naturally, the one you prioritize will depend on your business model.

Monthly Churn

In any given month, how many customers choose to leave the business and go elsewhere? What inhibits the customer experience? What features of the business encourage people to stick around? If you’re just starting as an e-commerce service, you may choose to place more weight on retention than revenue. Why? Because retention is a sign of greater things to come.

Trial Subscription/Purchase Conversion Rate

Lastly, another metric that holds value for e-commerce businesses is trial subscription conversion rate. Why? Because it tells you the number of people who trigger the Order Completed event after sampling your products or services. If nobody converts after testing a product or service, you need to know why. If conversion rates are strong after consumers take advantage of a trial, you know that the system is working efficiently.

How Events and Metrics Help Your Business

Personalized Messaging and Recommendations

Firstly, learning about how your customers/leads act and behave enables the business to reach out to them more effectively. As we all know, personalization is the not-so-secret secret to success in 2021. Customers appreciate a business that understands them, and personalization is the way to show your understanding. You recommend the right products, the customers convert, and everybody wins.

Fix Problems

Next, you’ve seen already but the events and metrics above allow you to highlight potential problems. For example, you might notice that many customers don’t return from one month to the next. If this happens, you need to explore what might have caused this trend. Is the brand receiving some negative press? Did you post something that could have gotten a poor reaction on social media?

Likewise, you could also learn that nobody is converting after visiting your online store. If this is the case, you’ll need to learn the factors that are preventing people from converting. While you can’t expect everyone to convert, you probably have issues with your online store if no single visitor spent money.

Is the checkout process long and arduous? Does your landing page fail to fulfill expectations generated by the ad? Are you hiding delivery costs from customers until the very end? Is your website not optimized for certain devices?

Furthermore, another problem could be that nobody converts after trialing your products and services. In this case, it suggests that the products/services themselves are causing the problems. Or it could be that you aren’t offering customers enough access to test properly.

Improve Email Marketing Campaigns

Next, you may have thought that the days of email were all over, but this is far from the truth. In fact, they’re making a sensational comeback as consumers have more control over their inboxes. With GDPR and other privacy regulations, consumers don’t get as much spam these days, and this means they pay more attention to their inboxes again — every email they receive has value (unlike the days of endless spamming).

As you monitor these events and metrics, you learn more about the customer journey and the value of your email marketing campaigns. As always, the aim is to avoid churn while also boosting engagement. If you can reach out to your audience with more relevant messaging, this will lead to more clicks on your online store and conversions.

Summary

What should you track? We hope that you have more confidence in this area now. As well as listening to our advice, you should also think carefully about your e-commerce service. What events and metrics will provide the information that you need to improve the customer experience? What information will generate more conversions, revenue, retention, and growth?

If you’re following metrics just for the sake of it, you’ll soon find that the insights don’t apply to your business. Instead, you need to think about your goals and how metrics will push you towards them.

Soon enough, you’ll learn about User Created, Lead Created, and Order Completed events. You’ll learn about your audience and how you can plug gaps in your marketing strategy.

This year, don’t be afraid of MarTech because all solutions and platforms are designed to assist your business. Once you lose the fear, you can finally use these tools to progress the business in the ways described in this guide!",9049
"Bringing The Necessity To The IT Industry

Who Necess-IT is and what they do.

Necess-IT provides high quality IT work within the Tampa area and beyond. While Covid-19 had many businesses shutting down in 2020, Necess-IT stepped up to the plate to provide the necessary equipment and upgrades required in many businesses to adapt to the changing landscape that the entire world needed to quickly adapt to. This consists of growth at the end of 2020 within Josh’s company that was unexpected but highly welcomed.

Check out my video on data findings over the last year.

I spoke with Josh who is the CEO of the company and he stated that they went from working from their home office to a fully staffed warehouse and expanded from 4 to 20 technicians who work on the field all over Florida in a matter of a year. He started the business in 2014 with his partner Kevin who is part owner. They worked out of the trunk of their car (called trunk slammers in the industry) endlessly for years to build up their team of skilled employees and gaining the necessary knowledge on the IT industry. Fast forward to 2021 and they work with big name brands such as Coca-Cola, IHG Hotels and Manor Care nursing homes. Providing everything from low voltage cabling projects, fiber optic site surveys and service calls daily to repair cable that provide data, phone connections that are not working properly and complex camera system setups at retail and office spaces. During the height of Covid-19 many businesses had to adapt, this included the addition of new ways to pay in retail such as NFC pucks for contactless payments and upgrading entire internet provider networks in nursing homes to provide better flow of data throughout local networks within the businesses so their daily employee tasks can resume as usual.

For more information on Necess-IT and public relations you can also visit my website.

The technology industry during the pandemic shifted in a major way. Entire companies went from working in offices to all working remotely and shutting their office spaces almost entirely. Below are some articles that I found interesting regarding the tech industry and Covid-19 and the changes that took place over the span of a year and beyond. Necess-IT worked hard to assist local businesses upgrade their network workload that it was going to have to now produce since most employees were starting to work from home and offices needed upgrades anticipating their return after the pandemic.

https://www.pwc.com/us/en/library/covid-19/coronavirus-technology-impact.html

In conclusion, Necess-IT has been a great example of a business that became a need for many when times got tough. They see no signs of slowing down and expanding their services and quality work for years to come.

For more public relations insights and information you can visit my Facebook page.",2866
"It’s about putting your people first for peak season 2021

Alex MacPherson, Director of Solution Consultancy and Account Management at Manhattan Associates, discusses peak season and how managing a work-life tech balance is going to be critically important during this peak season too. Top Business Tech Dec 20, 2021·5 min read

Alex MacPherson, Director of Solution Consultancy and Account Management at Manhattan Associates, discusses peak season and how managing a work-life tech balance is going to be critically important during this peak season too.

Whether it’s freight and potential wage increases, the shortage of HGV drivers or simply the challenges of recruiting and upskilling enough people to staff warehouses and stores, supply chains have never been more topical or talked about than they are right now.

With retailers gearing up for their busiest period of trading and already battling a number of external headwinds (more so than simply the effects of the pandemic), this year’s peak season feels like it will be more stressed, for more brands, than ever before.

Beyond the material challenges being spoken about however, there is also a welcome shift underway in softermessaging about the increasing value of managing employee well-being and engagement through periods of extended (and often excessive) stress like peak season.

This is the reality of the situation many retailers find themselves in. At a time when many have only just managed to weather the storm presented by the pandemic (successfully pivoting to new processes, methods of customer communications and fulfilment strategies to cope with shifting consumer demands). Warehouses now face the reality that many of their workforces have been working at consistently high levels of throughput, meaning that they need to adapt and be flexible to meet consumer demands.

If end of year sales targets are to be met, then it does not just supply chain processes that need to be effective; it’s the people who make the processes work that need to be considered far more too. Whether it’s warehouse employees or front of house store associates, much of the pressure of peak season 2021 (and ultimately the annual success of retailers) rests on the shoulders of these individuals.

Without wanting to continually look back, there are two key learnings we should take from the pandemic: first, the need for scalable and agile technology is the key to navigating fast-paced, changeable industry landscapes. And, secondly, for all the smart technology you might have in place within your supply chain network, it is ultimately, people, that power businesses and commerce.

Employee engagement in warehouse settings has received well-deserved media attention in recent months. The challenges faced during the pandemic have shown the importance of empathy and understanding the worries and anxieties of individual workers. They also highlighted the risks warehouse workers continue to take every day in order to make sure we, the consumer, still get what we want delivered to our door when we want it.

This surge in e-commerce over the last 18 months (and indeed its continued popularity) quickly highlighted the importance of productivity and an engaged, committed and healthy workforce. Having a loyal and engaged workforce is key to business success, but also crucially, it’s key to employee well-being too.

Achieving a happier, more engaged workforce requires more than simply reviewing performance data and rewarding high-performers. Increased expectations for fulfilment speed and volume are driving organizations to better understand and engage their workforce to differentiate and excel. Manhattan Active WM uses gamification theory and behavioral sciences to revolutionize warehouse labour management, with a focus on providing a more individual and rewarding work experience.

The result. A warehouse environment that helps promote employee productivity, satisfaction, well-being and reduces turnover too.

Managing a work-life tech balance is going to be critically important during this peak season too. Advances in consumer technology are bleeding into the business world, meaning enterprises are under increasing pressure to keep pace with the consumer market when it comes to user experience and innovation. Using outdated, archaic technology is a sure-fire way to exasperate your workforce and cause them unnecessary stress.

Warehouse employees and front of house store associates expect to be able to use the technology they are accustomed to in their personal lives, in their working space. This means having instant access to accurate information (such as in-store and online inventory, customer records or even social media purchasing and browsing trends), using technology that can connect them to other team members and having user-friendly operating systems that are intuitive and easy to get to grips with.

To switch from the back-end (warehouses) to front of house (stores) for a moment; ensuring a store Point of Sale (POS) solution is capable of handling orders and sales, managing inventory and customer-facing functionality (including loyalty, promotions or clienteling) will help retail associates both at the sales desks and in-flight on the shop floor stay ahead of trends, regardless of what might happen in future to shape them.

However, it will also arm the in-store associates with the information they need to provide a seamless customer experience for shoppers. And, as anyone who has ever worked in a high-street store over Christmas will know, being able to have accurate, real-time, information on hand to deal with customer requests makes life 100% less stressful!

The pandemic has provided individuals and organizations with an opportunity to rethink priorities, and in many cases, it has also afforded them the chance to reset what is truly important to them: whether it’s a greater emphasis on sustainability or a newfound appreciation of the true value and importance of people to organizations, there have been silver linings to the clouds of the last 18 months.

It is clear that options such as pick and ship from store, Click and Collect and micro-fulfilment are no longer nice to have options; they are critical areas of a brand’s ability to meet peak (and regular) seasonal challenges and people are key to the success of these capabilities.

As retailers continue to focus on remodeling the store, decentralizing fulfilment and managing direct-to-consumer requests, having an omnichannel proposition capable of managing an effective decentralized fulfilment network is now critical like never before.

READ MORE:

Let us hope that with the use of a smarter supply chain and retail technology (tempered in the heat of the pandemic), that peak season 2021 will see more emphasis placed on enabling, empowering and looking after the well-being of those people in warehouses and stores around the globe who continue to keep life and commerce running for us all.

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",7111
"The Windows Remote Desktop Connection software gives users the ability to connect to a remote Windows PC or server over the internet or on a local network, giving them full access to the tools and software installed. This is made possible by Microsoft’s Remote Desktop Protocol (or RDP for short). Remote Desktop Protocol (RDP) is a protocol to access and control a computer over a network. RDP can work as a remote access solution. It lets users operate their office desktop computer from anywhere in the world. So in this article we try to understand RDP in simple language like What is RDP and how it works Also we will explain how you can use RDP in Windows.

What is RDP

The Remote Desktop Protocol lets remote users to see and use Windows on a device from another location.It enables network administrators to remotely diagnose problems that individual users encounter as well as gives users remote access to their physical work desktop computers.Windows comes with a remote desktop client that can be used to access the complete Windows Desktop environment remotely. It’s very useful for people who use multiple computers for work.

How RDP works

RDP establishes a dedicated, encrypted network connection between the host system and the remote desktop. Through this connection, mouse and keyboard data flow upstream from the user while presentation data flow downstream from the host system. Application execution, data storage and processing remain on the host system. An RDP-enabled application or service packages the data that is to be transmitted and the Microsoft Communications Service directs the data to an RDP channel. From there, the OS encrypts the RDP data and adds it to a frame so that it can be transmitted.

How to use Remote Desktop Protocol ( RDP)

To use a remote desktop , a user or admin must install RDP client software to connect to the remote Windows PC or server, which must be running RDP server software. A graphical user interface enables the remote user open applications and edit files just as if they were sitting in front of their desktop. RDP clients are available for most versions of Windows as well as for macOS, Linux, Unix, Android and iOS

How to connect Windows PC or server via Remote Desktop Connection

Open the Remote Desktop Connection.( Start> All Programs> Accessories> Remote Desktop Connection ) Type the IP address of your Remote Computer or Server Enter Username and Password Click Connect

The remote desktop connection is established.

Benefits of RDP

128-bit encryption

32-bit color support

Audio, file system, printer, and port redirection to allow users to connect to local resources from within a terminal session

Support for a number of different network topologies

applications on a remote desktop can be run on a local computer

The ability to disconnect temporarily without logging off

Disadvantages of RDP",2883
"Scale-up Spotlight: A conversation with Co-founder and CEO of Sendcloud, Rob van den Heuvel

We spoke with the CEO and Co-founder of Sendcloud, Rob van den Heuvel, about the growth of his company and… Top Business Tech Dec 22, 2021·3 min read

We spoke with the CEO and Co-founder of Sendcloud, Rob van den Heuvel, about the growth of his company and how it has adapted to the post-Covid-19 world.

Sendcloud is an all-in-one shipping platform for e-commerce businesses that want to become international giants. Rob van den Heuvel came up with the idea for Sendcloud back in 2012 when he and his partners Bas Smeulders (Co-founder and COO) and Sabi Tolou (Co-founder and CCO) all ran an online store in phone accessories. “Things were going great, and our businesses were growing,” he explained, “but we were struggling with shipping as it was both time-consuming and expensive. After some beers, we decided to come up with a solution ourselves and asked a friend to help us build the software. Sendcloud was born, and the rest is history!”

Sendcloud started in the Netherlands but has quickly become one of the fastest growing scale-ups in Europe, with more than 23,000 customers across the UK, France, Germany, Spain, Italy, Belgium, and Austria. Its current customers range from small to enterprise-sized online retailers in industries ranging from fashion and electronics to food and drink.

Sendcloud differentiates itself in the market by being customer- and solution-centric, saying: “technology is at the heart of everything we do here”. Its end-to-end product covers the entire process, putting everything they need in one platform. In comparison, most retailers typically use three or four separate tools for shipping, labels, order picking, and returns.

Through its carrier and partner network, Sendcloud positions itself as one of the European leaders in enabling retailers to grow internationally. It’s also fully scalable, catering to businesses of all sizes, whether they ship 40 parcels a month or 400,000.

Covid-19 has massively affected businesses in the last two years, and e-commerce gained rapid momentum because of the lockdown measures as high street stores closed and people flocked to online shopping in huge numbers. When asked about Sendcloud’s last milestone as a company, van den Heuvel said: “it has to be rapidly expanding our team to keep up with this growth. We started with 140 employees in 2020 and recently reached the milestone of 300 employees” reiterating our status as one of the fastest growing companies in Europe.

Where other companies have had to slow their work down to adapt to the new changes, Sendcloud has found itself in a state of flux. This meant hiring people even faster than planned and managing all of this remotely. The biggest challenge for Sendcloud according to van der Heuvel has been scaling up the business even faster than originally expected whilst making sure new members are onboarded properly and staff remain motivated whilst working remotely.

E-commerce has grown tremendously during this past year, but now that shops have reopened and consumers have returned to the high street, this growth will naturally slow down. However, it can still be expected that many consumers will opt for online shopping in the long term. Lockdown has really caused an acceleration of an ongoing trend that ensures their safety and gives them peace of mind in a dangerous world.

Read More:

In the post-Covid world, many consumers may be craving the physical shopping experience and excitement that you simply can’t provide online. However, consumers have realized the full range of benefits of online shopping, from the peace of mind to deliveries built around their schedule, and have made it part of their routine. Covid-19 has set an irreversible trend when it comes to online shopping, and an omnichannel approach to shopping is here to stay as consumers get used to this new shopping environment.

Click here to discover more of our podcasts

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",4127
"Beyond Bitcoin: 2022’s defining crypto trends

As economies around the world continue to pivot to the crypto market, there’s no doubt that 2022 will see the innovation stakes raised even higher. Kristjan Kangro, CEO and founder of Change, a European cryptocurrency investing platform for retail investors, explores the defining cryptocurrency trends to come in the year ahead. Top Business Tech Dec 22, 2021·4 min read

As economies around the world continue to pivot to the crypto market, there’s no doubt that 2022 will see the innovation stakes raised even higher. Kristjan Kangro, CEO and founder of Change, a European cryptocurrency investing platform for retail investors, explores the defining cryptocurrency trends to come in the year ahead.

To say that 2021 has been a big year for the crypto market is an understatement. Cryptocurrency has now become more than just the choice of the ‘digitally savvy’ but a pillar of popular culture. Crypto is now not only recognised by major institutions but embedded within more leading businesses than ever before. Earlier this year the capitalisation of the market saw a threefold increase to an all-time high of $2.5 trillion, and this is just beginning.

Amid the rising case for a more transparent, digitalised approach to global finance, the crypto industry continues to evolve rapidly — going beyond its early Bitcoin origins, to effectively transform the face of finance as we know it. The result is an abundance of opportunity for the investor in-the-know to grow their portfolio and unlock new profit potential. But, with so many different crypto trends on the market, where to begin?

The DeFi Disruptor

One of the most important and exciting innovations to gain ground in the past twelve months is without a doubt decentralised finance. For those less familiar, decentralised finance (DeFi) is the concept of moving power and control away from traditional financial institutions, such as banks and brokerages, and into the hands of people like you and me.

It brings to life a world where financial transactions and products are all run on a public blockchain without a middleman. Effectively creating a more transparent, open and free financial system by reducing complicated procedures and costly checks associated with traditional finance. Applied en masse, it will result in a truly global economy; an economy where anyone, irrespective of the financial infrastructure or controls in their own country will have the same access and opportunities as everyone else.

Given that many millions of people are currently ‘underbanked’ or ‘unbanked’ it could help play a big role in tackling poverty too. For those already in the traditional financial world, the improved transparency and accountability makes the financial system as a whole more stable, meaning scenarios like the 2008 financial crisis would become far less likely.

Over the course of 2021 we’ve seen more companies and countries look at ways to embrace DeFi. Scores of DeFi projects have been launched and according to accountancy giant EY — the value of assets locked into DeFi grew from less than $1 billion in June 2020 to more than $98 billion in September 2021. This trend looks set to dominate in 2022 as new DeFi coins, projects and innovations continue to come to the fore.

More than a Token Movement

Non-fungible tokens (NFTs) are another incredibly exciting area, although NFTs are predominantly used in art, music and sports, they have recently expanded to include other types of digital property, including digital real estate. As the world becomes more digitalised and connected, the opportunity for fraud, identity theft and other cyber security breaches is on the rise. NFTs can play part of the solution.

Due to the way each asset is minted uniquely as an NFT on a blockchain, it offers the ultimate decentralized and immutable verification. In the future this could help everyone from brands, insurance companies and banks through to institutes and governments across the world. How? By speeding up all the types of application and identification processes through the ability to verify and trust the same single code.

One example might be a designer shoe brand which uses NFTs to verify its products — where each serialised pair of shoes has a unique ID embedded in the physical product which can be scanned by companies and consumers to prove its authenticity. In this way, in the future NFTs won’t just be confined to the big-tickets items like art, but instead could be used for everything from your cinema ticket and gym subscription through to your car key.

Crypto choices

As Bitcoin is the most established and widely used crypto asset, it is understandable why it may be the obvious first investment choice. However, in the next few years we’ll see other areas of the crypto world really start to take hold. Some great current examples include Cardano (ADA) & Polkadot (DOT). Both offer an interesting play to hedge against Ethereum as potential competitors for the best smart contracts platform. Solana (SOL) is also worth a mention, it’s one of the fastest growing blockchain ecosystems and offers a great way to scale user friendly apps.

Uniswap (UNI) is another one to watch. As the world’s largest decentralised exchange, Uniswap by market capitalisation, it’s a key player in its field and a driving force behind a lot of crypto innovation happening today. Alongside the wider DeFi movement, the public are going to see plenty more DeFi assets and services come to the forefront too and make themselves more known in the mainstream. We know this because in Change, over 90% of our community stated a clear interest in getting to know more DeFi assets alone last year.

Read More:

As we look to the year ahead, there’s no doubt that we’ll see a continued increase in market acceleration as more and more people turn to crypto. In this way, taking the time to keep on top of the market and joining a crypto community, or two, online could pay dividends.

Click here to discover more of our podcasts

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",6185
"Data analytics: employee empowerment or surveillance?

Caroline Lewis, sales director at data analytics organization Tiger, explains how businesses have adapted to this new era.

With the arrival of the pandemic, the working world changed forever. In fact, it’s been purported that business technology evolved more in a year than in the previous decade, as employees were armed with laptops and the facility to work from anywhere, via almost any device.

As hybrid working continues to take precedence and staff strive to find their own balance, so too do the organizations that have equipped team members with a previously unparalleled degree of autonomy. And this takes trust.

Adopting technologies to accommodate fresh ways of working

As the events of the past 18 months unfolded, the majority of firms have kept in touch via unified communications and collaboration (UC&C) platforms such as Microsoft Teams. The speed at which this facility was rolled out and adopted among workers was nothing short of astounding. A completely new concept to many, which would previously have taken a period of bedding in, training courses, and feedback, was implemented and embraced within a matter of weeks.

And there is no doubt that this process was what kept thousands of businesses afloat, throughout the world, as entire workforces were given no choice but to work from home for prolonged periods. In fact, in many instances, workers were forced to take increased control and responsibility over their workloads and responsibilities. Meanwhile, leaders faced various operational challenges as they adapted to managing their teams from a distance.

Now, as many companies begin to adopt a hybrid working approach in light of new Covid-19 variants — leaders must take stock of how to move forward, with a host of fresh ways to keep in touch considered an essential component of a company’s armoury.

A delicate balance

Hybrid working will remain one of the top considerations for businesses — who must assess whether this approach will work in the long term, to analyze how it will be implemented and managed, and to decide what additional tools they need in place to make it work.

And as the line between the workplace and home becomes increasingly blurred, organizations must implement new processes to monitor this — to the benefit of businesses and colleagues — to pick up on any nuances and keep abreast of their team’s wellbeing.

It will, no doubt, prove to be a nerve-wracking time for many. Some leaders will feel overwhelmed and out of control, with staff operating from kitchen tables and home offices, and enjoying the new-found flexibility that the pandemic has invoked. Meanwhile, other team members may feel disengaged, unable to retain the focus — or to enjoy the camaraderie — that the workplace once offered.

And with ‘the great resignation’ now a nationally debated topic — and job vacancies in some industries at an all-time high — it’s time that firms prioritized understanding how the pandemic has affected their teams, as well as their levels of service, and begun unlocking insight that could prove vital to their future success.

Don’t operate in the dark

In their haste to keep afloat, many organizations didn’t look beyond the immediate need to keep their company running. And understandably so. But as disruption continues to impact operations, with remote and hybrid working continuing for many, leaders needn’t feel out of touch.

Now is the time to prioritize how these platforms can become a permanent and useful feature, which increases efficiency, insight, and outcomes — both for teams and amongst a company’s client base. ‘Plugging in’ intelligent analytics tools which increase and contextualize the data available is just one of the ways that organizations can keep abreast of any employee trends, or areas of engagement and disengagement, amongst their teams.

For example, businesses can gain an understanding of how well video calls are working as a meeting tool. If the connection is constantly dropping, this could impact upon productivity and client satisfaction — jeopardizing both colleague wellbeing and customer retention. But with the context provided by analytics tools, organizations can gain valuable oversight which will help to inform strategy moving forward.

Not only will this knowledge empower leaders to ensure they’re offering the correct training, investing in the right technologies, and spending their time and money where it matters, but it will also ensure that hard work is visible, progression is measurable, and that targets are considered and achievable. All of this will contribute to ensuring that team members feel happy and supported in their employment.

Encourage ‘buy in’ across the board

Historically, the perception of analytics has proved controversial. Employees may worry that their activities are being ‘spied on’ or that their privacy is being invaded. But while these tools do indeed unlock relevant data — their main aim is to identify patterns of engagement, establish what is and isn’t working, and improve efficiency all round. It’s about empowering employees, not making them feel as if ‘Big Brother’ is watching them.

And this is just as beneficial to colleagues as it is to companies. Those intermittent connectivity issues which cause frustrating delays and video calls to glitch will be picked up, removing some of the hurdles which make achieving an employee’s goals, and indeed their targets, more easily attainable, as a result.

Just as any struggles can be picked up and supported, progress and growth can be identified and celebrated — making for unbiased observations based on data, rather than simply relying upon opinions that can be heavily influenced by external factors. Staff can use this data to support their own progression, pinpointing strengths, along with any areas for development or training, in order to create a robust case for career advancement.

And where any hesitancy remains, a transparent approach will help to remedy this. Introducing intuitive dashboards, for example, will bring data to the forefront for everyone. Once team members can clearly see what the strategic goals are, and how their contributions are being measured, in many cases it will address any cynicism and, instead, motivate them to seek improvement.

READ MORE:

Data analytics tools are not designed to snoop but, rather, are a key component that enables businesses to remain informed regardless of the physical whereabouts of their team members. With the ability to look out for patterns, identify difficulties, and address what’s working well, teams can collectively strive for success.

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",6809
"Can iterative digital change help businesses mitigate risk?

Clare Gledhill, operations and strategic development director at strategic change agency CDS explores how iterative digital change can help mitigate risk. Top Business Tech Dec 23, 2021·5 min read

Clare Gledhill, operations and strategic development director at strategic change agency CDS explores how iterative digital change can help mitigate risk.

Risk mitigation has always formed a key part of businesses’ success strategies, and that’s because all companies are faced with risk. While no one can always foresee the issues that will arise, having mechanisms in place to help mitigate, evaluate, manage, and resolve them can help an organization keep running smoothly, should a few bumps in the road occur.

Across the globe, the risk landscape arguably became even more unpredictable and turbulent with the onset of the pandemic, in many cases, highlighting the vulnerability of many organizations. As a result, firms worldwide have needed to demonstrate high levels of resilience and the ability to adapt and cope with the challenges thrown up by the virus or, alternatively, run the risk of having to close its doors permanently.

However, when it comes to planning for risk, there are many categories for business owners to be aware of; each comes with its own challenges. From operational and financial to compliance and reputational, companies need to proactively plan for these scenarios, not to eliminate risk but help preserve the trust and loyalty they’ve built up with their customers if the worst happens.

Accelerated digital transformation

It is no secret that to help navigate the unchartered waters of the pandemic, many companies looked to bolster their digital infrastructure to help them improve and streamline their processes and meet demand. For firms that had advanced technology in place, this simply meant investing in unified communications and collaboration (UC&C) software (such as Microsoft Teams or Zoom) to keep staff connected while working from home. In contrast, it meant a complete overhaul of their legacy systems and practices for others.

The wake-up call that came in the form of the pandemic caused many business owners to realize that their existing tech stacks and internal systems were no longer fit for purpose, especially with the shift to hybrid working models, or they didn’t meet the high expectations of both their staff and customers. Therefore, businesses looked to transform digitally to help weather the immediate storm and futureproof their systems and enhance their resilience.

Iterative digital change and risk mitigation

In today’s increasingly competitive marketplace, private sector firms strive to deliver more value and revenue, and public sector organizations are trying to offer better services. However, both share the common goal of increasing flexibility, leaner processes, and an optimized user experience.

Legacy systems can sometimes be a bottleneck in achieving these goals, In fact, a survey recently revealed that 70% of global CXOs see mainframe and legacy modernization as a top business priority. Another study interestingly also found that replacing legacy systems was one of the top 10 IT investments for companies in 2020.

Organizations don’t have to worry necessarily about re-platforming immediately, changing all existing systems and infrastructure at once, as, in the current economic climate, this may feel like a monumental task. Instead, choosing the right tech or understanding where an existing solution lets the company down is the logical first step. This can feel less daunting and more achievable than a complete re-platforming project.

By following a more iterative approach to digital change, organizations can implement newer, quicker, and futureproofed technology that supports, not hinders, at a comfortable rate for the business. As a result, this ensures effective scalability, efficiency, and sustainability, without pressure to adapt and change everything simultaneously.

Aging legacy technologies that no longer support developments, content personalization, or updates can leave companies vulnerable to a wide variety of risks. Whether a security breach, downtime, inefficient back-end processes, or user frustration, this often conflicts with society’s ‘inflated expectations.’

Savvy, intuitive technologies are being used by many people daily in their personal and professional lives. Those thatgenuinely delight in terms of their experience are the successful ones and used time and time again. Seamless, integrated, positive experiences are what people expect as standard from modern-day organizations. When executed with empathic experience and communications at its heart, this helps build authentic trust between brands and their audiences.

Looking at this from a financial and efficiency angle, if legacy systems are left in place and an organization builds upon these platforms without considering their constraints and risks, this inevitably leads to experiencing limitations and accruing technical debt. Therefore, ignoring legacy technologies can be a costly and complex exercise if addressed at a later stage.

Automation goes hand in hand with iterative digital change, too. Automating the more manual and operational tasks this also helps free up strategic resources, which can be dedicated to other areas of the firm.

Prioritizing behavioral insights

While many organizations think that investing in the latest, most advanced technology will solve all their problems, this really is not the case. Whether or not it involves iterative technical change, the cornerstone of any successful digital transformation journey is a user-centered approach from the start. Knowing exactly whothe target audience is and howa firm’s technology needs to operate to offer the best possible user experience is vital.

To achieve this, research needs to be carried out into users’ needs, priorities, and expectations, as this will shape the solution bespoke to the business. This ensures it is truly inclusive and accessible to all those who use it. Depending on the systems, this refers not only to external customers but also to internal employees. While a poor digital experience can switch consumers or service users off and impact the outward perception of a company, this can also prevent growth. For instance, if team members feel restricted by inefficient workflows or customers feel frustrated by a lack of empathetic communications, they will be more likely to take their skills and loyalty elsewhere.

Read More:

Ultimately, iterative technical change is one potential cog in the digital transformation machine. While it may not always be necessary, it’s pivotal to conduct behavioral insight research to determine this — removing any assumptions or guesswork. The research phase will always provide interesting findings, some of which can contradict a business’s views of what they thought they knew about their users.

However, ‘inflated expectations’ are only heading in one direction as technology continues to advance at a swift pace. Looking ahead to 2022 and beyond, meeting the true needs of end-users will arguably matter more than ever before, as both private and public sector organizations strive for a sustainable growth and success strategy with as minimal risk as possible.

Click here to discover more of our podcasts

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",7546
"Scale-up Spotlight: A conversation with the commercial director of Alvant, Richard Thompson

Richard Thompson, commercial director of materials specialist Alvant, talks about the importance of sustainability and innovation in post-Covid manufacturing and the role materials tech is playing across automotive, aerospace, consumer tech, and many other sectors. Top Business Tech Dec 28, 2021·5 min read

Richard Thompson, commercial director of materials specialist Alvant, talks about the importance of sustainability and innovation in post-Covid manufacturing and the role materials tech is playing across automotive, aerospace, consumer tech, and many other sectors.

Alvant — originally known as CMT — was established in 2003, specializing in designing, developing, testing, and manufacturing Aluminium Metal Matrix Composite materials and components (AMCs), a family of lightweight high-performance metals. AMCs are used in highly engineered products for multiple applications across many sectors, including aerospace, automotive, healthcare, the industrial, and high-end consumer. This can be parts for TVs, mobile devices, landing gear, electric motors, car interiors, biomechanical prosthetics, sports equipment, and wheelchairs — the list is as diverse as it is long!

AMCs provide the strength and stiffness of steel at less than half the weight and have superior damage tolerance and a higher thermal operating range. Engineers and manufacturers can use AMCs for more durable lightweight components for harsh environments. Product manufacturers and engineers are becoming more aware of how AMCs can sometimes be a better alternative than other composite materials or unreinforced metals.

Alvant provides ‘route to market’ services for integrating its AMCs; The focus is on solving problems with creative and commercially viable solutions. This is particularly important now as engineers face growing pressure to reduce weight to meet stringent market and legislative demands while simultaneously being cost-effective.

Richard Thompson informs that the method Alvant uses to create its materials, called Advanced Liquid Pressure Forming, has been developed specifically for its product and is patented. This sets the company apart from others in the market and makes them a viable alternative to other materials such as carbon and polymer composites, steel, titanium, and aluminum. These materials are either not recyclable or very difficult to recycle, unlike fiber-reinforced AMCs that have scope to recycle.

AMCs have many benefits, including having the strength and capability of steel yet the weight of aluminum while being more tolerant to physical and thermal damage than carbon composite materials. They also create less of an impact environmentally when it comes to sustainability issues. This is mainly because aluminum itself is abundant and has a less environmental impact during production than titanium.

It isn’t difficult to see why AMCs could provide a massive differential and offer game-changing potential across many industries as commercial demand increases for strong but lighter parts across many forms of transportation, as well as industrial and consumer applications. Companies are all looking for ways to increase product capabilities and performance while at the same time meeting ambitious goals for fuel efficiency and sustainability.

Thompson explains the company’s most significant achievement to be the part the company has contributed to the global-wide objective to achieve net-zero carbon emissions by 2050. Alvant has partnered with global-leading Safran Landing Systems on a two-year, £28 mn aerospace project titled ‘Large Landing Gear of the Future.’ Alvant’s contribution to the project is the design, manufacture, and testing of an AMC brake rod, targeting a 30% weight reduction over an equivalent titanium component while maintaining the same strength as steel. In fact, the current simulations suggest a 40% weight saving can be achieved over the original titanium part, citing a crucial project milestone scheduled for completion this year.

Alvant has partnered with Safran in the aerospace sector to create lighter weight landing gear

Besides weight reduction, the project aims to cut fuel burn and noise as part of the industry’s drive to reduce fuel consumption and carbon emissions while improving reliability and lowering ownership costs. This is of major significance for Alvant as a company because the project’s success will enable the validation of AMCs in areas where safety and reliability are essential. This puts Alvant firmly on course to help Safran not only solve some of their upcoming technological challenges but to help them remain competitive in the landing gear aerospace market and showcase innovative technologies for the industry’s future.

During Covid, the economy’s sudden plunge instantly impacted most automotive, industrial, and aerospace sectors, which unfortunately is where a significant chunk of Alvant’s market is based. Thompson says that “although we have been able to function throughout, it gave us an opportunity to bring forward part of our business growth plan and has enabled us to forge partnerships with manufacturers who still need to achieve the ambitious capability and sustainability goals, but with reduced internal resources. So, we’ve been quite lucky in many regards.”

Read More:

Moving forward, Thompson explains that the company has seen the biggest emissions decline since the Second World War during the Covid lockdown and explains that stakeholders are more concentrated now on Zero-emissions goals, so have different expectations on businesses. “Sustainability is redefining itself in the post-Covid world, and it has now changed scope, placing non-sustainable materials under far more scrutiny.” Focus has been placed largely on sustainable materials, and businesses are now considering the entire product life cycle and the ability to reuse, which is now very much a consideration in design development at Alvant.

AMCs are more sustainable thanks to the manner in which they separate the fibers from the aluminum at the end-of-life stage. Designers must increasingly factor ‘whole life cost’ into the design, and it’s an area where AMCs score well. “If we don’t use the past 12 months or so as a wake-up call and an opportunity to prompt radical change, then I don’t know what will. We may not get another chance,” Thompson states.

Click here to discover more of our podcasts

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter

About Richard Thompson

Commercial Director (BEng MBA CEng FIMechE)

Richard is a technology commercialization professional specializing in strategic market development, innovation, and new venture growth. Richard joined Alvant in October 2017 from Williams Advanced Engineering, the technology and engineering consultancy division of the Williams Formula 1 Group. He has worked for a range of high-performance engineering businesses and new venture companies developing and commercializing intellectual property. Richard was appointed as Commercial Director in February 2018.",7202
"Super Connect for Good 2021 Meet the Judges exclusive: Marcus Orton, Innovate UK EDGE

As we celebrate the success of Empact Ventures’ 2021 Super Connect for Good competition, we catch up with Regional Partner and Judge for the North of England region, Marcus Orton, Senior Innovation and Growth Specialist, Innovate UK EDGE. Top Business Tech Dec 17, 2021·7 min read

As we celebrate the success of Hays’ and Empact Ventures’ 2021 Super Connect for Good competition, we catch up with Regional Partner and Judge for the North of England region, Marcus Orton, Senior Innovation and Growth Specialist, Innovate UK EDGE.

Hays’ and Empact Ventures’ 2021 Super Connect for Good was a resounding success. The virtual final, which took place in November, saw the regional winners go head-to-head by pitching live for the Super Connect for Good 2021 Overall Champion. We spoke to Regional Partner and Judge for the North of England region, Marcus Orton, The Senior Innovation and Growth Specialist, Innovate UK EDGE. Innovate UK EDGE is an integral part of the grant funding organization, Innovate UK, which focuses exclusively on innovation-led companies with high growth potential. Innovate UK Edge was launched earlier in 2021, and is an initiative dedicated to helping such companies along growth journeys to scale.

Orton joined Innovate UK a year ago as an independent consultant, working with medical device sector advisors and supporting companies with regulatory compliance, risk management, product development, assessments, and services. He was also a managing director of NHS spinout company SwabTech, which sought to bring blood recovery technology to the market. Though SwabTech was unable to obtain the funding needed to enter the market, it brought Orton to the “very exciting” opportunity to work with the Innovate UK team, operating in the North of the UK. Innovate UK EDGE provides a national service, is led by seasoned specialists with diverse experience and international track records at the business-end of innovation.

Orton’s experience in product development, largely in medical devices, notably orthopaedic instrumentation and implant system development, positioned him well as a specialist in the organization. He shares that he joined a team with a holistic set of skills, including marketing, corporate finance, accountancy, local government and public sector, and support services, all from across various industries such as manufacturing, retail, hospitality and science and technology. “We’re an extended team of advisors,” he explains, “we can work either individually or as teams with clients from these sectors.”

Navigating a turbulent climate

Drawing on his experience at Innovate UK EDGE, Orton reflects on the current challenges facing startups today. He notes that startups must balance a focus on present operations while also remaining aware of the future as a business progresses. He shares that many contextual economic factors have impacted companies of all sizes in recent years: Britain’s decision to leave the EU, the Covid-19 pandemic, and the growing need for climate action vocalized at COP26. “These are immediate and long-term components to take into account, let alone rival and competitive competitor influences.

“At Innovate UK EDGE, we are a regular touchpoint and sounding board to what a startup’s or SME strategy is, and how its business plan responds to that strategy and evolves with it, both in the context of the business developing and changing economy,” he said.

As startups navigate this turbulent climate, Innovate UK EDGE provides a funded service “so that the cost to the participants in our work is primarily the time that they would invest in working with us,” explains Orton. Leveraging knowledge from Innovate UK, The Knowledge Transfer Network, The Catapults, and grant funding bodies such as Innovate UK and the EU Horizon project, Innovate UK EDGE provides a wealth of knowledge and connectivity to the businesses they work with, to meet the wider economic challenges head-on.

In the context of the healthcare industry, Orton shares that “organizations are coming from Europe as they see the UK as a very promising economy to be working with. There is more investment going into the healthcare economy, and the demand is increasing, almost as a consequence of how successful our health system is in tackling existing challenges.”

From an environmental aspect, society is taking a more active approach to mitigating climate change, which is reflected in emerging startups. “We’re seeing the increased interest in electric vehicles and alternatives to carbon-based gas, which will enable us to live more environmentally responsible lives.”

For both of these trends, Orton emphasizes, “if individual companies have an idea, they need to realize the work that is needed to bring that idea to market, to evaluate its potential and to engage successfully with relevant stakeholders; be they manufacturers, suppliers, or distributors. These are the themes we see across our work.”

The makings of a great startup

“Creativity is at the heart of a good startup,” says Orton, “And an openness to what is driving them to create a new solution. This can mean a driving passion and perseverance that is internal to the company, but a great startup also needs to have a realistic understanding that a solution can sustainably serve a market need, and that other bodies will engage with the need that they are responding to.” Orton adds that this is an iterative process where startups need to remain flexible as they grow and learn from customers and market peers while remaining open and actively listening to stakeholders.

Scaling up

To those startups looking to scaleup, Orton reminds these startups that many people have been through this process before. “Speak to others who have been through the scaleup process. You don’t have to like everything they say to you, but keep in mind; it’s quite a journey. We’re very privileged to work with a variety of companies in a short amount of time, and from this, we have gained an insight into a multitude of successes and failures, and being able to gain from this is key. It’s important to remember that you’re not on your own in this process.”

“I’ve been impressed by the Super Connect For Good programme. These hosts have a great knowledge of how virtual environments work, run excellent programmes and have provided a platform for some really fantastic businesses.” Orton emphasizes the importance of startups coming together to celebrate these accomplishments and the chance to meet other organizations that will support them in their journey. He adds that the quality of the companies he saw this year has given him hope that emerging technology will certainly have a role in overcoming the challenges that he previously outlined.

Innovate UK EDGE supports startups by facilitating grant funding applications, business cases, and presentations. He describes the quality of the presentations at the Super Connect for Good as sophisticated, but sobering. “While it’s exciting to see such a high standard of presentations, it is also sobering, as so few recipients can be awarded grant funding. This is where our support extends into other aspects of supporting business growth”.

Orton explains that there is a limitation on funding resources nationally, and if there is an successful appeal to the wider parts of the community, that have more resources, be it financial or otherwise, more businesses with much-needed solutions will thrive. “If investors took a lower return or placed more resources into businesses, the nation may see an overall improvement in economic progress.” He continues, “We’d also like to see more government money going into social services, transport systems, and businesses to not only to improve the support for our society and communities, but as an investment to develop and strengthen the next generation of products, services and professionals.”

READ MORE:

About Marcus Orton, Senior Innovation and Growth Specialist, Innovate UK EDGE

About Marcus Orton, Senior Innovation and Growth Specialist, Innovate UK EDGE

An experienced medical device developer, business leader and innovator, Marcus Orton is committed to delivering world-class products, services, business process solutions, sustainable and effective management of resources, founded on experience gained in leading multinational corporations, UK healthcare and academic organizations, startup and SME business settings.

As an innovation specialist, he provides product, business and regulatory development support and coaching. This work draws on work with medical device business teams, healthcare service providers and academic technology transfer and teaching programs. Services include gap analysis, planning, risk management, theme-based workshops, documentation review and direct support for the development of business plans, grant applications and product development projects.

As an NHS spinout, startup CEO, Orton leads the business and product development of an innovative enhancement to surgical blood recovery systems. While it did not succeed in reaching a commercial market position, the work successfully secured grant and investor resources, technology, supply-chain, and IP development. This drew on experience gained in medical device product development, product launch and scale-up delivery and has supported clinical innovation in orthopaedic medical device and surgical instrumentation sectors, the delivery of private healthcare change management, and technology transfer and transformation in academic research and business innovation management roles.

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",9822
"How will tech define the post-COVID era in healthcare?

Bhushan Patil, SVP EMEA, at Tech Mahindra gives an analysis of the ways technology will enhance healthcare following the covid pandemic. Top Business Tech Dec 16, 2021·5 min read

Bhushan Patil, SVP EMEA, at Tech Mahindra gives an analysis of the ways technology will enhance healthcare following the covid pandemic.

COVID-19 was an unprecedented health crisis, which not only put a strain on our healthcare infrastructure, but the way that healthcare is delivered across the world. Thankfully, indomitable human spirit aided by advanced technology has continued to provide solutions at every hurdle, and will be the key to a quick and effective recovery to pre-pandemic life.

While the vast benefits of smart technologies like 5G, artificial intelligence (AI) and the cloud were demonstrated during the pandemic, the sudden influx of patients (including the demand for vaccinations and virtual care) are having a lasting impact on the health sector. The pandemic highlighted just how inefficient paper-based processes and legacy IT systems were and we are now seeing rapid tech adoption by the industry to optimise their systems and processes to leverage these digital technologies to the fullest.

The healthcare challenges

The pandemic brought in four distinct challenges for health systems across the world including the UK. They are: care delivery mechanism, healthcare infrastructure stretched to capacity, supply chain logistics and urgency of rapid discovery of effective treatment.

The need for social distancing affected health professionals’ ability to deliver care. Not only was boosting care beds and staffing critical, but there were also delays in time sensitive treatments. While a massive ramp up was needed to vaccinate entire populations at least twice, severe restrictions on work and travel completely disrupted the global supply chain.

Despite the emergence of Omicron this new year, there has still been a lot of progress made in our ability to deal with the virus, namely due to the advancement of technology across the health ecosystem and huge learnings during the year. The rapid uptake of booster doses in the UK shows behavioural changes in the way patients self-treat and seek healthcare have also had a major impact on the way that healthcare is delivered, making virtual healthcare a priority, especially as uncertainty continues.

Patients want accessible and convenient health services that ultimately save them time and effort. However, as little as 48.4% of UK health organisations’ services were available digitally prior to COVID-19. The pandemic played a key role in pushing healthcare providers to deliver more digital solutions with research revealing that there were more than 11 million telehealth appointments in the lead up to March 2021 alone.

Medical Healthcare Research and Development Concept. Doctor in hospital lab with science health research icon show symbol of medical care technology innovation, medicine discovery and healthcare data.

How technology offered a way out of the pandemic

While teleconsultations acted as a solution to accessing healthcare, over half of patients in the UK want to continue remote consultations as it offers a way to bridge the gap between patients and doctors. Not only this, but it can also help patients with mobility restrictions avoid the need for physical travel. Due to its accessibility, teleconsultations have been one of the technologies that significantly aided both patients and doctors during the pandemic.

The record time that vaccines were developed and manufactured in countries like India, the US and the UK was also another sterling win for scientists and technology. The progress we have seen in vaccine development during the pandemic has been a significant transformation which will revolutionise the process of drug discovery in the future. By using AI in real-time, medical professionals were able to access high volumes of data to identify virus characteristics. This, in turn, has helped speed up the process of data management, analysis, diagnosis and the development of vaccinations.

Tech for the next generation of health

In 2022, emerging technologies such as AI, 5G and the cloud will empower significant developments in the way that healthcare is delivered to patients. With better management of data and process innovation, diagnosis and remote patient care is only at the beginning of its transformation.

Cloud innovation for example, has helped to unify health institutions. The NHS’ move to the cloud has enabled greater agility, scalability and efficiency so that the NHS can be streamlined using its 2018 solution, NHS Digital. Since it was first implemented, the NHS has moved more than 2.1 million NHS email servers to the cloud, representing the largest enterprise email migration to date globally.

A strategy that leverages a single infrastructure to function can greatly reduce costs, improve data security and streamlining management. According to Broadsoft research, 41% of data insufficiencies in hospitals can be attributed to disparate platforms. Meanwhile, 64% of health institutions were able to achieve real results from the benefits offered by unified operations.

5G use cases have also made a significant contribution during the pandemic. 5G connectivity enabled both remote and real-time patient care, which will continue to revolutionise methods used by health professionals as the technology develops. In fact, £3.2 million in 5G funding has been granted to Edinburgh Napier University so that researchers can leverage the technology to develop hearing aids. Further, O2 is also working on a ‘Smart Ambulance’ trial which leverages 5G with an aim to reduce hospital numbers and provide a better quality of care.

Ultimately, low latency, ultrafast connectivity enabled by 5G will be the pivotal function that connects healthcare innovations, making healthcare more efficient and accessible for all.

Read more:

Healthtech transformations for the future

The pandemic exposed gaps in the health sector in regard to delivering the quality of service required at scale. As we tentatively emerge from COVID-19 however, healthtech innovations driven by AI, 5G and the cloud are redefining the playbook. The response to finding solutions designed for diagnosis and treatment were overwhelming, but it is important to maintain the same momentum to create innovative and future-proof health services.

By embracing healthtech innovations as they become available, health institutions can optimise existing systems and processes. The pandemic has proven just how impactful adopting emerging technologies can be, so paying attention to where it can be used next should be a continuous priority for health institutions as we enter the next phase of healthcare.

Click here to discover more of our podcasts

For more news from Top Business Tech, don’t forget to subscribe to our daily bulletin!

Follow us on LinkedIn and Twitter",7027
"Getting remote and flexible working right across the generations

Within this article, Alex Graves, CEO of Silicon Reef, explores how remote, flexible and hybrid working are now part of life. Top Business Tech Dec 16, 2021·5 min read

Within this article, Alex Graves, CEO of Silicon Reef, explores how remote, flexible and hybrid working are now part of life.

85 per cent of managers believe it will become standard to have some remote workers in their team. This has presented huge challenges over the last eighteen months, mainly technical and cultural. As the dust settles, there’s a new problem to solve; making it work across generations and lifestyles.

Ironically, when considering remote and flexible working, it’s often with a one-size-fits-all mindset. Everyone gets the same tech and framework for how they should operate. Yet it’s quickly becoming clear that what works for one age group is woefully inadequate for another.

An early sign of this came at the start of the pandemic when middle aged workers with studies and spare bedrooms settled into a nice new lifestyle, while younger ones stuck in flat-shares balanced laptops on ironing boards and retreated into their bedrooms. There is now a long list of pitfalls that need to be considered if remote, flexible and hybrid working models are to fulfil their huge long-term potential. Each age group and type of worker needs to be taken into account from the younger to the older. From those with caring responsibilities to those without.

Remote working is becoming the new norm across generations

Gen Z Zoomers

Whilst younger office workers are often seen as digital natives who might love operating in a virtual team, 71 per cent of those under 25 say they want to go into the office at least two days a week. They don’t want to work remotely on an exclusive basis. This isn’t just about lacking space in shared housing. It’s also about learning, growing and developing. When everyone was in the office, juniors could be welcomed to the team and it was easier to learn on the job, listening to colleagues, learning from seniors, seeing how things were done.

This calls for two key initiatives. Firstly, collaboration tools need to be used from day one so they can be onboarded and introduced from the start, despite not being in the same room as colleagues. There are simple ways to achieve this, such as new starters creating a profile to be shared on intranets or emailed to the relevant people. They also need to use online training materials and find ways to interact and engage with their peers, using virtual groups to share experiences, thoughts, and to make friends. Digital buddy systems are also a good idea.

Secondly, it’s vital to put clear processes in place, enabled by the tech. This means offering regular interactions, monitoring the quality of work and ensuring feedback is given in a timely and useful manner. This will allow effective supervision and ensure their development is managed.

Thirdly, if this group of workers wants to be in the office, let them! We need to find ways to bring the office and virtual workers closer together, wherever they are.

Parents, carers and gen Xers

While younger workers often have the freedom to have a truly flexible working pattern, those with caring responsibilities might not. This makes remote and flexible working an absolute must. But in taking advantage of these rights, they run the risk of being side-lined in the workplace if it’s not managed effectively by employers.

Parents are keenly aware of this with 29 per cent saying they were concerned their caring responsibilities would make them more vulnerable to redundancy when furlough ended. And they were right to be worried. Catherine Mann, a member of the monetary policy committee of the Bank of England recently said that women who work remotely will damage their careers and be stuck in the lower part of a two-tier system where the real decisions are made in the office.

Employers simply can’t let this cohort of workers suffer this fate. Again, a mix of technology and process needs to be put in place to solve the challenge. It has to be made clear from the very top that people cannot be shut out of conversations simply because they’re not in the office — and anyone seen to be exploiting someone’s physical absence needs to be a disciplinary issue.

Every step needs to be taken to ensure the gap between the office and the remote location is as narrow as possible. Whilst in its infancy, the metaverse might be one avenue to explore. Put simply, this is a virtual world that can mirror our own, experienced through a VR headset. Once in, workers can interact with each other in a manner close to a face-to-face experience.

Boomers

It’s generally the case that older workers are more likely to want flexible and remote working than their younger peers. But at the same time, they’re perhaps less instantly capable of adopting new technologies and software to enable it. We need to be careful not to make wild generalisations, but YouGov data on multi-generational working shows that only 18 per cent of Baby Boomers say they feel informed and confident about new communication technology. In many cases, people like to stick with what they’re used to. This often ends up being email rather than Teams, SharePoint, Slack or any other collaboration platforms.

In this way, it seems that younger people favour the office environment yet are more capable of using remote collaboration software, and older people are more likely to ask for remote and flexible working yet are less able to use the tools that allow it. The irony of this situation underlines the stark need to avoid a one-size-fits-all approach. For Baby Boomers it’s vital to offer hands-on training opportunities, rather than the virtual training that under-25s often prefer. They also need the chance to engage with younger staff so they can share their experience and feel valued.

Flexible working calls for flexible implementation

This huge diversity of needs makes it crucial for businesses to understand people’s workplace preferences and then build flexible frameworks to suit. And while doing so, it’s vital that communication about decisions is crystal clear to avoid one group feeling like the other is getting special or even preferential treatment.

Remote, flexible and hybrid working are now part of life, and we have to make it work across ages and lifestyles. There are now four or even five distinct generations in the workforce, from Boomers to Zoomers. And let’s face it, what’s right for someone in their 50s or 60s is never going to be the same as what 20-somethings need.",6670
"Photo by André François McKenzie on Unsplash

Recently, the cryptocurrency Bitcoin reached 90 percent of its maximum offer. A study conducted by blockchain.com revealed that of the total distribution of 21 million Bitcoins, 18.89 million have already been mined and distributed to the market. The milestone comes almost 12 years after the first block, which consisted of 50 Bitcoins, was dug on January 9, 2009.

Bitcoin founder Satoshi Nakamoto has set the value of Bitcoin at 21 million, making cryptocurrency scarce for controlling inflation that may result from the unlimited supply. Bitcoin is “mined” by miners who solve math problems to verify and validate the transaction block that takes place on its network. It is a process of adding new Bitcoins to the stream. After making a successful transaction set, the miner is rewarded with a Bitcoins block.

It should be noted that every four years the value of Bitcoin mining is being halved. So, when Nakamoto created Bitcoin, the reward for securing a block of transactions was 50 Bitcoins. In 2012- 25 Bitcoins and dropped to 12.5 in 2016 and by the end of 2024 miners are expected to earn only 1.56 Bitcoins. This process is called splitting and will continue until the last Bitcoin is mined.

It may seem that the world’s most popular cryptocurrency is on the verge of collapse, but from the halving schedules, it is predicted that the remaining 10 percent of bitcoins will continue until February 2140, according to blockchain.com.

After reaching 21 million, Bitcoin will be very scarce and miners will rely on transaction funds, instead of block earnings. Miners will start to earn more from the transactions that take place on these blockchains than the mines themselves.

It is noteworthy that Bitcoin is not just a cryptocurrency, but a blockchain network that processes transactions in a distributed platform. Therefore, has a lot more to use than just a crypto asset.",1935
"This December, Braintrust announced a $100 million private token sale to an investment group led by Coatue with participation from Tiger Global, True Ventures, Hashkey, Blockchange Ventures. The infusion of funds is specifically meant “to grow the network and fuel rapid community-led innovation.” This indicates that this investment is less about what Braintrust does, match employers with talent, than how it does it, through community ownership and web3 protocols. In so many ways, this investment is about the future of digital transformation and how corporations will, or in this case will not, manage it.

The Disappointing Good Intentions of Digital Transformation

The digitalize or die wave that has gripped IT and Operations for a decade has only intensified over the last few years as dramatic commercial shifts strain bottom lines and remote work becomes the norm. In 2022, IT budgets are expected to eclipse those of the past 2 years and soar to 2018 levels and beyond.

The majority of this new spend will be on Web 2.0 projects that will fail. A 2020 Boston Consulting Group study concluded that an astonishing 70% of digital transformation projects fall short of their goals even when the organization is fully on board. Companies will fully commit to digital transformation this coming year and, sadly, they most likely won’t actually transform the organization in any significant way, leaving users underwhelmed and the IT team hobbled by an already outdated infrastructure. This failure will cost corporations nearly $111 billion.

Braintrust and the Web3 Approach

Braintrust is a web3-powered 2-sided marketplace currently focused on connecting high-end freelance tech and design talent with top employers. It’s main innovations, however, don’t surround its approach to recruiting, but its approach to operationalizing a business.

Instead of bloating an IT department with full-time personnel and expensive consulting firms, it uses grants and bounties to incentivize its freelancing tech-centric customer base to create value within the platform. The Braintrust platform’s initial use case of high-tech talent matching and job bidding is ideal because it attracts the exact people to the marketplace that Braintrust needs to help build out its core platform. In turn, talent is able to boost their value through increased technical experience in an emerging area, web3, further differentiating their talent supply.

Furthermore, The protocols that govern token ownership, voting, adjudication and eventually features like staking (enabling talent and employers to stake tokens to insure delivery on both sides) resolve user research deficiencies (because the users themselves are shaping the new functionality) and reduce bloat in other departments like legal and finance. Not coincidentally, Braintrust plans to expand its services to these other disciplines in the future, further enabling the ecosystem to successfully shape the organization.

The above information comes from Braintrust’s real-time Network Dashboard where community members can openly track Braintrust’s growth.

The $100 million token investment led by Coatue and Tiger Global, among other things, is an investment in the promise of web3 tech to change the IT and Operations cost structure of 2-sided marketplaces, optimizing the IT and operations spend and making them much more profitable, even as both sides get better deals. Braintrust openly targets what it calls “outdated, fragmented recruiting systems” by incentivizing community members to contribute technology, educational services and social marketing. This reduces friction on all sides, making connections between supply and demand quicker and cheaper to execute. Buying $100m in BTRST specifically “to grow the network and fuel rapid community-led innovation,” will act as a catalyst for this virtuous cycle.

Flipping the UpWork Model

To see how this might work long-term, let’s compare Braintrust’s model to that of UpWork, one of its web 2.0 competitors. UpWork generates revenues from membership and transaction fees charged to both talent and employers, with talent shouldering most of the cost.

Braintrust doesn’t charge talent for its services, instead choosing to take 10% of the contract value from the employer.

Because of this approach, the employer pays more in the Braintrust model, but the overall marketplace share (what UpWork calls the “take rate”) of the transaction is less.

Braintrust’s single-sided fees ecosystem is actually more economically efficient and obviously more attractive to top-tier talent. But will the employers pay more? Braintrust is betting that the employer will because it will want to be where the top-tier talent is. This may have them paying more in service fees, but also getting more for their money. One could also envision overall hiring costs dropping due to less friction in the end-to-end process, creating better employer terms as a result of this efficiency. At a cost of anywhere between $3000 and $4000 to hire a single employee, streamlining the hiring process and gaining access to adequately vetted candidates can lead to sizable reductions in the overall cost of recruiting and impactful increases in net productivity.

Expected Challenges Remain

Braintrust has some challenges. It’s talent pool appears web3 curious but more often than not doesn’t understand web3 itself or it’s role in the recruiting world. The talent is often simply there for globally accessible well-priced contracts with good companies. That in and of itself isn’t bad, but it does necessitate continued education concerning the value of the platform, and it means that the current talent pool isn’t likely to be enough to implement the required changes to the protocol. Fortunately, however, its community-based contribution model is designed specifically to address these shortcomings through education and ambassador grants.

Braintrust and the New Way Forward

Today, corporations are spending large sums on digital transformation largely driven and resourced in house. This is costly and often disorienting as the core business suffers under the distraction of technological pressures. The Braintrust web3 approach provides a solution to this model, creating potential for a win-win between corporations that must update their technological core to survive and the increasingly-empowered high-end talent needed to make such updates possible.",6443
"As technological advancements reach further heights and exceed expectations from decades ago, the rest of the world continues to catch on. In an effort to understand our gradual entrance into the revolutionary web3 era, it is key to look back into the Internet’s history and know where the digital world is headed.

The past & the present: Web1 through Web2

THE INTERNET has been around for over three decades now, with a history that dates back from its origin in the 1990s as the web1. This version of the internet was decentralized with community-governed open protocols. At the time, there was a lot of value generated by the network’s users and developers.

If you were around this period, you might remember prominent web1 sites like MySpace and LiveJournal where users enjoyed simple static websites. These were primarily informational to us at work and beyond. It was all about static websites which only displayed information and databases. You could only see web pages having texts, pictures, and an email by which you can find other newer websites. It was only by then that search engines became a feature.

At this point, the world has yet to know of the miles-deep potential of the internet.

Entering web2 began with the goal of global interactivity and information sharing across users, services, and businesses. It was the next generation of the internet, primarily corporate-run and centralized in its services. This took user experience to the next level through the doors it opened up for extensive features and platforms across web applications.

Among the corporations that have taken the most value out of the web2 era are the Big Tech companies including Google, Amazon, and Facebook. At this time, the internet amassed global success in networking a huge block of the world and it took consumer services to the next level, as it gave birth to many newer markets such as social media and e-commerce that took the multiple traditional industries by storm.

However, technology continues to prove its never-ending pursuit for innovation. With a lot of budding ethical issues around user data and centralized services, the future of the Internet finds its way in web3.

Easing into the Web3 and the future

As a sophisticated hybrid of the previous versions of the internet, web3 combines the web1 ethos of a decentralized, community-governed network with the advanced and modern functions of web2. We are currently living it now and it is gaining momentum in its early stages.

Powered by the open-source breakthrough technology, the Blockchain, web3 is all about the decentralization of data and data servers, internet of value, and access inclusivity through permissionless networks run by tokens that may be fungible and non-fungible (NFTs).

Users and developers of the web3 can own parcels of internet services by owning the mentioned tokens as these give individuals various property rights. For instance, NFTs grant owners the ability to own digital objects and assets that may be in the form of art, photos, code, music, game items, access passes, and beyond.

This new tech frontier is brought by Blockchain technology geared at decentralization, where individuals are entitled to a unique code that enables them to share data across multiple special computers which are then encrypted and infallible from manipulation. This is unlike the soiled centralized data servers in web2 which are massively run by specific giant tech companies such as Google.

As a result of the new innovation, the mechanism by which data is stored and exchanged across the world is revolutionized, affording everyone the right to preserve and protect their privacy. Once more users join the network, the deeper problems brought by previous versions of the internet will soon be mended by the prominence of web3.

All that said, Web3 changes the internet digital game. Beyond gaining back full control over our individual personal data through the decentralization of servers, the cyber world it introduces us to is one where inclusive access, goodwill, and security are generated, and new technologies are adopted with an open mind.

Web3 is the internet’s future, and it has already arrived. It does matter and it is here to stay in the long haul.",4257
"Source: Carterra, BusinessWire

Dec.16.2021, Shanghai

Carterra, Inc. the world leader in label-free high throughput antibody screening and characterization, today announced that the company has signed an exclusive Asia-Pacific and Oceania region distribution agreement with PerkinElmer, Inc. a global leader committed to innovating for a healthier world. With the agreement, PerkinElmer will market, sell, and service Carterra’s flagship LSA high-throughput surface plasmon resonance (SPR) platform and software used for accelerating the discovery of therapeutic antibodies.

​This move will assist Carterra in meeting growing customer demand by leveraging PerkinElmer’s long-standing channel infrastructure in those regions. It will also add additional tools to PerkinElmer’s growing portfolio of science and biologics discovery offerings, which includes its world-leading BioLegend antibody and reagent technologies.

The new agreement builds on the equity investment PerkinElmer made in Carterra last December, which has resulted in several collaborations between the companies involving assay development and software integration projects over the past year.

Commenting on this latest distribution arrangement, Alan Fletcher, Senior Vice President, Life Science, PerkinElmer, said, “We are delighted to add the distribution of Carterra’s highly innovative LSA technology to help our Asia-Pacific and Oceania customers further streamline and advance their biologic and biotherapeutic discovery and development work.”

“Carterra recognized early on that Asia-Pacific and Oceania would be important markets for our game-changing biologics discovery products and we are at the point where we need additional reach and channel power to help us take our customer delivery there to the next level, ” stated Tim Germann, Chief Commercial Officer at Carterra. “We are excited to collaborate with PerkinElmer to add that extra horsepower in sales, marketing, and service as well as benefit from their deep life science expertise across drug development workflows.”

​About Carterra, Inc.

Carterra® is privately held and is the leading provider of high throughput technologies designed to accelerate and improve the discovery of novel therapeutic candidates. Carterra’s LSA® instrument, software, and consumables for monoclonal antibody (mAb) characterization deliver up to 100 times the throughput of existing platforms in 10% of the time while using only 1% of the sample required by other systems. The LSA combines patented microfluidics technology with real-time high throughput Surface Plasmon Resonance (HT-SPR) and industry-leading data analysis and visualization software to revolutionize mAb screening. Carterra, Inc. is based in Salt Lake City, Utah, and has Customer Experience Centers in San Francisco, Salt Lake City, Boston, and Munich, Germany. For additional information, please visit www.carterra-bio.com.",2918
"Export collection from DB A

A is an example of DB name

open the terminal and type. For clarification on the commands type mongoexport — — help on the terminal

Instance 1.

When the DB is not local and it has credentials. edit all the fields in bold.

sudo mongoexport --host=your_IP --port=port --username=your_username --password=your_pwd --forceTableScan --db your_db --collection your_collection --type json --out name_of_json.json

Example:

sudo mongoexport --host=182.158.10.1 --port=2987 --username=mercy --password=mercy_pwd --forceTableScan --db UsersDB --collection Users --type json --out Users.json ///to specify your path

/Downloads/Users.json

Instance 2.

When the DB is local and it has no credentials. edit all the fields in bold. You remove the credential if they don’t exist and change Ip to localhost.

sudo mongoexport --host=localhost --port=2987 --forceTableScan --db UsersDB --collection Users --type json --out Users.json

N.B don’t forget to change the port

To view your document, just check on your present working directory. Type pwd on the terminal.

Import collection from DB A to DB B

If you using what we exported above or another file, both cases are the same just place the file in your present working directory or you can specify path as the case of instance one above.

Instance one with hosted and secured DB

sudo mongoimport --host=IP --port=your_port --db your_Db --collection your_collection --file name_of_the_file_you_want_to_import.json

Example

sudo mongoimport --host=182.158.10.1 --port=3444 --db SuppliersBD --collection Users --file Users.json

N.B if it is authenticated add password and username

--username=mercy --password=mercy_pwd

Instance 2.

Importing from a local insecured(if it is secured, add password as above)

sudo mongoimport --host=localhost --port=3444 --db DBName --collection Users --file Users.json

Hope this helps.

Happy coding!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",1950
"New Distribution Capability (NDC) is a travel industry standard for distributing and selling flight content and implements to airlines, consolidators, and travel suppliers. Altogether, NDC is the capability to distribute travel content in a new way. The IATA developed a New Distribution Capability to enhance communications between airlines and travel suppliers and remodel air products retail to leisure and business travellers.

The IATA developed a New Distribution Capability to enhance communications between airlines and travel suppliers and remodel air products retail to leisure and business travellers. NDC is a standard for exchanging data between airlines and travel suppliers and not just a system, database, or piece of software.

NDC is not simply a communication standard, it is about converting you into an Airline Retailer

Peering through the implementation of the new communication standard to develop communications between airlines and the travel industry, the vision of NDC is well overwhelmed in IATA’s slogan — “Together, Let’s Build Airline Retailing“. Presenting yourself as an airline retailer means developing distribution channels & products, managing the offer, enabling full transparency of the customer experience and product differentiation and, in the process, unfastening total retail capacity for your airline. Airlines spend a fortune on their product; however, how Airlines currently retail does not discriminate against competitors. NDC promotes product offers and rich content to furnish customers and agents with more compelling information to differentiate. NDC is not just a message standard.

NDC, a Distribution Strategy and a complete solution

There is always too much debate on whether New Distribution Capability will or won’t benefit airlines, travellers, and the industry. While it is true that NDC has the prospects to disrupt the GDS’s, it is not practical or even reasonable to believe that they will cease. The notion that network airlines can manage their overall distribution demands with substantial volumes on their own is neither realistic nor achievable. There’s still a position for GDS’s, and NDC will bring the esteemed change of the current capability they present.

NDC supports airlines to take charge of distribution

NDC prompts a reconsideration of airlines inventory administration and channel distribution approach. Several airlines have dropped the control of their inventory and are reliant on 3rd parties both commercially and technically to access it.",2528
"Blockchain technology, or distributed ledger technology, has been a frequent and popular topic in travel for some time now. While it still remains in the early days, akin to the internet in the 1990s, we still strongly believe that the technology holds promise for the travel industry.

Much has changed since we hosted our Blockchain in Travel Summit, where we convened industry insiders and blockchain experts to discuss all aspects of blockchain. For context, when we held the summit back in March 2019, the total crypto market capitalization was $130 billion. Today, the industry has pushed past $2 trillion. That’s over 15x growth in just two short years.

So, what’s the state of play with blockchain in travel today? Has the blockchain’s ability to increase transparency, lower costs, and instant traceability “taken flight”? Let’s dive into some of the projects bringing blockchain to travel.

Blockchain for operations

There are three primary areas that blockchain technology can support airline operations — streamlining processes, improving supply chain management, and reducing payment fees. Airline operations are complex, require many partners, and are mission critical, making it ripe for distributed technology. However, given the scale of incumbent platforms, the change will take time.

Operational processes: Blockchain technology has the potential to unlock a new world of smarter, more efficient operations for airlines. If every department and external vendor had the ability to work from the same data set, labor-intensive processes would not only be seamless, but airlines would have more control over sharing that data across traditionally disparate systems.

What does this look like in practice? Today, AirAsia uses Freightchain to optimize cargo revenues. Freightchain matches airlines with shippers and utilizes digital contracts to enable real-time bookings and fast settlement to optimize cargo capacity usage.

In another example, Deepair leverages smart contracts and real-time settlement so that airlines can augment ancillary revenues by cross-selling content across interline partners. Secure data on the distributed ledger means that these offers can be personalized, at scale, without sacrificing security or violating privacy.

Supply chain: Airlines can also enhance visibility into their global supply chain by tracking components on the blockchain. Capabilities like inventory management and procurement are augmented with greater transparency, tracking, and control of critical aircraft parts or operational inventory.

VeriTX is a digital supply chain for aircraft parts that verifies the origin and authenticity of inventory to ensure compliance with regulations and quality standards. The system also uses 3D printing to reduce the time that an aircraft is out of service by streamlining on-demand printing of verified digital parts.

Payments: Travel companies spend millions each year on payment processing. The settlement times are also lengthier, due in part to the global nature of the industry and the number of payment methods available. Payments can be faster, less costly, and more secure with distributed ledger technology

UATP, an airline-owned payment network partnered with BitPay to enable payments in cryptocurrency. While crypto has not become a popular means of payment in travel just yet, these payment rails are a requirement for serious adoption.

Blockchain for distribution

Airlines, hotels, property managers, and other accommodation providers often struggle with profitable distribution, as it requires a delicate balance between commission-free direct bookings and tapping into the global reach of the most recognized online travel agencies (OTAs) like Expedia and Booking.

In the case of OTAs, an airline typically goes through a global distribution system (GDS) company such as Sabre or Amadeus, which distributes air and hotel plus ancillary products to the OTAs before the final product reaches the end consumer.

This multi-layered process is highly inefficient for both the airlines and customers and is ripe to be streamlined utilizing distributed ledger technology.

To optimize profitability, providers are pushing for more direct bookings. Today, Blockskye is partnered with United Airlines to provide a direct booking solution for business travelers. This distributed platform allows travelers to book directly with the airline, resulting in higher margins for the airline from both the price of the ticket and the reduction of fees associated with the GDS and OTA.

OTAs are also evolving to include travel marketplaces that accept cryptocurrency. Travala allows customers to book over two million accommodations on its platform and is partnered with Binance to allow customers to book trips directly from the exchange. Booking from Binance Pay provides crypto holders the ease of using a single wallet and more options to utilize tokens.

Blockchain for identity

As it has in other industries, COVID-19 has acted as an accelerant, pushing the adoption of blockchain-based identity. This has been one of the most visible and active applications of blockchain in the industry:

IATA’s TravelPass, which uses the distributed ledger technology from Evernym to secure user identities, is being used by Etihad Airways.

Aruba has also built a health app with SITA and Indicio.tech, which allows visitors to share health data privately and securely.

Germany is hosting a pilot across 120 hotels to use decentralized digital IDs to allow employees from German corporations to check-in. This is one of the most prominent pilots to date in using device-stored digital IDs in travel.

Amadeus integrated IBM’s Digital Health Pass to enable secure and seamless health data verification.

What’s next?

Blockchain’s ability to provide scalable, secure, transparent, and traceable data makes it an ideal solution in travel. The industry’s complex infrastructure is perfect for blockchain technology, as it can streamline processes and systems spread across geographies, companies, and even industries.

The challenge ahead is what every emerging technology — and the startup ecosystem surrounding it — faces: adoption. There must be a critical mass of both users and companies piloting the technology. Otherwise, there won’t be significant learnings to improve systems and pave the way for widespread adoption.

Yet, even with the groundwork required to put the blockchain to work in travel, we remain bullish on blockchain’s role in travel. As we navigate global governance, private vs public blockchains, and regulations, the industry will find its path towards enabling a new way to transact, interact, and engage. These questions will be answered with actual usage. That’s the inflection point where travel finds itself: with ongoing pilots, the lessons learned are being translated into better products primed for global service.",6910
"Currently, travelers are required to show their IDs at multiple checkpoints — from entering the airport to the luggage drop-off to buying stuff from the duty-free shops. Further, the long queues at security check-in and border control checkpoints are some of the other key pain points that travelers have to go through. Blockchain technology has the potential to simplify this process and eliminate the need to shuffle your bag repeatedly for taking out IDs and various other documents for verification.

Digital ID on Blockchain

With a Blockchain-based system, a user will be able to create his digital ID, entirely controlled by him. In other words, he can decide who can view his information stored on the ID, and he can even decide what part of the information should be shared with any specific viewer.

For this, first, the user will be required to download the Blockchain-based Digital ID creator app for uploading his personal verification documents like passport, visa, etc., and biometric data like fingerprints, eye scan, and voice. This data will then be uploaded on the Blockchain, and a unique hash will be generated for the user’s information.

Creating digital ID on Blockchain

Now, this data will be sent to a government agency for verification. This agency will check and verify the user’s data against the central database and will either approve it or disapprove it. The verification status of the uploaded documents and ID will get stored on the Blockchain.

When a user needs to take a flight, after reaching the airport, he can then simply go to a self-check-in security booth, where the user will be required to share his verified digital ID. After this, the user’s biometrics like the fingerprint or facial recognition will be captured by the booth and will be verified against the verified digital ID provided by the user. On verification, a QR code will be generated, which can be scanned by the user through his mobile app and can be used in further rounds of verification done at the airport. Thus, saving him a lot of precious time and unwanted hassle to take out his passport and other documents again and again to verify his credentials.

At the self-check-in booth, a QR code is generated after the verification of digital ID, which can be used for further verification done at the airport

In the case of international travel, the user can also share his verified digital ID with the border agencies well before his arrival dates. This will initiate the risk assessment in advance, and the actual process would be much faster and smoother when the user actually arrives at the border site. As the digital ID is government verified, the user can even be saved from the hassle of showing the passport.

Baggage Tracking through Blockchain

What comes to your mind when you think of travel. Packing your bags and leaving for that vacation you have been planning all month long. With your bags ready, you leave for the airport thinking about the wonderful beaches that you will hit or the unconquered mountains you will conquer. But as you leave your bags at the baggage drop-in counter, a slight sense of skepticism creeps in, and you become slightly worried about your luggage carrying all your important stuff and gears. You just wish that it gets loaded on the right plane, and in case you have a connecting flight, your concern reaches the next level. And all of this happens because you have no clear visibility of your luggage. In the current system, there is no way to track your luggage even at a single point during your whole journey.

The main reason behind this is the lack of data exchange among the parties involved in the luggage transfer. The responsibility for every luggage item repeatedly changes during the journey. And when the baggage changes hands, the relevant information is uploaded and stored in their respective local and private systems. This information stored in the local systems is not shared with other stakeholders. Thus, making the backtracking of any luggage difficult and complicated. This is especially applicable to multi-stop flights because an increased number of airports and authorities are involved in the luggage transfer, which in turn makes the data sharing complex and confusing.

When baggage changes hands, information is stored in local systems, which is not shared with other stakeholders

But in a Blockchain-based system, the data stored will be secure, immutable, and accessible to all the stakeholders involved. Each bag will be marked with a unique code or number, and each traveler will be given a corresponding unique number to track their luggage.

When the bag moves through a security scanner or tracking system, the data will be uploaded and recorded on the blockchain. Every stakeholder on the Blockchain can track the entire journey of the baggage. Through this, every passenger can see the location of their bags in real-time. This will give him the mental satisfaction that his bags have been loaded into the right plane, even during multi-stop flights. Further, it will also fasten the tracking of lost baggage as every stakeholder will have a clear picture of the luggage, its last tracking point, and the authority responsible for it. Thus, leading to a significant improvement in the customer experience for the flyers.

Tracking bags in real-time on the Blockchain

Redeeming Loyalty Points on Blockchain

Loyalty points and air miles are some of the very crucial ways through which airlines tap repeated customers. But in the current system, customers have to wait until they have substantial loyalty points accrued in their account. Further, the usage of these points is limited to very specific places, which leads to a lot of points being left unused or expired.

But with Blockchain-based royalty points, the points can be redeemed at various other partner outlets. Thus, increasing the usage and attractiveness of these points to the consumers. Very recently, a Blockchain-based loyalty program has been launched by Singapore Airlines for its flyers. They have developed a digital wallet named Krispay, in partnership with Microsoft and KPMG where their flyers can convert their air miles into units of payment that can be used at partner outlets in Singapore. Customers are provided with a mobile app. Using this app, the customers can convert their miles into units of payment and use these units to pay at registered outlets by simply scanning a QR code.

Flight Insurance Payout using smart contracts

Many airline carriers and their travel partners sell flight delay insurance along with flight tickets. But when the flight gets delayed, we don’t know whom to approach to get our insurance claim. The whole claim process is opaque, and no one knows what steps are involved or what compensation they will get if their flight gets delayed.

But with Blockchain and smart contracts, the process will be simplified. This has been implemented by AXA, where Flyers can buy flight delay insurance by paying a premium. Under this insurance scheme, if the flight gets delayed by more than 2 hours, the smart contract gets executed, and the insurance claim amount gets automatically transferred in the accounts of the flyers.

Flight Insurance payout on Blockchain

For buying this smart contract-based insurance, a person needs to first register their flight details on the service provider’s platform and fill in their identity and account details. Then they can pay and buy the insurance plan. And if the flight gets delayed more than the stipulated time defined in the smart contract, an automatic payout is triggered by the smart contract into the flyer’s bank account. This payout is done without any manual intervention from any of the stakeholders involved. Thus, making it swift and transparent.

Thanks for reading!",7832
"Scraper bots make up the worst of bad bot traffic for the travel industry, with sites witnessing over 90% of traffic attributed to fare scraping. Whilst this activity can be benign or even used for positive means, if uncontrolled it can impact top line revenue, bottom line profits and customer experience.

In a recent webinar, Netacea’s Head of Threat Research, Matthew Gracey-McMinn, and Enterprise Sales Manager for Travel and Tourism, Graeme Harvey, were joined by Ann Cederhall, Travel Technology Specialist at LeapShift.

The panel discussed the effect of the Covid-19 pandemic on the travel and tourism industry, the damage of fare scraping and excess web requests on travel booking sites, and how travel companies can take back control of their look-to-book ratio.

Catch up on all the key takeaways below or watch the full webinar on demand here.

What is the look-to-book ratio?

The look-to-book ratio is the number of requests made per booking on an online travel site.

Requests can be made by humans or bots, and the lower the look-to-book ratio, the better. A low look-to-book ratio means conversions are high from genuine customers browsing the website. However, scraper activity can cause look-to-book ratios to exceed several thousand. A high look-to-book ratio inflates the number of requests versus the number of conversions.

When scraper bots pull information from a website, they create excess web requests which, in turn, negatively impacts your look-to-book ratio. Increased competition driven from the pandemic, and the popularity of dynamic pricing, means this is fast becoming a top threat for the travel industry.

How does fare scraping work on travel booking websites?

In travel, web scraper bots are mainly used to collect fare and availability information by rival companies and aggregator sites, used for price comparison.

But also targeting travel booking sites are scraper bots, used to discover and publicize the availability of products and services such as flights, hotels or car rentals.

Attackers advertise the scraped information at lower price points on a secondary site, motivated by the financial reward of charging commission, stealing personal data, or generating advertising revenue.

Scraping is also often used to gather the data needed for more sophisticated or damaging attacks such as ticket spinning or denial of inventory. Preventing malicious scraper bots can cut out these further attacks early as the attackers do not have the data they need to progress.

‘The attackers who were working in travel have moved out of the space over the last year, but they are now coming back with new skills and techniques learned from other industries [during the Covid-19 pandemic].’ – Matthew Gracey-McMinn, Head of Threat Research at Netacea

What damage is inflated look-to-book ratios causing to travel companies?

‘The cost for an airline of having excess transactions — of having pricing systems being queried — can be very substantial, and also uncontrollable.’ – Ann Cederhall, Travel Technology Specialist at LeapShift

Excess traffic caused by aggressive scraping and high look-to-book ratios negatively impact airlines and travel companies both on their bottom line and on their technical performance.

Business costs

Additional costs (up to millions per year) to third-party services like Metasearch engines and GDS booking fees, which charge based on traffic volumes

Extra costs for SIEM and anti-fraud solutions, again which charge based on traffic volumes

Loss of pricing visibility leading to competitive pricing disadvantage

Misleading analytics from inaccurate number of website viewers interested in a certain product

Loss of ancillary revenues (or “add-ons”) e.g., hotels, travel insurance and car hire

Technical costs

Excessive infrastructure costs (up to 50%) used to serve bots

IT teams stretched to deal with bots away from daily tasks

Slowed website performance leading to negative user experience

Costly downtime in extreme cases

How to prevent scraping, excess transactions and high look-to-book ratios

The travel industry is one of the most severely affected by bad bots and has been since the advent of online travel. As bots grow in sophistication and volume, it is crucial for travel websites to accurately detect and bad bots without affecting good bots necessary for the steady running of your website, and genuine users’ experience.

As Ann Cederhall concluded in the webinar:

‘It’s all about control.’

Download the full guide: Fare Scraping and Excess Transactions: The Real Cost to the Travel Industry

The post How much is fare scraping costing the travel industry? appeared first on Netacea.",4690
"What is Trello?

Trello is a task management software based on the principles of Kanban boards. A kanban board is an agile project management tool designed to help visualize work, limit work-in-progress, and maximize efficiency (Atlassian). Trello has helped millions of customers improve their process management with the help of Kanban boards.

The need of Kanban in SugarCRM

A better way of organizing CRM data

SugarCRM is a great tool to view your sales pipeline, but it is limited by its default views and configurations which struggle to provide helpful insights that can prompt action.

Consider the typical flow for a SugarCRM user:

Open a module. See a list of records. Open a particular record in detail view.

The detail view gives us the option to make changes in the fields which can move an item along the sales pipeline, or which can change the qualification status for a lead, but the process is cumbersome, and doesn’t give us the bigger picture of where we are relative to other records in the same module.

The list view isn’t as helpful as you’d expect either. Let’s take an example of just one module.

In the Opportunities list view:

Can we categorize Opportunities by their assigned user?

Can we change the assigned user to another salesperson?

Can we evaluate which salesperson is prospecting for higher value opportunities than others?

Is there a way to distribute opportunities by their sales stage?

We need to call or write emails to records of a specific category. Can you interact with a record without going to the detail view of each?

The answer is an unequivocal no. The traditional view showing items in a list is not very helpful in either getting overall insights about a record relative to the others, or in interacting with them without going through individual fields in the detail view.

SugarBoards not only works with Opportunities and Leads but with almost all modules

Now, imagine a Kanban board of the same Opportunities module, with each salesperson assigned a column and opportunities distributed among them. The total number of opportunities assigned to each user is mentioned against their name at the top. You have an organized, clear view of all the records, and which one is being worked on by whom. You can even see the major details of each opportunity displayed on each card.

That’s what Trello does. It organizes the records into groups and gives a bird’s eye view of the whole pipeline to the viewer. The tool is used by more than 1 million teams including at companies such as Google, Ebay, and SquareSpace.

Our Solution

In order to make use of the power of Trello boards within SugarCRM, Rolustech has introduced a tool. It works on all sidecar modules, even the custom built ones. It can handle all the scenarios mentioned above and much more. Look for RT SugarBoards on SugarOutfitters for details.

RT SugarBoards can offer Trellio board view of your CRM modules, including Leads

What can RT SugarBoards do for me?

Considering only the Leads and Opportunities for a start, following are a few of the things you can accomplish with the help of RT SugarBoards:

See SugarCRM data drawn on a Trello-style board with the details you need (Name, Likely value, Assigned to and more). See leads by their source, and group them by different criteria of your choosing. Simply drag and drop an opportunity from one column to another to change its status, or drop it in a special box to mark it closed won or closed lost. All the necessary information will be updated for you. Take notes against a record. Associate a calendar event with a record.

The best part is, it not only works with Opportunities and Leads but with almost all modules and can create groups by their respective fields so that you can make informed decisions, plan out strategy and understand what is cooking organization-wide.

How do I get RT SugarBoards for my CRM?

If you use a SugarCRM instance in your business, and would like to reap the benefits that RT SugarBoards offers over the default views of different modules, look for RT SugarBoards on SugarOutfitters, or click here. For any queries, please contact support@rolustech.com.",4177
"Passionate about history, innovative technologies and writing good articles, our next guest in the #WeAreModex series of interviews believes that doing PR in the tech world is nicer than in other industries. If he had the chance, Robert would have liked to be a film director, but for the moment he is smoothly ‘directing’ Modex’s Communications department. Here’s what he has to say about blockchain, journalism, PR and… specialty coffee.

Tell us a bit about your studies: high-school, University, Master’s Degree.

I was born in a small, communist style town in the South-East of Romania, where I grew up and lived, like every young Romanian in search of happiness at the beginning of the 21st century. The funny thing is that I’ve graduated from a mathematics and economy-oriented high-school, and then I made a living from writing.

After high-school I was accepted at the University of Bucharest. I became a student at the Faculty of History, my first passion which, most probably, will stay with me for my entire life. When I was 5 years old, I started learning about Romanian and Universal History. So, for me it was quite easy to choose history in college, because at that time I thought I knew almost everything about this domain. A few months later, I discovered it that I knew nothing, so in a way it was very interesting for me to learn what history represents. I then took the Master’s Degree in History of Modernism, with a dissertation paper about Sexuality in the traditional Romanian society.

Find out more about Robert and his professional path.",1565
"Photo by Prateek Katyal on Unsplash

Social media’s original pitch was that of an effective way to connect with all your family and friends, especially long lost friends. To be fair, that premise — centered around family and friends — was simple, and it worked. I was able to see posts by my uncles, aunts and cousins for the first time — their photos, their jokes 🤣 and their likes and shares. I was able to keep in touch with friends from college. Heck, I even live streamed my wedding on a private Facebook group — and this was well before 2020.

But instead of becoming the fabric that kept communities together, social media has become a substitute for our televisions and radios — they became content creation engines.

And so while they are not solving challenges around personal communications, these content platforms are providing solutions to an entirely different set of problems. They are creating entire industries and jobs that seem exotic to us today, but will likely be economic engines of tomorrow.

Effectively, this does mean that social media is pretty much out of the personal communications business. And not surprisingly people flocked over to apps like WhatsApp, Signal and Zoom in the depths of this pandemic.

One reason that Zoom exists today: Facebook did not think that video calls were worth the trouble. If companies like Facebook and Twitter were really about focused on 1:1 contacts, they would have figured out the business and technology case for video calls. I think the simple truth of the matter is that they are not interested.",1567
"Do you want to cooperate with an outsourcing company, but you do not know which cooperation model to choose? This article will compare the most popular outsourcing models and help you decide which one will be the best for your business.

Location-Based Types of IT Outsourcing

Outsourcing can be divided into two ways, depending on the location or form of collaboration. In the case of the first one, these are:

Onshore Software Development — the outsourcing vendor and its client, are from the same country.

— the outsourcing vendor and its client, are from the same country. Nearshore Software Development — the outsourcing vendor and its client, are from the same continent.

— the outsourcing vendor and its client, are from the same continent. Offshore Software Development — the outsourcing vendor is from a different continent than its client.

For more about these models, read the article: Onshore, Nearshore & Offshore — What Is It?

In this case, the issues you should consider are:

- cultural proximity,

- time zone,

- level of English,

- level of programming skills,

- cost.

When deciding on one of these models, you should compare the differences between specific countries. In this article: Top IT Outsourcing Destinations in 2021 and 2022. In Which Country Can You Find the Best Developers? we compared 11 most popular outsourcing destinations, where we took into account issues such as programming skills, cultural proximity, and average developer salary.

Relationship-Based Types of IT Outsourcing

Another important decision is the type of partnership with the outsourcing company.

Here you have 4 ways to choose:

It’s a critical choice, so we’ll focus on just that division and help you make the best decision for your business in this article.

1. Project-based IT outsourcing

The most classic outsourcing model is project outsourcing, where the outsourcing company is responsible for the entire project. It is its responsibility to deliver the project on time, manage and hire employees. The project team has all the needed professionals to execute your application or software.

In this model, the outsourcing company is responsible for managing risks such as rotations or other factors that cause possible delays. Notably, a project manager is assigned to the project, and he manages the entire software development process. Of course, you will be able to monitor the progress of the work on an ongoing basis, but it is the provider who decides which people to hire or which tools to use to complete the project.

Key benefits:

Saving time — you don’t have to spend time and energy recruiting, onboarding and managing the team responsible for delivering the project.

— you don’t have to spend time and energy recruiting, onboarding and managing the team responsible for delivering the project. Cost optimization — when you decide on such a cooperation model, you know exactly how much it will cost. You do not have to worry about salaries, benefits, office or administrative costs.

— when you decide on such a cooperation model, you know exactly how much it will cost. You do not have to worry about salaries, benefits, office or administrative costs. Less responsibility — the supplier manages all the risks connected with the project, and he is responsible for delivering the project on time.

When to choose a project-based outsourcing model?

Choose this solution if:

Your IT team is busy with other tasks.

You don’t need such a large IT team permanently as you would need for this project.

You don’t have your own internal IT team.

Your project requires a lot of experience and knowledge, which your company doesn’t have.

You want to focus on other tasks and pass the entire responsibility to the provider.

You need a specific thing to do (e.g. application, software, functionality), but you don’t need a whole team to maintain it in the future.

You are the founder of a startup, and you don’t yet have the knowledge and experience to do this project internally.

2. Staff Augmentation

Staff augmentation is adding external specialists for the internal team through an outsourcing company. You can choose the specific people you want to work with by reviewing the resumes offered by the vendor. It is usually temporary cooperation chosen by companies that need support in implementing larger projects. In this model, team and project management is on your side. You are also responsible for delivering the project on time.

[ Read also: What is Staff Augmentation? ]

Benefits of staff augmentation:

More control over the project — external employees are an integral part of the team and work on your terms.

external employees are an integral part of the team and work on your terms. Quick fill of staff shortages — you don’t waste time on internal recruitment

— you don’t waste time on internal recruitment Flexibility — you can scale your team up or down at any time.

— you can scale your team up or down at any time. No administrative costs — people who join your team can be hired directly by the provider.

— people who join your team can be hired directly by the provider. Increased productivity — you get programmers dedicated exclusively to your project.

When should you choose staff augmentation?

This model is the best choice if:

You have your own IT team working on a project but need temporary support to finish your work faster.

You have problems finding qualified specialists with niche skills to join your team and care about time.

You want to have an impact on what kind of people take part in your project.

You want to have control over every aspect of the project.

3. Managed team

This model involves hiring an external team of developers to perform a specific scope of work. This team does not work directly with the internal team as in the case of staff augmentation but reports its tasks to the PM, CTO or the client. Like the augmentation model, you get specialists entirely focused on your product, but they do not become part of your internal team.

It is an intermediate solution between staff augmentation and project outsourcing.

What kind of specialists can you hire in this model? The number and roles depend on your project. It may be a complete IT team, or it may be, for example, a dedicated testing team. The outsourcing provider should be able to select specialists according to your requirements.

Benefits of a managed team:

More control over the process — in this model, you communicate directly with all project members, and they report directly to you.

— in this model, you communicate directly with all project members, and they report directly to you. Better communication — members of a dedicated team usually know each other well and can work together effectively.

— members of a dedicated team usually know each other well and can work together effectively. Reduced risk — a dedicated team has experience in delivering the solution you need, so there is a greater chance of success on this project.

— a dedicated team has experience in delivering the solution you need, so there is a greater chance of success on this project. Time savings — you don’t have to waste time recruiting additional people to your team. What’s more, if you don’t have time to manage, you can assign an external PM responsible for managing the team

When should you choose a managed team?

This model is the best solution when you need to perform tasks that you don’t want to involve an internal team in, or you need specific skills that your employees don’t have and the company doesn’t want to invest in.

4. Body leasing

Body leasing is a model in which an external supplier finds an employee for you. It is the best solution for companies looking for an employee with specific skills and experience temporarily or for a particular project. You can even enlist the support of this specialist within a specific time frame — for example, on specific days of the month when your internal team needs additional support.

The person who joins your team in this model signs a contract with the outsourcing provider, so you don’t have to worry about administrative issues or providing them with full-time work assignments.

[ Read also: Body Leasing — 3 Points to Consider While Choosing This Outsourcing Model ]

Benefits of IT body leasing:

Time savings — instead of recruiting an employee for your team, you can focus on other tasks.

— instead of recruiting an employee for your team, you can focus on other tasks. Flexibility — supporting the person hired in this model is up to you. You decide when and for how long you need their support.

— supporting the person hired in this model is up to you. You decide when and for how long you need their support. Convenience — you do not have to worry about administrative issues and finding tasks for this employee to fill his full-time position.

When should you choose IT body leasing?

If you need to quickly find IT specialists with specific technological skills, but:

You don’t have time to recruit them.

You need occasional and temporary part-time support.

You are looking for a specialist with niche skills.

Are you looking for qualified IT professionals to work within one of the above models? Contact us and tell us about your needs.",9284
"Almost nothing. Self-driving cars are insanely cheaper to operate than a normal car

The Origin, a self-driving car with no steering wheels or pedals, designed by GM-owned Cruise

This is the first in a series of stories in which I will show you just how HUGE the impact of AVs will be in our lives.

We’ll begin by looking at how much it would cost for a self-driving car to take you from Downtown San Francisco to Ocean Beach, on the city’s western end — and how this compares to the costs that an Uber or Lyft driver would face on the same trip.

Before we start, let’s agree on how to calculate these costs

First, we’ll only include variable costs, that is, those that occur directly as a result of the ride. We’ll ignore fixed expenses like software development, because they are not causally linked to the trip.

Second, we’ll include the following variable costs: electricity, depreciation, insurance, cleaning, maintenance, and driver cost. We won’t include customer service cost, as these are harder to estimate, and usually low.

Third, we’ll choose the Cruise Origin as our benchmark for AVs. The Origin has no steering wheel or pedals, and is expected to be released in 2023 by Cruise, a General Motors-owned company. Even if you wanted to, you couldn’t drive it, so it’s the quintessential autonomous vehicle.

Fourth, our non-AV benchmark will be an Uber driver riding a Chevy Bolt. The Bolt is electric, and choosing an electric car allows for a fairer comparison, as the Origin is battery-powered too.

Fifth, we will focus on cost, and not on price. Cost is a measure of the resources that are consumed when making the drive, and is straightforward to calculate. Price is how much the service provider (Uber or Cruise) will charge you for the ride, and is more complex to determine.

Sixth: We’ll assume that both cars have a 60% utilization, which is approximately what we see in New York City. They’ll be driving with a passenger 60% of the time, while the other 40% will be “empty” miles in which the cars will be driving to pick up a new passenger or waiting for a new call. We’ll assume that the 40% is all driving, and that there is no “parking” involved.

Finally, we’ll use estimates and common sense when we cannot find reliable data for any given cost component.

Alright, now that we have set the ground rules, let’s dive into the costs.

San Francisco, California, and our simulated ride, from the Civic Center to the Dutch Windmill at Ocean Beach

COST BREAKDOWN

Electricity

Our trip from Downtown to the Beach is 5.7 miles long (or 8.5 miles including the “empty” distance driven to the next passenger).

The Origin will probably need around 29 kwh / 100 miles (similar to the Bolt), so that’s 1.65 kwh for the trip. We’re only missing the electricity cost, which in San Francisco is about $0.25 per kwh. That gets us to $0.41 for the trip, and adding the “empty” distance to the next rider, 3.8 miles, we get to $0.62.

That’s pretty cheap. If you drove this same route with a Chevy Malibu, which does about 24 mpg, it would cost you about 1.41 cents at the current California gas price of $4.5 / gallon.

But remember: our Uber driver has an electric Chevy Bolt and gets the same 62 cents. So the AV cost advantage is not really related to the electricity cost.

Tie between Uber and Cruise at 62 cents

2. Depreciation

This is where the Origin starts pulling ahead. According to Cruise, the Origin will cost about $50,000 and last about one million miles before it needs to be retired. That’s a lot more than the useful life of the Bolt, which is 400,000 miles.

Quick math shows that the depreciation cost per mile is 5 cents per mile ($50,000 / 1,000,000) and that the depreciation cost for our trip to the beach is then 5.7 * 5 = 29 cents, or 44 cents with 60% utilization.

If we drove the Bolt, which costs the driver $33,000, the cost would be 71 cents. Advantage Origin.

Cruise beats Uber, 44 cents vs. 71 cents

The San Francisco Civic Center, our starting point

3. Insurance

This is another cost line where AVs will crush competition, if we make one simple assumption: that self-driving vehicles will never be at fault for a traffic accident.

Wait, what? That’s kinda hard to believe, right? Well, it’s closer to the truth than you may think. Data from Waymo — Cruise’s Google-owned competitor — can help us.

Waymo’s cars drove 6.2 million miles autonomously around Phoenix, Arizona, from January 2019 to September 2020. During this time, the firm’s cars were involved in 47 collisions and, according to the company, nearly all of those were not Waymo’s fault. How many does “nearly all” mean? Waymo doesn’t specify, but I’m guessing no more than 2 or 3. Which would basically mean 4–6% of all accidents. That’s very close to ZERO, and, to simplify, we’ll call it zero.

What type of insurance do app drivers and Cruise need?

With this controversial but credible assumption, we can move on to understand the two insurance policies we might need:

Liability coverage for app drivers: California requires Uber drivers to carry $1 million in liability insurance to cover the passengers and property involved in an accident. This covers accidents where the insured Uber driver is to blame for the accident, so the Origin doesn’t need it.

California requires Uber drivers to carry $1 million in liability insurance to cover the passengers and property involved in an accident. This covers accidents where the insured Uber driver is to blame for the accident, so the Origin doesn’t need it. Uninsured /underinsured driver: When someone other than the app driver is responsible for the accident, the coverage above does not apply. But maybe the other party at fault doesn’t have insurance, and can’t pay for the damages of the crash. To guard their users against such risk, Uber offers this “uninsured driver coverage”, even though this is not required by law. Cruise should too, because as long as there are bad drivers sharing the road with the Origin, this risk still exists.

How much does all of this cost? According to Allstate, which offers insurance to app drivers in California:

Liability coverage adds up to $1,248 per year for 50,000 miles, or 14 cents for our 5.7-mile trip, 21 cents with the “empty” miles. Again, Cruise doesn’t have to pay for this, but Uber drivers do.

Uninsured / underinsured coverage costs $360 a year for 50,000 miles → 4 cents for our trip, or 6 cents including the “empty” miles. Both Cruise and our Uber driver need to pay for this.

Cruise beats Uber, 6 cents vs. 27 cents

Ocean Beach, our destination, on the city’s Western End

4. Maintenance

The Origin and the Bolt need to be maintained from time to time so they can safely tranport passengers. There is no data available on how much it will cost to maintain the Origin…but we do have a good estimate for the Bolt. That comes from Edmunds, a well-known car-review portal.

We also know that, as a rule of thumb, the more it costs to build a car, the more it costs to maintain it. And we know the Origin’s and the Bolt’s production cost: $50,000 and $29,000, respectively. (You’ll pay $33,000 for a new Bolt because GM and the dealer need to make a profit, but $29,000 is what it costs to make it).

Now for the actual cost: Edmunds expects you to spend $3,021 on maintenance for every 75,000 miles driven on your Bolt, or 23 cents for our trip, 35 cents with the “empty” miles. The Origin costs 72% more to produce, so we’ll just assume that the maintenance costs are 72% higher → 60 cents including the “empty” miles.

Uber wins, with 35 cents, vs. 60 cents for the Origin

5. Cleaning

From time to time, Uber drivers need to vacuum and wipe the vehicle’s floor and seats to keep it looking and smelling fresh. Less often, they will wash the outside too, to give the vehicle that shiny new car look.

How often do they do it? The Uber sub-reddit seems to indicate once a day for the interior, and a once a week for the exterior. The same would apply for Cruise, as their passengers will still demand a spotless vehicle.

For the costs, let’s assume that a car drives about 150 miles per day:

Interior cleaning: 15 minutes per day + $1 of cleaning products per day. With a cleaner costing about $20 / hour, that comes to $6 / day, 4 cents a mile → 35 cents per trip with “empty” miles

Exterior: A standard cash wash is about $30, or 3 cents per mile → 25 cents per trip

The total adds up to 40 cents per trip, which is a lot! More than insurance, and almost the same as electricity.

If I were to be super rigorous, I’d say that the Origin has a small disadvantage over the Bolt: its exterior sensors, like LIDARs, cameras and radars, need constant cleaning, because it relies on them to “see” the road and drive safely. How often is this required? Maybe once a week? Maybe never because they have some “auto-cleaning” function? Not sure.

But we know that the Origin is already going to be cleaned once a day, so the marginal cost of cleaning the sensors would be very small. It’s just a matter of spending an extra 2–3 minutes wiping them. We’ll call it not relevant for our calculations.

Uber and Cruise tie at 60 cents per trip

Waymo’s vehicles like this adapted Jaguar have been roaming the streets of San Francisco for a while. Notice the driver inside, who can still take over if needed. This won’t be possible — or hopefully needed — in the Origin

6. Driver cost

You guessed it: it’s zero for Cruise. The company is spending a lot of money on developing its self-driving software, but this is a fixed cost. Once the program is installed in the car, it doesn’t cost anything more to take passengers from one point to another.

For an Uber driver, it’s a different story: Everytime they ride, they expect to earn a wage. It’s how they make a living, afterall. So there is a cost here. We’ll calculate it based on the trip length, utilization rate, and on the wage that a driver would demand. Let’s dive into the math:

The trip would take 31 minutes at the SF downtown average speed of 11 mph…

…but the drivers expect to be paid for more than just these 31 minutes. They are only carrying a passenger 60% of the time they are working, but also want money for the other 40% of the time. If we didn’t factor this time in, we’d get a much lower pay, one at which drivers would prefer to work somewhere else. Factoring in the 40%, the actual duration of the trip is 52 minutes.

At $16.32 per hour, which is the San Francisco minimum wage, these 52 minutes cost $14.06, vs. ZERO for Cruise. That’s an insane advantage. Let’s discuss it below.

Cruise beats Uber, $0 to $14.06

COST SUMMARY, PRICING AND IMPLICATIONS

Cost summary

Adding up everything, the total cost for an Uber driver to move you from the Civic Center to Ocean Beach comes to $16.60. Out of this total, $2.54 comes from non-driver costs, and $14.06 from the driver.

Summary of all trip-related costs. The ‘Driver’ line is the main difference between the two alternatives

For a self-driving vehicle, the cost for the same trip is $2.31, all from non-driver costs. That is 86% lower than the Uber driver.

86% is A LOT. Like really a lot. It’s all the more significant because transportation is a very significant expense to all of us. The average American family spends $9,600 on transportation a year, the second highest budget category behind housing. Imagine if I told you that in 10 years, all houses would cost 86% less. You’d call me crazy, for sure.

Pricing

But let’s remember that it’s one thing to calculate the variable cost of the ride, and another one to calculate how much Cruise or Waymo will charge us for it. They are companies and want to maximize their profits, so we can expect to pay more than $2.31.

Exactly how much? This will be the subject of my next post. To calculate the price, I will borrow concepts from microeconomics like demand curve, marginal revenue and profit maximization. Another key aspect of price calculation is the variable cost of the ride — and this is the reason why this text focuses on variable costs.

But while we are still not sure about the final price, one thing is certain: it will be much cheaper. This brings powerful implications to the transportation world as we know it.

What does this lower cost even mean? Implications

At the very least, lower prices will drive a lot more people to use AV ride-hailing services vs. other alternatives like buses, the subway, or even walking or cycling.

More demand means more cars on the streets, and potentially more traffic jams. But it’s not all bad: it also means less public space reserved to parking, as AVs will either be parked in their company’s garage when they are charging or undergoing maintenance work, or not parked at all.

To understand exactly how much more traffic, or how much less parking space is needed, we’ll first need to understand how many people will use AVs. And this number depends on how much Cruise or Waymo will charge for their services.

So see you on our next post about pricing!",13040
"While bitcoin has been around for quite some time now, adoption hasn’t been as fast and, when it comes to gaming and gambling, digital currency is a relatively new player.

It’s catching on pretty well, however, and is now a huge trend in the industry as bitcoin is accessible to anyone with access to the internet.

As with everything else, there are advantages and disadvantages to using bitcoin to fund your gambling. We take a look at some of the pros and cons right below.

Related Post: Sports Fan? Hacks for getting the best sportsbooks deals and earnings

Pros

Gambling with bitcoin gives tech-savvy gamblers an edge considering how fast transactions can take place via blockchain technology. While other payment methods are dependent on banks or other third parties for processing, BTC is decentralized and offers transparency. Transactions are quick and can be facilitated from anywhere in the world.

They’re also anonymous and all that needs to be submitted is a bitcoin wallet address. Anonymity is a big thing in today’s society as it’s quite hard to stay off the radar in the current technological era but the use of bitcoin provides just that. BTC also eradicates the need for additional fees that would take several business days otherwise.

This makes bitcoin one of the most trusted and reliable methods of payment.

Cons

There are plenty of good things to say about bitcoin but it is far from perfect and isn’t always the best way to go. One should put some time into researching the coin and cryptocurrency in general before acquiring it but it’s a pretty great asset to have. Finding the right wallet could be pretty challenging but there are plenty of good ones out there.

Getting the best wallet for you and learning how to store your BTC is of the utmostOF importance. Bitcoin can be acquired through crypto exchanges that also provide storage but it’s best stored in an external wallet.

Also Read: What Your Small Business Needs to Invest in to Grow

One of the biggest concerns when it comes to BTC and other cryptos is volatility. Some digital currencies such as USDC and Tether do not change in value as they’re always equivalent to the dollar but bitcoin has proven to be pretty wild. It was valued at less than $20,000 per coin in early 2020 to over $64,000 in early 2021 before crashing on the back of a tweet from Elon Musk. It saw another all-time high towards the end of the year, climbing to over $68,000 but was just below $47,000 at the time of writing.

Also, the anonymity factor mentioned above doesn’t necessarily mean it’s great for gambling sites as many of them are required to complete KYC and AML checks. So, while your activities cannot be tracked on the blockchain, they can be on gambling sites.

How can you gamble with bitcoin?

Bitcoin offers holders plenty of options when it comes to gambling. One of the most popular is online sportsbooks. The demand for sportsbooks that accept BTC and other cryptos is growing and there are already several top bitcoin sportsbooks to choose from including Bookmaker, BetOnline, Heritage Sports, Bovada, and Nitrogen Sports.

Also Read: 7 Top Digital Marketing Strategies for Online Gaming Sites

Sportsbooks that accept cryptos are becoming commonplace given that the crypto space has forced its way into sports, sponsoring teams, and securing naming rights to arenas. One can use bitcoin to bet just as they would with regular cash, placing punts on moneyline, point spreads, or totals in traditional fashion.

“The best online sports betting sites offer a broad selection of betting options on big games. You should be able to bet on alternative spreads and totals, and lots of game props and player props, plus halves, quarters and periods, and futures betting markets,” Bookmakers Review reports on that note. “The top online betting sites have sophisticated live betting products offering a range of in-play markets, with live streaming or stats and infographics. You should find features like cash out and a same game parlay builder too. The best Bitcoin betting sites include these sportsbook features, while furnishing their customers with a variety of exciting wagering options.”

There are also bitcoin casinos popping up all over. Initially, they would have converted regular cash into BTC then allow you to play various games or you’d still see FIAT figures while playing but then have your bankroll updated in BTC.

Also Read: 5 Things you must Know about Blockchain

Now, most casinos offer direct access to bitcoin games, with the transition from FIAT to BTC and vice versa a lot smoother. There’s also the option of playing with mBTC, or millibitcoin that allows for better management of one’s money via the platform.

Bitcoin casinos are ahead when it comes to crypto gambling. Users can find no end of games with which to entertain themselves and win money.

Bitcoin poker rooms are also a thing nowadays and gamers could find Texas Hold’em or Omaha Hi-Lo games online that offer BTC options.

The gameplay will be the same as one would be used to playing in a regular online poker room, just with the added option of fronting crypto.",5147
"A Complete Beginner’s Guide to 3D Printing

General tips for everyone and specific tips for Prusa I3 MK3S​ users to get started with 3D printing Parmin Sedigh Follow Dec 31, 2021 · 6 min read

Photo by Kadir Celep on Unsplash

Watching an entirely new object being created out of some plastic is extremely satisfying, and it’s something I’ve been fortunate enough to witness through the magic of 3D printing.

Ever since I found out that my local library has a 3D printer, a Prusa I3 MK3S​ to be exact, I’ve become somewhat obsessed with 3D printing. I’m by no means an expert but as a complete beginner who has had to figure out how to find good 3D models, slice them, and print them, I think it’s important that I share my experience. So let’s dive into all of these topics and learn how to 3D print.

Note that I will be discussing the Prusa I3 MK3S specifically as that is what I have used but the first few topics I’ll discuss (about dealing with 3D models) apply to any printer.

Step I: Finding a Model

Naturally, the first step is to find or create a model you’d like to print out. This could be anything from a figurine of your favourite character to something functional like a keychain or a prototype. The choice is yours.

Gif by author. A Prusa I3 MK3S at work.

I’ll walk you through my personal experience; I wanted to print out a model of the human brain, but I didn’t want to create my own model as I was sure that there must be some suitable model out there. The 3D community is very active and so after some searching, I found exactly what I was looking for.

So how did I find it? Through lots of searching on websites like…

Thingiverse : As far as I know, this is the most popular website for finding 3D models. It has lots of users, and even better, many of them comment after printing out the models, giving you clues as to what you should do. For example, I learned from a comment that I should make a certain part of the model slightly smaller so that it would fit well with the other parts. Plus, the search function of the website is quite strong with good filters and categories. Oh, and did I mention that all the models are free ?

: As far as I know, this is the website for finding 3D models. It has lots of users, and even better, many of them after printing out the models, giving you clues as to what you should do. For example, I learned from a comment that I should make a certain part of the model slightly smaller so that it would fit well with the other parts. Plus, the function of the website is quite strong with good filters and categories. Oh, and did I mention that all the models are ? MyMiniFactory : This is a great place to look in case you don’t find what you’re looking for on Thingiverse. It has some very unique models but beware that some of them do cost money .

This is a great place to look in case you don’t find what you’re looking for on Thingiverse. It has some very unique models but beware that some of them do cost . Pinshape : Similar to MyMiniFactory, there are some great free models as well as some that you need to pay for. On the plus side, if you decide to begin making models yourself, you can sell them on the platform.

: Similar to MyMiniFactory, there are some great free models as well as some that you need to pay for. On the plus side, if you decide to begin making models yourself, you can them on the platform. YouMagine: Though I don’t find the search function of this platform very robust, it’s great if you don’t have a specific model in mind and simply want to find something to print. They have a way to look at their Popular and Trending designs as well as different collections such as “Quick Prints” to get you started.

What if you’d like to create your own model instead? That’s where I’d recommend finding a different resource as it isn’t my area of expertise. I have, however, heard that Tinkercad is a great platform for beginning your 3D model creation journey.

Step II: Preparing Your Model

When you’ve downloaded your model, chances are that it’s either a .stl or .obj file, both common file types for 3D models. But you can’t print this file type directly on the Prusa I3 MK3S and we need to go through a process called slicing.

Before we get there though, we first need to resize the model (if you’d like to). Going back to my example of the brain model, I wanted to make it slightly smaller. And I did that using a free program called Ultimaker Cura. You can download it here.

Here are step-by-step instructions of what you’ll do next…

Gif by author. A demonstration of how to resize and slice using Ultimaker Cura.

Start by launching the app. Select your printer by clicking on the dropdown menu on the left. Then, click “Add printer.” If you’re using a Prusa I3 MK3S, click on “Add a non-networked printer” and then find your printer. This makes it so the correct printer bed size is displayed and that helps you set the right dimensions for your model. Drag your .stl or .obj file into the app and watch it appear. Click on the model to select it. Then, move it around if you wish to. To resize, click on the second option from the left-hand side menu. Make sure “Uniform Scaling” is on and then resize how ever you’d like.

Once you’re done with this, we can get to the slicing. All this is doing is changing the file type from a .stl or .obj to G-code which is the only file type the Prusa I3 MK3S​ can print from.

Before we slice it though, you should insert your SD card into your computer. Another limitation of the Prusa I3 MK3S​ is that it can only print from SD cards. We also need to ensure that the SD card is formatted in FAT32. If you’re a Windows user, you can find how to do that here and if you’re a Mac user, click here.

Once that’s done, we’ll return to Ultimaker Cura. See that blue “Slice” button in the bottom right corner? We’re going to click on that. If your SD card is inserted, you’ll see an option to directly save to the SD card. And voila, your file is ready to print.

Step III: Printing

With all of that preparation out of the way, it’s finally time to print!

Let’s begin by cleaning the printer bed. You can use isopropyl alcohol for this.

Now you can load the filament. I’m using PLA (a specific type of filament) so that’s what you’ll see me selecting. You’ll first place the spool of filament on the handle on top of the printer. You’ll then cut the end of the filament at a sharp 45-degree angle. After that, select “Load filament” from the printer screen and choose the filament type you’re using.

Once the extruder (the part of the printer that extrudes the filament) has reached the correct temperature, the screen will prompt you to insert the filament. No need to press the filament in too hard, just guide it until it’s being pulled in by the printer itself.

Video by author. A demonstration of how to load filament.

Next up, the printer will begin extruding the filament, hoping to clean out the previous colour that was in there. Just follow the instructions on the screen until you’re only seeing your new colour.

Once that’s all done, it’s time to insert your SD card! Once inserted, the contents of your SD card will be displayed on the printer screen. Use the knob to select the correct file. Then stand back and watch as your model is printed.

Once done, gently remove your model and unload the filament (unless you plan to use the same filament for another print within the same printing session.) Select “Unload filament” from the printer screen and then the filament type. When the printer is ready, you’ll be prompted to gently pull out the filament.

Video by author. A demonstration of how to unload filament.

That brings us to the end! In case you’re curious, here’s a final picture of my completed model of the brain. (The model is from Thingiverse as mentioned before and was created by Laweez.)

Image by author.

Now it’s your turn. Go out there and create some cool things using 3D printers.

Additional Resources

Here are a few other good resources to help you master the art of 3D printing.",8076
"When you hear 3D Printing & Additive Manufacturing you thought both different things

Right?

Yes, I also Thought like this. But, We are wrong here, 3D Printing is an Additive Manufacturing process that brings your idea to life.

There are many parameters to choose from different 3d printing technologies such as type of material, Application, Lead time, Surface finish, Cost, etc.

How Does 3d Printing Work?

When we go for 3d printing there are mainly 3 printing stages were First, You need to create 3d object file for your innovative idea. Then You need to choose the best 3d printing technology & materials for your idea. When Printer Completes the 3d printing You need to work on post-processing like making 3d object as you thought.

3D Printing Technologies

There are many 3d printing technologies such as fused deposition modeling (FDM), Stereolithography (SLA), Selective Laser Sintering (SLS), Direct Metal Laser Sintering (DMLS), Digital Light Process (DLP), Multi Jet Fusion (MJF), PolyJet, Electron Beam Melting (EBM), Continuous Liquid Interface Production (CLIP), MultiJet printers, binder jetting, Selective Deposition Lamination, Selective Deposition Lamination.

Vexmatech provides 3d Printing Technology

Stereolithography (SLA)

SLA 3D Printing Technology is an industrial printing technology that prints objects layer by layer. Mostly, Resin 3d printing material is used for SLA technology. There are UV Laser print pre-designed shapes on the surface of the photopolymer vat. The resin photochemically solidified, single layer formation of 3d objects because Photopolymers are sensitive to UV lights. Resin & Wax kinds of material can be used for SLA 3D Printing.

Fused Deposition Modeling (FDM)

FDM 3d Printing is a very popular technology in the market. FDM is like its Name It’s fused material layers into One object. It worked both horizontally & vertically side. It is also cheap 3d printing. In this technology, 3D Printer melt material is fused into objects by a nozel. It is useful to Multicolor & Complex 3d Printing. Plastic & Alumide type of material can be used for printing.

Selective laser sintering (SLS)

SLS 3d Printing is a laser sintering 3d printing technology. It prints object by sintering powder material & fusing them together. It will print your 3d object layer by layer. Alumide & plastic kind of materials Can be used for SLS 3D Printing.

Direct Metal Laser Sintering (DMLS)

DMLS 3D Printing It is a laser sintering 3d printing technology. It prints object by sintering metal powder material & fusing them together. It will print your 3d object layer by layer. The metal material can be used for DMLS 3D Printing.

DMLS 3D Printing It is a laser sintering 3d printing technology. It prints object by sintering metal powder material & fusing them together. It will print your 3d object layer by layer. The metal material can be used for DMLS 3D Printing.",2910
"e

Augmented Reality (AR) is one of the technologies that nowadays has experienced a boom in its expansion and that is showing to have an impact not only in industrial fields but in people’s lives in many ways. This technology aims to enhance one’s experience by involving overlaying visual, auditory, or other sensory information onto the world. Based on a report by ResearchAndMarkets.com’s the AR market was valued at USD 14.7 billion in 2020 and is projected to reach USD 88.4 billion by 2026. Here we can add that the key factors driving the growth of the AR market include surging demand for AR devices and applications in healthcare, growing demand for AR in retail and e-commerce sectors due to COVID-19, rising investments in the AR market, increasing demand for AR devices and technology in the global automotive industry, and many other industries as well. As AR technology finds new applications day by day, saves costs, and helps to keep the business continuity, demand and investment in AR will continue to grow. AR application is diverse, so let’s see below some of its existing applications in emerging market industries:

1. Automotive

Many businesses operating in the automotive industry have successfully embraced new technologies such as AR, as a need to change the way of operations and to build a bridge of collaboration between experts and field workers. Nowadays with the help of AR-based remote support integrated into smart glasses experts are able to make live annotations that overlay into real-world during work, field workers can work hands-free without distractions while receiving instructions on how to assemble a specific machine. With this AR-based remote collaboration, the accuracy of field workers can go up to 96% and they are able to work 30% faster. In addition, AR remote support has demonstrated great potential for fieldworkers' training. Connected in real-time with an experienced expert, field workers from different locations all around the world, are trained for different assembly tasks before they start performing their work. Training field workers to perform their job correctly have productive results in fewer operator errors that eventually cause slowdowns and downtime.

2. Manufacturing

Given all the many benefits that remote support offers, the manufacturing industry is utilizing AR technology in order to stay relevant, innovative, and competitive. AR-remote support is one of the main tools that help operating businesses in the manufacturing industry to achieve functional goals such as reducing costs, improving efficiency, increasing safety, and expanding production capacity. In manufacturing businesses, production downtime due to machine breakdowns is a serious threat and it could cost hundreds of thousands of dollars. Machinery malfunctions need to be prevented to minimize manufacturing downtime. But with the help of AR remote support, the field workers from received instructions on their smart devices can quickly visually identify the problem and resolve it or even prevent the downtime from ever occurring. Businesses of the manufacturing industry through implementing AR remote support monitor, inspect, update, and repair their machinery, by this, are able to create the right atmosphere for meeting goals to help to eliminate machinery downtime.

Trending AR VR Articles:

3. Logistics and Transportation

Innovative technologies such as AR enable logistics and transportation businesses to optimize efficiency by automating and standardizing the workflows. Using AR can enhance warehouses’ maintenance, improve packaging and order picking processes, and reduce the time spent on transportation and delivery. To lower the error rates and ease the workers’ decision-making process in logistics, it is necessary to use technologies like AR and simplify tasks. It will also clarify most of the logistic business steps like assembly, storage, sorting, inventory, order picking, transportation, and more.

4. Oil and Gas

AR applications are already enabling oil and gas companies to transform their operational processes by reducing expenditures, enhancing remote collaboration, and helping to stay competitive in the market. Considering all the features and benefits, the implementation of AR in oil and gas operations is proving to be really efficient and indeed can transform the sector. Due to the harmful environment in which field workers are exposed, they are prone to health damage that can lead to loss of life. Through expert guidance field, workers perform their tasks in safe environments.

5. Healthcare

The tremendous growth of AR is being experienced especially in the healthcare sector, creating business opportunities for companies with AR expertise. In the healthcare field, the potential of AR is manifested in the ability that enable the presentation of 3D medical models in real-time at remote locations. The AR-based remote support can help doctors diagnose and treat patients accurately. With access to real-time patient data, they can perform functions better than before. In addition, by ignoring distance and boundaries AR brings the most specialized surgeons to the same operating room alongside the specialists who will perform the operation. Furthermore, doctors can record video and audio while examining a patient and can use them at a later time to train their students across the globe.

AR knows no boundaries but plays an important role in the operation of businesses despite the many and quite difficult challenges. This technology can be seen as an innovative solution to be adopted and that provides many benefits for businesses operating in different industrial fields. The power of AR connects experts with field workers around the world with real-time data, guidance, and the power of remote support. The impact of this technology is an excellent example of how the operational process of different industry types has been transformed and how the quality and productivity of work have increased.

Don’t forget to give us your 👏 !",6049
"(Live — Streaming) : 2021 Soul Train Music Awards | [Online Streaming]

2021 Soul Train Music Awards, at New York’s Apollo Theater, Sunday Nov 28, 2021 at 20PM

<< GO LIVE NOW >>

▶▶ https://cutt.ly/8T2eQlA

SHOW INFO :

Event : 2021 Soul Train Music Awards

Date/Time : Sunday Nov 28, 2021 at 20PM

Venue : New York’s Apollo Theater, USA

Hosted by Tisha Campbell and Tichina Arnold

Ashanti will be honored with the Lady of Soul Award, while Maxwell will receive the Legend Award.

Song of the Year: Blxst feat. Ty Dolla $Ign & Tyga — “Chosen”; Bruno Mars, Anderson .Paak (Silk Sonic) — “Leave the Door Open”; H.E.R. — “Damage”; Jazmine Sullivan — “Pick Up Your Feelings”; Wizkid feat. Tems — “Essence”; Yung Bleu feat. Drake — “You’re Mines Still”

Album of the Year: Blxst — No Love Lost; Doja Cat — Planet Her; Giveon — When It’s All Said and Done… Take Time; H.E.R. — Back of My Mind; Jazmine Sullivan — Heaux Tales; Wizkid — Made in Lagos

Video of the Year: Bruno Mars, Anderson .Paak (Silk Sonic) — “Leave the Door Open”; Chris Brown, Young Thug feat. Future, Lil Durk, Latto — “Go Crazy (Remix)”; H.E.R. — “Damage”; Jazmine Sullivan — “Pick Up Your Feelings”; Normani feat. Cardi B — “Wild Side”; Wizkid feat. Tems — “Essence”.

❖ ALL CATEGORY WATCHTED ❖

An action story is similar to adventure, and the protagonist usually takes a risky turn, which leads to desperate scenarios (including explosions, fight scenes, daring escapes, etc.). Action and adventure usually are categorized together (sometimes even while “action-adventure”) because they have much in common, and many stories are categorized as both genres simultaneously (for instance, the James Bond series can be classified as both).

Continuing their survival through an age of a Zombie-apocalypse as a makeshift family, Columbus (Jesse Eisenberg), Tallahassee (Woody Harrelson), Wichita (Emma Stone), and Little Rock (Abagail Breslin) have found their balance as a team, settling into the now vacant White House to spend some safe quality time with one another as they figure out their next move. However, spend time at the Presidential residents raise some uncertainty as Columbus proposes to Wichita, which freaks out the independent, lone warrior out, while Little Rock starts to feel the need to be on her own. The women suddenly decide to escape in the middle of the night, leaving the men concerned about Little Rock, who’s quickly joined by Berkley (Avan Jogia), a hitchhiking hippie on his way to place called Babylon, a fortified commune that’s supposed to be safe haven against the zombies of the land. Hitting the road to retrieved their loved one, Tallahassee and Columbus meet Madison (Zoey Deutch), a dim-witted survivor who takes an immediate liking to Columbus, complicating his relationship with Wichita.

✅ ANALYZER GOOD / BAD ✅

To be honest, I didn’t catch Zombieland when it first got released (in theaters) back in 2009. Of course, the movie pre-dated a lot of the pop culture phenomenon of the usage of zombies-esque as the main antagonist (i.e Game of Thrones, The Maze Runner trilogy, The Walking Dead, World War Z, The Last of Us, etc.), but I’ve never been keen on the whole “Zombie” craze as others are. So, despite the comedy talents on the project, I didn’t see Zombieland….until it came to TV a year or so later. Surprisingly, however, I did like it. Naturally, the zombie apocalypse thing was fine (just wasn’t my thing), but I really enjoyed the film’s humor-based comedy throughout much of the feature. With the exception of 2008’s Shaun of the Dead, majority of the past (and future) endeavors of this narrative have always been serious, so it was kind of refreshing to see comedic levity being brought into the mix. Plus, the film’s cast was great, with the four main leads being one of the film’s greatest assets. As mentioned above, Zombieland didn’t make much of a huge splash at the box office, but certainly gained a strong cult following, including myself, in the following years.

Flash forward a decade after its release and Zombieland finally got a sequel with Zombieland: Double Tap, the central focus of this review post. Given how the original film ended, it was clear that a sequel to the 2009 movie was indeed possible, but it seemed like it was in no rush as the years kept passing by. So, I was quite surprised to hear that Zombieland was getting a sequel, but also a bit not surprised as well as Hollywood’s recent endeavors have been of the “belated sequels” variety; finding mixed results on each of these projects. I did see the film’s movie trailer, which definitely was what I was looking for in this Zombieland 2 movie, with Eisenberg, Harrelson, Stone, Breslin returning to reprise their respective characters again. I knew I wasn’t expecting anything drastically different from the 2009 movie, so I entered Double Tap with good frame of my mind and somewhat eagerly expecting to catch up with this dysfunctional zombie killing family. Unfortunately, while I did see the movie a week after its release, my review for it fell to the wayside as my life in retail got a hold of me during the holidays as well as being sick for a good week and half after seeing the movie. So, with me still playing “catch up” I finally have the time to share my opinions on Zombieland: Double Tap. And what are they? Well, to be honest, my opinions on the film was good. Despite some problems here and there, Zombieland: Double Tap is definitely a fun sequel that’s worth the decade long wait. It doesn’t “redefine” the Zombie genre interest or outmatch its predecessor, but this next chapter of Zombieland still provides an entertaining entry….and that’s all that matters.

Returning to the director’s chair is director Ruben Fleischer, who helmed the first Zombieland movie as well as other film projects such as 30 Minutes or Less, Gangster Squad, and Venom. Thus, given his previous knowledge of shaping the first film, it seems quite suitable (and obvious) for Fleischer to direct this movie and (to that affect), Double Tap succeeds. Of course, with the first film being a “cult classic” of sorts, Fleischer probably knew that it wasn’t going to be easy to replicate the same formula in this sequel, especially since the 10-year gap between the films. Luckily, Fleischer certainly excels in bringing the same type of comedic nuances and cinematic aspects that made the first Zombieland enjoyable to Double Tap; creating a second installment that has plenty of fun and entertainment throughout. A lot of the familiar / likeable aspects of the first film, including the witty banter between four main lead characters, continues to be at the forefront of this sequel; touching upon each character in a amusing way, with plenty of nods and winks to the original 2009 film that’s done skillfully and not so much unnecessarily ham-fisted. Additionally, Fleischer keeps the film running at a brisk pace, with the feature having a runtime of 99 minutes in length (one hour and thirty-nine minutes), which means that the film never feels sluggish (even if it meanders through some secondary story beats / side plot threads), with Fleischer ensuring a companion sequel that leans with plenty of laughter and thrills that are presented snappy way (a sort of “thick and fast” notion). Speaking of which, the comedic aspect of the first Zombieland movie is well-represented in Double Tap, with Fleischer still utilizing its cast (more on that below) in a smart and hilarious by mixing comedic personalities / personas with something as serious / gravitas as fighting endless hordes of zombies every where they go. Basically, if you were a fan of the first Zombieland flick, you’ll definitely find Double Tap to your liking.

In terms of production quality, Double Tap is a good feature. Granted, much like the last film, I knew that the overall setting and background layouts weren’t going to be something elaborate and / or expansive. Thus, my opinion of this subject of the movie’s technical presentation isn’t that critical. Taking that into account, Double Tap does (at least) does have that standard “post-apocalyptic” setting of an abandoned building, cityscapes, and roads throughout the feature; littered with unmanned vehicles and rubbish. It certainly has that “look and feel” of the post-zombie world, so Double Tap’s visual aesthetics gets a solid industry standard in my book. Thus, a lot of the other areas that I usually mentioned (i.e set decorations, costumes, cinematography, etc.) fit into that same category as meeting the standards for a 202 movie. Thus, as a whole, the movie’s background nuances and presentation is good, but nothing grand as I didn’t expect to be “wowed” over it. So, it sort of breaks even. This also extends to the film’s score, which was done by David Sardy, which provides a good musical composition for the feature’s various scenes as well as a musical song selection thrown into the mix; interjecting the various zombie and humor bits equally well.

There are some problems that are bit glaring that Double Tap, while effectively fun and entertaining, can’t overcome, which hinders the film from overtaking its predecessor. Perhaps one of the most notable criticism that the movie can’t get right is the narrative being told. Of course, the narrative in the first Zombieland wasn’t exactly the best, but still combined zombie-killing action with its combination of group dynamics between its lead characters. Double Tap, however, is fun, but messy at the same time; creating a frustrating narrative that sounds good on paper, but thinly written when executed. Thus, problem lies within the movie’s script, which was penned by Dave Callaham, Rhett Reese, and Paul Wernick, which is a bit thinly sketched in certain areas of the story, including a side-story involving Tallahassee wanting to head to Graceland, which involves some of the movie’s new supporting characters. It’s fun sequence of events that follows, but adds little to the main narrative and ultimately could’ve been cut completely. Thus, I kind of wanted see Double Tap have more a substance within its narrative. Heck, they even had a decade long gap to come up with a new yarn to spin for this sequel…and it looks like they came up a bit shorter than expected.

Another point of criticism that I have about this is that there aren’t enough zombie action bits as there were in the first Zombieland movie. Much like the Walking Dead series as become, Double Tap seems more focused on its characters (and the dynamics that they share with each other) rather than the group facing the sparse groupings of mindless zombies. However, that was some of the fun of the first movie and Double Tap takes away that element. Yes, there are zombies in the movie and the gang is ready to take care of them (in gruesome fashion), but these mindless beings sort take a back seat for much of the film, with the script and Fleischer seemed more focused on showcasing witty banter between Columbus, Tallahassee, Wichita, and Little Rock. Of course, the ending climatic piece in the third act gives us the best zombie action scenes of the feature, but it feels a bit “too little, too late” in my opinion. To be honest, this big sequence is a little manufactured and not as fun and unique as the final battle scene in the first film. I know that sounds a bit contrive and weird, but, while the third act big fight seems more polished and staged well, it sort of feels more restricted and doesn’t flow cohesively with the rest of the film’s flow (in matter of speaking).

What’s certainly elevates these points of criticism is the film’s cast, with the main quartet lead acting talents returning to reprise their roles in Double Tap, which is absolutely the “hands down” best part of this sequel. Naturally, I’m talking about the talents of Jessie Eisenberg, Woody Harrelson, Emma Stone and Abigail Breslin in their respective roles Zombieland character roles of Columbus, Tallahassee, Wichita, and Little Rock. Of the four, Harrelson, known for his roles in Cheers, True Detective, and War for the Planet of the Apes, shines as the brightest in the movie, with dialogue lines of Tallahassee proving to be the most hilarious comedy stuff on the sequel. Harrelson certainly knows how to lay it on “thick and fast” with the character and the s**t he says in the movie is definitely funny (regardless if the joke is slightly or dated). Behind him, Eisenberg, known for his roles in The Art of Self-Defense, The Social Network, and Batman v Superman: Dawn of Justice, is somewhere in the middle of pack, but still continues to act as the somewhat main protagonist of the feature, including being a narrator for us (the viewers) in this post-zombie apocalypse world. Of course, Eisenberg’s nervous voice and twitchy body movements certainly help the character of Columbus to be likeable and does have a few comedic timing / bits with each of co-stars. Stone, known for her roles in The Help, Superbad, and La La Land, and Breslin, known for her roles in Signs, Little Miss Sunshine, and Definitely, Maybe, round out the quartet; providing some more grown-up / mature character of the group, with Wichita and Little Rock trying to find their place in the world and how they must deal with some of the party members on a personal level. Collectively, these four are what certainly the first movie fun and hilarious and their overall camaraderie / screen-presence with each other hasn’t diminished in the decade long absence. To be it simply, these four are simply riot in the Zombieland and are again in Double Tap.

With the movie keeping the focus on the main quartet of lead Zombieland characters, the one newcomer that certainly takes the spotlight is actress Zoey Deutch, who plays the character of Madison, a dim-witted blonde who joins the group and takes a liking to Columbus. Known for her roles in Before I Fall, The Politician, and Set It Up, Deutch is a somewhat “breath of fresh air” by acting as the tagalong team member to the quartet in a humorous way. Though there isn’t much insight or depth to the character of Madison, Deutch’s ditzy / air-head portrayal of her is quite hilarious and is fun when she’s making comments to Harrelson’s Tallahassee (again, he’s just a riot in the movie).

The rest of the cast, including actor Avan Jogia (Now Apocalypse and Shaft) as Berkeley, a pacifist hippie that quickly befriends Little Rock on her journey, actress Rosario Dawson (Rent and Sin City) as Nevada, the owner of a Elvis-themed motel who Tallahassee quickly takes a shine to, and actors Luke Wilson (Legally Blonde and Old School) and Thomas Middleditch (Silicon Valley and Captain Underpants: The First Epic Movie) as Albuquerque and Flagstaff, two traveling zombie-killing partners that are mimic reflections of Tallahassee and Columbus, are in minor supporting roles in Double Tap. While all of these acting talents are good and definitely bring a certain humorous quality to their characters, the characters themselves could’ve been easily expanded upon, with many just being thinly written caricatures. Of course, the movie focuses heavily on the Zombieland quartet (and newcomer Madison), but I wished that these characters could’ve been fleshed out a bit.

Lastly, be sure to still around for the film’s ending credits, with Double Tap offering up two Easter Eggs scenes (one mid-credits and one post-credit scenes). While I won’t spoil them, I do have mention that they are pretty hilarious.

✅ FINAL THOUGHTS ✅

It’s been awhile, but the Zombieland gang is back and are ready to hit the road once again in the movie Zombieland: Double Tap. Director Reuben Fleischer’s latest film sees the return the dysfunctional zombie-killing makeshift family of survivors for another round of bickering, banting, and trying to find their way in a post-apocalyptic world. While the movie’s narrative is a bit messy and could’ve been refined in the storyboarding process as well as having a bit more zombie action, the rest of the feature provides to be a fun endeavor, especially with Fleischer returning to direct the project, the snappy / witty banter amongst its characters, a breezy runtime, and the four lead returning acting talents. Personally, I liked this movie. I definitely found it to my liking as I laugh many times throughout the movie, with the main principal cast lending their screen presence in this post-apocalyptic zombie movie. Thus, my recommendation for this movie is favorable “recommended” as I’m sure it will please many fans of the first movie as well as to the uninitiated (the film is quite easy to follow for newcomers). While the movie doesn’t redefine what was previous done back in 2009, Zombieland: Double Tap still provides a riot of laughs with this make-shift quartet of zombie survivors; giving us give us (the viewers) fun and entertaining companion sequel to the original feature.",16926
"[Live Stream] 2021 Soul Train Music Awards | Full Show On “BET”

2021 Soul Train Music Awards, at New York’s Apollo Theater, Sunday Nov 28, 2021 at 20PM

<< GO LIVE NOW >>

▶▶ https://cutt.ly/8T2eQlA

SHOW INFO :

Event : 2021 Soul Train Music Awards

Date/Time : Sunday Nov 28, 2021 at 20PM

Venue : New York’s Apollo Theater, USA

Hosted by Tisha Campbell and Tichina Arnold

Ashanti will be honored with the Lady of Soul Award, while Maxwell will receive the Legend Award.

Song of the Year: Blxst feat. Ty Dolla $Ign & Tyga — “Chosen”; Bruno Mars, Anderson .Paak (Silk Sonic) — “Leave the Door Open”; H.E.R. — “Damage”; Jazmine Sullivan — “Pick Up Your Feelings”; Wizkid feat. Tems — “Essence”; Yung Bleu feat. Drake — “You’re Mines Still”

Album of the Year: Blxst — No Love Lost; Doja Cat — Planet Her; Giveon — When It’s All Said and Done… Take Time; H.E.R. — Back of My Mind; Jazmine Sullivan — Heaux Tales; Wizkid — Made in Lagos

Video of the Year: Bruno Mars, Anderson .Paak (Silk Sonic) — “Leave the Door Open”; Chris Brown, Young Thug feat. Future, Lil Durk, Latto — “Go Crazy (Remix)”; H.E.R. — “Damage”; Jazmine Sullivan — “Pick Up Your Feelings”; Normani feat. Cardi B — “Wild Side”; Wizkid feat. Tems — “Essence”.

❖ ALL CATEGORY WATCHTED ❖

An action story is similar to adventure, and the protagonist usually takes a risky turn, which leads to desperate scenarios (including explosions, fight scenes, daring escapes, etc.). Action and adventure usually are categorized together (sometimes even while “action-adventure”) because they have much in common, and many stories are categorized as both genres simultaneously (for instance, the James Bond series can be classified as both).

Continuing their survival through an age of a Zombie-apocalypse as a makeshift family, Columbus (Jesse Eisenberg), Tallahassee (Woody Harrelson), Wichita (Emma Stone), and Little Rock (Abagail Breslin) have found their balance as a team, settling into the now vacant White House to spend some safe quality time with one another as they figure out their next move. However, spend time at the Presidential residents raise some uncertainty as Columbus proposes to Wichita, which freaks out the independent, lone warrior out, while Little Rock starts to feel the need to be on her own. The women suddenly decide to escape in the middle of the night, leaving the men concerned about Little Rock, who’s quickly joined by Berkley (Avan Jogia), a hitchhiking hippie on his way to place called Babylon, a fortified commune that’s supposed to be safe haven against the zombies of the land. Hitting the road to retrieved their loved one, Tallahassee and Columbus meet Madison (Zoey Deutch), a dim-witted survivor who takes an immediate liking to Columbus, complicating his relationship with Wichita.

✅ ANALYZER GOOD / BAD ✅

To be honest, I didn’t catch Zombieland when it first got released (in theaters) back in 2009. Of course, the movie pre-dated a lot of the pop culture phenomenon of the usage of zombies-esque as the main antagonist (i.e Game of Thrones, The Maze Runner trilogy, The Walking Dead, World War Z, The Last of Us, etc.), but I’ve never been keen on the whole “Zombie” craze as others are. So, despite the comedy talents on the project, I didn’t see Zombieland….until it came to TV a year or so later. Surprisingly, however, I did like it. Naturally, the zombie apocalypse thing was fine (just wasn’t my thing), but I really enjoyed the film’s humor-based comedy throughout much of the feature. With the exception of 2008’s Shaun of the Dead, majority of the past (and future) endeavors of this narrative have always been serious, so it was kind of refreshing to see comedic levity being brought into the mix. Plus, the film’s cast was great, with the four main leads being one of the film’s greatest assets. As mentioned above, Zombieland didn’t make much of a huge splash at the box office, but certainly gained a strong cult following, including myself, in the following years.

Flash forward a decade after its release and Zombieland finally got a sequel with Zombieland: Double Tap, the central focus of this review post. Given how the original film ended, it was clear that a sequel to the 2009 movie was indeed possible, but it seemed like it was in no rush as the years kept passing by. So, I was quite surprised to hear that Zombieland was getting a sequel, but also a bit not surprised as well as Hollywood’s recent endeavors have been of the “belated sequels” variety; finding mixed results on each of these projects. I did see the film’s movie trailer, which definitely was what I was looking for in this Zombieland 2 movie, with Eisenberg, Harrelson, Stone, Breslin returning to reprise their respective characters again. I knew I wasn’t expecting anything drastically different from the 2009 movie, so I entered Double Tap with good frame of my mind and somewhat eagerly expecting to catch up with this dysfunctional zombie killing family. Unfortunately, while I did see the movie a week after its release, my review for it fell to the wayside as my life in retail got a hold of me during the holidays as well as being sick for a good week and half after seeing the movie. So, with me still playing “catch up” I finally have the time to share my opinions on Zombieland: Double Tap. And what are they? Well, to be honest, my opinions on the film was good. Despite some problems here and there, Zombieland: Double Tap is definitely a fun sequel that’s worth the decade long wait. It doesn’t “redefine” the Zombie genre interest or outmatch its predecessor, but this next chapter of Zombieland still provides an entertaining entry….and that’s all that matters.

Returning to the director’s chair is director Ruben Fleischer, who helmed the first Zombieland movie as well as other film projects such as 30 Minutes or Less, Gangster Squad, and Venom. Thus, given his previous knowledge of shaping the first film, it seems quite suitable (and obvious) for Fleischer to direct this movie and (to that affect), Double Tap succeeds. Of course, with the first film being a “cult classic” of sorts, Fleischer probably knew that it wasn’t going to be easy to replicate the same formula in this sequel, especially since the 10-year gap between the films. Luckily, Fleischer certainly excels in bringing the same type of comedic nuances and cinematic aspects that made the first Zombieland enjoyable to Double Tap; creating a second installment that has plenty of fun and entertainment throughout. A lot of the familiar / likeable aspects of the first film, including the witty banter between four main lead characters, continues to be at the forefront of this sequel; touching upon each character in a amusing way, with plenty of nods and winks to the original 2009 film that’s done skillfully and not so much unnecessarily ham-fisted. Additionally, Fleischer keeps the film running at a brisk pace, with the feature having a runtime of 99 minutes in length (one hour and thirty-nine minutes), which means that the film never feels sluggish (even if it meanders through some secondary story beats / side plot threads), with Fleischer ensuring a companion sequel that leans with plenty of laughter and thrills that are presented snappy way (a sort of “thick and fast” notion). Speaking of which, the comedic aspect of the first Zombieland movie is well-represented in Double Tap, with Fleischer still utilizing its cast (more on that below) in a smart and hilarious by mixing comedic personalities / personas with something as serious / gravitas as fighting endless hordes of zombies every where they go. Basically, if you were a fan of the first Zombieland flick, you’ll definitely find Double Tap to your liking.

In terms of production quality, Double Tap is a good feature. Granted, much like the last film, I knew that the overall setting and background layouts weren’t going to be something elaborate and / or expansive. Thus, my opinion of this subject of the movie’s technical presentation isn’t that critical. Taking that into account, Double Tap does (at least) does have that standard “post-apocalyptic” setting of an abandoned building, cityscapes, and roads throughout the feature; littered with unmanned vehicles and rubbish. It certainly has that “look and feel” of the post-zombie world, so Double Tap’s visual aesthetics gets a solid industry standard in my book. Thus, a lot of the other areas that I usually mentioned (i.e set decorations, costumes, cinematography, etc.) fit into that same category as meeting the standards for a 202 movie. Thus, as a whole, the movie’s background nuances and presentation is good, but nothing grand as I didn’t expect to be “wowed” over it. So, it sort of breaks even. This also extends to the film’s score, which was done by David Sardy, which provides a good musical composition for the feature’s various scenes as well as a musical song selection thrown into the mix; interjecting the various zombie and humor bits equally well.

There are some problems that are bit glaring that Double Tap, while effectively fun and entertaining, can’t overcome, which hinders the film from overtaking its predecessor. Perhaps one of the most notable criticism that the movie can’t get right is the narrative being told. Of course, the narrative in the first Zombieland wasn’t exactly the best, but still combined zombie-killing action with its combination of group dynamics between its lead characters. Double Tap, however, is fun, but messy at the same time; creating a frustrating narrative that sounds good on paper, but thinly written when executed. Thus, problem lies within the movie’s script, which was penned by Dave Callaham, Rhett Reese, and Paul Wernick, which is a bit thinly sketched in certain areas of the story, including a side-story involving Tallahassee wanting to head to Graceland, which involves some of the movie’s new supporting characters. It’s fun sequence of events that follows, but adds little to the main narrative and ultimately could’ve been cut completely. Thus, I kind of wanted see Double Tap have more a substance within its narrative. Heck, they even had a decade long gap to come up with a new yarn to spin for this sequel…and it looks like they came up a bit shorter than expected.

Another point of criticism that I have about this is that there aren’t enough zombie action bits as there were in the first Zombieland movie. Much like the Walking Dead series as become, Double Tap seems more focused on its characters (and the dynamics that they share with each other) rather than the group facing the sparse groupings of mindless zombies. However, that was some of the fun of the first movie and Double Tap takes away that element. Yes, there are zombies in the movie and the gang is ready to take care of them (in gruesome fashion), but these mindless beings sort take a back seat for much of the film, with the script and Fleischer seemed more focused on showcasing witty banter between Columbus, Tallahassee, Wichita, and Little Rock. Of course, the ending climatic piece in the third act gives us the best zombie action scenes of the feature, but it feels a bit “too little, too late” in my opinion. To be honest, this big sequence is a little manufactured and not as fun and unique as the final battle scene in the first film. I know that sounds a bit contrive and weird, but, while the third act big fight seems more polished and staged well, it sort of feels more restricted and doesn’t flow cohesively with the rest of the film’s flow (in matter of speaking).

What’s certainly elevates these points of criticism is the film’s cast, with the main quartet lead acting talents returning to reprise their roles in Double Tap, which is absolutely the “hands down” best part of this sequel. Naturally, I’m talking about the talents of Jessie Eisenberg, Woody Harrelson, Emma Stone and Abigail Breslin in their respective roles Zombieland character roles of Columbus, Tallahassee, Wichita, and Little Rock. Of the four, Harrelson, known for his roles in Cheers, True Detective, and War for the Planet of the Apes, shines as the brightest in the movie, with dialogue lines of Tallahassee proving to be the most hilarious comedy stuff on the sequel. Harrelson certainly knows how to lay it on “thick and fast” with the character and the s**t he says in the movie is definitely funny (regardless if the joke is slightly or dated). Behind him, Eisenberg, known for his roles in The Art of Self-Defense, The Social Network, and Batman v Superman: Dawn of Justice, is somewhere in the middle of pack, but still continues to act as the somewhat main protagonist of the feature, including being a narrator for us (the viewers) in this post-zombie apocalypse world. Of course, Eisenberg’s nervous voice and twitchy body movements certainly help the character of Columbus to be likeable and does have a few comedic timing / bits with each of co-stars. Stone, known for her roles in The Help, Superbad, and La La Land, and Breslin, known for her roles in Signs, Little Miss Sunshine, and Definitely, Maybe, round out the quartet; providing some more grown-up / mature character of the group, with Wichita and Little Rock trying to find their place in the world and how they must deal with some of the party members on a personal level. Collectively, these four are what certainly the first movie fun and hilarious and their overall camaraderie / screen-presence with each other hasn’t diminished in the decade long absence. To be it simply, these four are simply riot in the Zombieland and are again in Double Tap.

With the movie keeping the focus on the main quartet of lead Zombieland characters, the one newcomer that certainly takes the spotlight is actress Zoey Deutch, who plays the character of Madison, a dim-witted blonde who joins the group and takes a liking to Columbus. Known for her roles in Before I Fall, The Politician, and Set It Up, Deutch is a somewhat “breath of fresh air” by acting as the tagalong team member to the quartet in a humorous way. Though there isn’t much insight or depth to the character of Madison, Deutch’s ditzy / air-head portrayal of her is quite hilarious and is fun when she’s making comments to Harrelson’s Tallahassee (again, he’s just a riot in the movie).

The rest of the cast, including actor Avan Jogia (Now Apocalypse and Shaft) as Berkeley, a pacifist hippie that quickly befriends Little Rock on her journey, actress Rosario Dawson (Rent and Sin City) as Nevada, the owner of a Elvis-themed motel who Tallahassee quickly takes a shine to, and actors Luke Wilson (Legally Blonde and Old School) and Thomas Middleditch (Silicon Valley and Captain Underpants: The First Epic Movie) as Albuquerque and Flagstaff, two traveling zombie-killing partners that are mimic reflections of Tallahassee and Columbus, are in minor supporting roles in Double Tap. While all of these acting talents are good and definitely bring a certain humorous quality to their characters, the characters themselves could’ve been easily expanded upon, with many just being thinly written caricatures. Of course, the movie focuses heavily on the Zombieland quartet (and newcomer Madison), but I wished that these characters could’ve been fleshed out a bit.

Lastly, be sure to still around for the film’s ending credits, with Double Tap offering up two Easter Eggs scenes (one mid-credits and one post-credit scenes). While I won’t spoil them, I do have mention that they are pretty hilarious.

✅ FINAL THOUGHTS ✅

It’s been awhile, but the Zombieland gang is back and are ready to hit the road once again in the movie Zombieland: Double Tap. Director Reuben Fleischer’s latest film sees the return the dysfunctional zombie-killing makeshift family of survivors for another round of bickering, banting, and trying to find their way in a post-apocalyptic world. While the movie’s narrative is a bit messy and could’ve been refined in the storyboarding process as well as having a bit more zombie action, the rest of the feature provides to be a fun endeavor, especially with Fleischer returning to direct the project, the snappy / witty banter amongst its characters, a breezy runtime, and the four lead returning acting talents. Personally, I liked this movie. I definitely found it to my liking as I laugh many times throughout the movie, with the main principal cast lending their screen presence in this post-apocalyptic zombie movie. Thus, my recommendation for this movie is favorable “recommended” as I’m sure it will please many fans of the first movie as well as to the uninitiated (the film is quite easy to follow for newcomers). While the movie doesn’t redefine what was previous done back in 2009, Zombieland: Double Tap still provides a riot of laughs with this make-shift quartet of zombie survivors; giving us give us (the viewers) fun and entertaining companion sequel to the original feature.",16919
